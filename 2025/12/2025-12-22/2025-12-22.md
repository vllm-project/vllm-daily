# vLLM Merged PR Report

**Report Date:** 2025-12-22 PST

**Total Merged PRs:** 24

---

## 1. [[Bugfix] Fix Jais2ForCausalLM](https://github.com/vllm-project/vllm/pull/31198)


### Base Information

- **PR Number:** #31198
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-22 23:44:02
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31198/files)


### Summary

**What changed and why**  
The changes remove incorrect LoRA (Low-Rank Adaptation) logic from the Jais2ForCausalLM implementation to fix issue #31148. Specifically, the code eliminates LoRA-related vocabulary size adjustments and padding calculations that were improperly applied to the base model.

**Technical impact**  
These modifications simplify the model architecture by decoupling it from LoRA-specific configurations. The vocabulary size now directly uses the base model's `config.vocab_size` without LoRA adjustments, and the rotary embedding initialization no longer includes an explicit `rotary_dim` parameter. The LogitsProcessor is also updated to use a simpler constructor.

**Potential risks**  
If LoRA support is actually required for Jais2 models, this fix may break that functionality. The removal of vocabulary padding logic could affect kernel compatibility when LoRA is enabled. There's also a risk that other parts of the codebase might still expect the previous LoRA-adjusted vocabulary sizes.

**Key insights**  
This appears to be a corrective fix for a model that shouldn't have had LoRA logic in the first place. Developers should verify that Jais2 models are indeed incompatible with LoRA before merging. The changes align the implementation with standard causal LM patterns, but any future LoRA integration would need to be re-implemented correctly.

---

## 2. [[XPU] decrease IGC_ForceOCLSIMDWidth for speculative decoding triton-xpu kernel compilation](https://github.com/vllm-project/vllm/pull/30538)


### Base Information

- **PR Number:** #30538
- **Author:** [yma11](https://github.com/yma11)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2025-12-22 21:22:15
- **Type:** `SUBSTANTIAL`
- **Changed Files (3):** [changed files link](https://github.com/vllm-project/vllm/pull/30538/files)


### Summary

**What changed and why**  
This PR addresses a compilation error in speculative decoding on XPU platforms by decreasing the Triton kernel compilation scratch space. The error occurred when the scratch space exceeded hardware limits (1,164,736 bytes vs 262,144 byte maximum). The fix reduces the `IGC_ForceOCLSIMDWidth` environment variable to 16 when speculative decoding is enabled, and updates documentation to reflect XPU support.

**Technical impact**  
The change modifies the XPU platform configuration to dynamically set a compiler environment variable (`IGC_ForceOCLSIMDWidth=16`) when speculative decoding is active. This reduces SIMD width to decrease memory usage during kernel compilation. The Dockerfile now installs `arctic-inference` for suffix method speculative decoding and switches to a staging graphics repository.

**Potential risks**  
Reducing SIMD width could potentially impact kernel performance by limiting parallelism. The environment variable change is global within the process when speculative decoding is enabled, which might affect other XPU operations. The switch to a staging PPA (`intel-graphics-staging`) could introduce less stable dependencies.

**Key insights**  
This is a workaround for hardware limitations rather than an architectural fix. Developers should monitor performance to ensure the SIMD width reduction doesn't degrade throughput significantly. The solution is targeted and only activates when speculative decoding is configured, minimizing broader impact. Consider if this should be a configurable parameter rather than a hardcoded value.

---

## 3. [[Chore] Update more locations to use `attention_config.backend`](https://github.com/vllm-project/vllm/pull/31153)


### Base Information

- **PR Number:** #31153
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 19:19:50
- **Type:** `TRIVIAL`
- **Changed Files (2):** [changed files link](https://github.com/vllm-project/vllm/pull/31153/files)


### Summary

**What changed and why**  
This PR updates two test files to use the new `attention_config.backend` parameter instead of the deprecated `VLLM_ATTENTION_BACKEND` environment variable. These changes follow up on a previous migration (#26315) to ensure consistency across the codebase.

**Technical impact**  
The changes migrate test configurations from environment-based attention backend selection to explicit parameter passing in model initialization. This improves test clarity and aligns with the library's move away from global environment variables for configuration.

**Potential risks**  
If other test files still rely on the `VLLM_ATTENTION_BACKEND` environment variable, they may fail or produce inconsistent results. The removal of the environment variable setting could affect tests that depend on its side effects or that run in parallel with other tests using the same environment.

**Key insights**  
Developers should verify that all test files have been migrated from `VLLM_ATTENTION_BACKEND` to `attention_config.backend`. When writing new tests, always use the explicit parameter approach rather than environment variables for attention backend configuration to ensure test isolation and reproducibility.

---

## 4. [[Feature] Batch invariant: Lora](https://github.com/vllm-project/vllm/pull/30097)


### Base Information

- **PR Number:** #30097
- **Author:** [quanliu1991](https://github.com/quanliu1991)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2025-12-22 18:32:47
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/30097/files)


### Summary

**What changed and why**  
This PR extracts LoRA-related changes from a larger PR (#30018) to separate concerns. The changes modify LoRA operation configuration logic to handle batch-invariant cases differently by avoiding custom configuration loading and adjusting kernel parameters for improved performance.

**Technical impact**  
The modifications introduce conditional logic based on batch invariance detection, affecting how LoRA operations are configured. When batch-invariant mode is active, the system bypasses user-defined configurations and uses optimized kernel parameters (specifically setting split_k=1) for the "shrink" operation type, which could improve performance in certain scenarios.

**Potential risks**  
The changes introduce a new dependency on `vllm_is_batch_invariant()` which must be properly initialized and maintained. There's a risk that the batch invariance detection might not align with actual runtime behavior, potentially causing suboptimal configuration selection. The hardcoded split_k=1 for batch-invariant cases may not be optimal for all hardware configurations.

**Key insights**  
Developers should ensure the `vllm_is_batch_invariant()` function accurately reflects the system state. The separation of LoRA changes from FA2 implementation is a good architectural decision that improves code maintainability. Consider making the split_k value for batch-invariant cases configurable rather than hardcoded to accommodate different hardware profiles.

---

## 5. [Revert "[SM100] Enable fp8 compute for prefill MLA (#30746)"](https://github.com/vllm-project/vllm/pull/31197)


### Base Information

- **PR Number:** #31197
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 18:15:33
- **Type:** `SUBSTANTIAL`
- **Changed Files (3):** [changed files link](https://github.com/vllm-project/vllm/pull/31197/files)


### Summary

**What changed and why**  
This PR reverts commit b10f41c, which enabled FP8 compute for prefill MLA (Multi-Head Latent Attention). The revert is temporary, pending the merge of PR #30993, indicating that the FP8 feature introduced issues or dependencies that need to be resolved before it can be safely integrated.

**Technical impact**  
The changes remove FP8 support from the MLA attention backend, reverting to using the model's base dtype (e.g., FP16/BF16) for prefill computations. This affects the `common.py` backend logic, test configurations, and a minor adjustment in MoE quantization. The revert simplifies the code by eliminating FP8-specific data type handling and conditional logic across multiple prefill methods (FlashAttention, FlashInfer, Cudnn, TRT-LLM).

**Potential risks**  
Reverting may temporarily reduce performance or memory efficiency for prefill operations that benefited from FP8 compute. There is a risk of reintroducing the original issues that PR #30993 aims to address. Additionally, the revert changes the KV cache spec type in tests from `MLAAttentionSpec` to `FullAttentionSpec`, which could affect test coverage if not fully aligned with production code paths.

**Key insights**  
This is a strategic rollback to stabilize the codebase while dependencies are resolved. Developers should ensure that PR #30993 is merged before re-enabling FP8 support. The test suite must be validated to confirm that the revert does not introduce regressions, especially around attention backend correctness and MoE quantization. Future FP8 re-implementation should include robust testing for all supported backends.

---

## 6. [[ci] Fix Pytorch compilation test oom in 2.10](https://github.com/vllm-project/vllm/pull/31194)


### Base Information

- **PR Number:** #31194
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2025-12-22 17:56:48
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31194/files)


### Summary

**What changed and why**  
Added `max_model_len=1024` parameter to the model initialization in a PyTorch compilation test. This reduces the KV cache memory requirement from 16 GiB to 0.44 GiB to prevent OOM errors in CI runners with limited memory.

**Technical impact**  
The change lowers memory consumption during testing by constraining the maximum sequence length, allowing the test to run within CI resource constraints. It does not affect production code or model behavior outside this specific test.

**Potential risks**  
If the test relies on longer sequences to validate dynamic shape compilation, artificially limiting `max_model_len` could mask issues related to large KV caches. The fix is temporary and may need adjustment if CI memory changes or test requirements evolve.

**Key insights**  
This is a pragmatic workaround for CI limitations, but developers should ensure the reduced sequence length still adequately tests compilation logic. Consider documenting the memory constraint rationale and monitoring for future CI upgrades that might allow restoring the original `max_model_len`.

---

## 7. [[AMD][CI] fix v1/engine test_preprocess_error_handling](https://github.com/vllm-project/vllm/pull/31192)


### Base Information

- **PR Number:** #31192
- **Author:** [divakar-amd](https://github.com/divakar-amd)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2025-12-22 17:28:19
- **Type:** `TRIVIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31192/files)


### Summary

**What changed and why**  
A conditional skip was added to `test_preprocess_error_handling.py` for ROCm platforms. The test relies on the `fork` multiprocessing start method, but ROCm uses `spawn`, causing incompatibility. This change prevents test failures on AMD GPU environments.

**Technical impact**  
The modification ensures the test suite passes on ROCm without altering test logic or engine behavior. It introduces a platform-specific dependency (`current_platform.is_rocm()`) and maintains test coverage for non-ROCm platforms where `fork` is available.

**Potential risks**  
If `current_platform.is_rocm()` incorrectly identifies the platform, tests may be skipped unnecessarily or run in incompatible environments. Additionally, this skip could mask deeper issues if ROCm’s `spawn` method reveals latent bugs in error-handling paths that `fork` does not.

**Key insights**  
Platform-aware test skipping is a pragmatic solution for multiprocessing incompatibilities. Consider documenting this limitation in test files and evaluating whether error-handling logic should be validated under `spawn` via a separate test. Ensure platform checks are robust and consistent across the codebase.

---

## 8. [[MoE Refactor][7/N] AITER MK](https://github.com/vllm-project/vllm/pull/31102)


### Base Information

- **PR Number:** #31102
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2025-12-22 15:42:59
- **Type:** `SUBSTANTIAL`
- **Changed Files (4):** [changed files link](https://github.com/vllm-project/vllm/pull/31102/files)


### Summary

**What changed and why**  
This PR continues the MoE refactoring by converting the ROCm AITER backend to use the modular kernel (MK) framework. It adds support for `torch.float8_e4m3fnuz` data type in Triton kernels, introduces a `defer_input_quant` flag to handle quantization differences between backends, and integrates AITER as a new FP8 MoE backend option. The changes also fix an issue where FP8 Triton was broken on ROCm.

**Technical impact**  
The AITER backend is now fully integrated into the modular kernel architecture, making it consistent with other backends like Triton, DeepGEMM, and Marlin. The `fp8.py` file now handles AITER through the same `FusedMoEModularKernel` interface, eliminating special-case code paths. This improves code maintainability and allows AITER to benefit from the standardized MoE infrastructure.

**Potential risks**  
The `defer_input_quant` mechanism introduces conditional logic that could lead to inconsistencies if not properly handled across all backends. AITER still lacks support for chunking and all2all operations, which may limit its functionality in distributed scenarios. The changes assume AITER manages its own workspace internally, which could cause issues if memory management expectations differ from other backends.

**Key insights**  
Developers should note that AITER is now a first-class backend option (`Fp8MoeBackend.AITER`) with proper environment variable controls. The refactoring removes duplicate code paths and centralizes backend selection logic. When adding new backends, ensure they properly implement the modular kernel interfaces and handle quantization consistently. The fix for `torch.float8_e4m3fnuz` support in Triton kernels is critical for ROCm compatibility.

---

## 9. [[CI Failure] Disable mosaicml/mpt-7b and databricks/dbrx-instruct tests](https://github.com/vllm-project/vllm/pull/31182)


### Base Information

- **PR Number:** #31182
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 15:40:35
- **Type:** `TRIVIAL`
- **Changed Files (3):** [changed files link](https://github.com/vllm-project/vllm/pull/31182/files)


### Summary

**What changed and why**  
This PR disables CI tests for two deleted Hugging Face models (`mosaicml/mpt-7b` and `databricks/dbrx-instruct`) by marking them as unavailable online in the test registry and removing references in documentation and tokenizer tests. The changes prevent CI failures caused by attempting to fetch models that no longer exist.

**Technical impact**  
The modifications ensure CI stability by preventing network requests to deleted model repositories. The test registry now correctly flags these models as offline-only, while documentation and tokenizer tests are updated to avoid referencing unavailable resources. This maintains test suite integrity without altering core functionality.

**Potential risks**  
If any tests still implicitly depend on these models, they may fail unexpectedly. The documentation change swaps the example model but does not verify that `Qwen/Qwen3-4B` is a suitable replacement in all contexts. There is also a risk that other tests or code paths might still reference the deleted models.

**Key insights**  
Always verify model availability before updating test fixtures. Consider adding a validation step in CI to detect broken external dependencies proactively. For documentation, ensure replacement models are compatible with the examples provided (e.g., similar tokenizer behavior or licensing).

---

## 10. [[Perf] Remove blocking copy in GDN Attention](https://github.com/vllm-project/vllm/pull/31167)


### Base Information

- **PR Number:** #31167
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 14:25:22
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31167/files)


### Summary

**What changed and why**  
The change modifies a single line in the GDN attention backend to add `non_blocking=True` when copying `context_lens` from CPU to GPU. This removes a blocking synchronization point that was preventing async scheduling from working effectively for Qwen3-Next, enabling follow-up optimizations.

**Technical impact**  
This minor change allows the CPU-to-GPU tensor copy operation to become asynchronous, which improves GPU utilization by reducing pipeline stalls. It enables better overlap of computation and data transfer, particularly benefiting the Qwen3-Next model architecture where async scheduling and MTP optimizations are being implemented.

**Potential risks**  
The `non_blocking=True` flag requires proper synchronization later in the kernel execution to ensure the data is ready before use. If the attention kernel attempts to access `context_lens_tensor` before the asynchronous copy completes, it could lead to incorrect results or crashes. The risk is mitigated by typical CUDA stream semantics but requires careful attention in subsequent kernel launches.

**Key insights**  
This is a targeted optimization that demonstrates how small synchronization bottlenecks can impact overall performance. Developers should audit similar CPU-to-GPU copies throughout the codebase for potential async conversion, but must ensure proper synchronization downstream. The 1% speedup at batch size 1 suggests this was indeed a meaningful bottleneck for the specific workload.

---

## 11. [[Bug] Fix `'CutlassMLAImpl' object has no attribute '_workspace_buffer'`](https://github.com/vllm-project/vllm/pull/31173)


### Base Information

- **PR Number:** #31173
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 14:24:27
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31173/files)


### Summary

**What changed and why**  
The fix addresses an `AttributeError` where `CutlassMLAImpl` objects lacked a `_workspace_buffer` attribute when using TRT-LLM ragged prefill with DeepSeek models. The changes store the workspace buffer in `PrefillMetadata` during metadata construction and pass it through metadata instead of accessing it directly from the implementation object.

**Technical impact**  
This modifies the data flow for workspace buffers in TRT-LLM ragged prefill operations. The workspace buffer is now encapsulated within `PrefillMetadata`, making the implementation more modular and reducing direct dependency on the backend object's internal state during execution.

**Potential risks**  
If `workspace_buffer` is not properly initialized in `PrefillMetadata` for all code paths, similar attribute errors could occur. The `assert` statements added provide runtime checks but do not guarantee initialization in all scenarios. Additionally, changes to buffer reuse or lifecycle management could affect performance or memory usage.

**Key insights**  
The fix highlights the importance of proper state encapsulation in attention backends. Developers should ensure that all metadata fields are consistently populated across different execution paths. Consider adding validation during metadata construction to catch missing fields earlier.

---

## 12. [[SpecDecode] Simplified alternative padded-speculation acceptance rate fix](https://github.com/vllm-project/vllm/pull/29845)


### Base Information

- **PR Number:** #29845
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 13:06:10
- **Type:** `SUBSTANTIAL`
- **Changed Files (8):** [changed files link](https://github.com/vllm-project/vllm/pull/29845/files)


### Summary

**What changed and why**  
This PR introduces an alternative fix for padded speculation acceptance rate in speculative decoding. The main change modifies how rejected tokens are handled during speculative decoding by computing rejection counts on GPU and adjusting sequence lengths directly in the attention metadata, avoiding CPU synchronization overhead. This improves throughput by reducing GPU-CPU sync points.

**Technical impact**  
The changes affect the speculative decoding pipeline by modifying the `prepare_inputs_padded` method to return GPU-calculated rejection counts, which are then used to adjust `seq_lens` in the attention metadata. This eliminates the need for CPU-side sequence length adjustments and prevents unnecessary synchronization. The attention backends (MLA, FlashAttn) are updated to accept `max_seq_len` directly instead of computing it from CPU tensors.

**Potential risks**  
There's a risk of incorrect sequence length adjustments if `num_rejected_tokens_gpu` contains invalid values, which could lead to attention computation errors. The changes assume rejection counts are only applied when `num_speculative_tokens > 1`, which might not cover all edge cases. Additionally, invalidating CPU-side shadows (`_seq_lens_cpu`) could cause issues if other components rely on synchronized CPU data.

**Key insights**  
The PR achieves measurable throughput improvements (up to 4.5% for K=10) by minimizing GPU-CPU synchronization. Developers should ensure that `num_rejected_tokens_gpu` is properly validated and that all attention backends consistently handle the new `max_seq_len` parameter. The test update verifies rejection count calculations, but additional validation for edge cases (e.g., zero rejections, max speculative tokens) is recommended.

---

## 13. [[Doc] Add vllm-metal to hardware plugin documentation](https://github.com/vllm-project/vllm/pull/31174)


### Base Information

- **PR Number:** #31174
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-22 12:06:29
- **Type:** `TRIVIAL`
- **Changed Files (2):** [changed files link](https://github.com/vllm-project/vllm/pull/31174/files)


### Summary

**What changed and why**  
This PR adds documentation for the vllm-metal hardware plugin to the vLLM installation guide. It includes the plugin in the main hardware backends table and adds a tip in the macOS-specific installation section to inform users about GPU-accelerated inference options for Apple Silicon.

**Technical impact**  
The changes are purely documentation updates that improve discoverability of community-maintained hardware plugins. They provide clear guidance to macOS users about available acceleration options beyond the existing CPU implementation, potentially improving their inference performance.

**Potential risks**  
No technical risks since these are documentation-only changes. However, the documentation now references an external community-maintained project, so future maintenance depends on that project's stability and compatibility with vLLM core updates.

**Key insights**  
These documentation improvements help users find appropriate hardware acceleration options for their Apple Silicon devices. Developers should ensure that any future changes to the hardware plugin architecture or macOS support consider the existence of this community plugin. The tip format effectively guides users without disrupting the main installation flow.

---

## 14. [[SM100] Enable fp8 compute for prefill MLA](https://github.com/vllm-project/vllm/pull/30746)


### Base Information

- **PR Number:** #30746
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [pavanimajety](https://github.com/pavanimajety)
- **Merged time:** 2025-12-22 11:15:58
- **Type:** `SUBSTANTIAL`
- **Changed Files (3):** [changed files link](https://github.com/vllm-project/vllm/pull/30746/files)


### Summary

**What changed and why**  
This PR enables FP8 compute for prefill MLA (Multi-Latent Attention) kernels on SM100+ GPUs when `--kv-cache-dtype fp8` is set. It adds FP8 support to FlashInfer Cutlass FMHA and TRT-LLM Ragged prefill kernels by conditionally converting Q/K/V tensors to FP8 dtype before kernel execution, similar to the existing decode path.

**Technical impact**  
The changes introduce FP8 tensor conversion hooks in prefill computation paths for FlashInfer and TRT-LLM backends, while maintaining backward compatibility with existing FP16/BF16 workflows. The `q_data_type` field is now propagated through metadata to control kernel execution precision, and FP8 support is explicitly disabled for FlashAttention and CUDNN backends via assertions.

**Potential risks**  
The FP8 conversion occurs via `.to(fp8_dtype)` calls within each kernel wrapper, which may introduce overhead from additional memory operations. There is a risk of precision loss affecting model accuracy, though the PR mentions accuracy evaluations were performed. The changes assume FlashInfer PR #2047 is merged, creating a dependency risk.

**Key insights**  
Developers should ensure the external FlashInfer dependency is satisfied before integration. The FP8 support is currently limited to specific backends (FlashInfer/TRT-LLM), and performance should be validated against the reported GSM8k benchmarks. Future optimizations should focus on fusing quantization with RoPE operations to eliminate the extra conversion overhead.

---

## 15. [[MoE Refactor][9/N] Use modular kernel for unquantized Triton MoE](https://github.com/vllm-project/vllm/pull/31052)


### Base Information

- **PR Number:** #31052
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-22 09:34:19
- **Type:** `SUBSTANTIAL`
- **Changed Files (2):** [changed files link](https://github.com/vllm-project/vllm/pull/31052/files)


### Summary

**What changed and why**  
The changes refactor unquantized Triton MoE to use the modular kernel approach. Specifically, the `TritonExperts` modular kernel methods replace the direct `fused_experts` call for GPU execution. A test file is updated to initialize the workspace manager and call a weight processing method.

**Technical impact**  
This change centralizes MoE kernel logic through the modular kernel abstraction (`mk.FusedMoEModularKernel`), improving code structure and maintainability. The `forward_cuda` method now delegates to `self.kernel` instead of directly invoking `fused_experts`, aligning with a more modular architecture.

**Potential risks**  
The test includes a FIXME comment about moving `self.kernel` assignment in `FusedMoE.__init__`, indicating potential initialization order issues. There's also a risk if `TritonExperts` or `MoEPrepareAndFinalizeNoEP` are not properly configured for all GPU scenarios, especially given the conditional import for non-CUDA platforms.

**Key insights**  
Developers should ensure the modular kernel is correctly initialized for all GPU paths and address the FIXME to prevent test instability. The refactor simplifies future extensions but requires validation that performance and correctness match the previous implementation.

---

## 16. [[ROCm][CI/Build] Fix triton version to one that has triton_kernels required for gpt-oss to run](https://github.com/vllm-project/vllm/pull/31159)


### Base Information

- **PR Number:** #31159
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2025-12-22 09:19:27
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31159/files)


### Summary

**What changed and why**  
The change updates the Triton commit hash from `a272dfa8` to `57c693b6` in the ROCm base Dockerfile. This fixes an `ImportError` related to missing `GFX950MXScaleLayout` in Triton kernels, which was preventing GPT-OSS from running on gfx950 hardware due to a missing feature in Triton 3.5.x on ROCm.

**Technical impact**  
This update aligns the Triton version with the new `rocm/vllm-dev:base` image, ensuring compatibility with gfx950 GPUs. The change is minimal and only affects the Triton dependency, but it is critical for enabling GPT-OSS functionality on specific ROCm hardware.

**Potential risks**  
While the fix addresses the immediate import error, there is a risk that the new Triton commit may introduce regressions or incompatibilities with other components. Additionally, the change is tied to a specific hardware target (gfx950), so testing on other ROCm architectures is advisable to ensure broader compatibility.

**Key insights**  
Developers should verify that the new Triton commit (`57c693b6`) is stable and fully compatible with the entire ROCm software stack. It is also recommended to document hardware-specific dependencies and consider implementing version pinning or conditional builds to manage such hardware-specific fixes in the future.

---

## 17. [[UX] improve profiler error message](https://github.com/vllm-project/vllm/pull/31125)


### Base Information

- **PR Number:** #31125
- **Author:** [BoyuanFeng](https://github.com/BoyuanFeng)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 08:46:00
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31125/files)


### Summary

**What changed and why**  
The PR improves an error message in the GPU worker's `profile()` method. Previously, the error simply stated "Profiling is not enabled." Now it includes actionable guidance, showing users how to enable profiling via command-line arguments with a concrete example.

**Technical impact**  
This change enhances user experience by providing immediate, actionable feedback when profiling is attempted without proper configuration. It does not affect system behavior or architecture—only the error message content is modified, which aids in debugging and reduces user confusion.

**Potential risks**  
The example path (`YOUR_DIR_PATH_TO_DUMP_TRACE`) uses a placeholder that may be misinterpreted if users copy it verbatim without substitution. Additionally, the error message is hardcoded; if profiling configuration options change in the future, this message could become outdated.

**Key insights**  
Always provide actionable error messages to improve developer experience. Consider using dynamic error messages that reflect current valid configuration options, or document these options clearly elsewhere. For placeholders, ensure they are obviously placeholders (e.g., `<YOUR_DIR_PATH>`) to prevent literal usage.

---

## 18. [[ROCm] [Critical]: Remove unused variable](https://github.com/vllm-project/vllm/pull/31156)


### Base Information

- **PR Number:** #31156
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 08:28:22
- **Type:** `TRIVIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31156/files)


### Summary

**What changed and why**  
Removed an unused variable `rotary_dim` that was declared but never referenced in the code. This change resolves a ROCm build failure caused by the `-Werror=unused-variable` compiler flag, which treats unused variables as errors.

**Technical impact**  
The modification is purely cosmetic and has no functional impact on the kernel's behavior. It eliminates a compiler warning that was blocking successful builds on ROCm platforms, ensuring compatibility with the strict warning settings used in the ROCm toolchain.

**Potential risks**  
There is minimal risk since the variable was unused. However, if `rotary_dim` was intended for future use or debugging, its removal could require re-adding it later. The change should be validated to ensure no hidden dependencies exist, though the test results indicate no regressions.

**Key insights**  
This fix highlights the importance of maintaining warning-free code, especially when cross-platform support involves strict compiler flags. Developers should regularly audit for unused variables to prevent similar build issues. The PR effectively addresses a critical blocker for ROCm builds with minimal code impact.

---

## 19. [[AMD][CI] Add "V1 Test e2e + engine" to mi325_8 Agent Pool](https://github.com/vllm-project/vllm/pull/31040)


### Base Information

- **PR Number:** #31040
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2025-12-22 07:41:56
- **Type:** `TRIVIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31040/files)


### Summary

**What changed and why**  
The PR moves the "V1 Test e2e + engine" test group from the `mi325_4` agent pool to the `mi325_8` pool. This change aims to resolve intermittent "agent lost" failures observed when resource-intensive tests (specifically `test_eagle_correctness`) run concurrently on the same node, likely due to resource contention.

**Technical impact**  
By scheduling the test on 8-GPU machines instead of 4-GPU ones, the test will have exclusive access to the node, preventing co-location with other demanding tests. This reduces resource competition (e.g., GPU memory, CPU, or bandwidth) and should improve test stability without altering the test logic or runtime environment.

**Potential risks**  
Using larger machines for a 4-GPU test may reduce overall CI resource efficiency, potentially increasing queue times or costs if the `mi325_8` pool is limited. Additionally, if the root cause is not purely resource contention (e.g., a subtle race condition or driver issue), the failures might resurface even on isolated nodes.

**Key insights**  
This is a pragmatic stopgap to stabilize flaky tests, but the team should continue investigating the underlying cause (e.g., profiling resource usage during failures). Consider adding monitoring or resource limits to the test to ensure it doesn’t exceed expected consumption, and evaluate if similar changes are needed for other test groups.

---

## 20. [[CI][Bugfix] Fix `entrypoints/openai/test_audio.py`](https://github.com/vllm-project/vllm/pull/31151)


### Base Information

- **PR Number:** #31151
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 07:21:40
- **Type:** `TRIVIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31151/files)


### Summary

**What changed and why**  
The change modifies a single test case in the OpenAI audio API test suite by updating the text prompt from "What's happening in this audio?" to "What's a short title for this audio?". This appears to be a test-specific adjustment, likely addressing an issue reported in GitHub issue #30878, potentially related to test expectations or model responses.

**Technical impact**  
This change only affects test behavior and has no impact on production code, system architecture, or runtime functionality. It updates the expected input for a specific test scenario, which may improve test reliability or align with updated model capabilities or API expectations.

**Potential risks**  
The main risk is potential test flakiness if the new prompt produces inconsistent responses from the model. There's also a risk that this change might not fully address the underlying issue #30878 if other test parameters need adjustment. The minimal nature of the change suggests it's a targeted fix, but the exact problem context isn't provided.

**Key insights**  
This is a low-risk test-only change that developers should verify resolves the referenced GitHub issue. When reviewing such changes, ensure the new prompt aligns with the test's purpose and doesn't introduce new failure modes. Consider whether similar test cases might need analogous updates for consistency.

---

## 21. [[BugFix] skip language model in Encoder](https://github.com/vllm-project/vllm/pull/30242)


### Base Information

- **PR Number:** #30242
- **Author:** [Bounty-hunter](https://github.com/Bounty-hunter)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 05:25:59
- **Type:** `SUBSTANTIAL`
- **Changed Files (8):** [changed files link](https://github.com/vllm-project/vllm/pull/30242/files)


### Summary

**What changed and why**  
This PR introduces a new `mm_encoder_only` conversion type to skip language model initialization in vision-language models for encoder-only instances, reducing GPU memory usage. The change follows the pattern used for pooling models and requires VL models to implement a `get_language_model_spec` method to identify which components to exclude.

**Technical impact**  
The changes add a new conversion pipeline that creates a specialized model class without the language model component, modifies weight loading to skip language model weights, and updates dummy run methods to handle encoder-only models. This enables more efficient encoder-only deployments for VL models by eliminating unnecessary LM memory overhead.

**Potential risks**  
The monkey-patching approach (temporarily replacing `__init__` methods) is fragile and could break if internal class hierarchies change. There's also a risk that models not implementing `get_language_model_spec` will fail unexpectedly. The dummy run skipping might hide initialization issues that would normally be caught during warmup.

**Key insights**  
Developers must implement `get_language_model_spec` for any new VL models to support this optimization. The solution is architecturally similar to existing pooling model conversions, maintaining consistency. Consider adding validation to ensure skipped components don't affect encoder functionality, and document that this is specifically for encoder-only deployments.

---

## 22. [[CI] add polling for precompiled wheel in python_only_compile.sh, fix index generation for releases](https://github.com/vllm-project/vllm/pull/30781)


### Base Information

- **PR Number:** #30781
- **Author:** [Harry-Chen](https://github.com/Harry-Chen)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2025-12-22 05:24:21
- **Type:** `SUBSTANTIAL`
- **Changed Files (3):** [changed files link](https://github.com/vllm-project/vllm/pull/30781/files)


### Summary

**What changed and why**  
This PR addresses two CI-related issues: 1) It adds polling with architecture validation in `python_only_compile.sh` to handle timing issues where precompiled wheels aren't yet available during CI test execution. 2) It fixes index generation for releases by adding a `--wheel-dir` parameter to correctly point indices to the commit-specific wheel directory instead of the version directory.

**Technical impact**  
The polling enhancement makes the test script more robust by verifying that a compatible wheel exists before proceeding, preventing architecture mismatch errors. The index generation fix ensures release indices contain correct paths to wheel files, resolving broken links in release artifacts. Both changes improve CI reliability and artifact distribution.

**Potential risks**  
The extended 5-minute retry interval (increased from 3 minutes) could prolong CI runtime if wheels are delayed. The architecture check assumes `platform.machine()` output is contained within the `platform_tag` string, which may not match all wheel naming conventions. The `--wheel-dir` parameter default behavior could cause confusion if not explicitly set in all usage scenarios.

**Key insights**  
The architecture validation logic should be aligned with `determine_wheel_url` in `setup.py` to ensure consistency. Consider adding a timeout limit to the polling loop to prevent indefinite hanging. Document the new `--wheel-dir` parameter requirement for release workflows to prevent regression.

---

## 23. [[gpt-oss] Fix harmony parser in streaming responses](https://github.com/vllm-project/vllm/pull/30205)


### Base Information

- **PR Number:** #30205
- **Author:** [AlonKejzman](https://github.com/AlonKejzman)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2025-12-22 04:56:06
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/30205/files)


### Summary

**What changed and why**  
The fix addresses a bug in the harmony parser during streaming responses where final tokens could be missed in speculative decoding scenarios. Specifically, it handles cases where multiple tokens are generated simultaneously, including the final token, which results in text delta but leaves the current channel empty (in start state).

**Technical impact**  
This change ensures that when the harmony parser's current channel is empty but there is delta text present (indicating final token generation), the channel is explicitly set to "final". This prevents the loss of final streaming tokens and maintains proper token delivery in speculative decoding workflows.

**Potential risks**  
The main risk is that setting the channel to "final" when it's empty but delta text exists might incorrectly categorize some intermediate states. Edge cases could arise if the harmony parser enters a legitimate empty-channel state with delta text that isn't actually final output, potentially causing incorrect channel assignment.

**Key insights**  
Developers should verify that the "final" channel assignment logic aligns with all harmony parser state transitions. This fix is critical for streaming response completeness but requires careful testing to ensure it doesn't interfere with other parser states or introduce new edge cases in token generation sequences.

---

## 24. [[Model] Fix bagel failed to run](https://github.com/vllm-project/vllm/pull/31132)


### Base Information

- **PR Number:** #31132
- **Author:** [Potabk](https://github.com/Potabk)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 02:15:54
- **Type:** `SUBSTANTIAL`
- **Changed Files (1):** [changed files link](https://github.com/vllm-project/vllm/pull/31132/files)


### Summary

**What changed and why**  
The PR renames the `get_multimodal_embeddings` method to `embed_multimodal` in the Bagel model class. This change aligns the method name with the expected interface used by `modelrunner`, resolving a mismatch that was causing the model to fail during execution.

**Technical impact**  
This is a straightforward interface correction that ensures the Bagel model conforms to the expected API contract. The change is minimal and localized, affecting only the method signature without altering the underlying logic or data flow of the multimodal embedding generation.

**Potential risks**  
If any other components directly call `get_multimodal_embeddings` by name (e.g., via reflection or dynamic imports), they will break. Additionally, the PR description lacks test details, so it's unclear whether the fix has been validated beyond resolving the immediate runtime error.

**Key insights**  
Always verify that model implementations adhere to defined interfaces, especially when integrating new models. The PR should include a test plan and results to confirm the fix works end-to-end. Consider deprecating the old method name temporarily if there are external dependencies, or ensure all callers are updated simultaneously.

---

