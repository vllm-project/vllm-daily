# vLLM Merged PR Report

**Report Date:** 2025-12-23 PST

**Total Merged PRs:** 36

---

## 1. [[CI/Build] Ignore data_parallel_size_local](https://github.com/vllm-project/vllm/pull/30281)


### Base Information

- **PR Number:** #30281
- **Author:** [rjrock](https://github.com/rjrock)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2025-12-23 23:40:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30281/files) (1):**
  - `vllm/config/parallel.py`

### Summary

**What changed and why**  
The change adds `"data_parallel_size_local"` to the list of ignored parameters in the configuration hash computation. This allows API-only servers with headless engines to run successfully when `data_parallel_size_local` differs across nodes, resolving a configuration mismatch error.

**Technical impact**  
This modification prevents the hash-based consistency check from failing due to differences in `data_parallel_size_local` values between nodes. It enables flexible deployment scenarios where local data-parallel sizes vary, such as in headless engine configurations, without affecting collective communication parameters.

**Potential risks**  
If `data_parallel_size_local` influences other collective communication aspects not covered by the remaining hash parameters, ignoring it could mask genuine configuration mismatches. Additionally, future changes to data-parallel semantics might require revisiting this exclusion to ensure consistency.

**Key insights**  
The fix is minimal and targeted, addressing a specific deployment use case. Developers should verify that `data_parallel_size_local` does not impact other synchronized settings. Consider documenting this behavior to clarify that local size differences are permissible in API-only headless setups.

---

## 2. [[ROCm][CI] Set TORCH_NCCL_BLOCKING_WAIT Distributed Tests On ROCm](https://github.com/vllm-project/vllm/pull/31259)


### Base Information

- **PR Number:** #31259
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2025-12-23 23:19:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31259/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The PR adds `export TORCH_NCCL_BLOCKING_WAIT=1` to three distributed test steps in the AMD CI configuration. This is a workaround for a known HIP runtime bug that causes NCCL operations to fail with "HIP error: operation not permitted when stream is capturing" during distributed tests.

**Technical impact**  
Setting this environment variable forces NCCL operations to use blocking waits instead of asynchronous progress, which mitigates the HIP bug. The change only affects AMD CI test runs and does not impact production code or other platforms. It ensures distributed tests pass reliably until the underlying ROCm issue is resolved.

**Potential risks**  
The workaround may introduce slight performance overhead in CI due to blocking synchronization, but this is acceptable for test environments. There's a risk of forgetting to remove the workaround once the HIP bug is fixed, though the added TODO comments help track this. If the bug persists beyond expected ROCm releases, tests may start failing again.

**Key insights**  
This is a well-documented, platform-specific workaround that addresses a root cause tracked externally. Developers should monitor the linked HIP issue for fixes and remove these exports once a new ROCm release resolves the bug. The changes are minimal and focused, avoiding unnecessary modifications to other test configurations.

---

## 3. [[ROCm][CI] Fix "Distributed Tests (H200)" Test](https://github.com/vllm-project/vllm/pull/31227)


### Base Information

- **PR Number:** #31227
- **Author:** [kliuae](https://github.com/kliuae)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2025-12-23 22:56:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31227/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The PR modifies a single CI test command in `.buildkite/test-amd.yaml` to fix a failing distributed test on ROCm/H200 hardware. The original command used the `--all2all-backend=deepep_high_throughput` argument, which is unsupported on ROCm, causing an unrecognized argument error. The fix replaces it with the ROCm-compatible `allgather_reducescatter` backend and adds `--disable-nccl-for-dp-synchronization` to avoid CUDA graph breaking issues.

**Technical impact**  
This change ensures the distributed data-parallel test runs successfully on ROCm platforms by switching to a supported collective communication backend and using CPU-based synchronization for graph mode. It aligns the test configuration with ROCm’s current capabilities, preventing dependency errors and runtime failures.

**Potential risks**  
The change is minimal and specific to ROCm CI, but it assumes `allgather_reducescatter` and CPU-based synchronization are functionally equivalent to the original deepep backend for testing purposes. If future updates introduce platform-specific behavior differences, the test may not catch regressions that would appear on non-ROCm systems.

**Key insights**  
This fix highlights the importance of platform-aware CI configurations, especially for distributed operations. Developers should ensure backend options are validated per hardware target and consider adding conditional test logic for unsupported features. The use of `--disable-nccl-for-dp-synchronization` is a notable workaround for ROCm’s CUDA graph limitations.

---

## 4. [[ROCm][CI] Fix entrypoints tests and Python-only installation test on ROCm](https://github.com/vllm-project/vllm/pull/28979)


### Base Information

- **PR Number:** #28979
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 22:42:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28979/files) (26):**
  - `setup.py`
  - `tests/entrypoints/openai/conftest.py`
  - `tests/entrypoints/openai/test_chat.py`
  - `tests/entrypoints/openai/test_optional_middleware.py`
  - `tests/entrypoints/openai/test_response_api_with_harmony.py`
  - `tests/entrypoints/openai/test_serving_tokens.py`
  - `tests/entrypoints/openai/test_shutdown.py`
  - `tests/entrypoints/openai/test_transcription_validation.py`
  - `tests/entrypoints/openai/test_translation_validation.py`
  - `tests/entrypoints/openai/test_video.py`
  - `tests/entrypoints/openai/test_vision.py`
  - `tests/entrypoints/openai/test_vision_embeds.py`
  - `tests/entrypoints/pooling/basic/test_encode.py`
  - `tests/entrypoints/pooling/basic/test_truncation.py`
  - `tests/entrypoints/pooling/embed/conftest.py`
  - `tests/entrypoints/pooling/embed/test_correctness_mteb.py`
  - `tests/entrypoints/pooling/embed/test_offline.py`
  - `tests/entrypoints/pooling/embed/test_online.py`
  - `tests/entrypoints/pooling/embed/test_online_dimensions.py`
  - `tests/entrypoints/pooling/embed/test_online_long_text.py`
  - `tests/entrypoints/pooling/score/test_correctness_mteb.py`
  - `tests/entrypoints/pooling/score/test_offline.py`
  - `tests/entrypoints/pooling/score/test_online_rerank.py`
  - `tests/entrypoints/pooling/score/test_online_score.py`
  - `tests/standalone_tests/pytorch_nightly_dependency.sh`
  - `vllm/entrypoints/pooling/embed/conftest.py`

### Summary

**What changed and why**  
This PR fixes ROCm CI failures by adjusting attention backends for encoder-only models and audio transcription tests, and improves Python-only installation detection. Changes include setting `FLEX_ATTENTION` for encoder-only pooling tests (the only ROCm-supported backend for encoder self-attention) and `ROCM_AITER_FA` for audio transcription/translation. It also enhances platform auto-detection in `setup.py` to properly identify ROCm systems and adds ROCm-specific wheel fetching logic.

**Technical impact**  
The modifications enable ROCm CI tests to pass by ensuring compatible attention backends are used for specific model types and operations. The build system now correctly auto-detects ROCm environments and handles ROCm-specific library inclusion (e.g., `_rocm_C.abi3.so`). Test timeouts and retries are adjusted for ROCm to account for slower initialization or network delays, and SDP backends are disabled to avoid HuggingFace Transformers accuracy issues on ROCm.

**Potential risks**  
Platform-specific logic (`current_platform.is_rocm()`) may complicate maintenance if not consistently applied. Disabling flash/memory-efficient SDP backends could impact performance on ROCm. The added ROCm wheel-fetching mechanism depends on external indices, which may introduce reliability concerns. Conditional test modifications (e.g., reduced `max-model-len` for audio LoRA tests) might mask underlying issues.

**Key insights**  
Developers should ensure attention backend configurations align with platform capabilities, especially for encoder-only and audio models. The ROCm auto-detection improvements in `setup.py` are critical for correct installation. When adding ROCm-specific test adjustments, consider whether they are temporary workarounds or long-term solutions, and document accordingly. Regularly review platform-specific branches to avoid divergence from main code paths.

---

## 5. [Add `--max-model-len auto` to auto-fit context to available memory](https://github.com/vllm-project/vllm/pull/29431)


### Base Information

- **PR Number:** #29431
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 21:37:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29431/files) (7):**
  - `tests/engine/test_arg_utils.py`
  - `tests/v1/core/test_kv_cache_utils.py`
  - `vllm/config/model.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/v1/core/kv_cache_utils.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR adds support for `--max-model-len auto` (or `-1`) to automatically determine the maximum context length that fits in available GPU memory. The system uses binary search via `estimate_max_model_len` to find the largest supported length, falling back from the model's full context length if necessary. This feature is particularly useful for memory-constrained deployments where manual tuning is impractical.

**Technical impact**  
The auto-fit logic is encapsulated in `_auto_fit_max_model_len()` within `kv_cache_utils.py` and triggers during KV cache configuration when `original_max_model_len == -1`. For distributed setups (TP/PP), the engine core detects reduced `max_model_len` and synchronizes the new value to all workers via `collective_rpc("update_max_model_len")`, ensuring consistency since workers are spawned before memory profiling. The implementation correctly handles hybrid models and memory calculations across multiple workers.

**Potential risks**  
Binary search could introduce performance overhead during engine initialization for very large context windows. Edge cases where minimal memory cannot fit even one token may cause initialization failures. The synchronization mechanism assumes all workers are reachable via RPC; network issues could lead to inconsistent state. There's also a risk that auto-calculated lengths might not align with application requirements for specific sequence lengths.

**Key insights**  
The implementation is robust with proper cleanup (using try-finally to restore configs) and comprehensive logging. Developers should note that `-1` and `auto` are now valid CLI values, and the system handles distributed setups transparently. Testing shows effective memory adaptation across TP/PP configurations. Consider adding a configuration flag to allow users to set a minimum acceptable context length when using auto-fit mode.

---

## 6. [[docker] Fix downloading sccache on aarch64 platform](https://github.com/vllm-project/vllm/pull/30070)


### Base Information

- **PR Number:** #30070
- **Author:** [NickCao](https://github.com/NickCao)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 21:36:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30070/files) (1):**
  - `docker/Dockerfile`

### Summary

**What changed and why**  
The Dockerfile was modified to dynamically determine the correct sccache binary architecture based on the build platform (`TARGETPLATFORM`). Previously, the download URL was hardcoded to `x86_64`, which caused failures on `aarch64` systems. Now, the script selects `aarch64` for `linux/arm64` and `x86_64` for `linux/amd64`, with a fallback to the default URL if `SCCACHE_DOWNLOAD_URL` is not provided.

**Technical impact**  
This change enables cross-platform Docker builds by ensuring the correct sccache binary is downloaded for the target architecture. It improves maintainability by centralizing the architecture detection logic within the conditional block and allows override via the `SCCACHE_DOWNLOAD_URL` build argument. The build process now supports both AMD64 and ARM64 platforms seamlessly when sccache is enabled.

**Potential risks**  
If the `TARGETPLATFORM` environment variable is not set (e.g., in non-Docker BuildKit contexts), the script will fail with an "Unsupported TARGETPLATFORM" error. Additionally, the hardcoded sccache version (`v0.8.1`) and musl variant may become outdated or incompatible with future system requirements. The error handling only logs to stderr and exits, which could abruptly stop the build without detailed guidance.

**Key insights**  
Always validate that `TARGETPLATFORM` is available in the build environment, especially for non-BuildKit users. Consider parameterizing the sccache version to simplify updates. For robustness, add a default case that warns and skips sccache installation instead of exiting, unless it is strictly required. This pattern of platform detection can be reused for other architecture-specific dependencies in the Dockerfile.

---

## 7. [[XPU] Remove distributed_executor_backend check](https://github.com/vllm-project/vllm/pull/30760)


### Base Information

- **PR Number:** #30760
- **Author:** [1643661061leo](https://github.com/1643661061leo)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 21:34:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30760/files) (1):**
  - `vllm/platforms/xpu.py`

### Summary

**What changed and why**  
Removed 27 lines of code from the XPU platform module that handled `distributed_executor_backend` configuration. This change eliminates platform-specific backend validation and fallback logic for XPU, fixing a test failure in `test_custom_executor_async` where a marker file assertion failed due to interference from this logic.

**Technical impact**  
The XPU platform no longer overrides or validates the `distributed_executor_backend` setting. Configuration decisions (like defaulting to "ray" for multi-GPU) and warnings about unsupported backends are removed, making XPU behavior more consistent with other platforms. The `kv_transfer_config` adjustment remains unchanged.

**Potential risks**  
Without platform-specific validation, users might configure incompatible backends (like "mp") on XPU without warnings, potentially causing runtime failures. The removal of the spawn method enforcement for multiprocessing could lead to fork-related issues on XPU if not handled elsewhere. Test coverage gaps could hide regressions in distributed execution.

**Key insights**  
This fix addresses a specific test failure by simplifying XPU's backend handling, but the change should be accompanied by validation that distributed execution still works correctly on XPU hardware. Consider adding the removed logic to a more appropriate location if backend compatibility constraints still exist, or ensure comprehensive testing of all supported backends on XPU.

---

## 8. [[Qwen3-Omni] fixed _get_feat_extract_output_lengths function](https://github.com/vllm-project/vllm/pull/31007)


### Base Information

- **PR Number:** #31007
- **Author:** [wangxiongts](https://github.com/wangxiongts)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 21:33:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31007/files) (1):**
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
The `_get_feat_extract_output_lengths` function was modified to return only `output_lengths` instead of both `feat_lengths` and `output_lengths`. All callers were updated to use the single return value, and the `aftercnn_lens` parameter in the audio tower call now receives `audio_output_lengths` instead of the previously computed `audio_feat_lengths`.

**Technical impact**  
This change simplifies the function's interface by eliminating an unused intermediate value (`feat_lengths`). The audio processing pipeline now consistently uses the final output lengths throughout, which improves code clarity and reduces potential confusion between intermediate and final feature lengths.

**Potential risks**  
The main risk is that `audio_output_lengths` might not be mathematically equivalent to the original `feat_lengths` for the `aftercnn_lens` parameter. If the audio tower expects different length values for its two parameters, this change could introduce incorrect behavior. Additionally, any downstream code that might have depended on the removed `feat_lengths` return value would now fail.

**Key insights**  
The change appears to be a cleanup that removes unused code, but developers should verify that the audio tower's `aftercnn_lens` parameter truly expects the same values as `output_lengths`. A comment explaining the relationship between these lengths would help maintain clarity. The modifications are consistent across all call sites, suggesting thorough refactoring.

---

## 9. [[DeepSeek v3.2] Remove unnecessary syncwarps](https://github.com/vllm-project/vllm/pull/31047)


### Base Information

- **PR Number:** #31047
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 21:33:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31047/files) (1):**
  - `csrc/cache_kernels.cu`

### Summary

**What changed and why**  
Removed two unnecessary `__syncwarp()` calls in the CUDA kernel `indexer_k_quant_and_cache_kernel`. The synchronization was redundant because `__shfl_xor_sync()` already ensures warp synchronization, and the removal avoids unnecessary overhead.

**Technical impact**  
This change reduces synchronization overhead in the kernel, potentially improving performance slightly. The logic remains correct because `__shfl_xor_sync()` implicitly synchronizes the warp, maintaining data consistency for the reduction operation.

**Potential risks**  
Minimal risk, as the removed syncwarps were redundant. However, if any code path relied on explicit synchronization beyond what `__shfl_xor_sync()` provides (e.g., for memory operations), it could introduce subtle bugs. The conditional `#ifndef USE_ROCM` suggests platform-specific considerations, but ROCm builds are unaffected.

**Key insights**  
The change is a safe optimization that eliminates redundant synchronization. Developers should verify that no hidden dependencies exist on the removed syncwarps and ensure similar patterns are reviewed elsewhere in the codebase. The test results show no regression, confirming correctness.

---

## 10. [[Bugfix][ROCm][Dynamo][DS 3.1][FP8] fix unsupported hasattr call when Dynamo tracing for ROCm device](https://github.com/vllm-project/vllm/pull/31149)


### Base Information

- **PR Number:** #31149
- **Author:** [zejunchen-zejun](https://github.com/zejunchen-zejun)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 21:32:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31149/files) (1):**
  - `vllm/_aiter_ops.py`

### Summary

**What changed and why**  
Added a new custom op `rocm_aiter_triton_gemm_a8w8_blockscale` to bypass Dynamo tracing for the `aiter.ops.triton.gemm_a8w8_blockscale` operation on ROCm. The issue was that Dynamo failed when tracing `hasattr` calls on user-defined variables within this op. The fix registers the op with `torch.compile` to prevent internal tracing, allowing DeepSpeed FP8 to run successfully on MI355 hardware.

**Technical impact**  
This change introduces a new custom op implementation and fake function, integrating it into the existing ROCm operator registration flow. It moves the direct import of `gemm_a8w8_blockscale` from the static method into the custom op wrapper, ensuring the operation is treated as a black box during Dynamo compilation. This maintains performance and accuracy while avoiding tracing issues.

**Potential risks**  
If the custom op registration fails or is misconfigured, it could lead to silent fallbacks or runtime errors. The fake implementation assumes output tensor dimensions based on input shapes, which may not hold for all edge cases. There is also a risk of inconsistency if the underlying `aiter.ops.triton.gemm_a8w8_blockscale` implementation changes without updating the wrapper.

**Key insights**  
Always register external or complex operations as custom ops when using Dynamo to avoid tracing failures. Ensure fake implementations accurately reflect the real op's output characteristics. Consider adding validation for tensor shapes and dtypes in the fake function to catch mismatches early. This pattern should be applied consistently to similar ops in the codebase.

---

## 11. [Revert "[bench] Support common prefix len config (for decode-only bench)"](https://github.com/vllm-project/vllm/pull/31240)


### Base Information

- **PR Number:** #31240
- **Author:** [minosfuture](https://github.com/minosfuture)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 21:17:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31240/files) (2):**
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
This PR reverts a previous change that added support for a `--common-prefix-len` configuration parameter in the benchmarking system. The rationale is that the existing `--random-prefix-len` argument should be used instead, simplifying the interface and removing redundant functionality.

**Technical impact**  
The removal eliminates a dataset sampling parameter (`prefix_len`) from the random dataset generator and its corresponding CLI argument. This simplifies the codebase by reducing configuration options and ensures all prefix length control flows through the established `--random-prefix-len` mechanism.

**Potential risks**  
If any benchmarking scripts or documentation were relying on the `--common-prefix-len` parameter, they will now fail or behave unexpectedly. There's also a risk that the `--random-prefix-len` argument might not provide identical functionality, potentially affecting benchmark consistency if not fully equivalent.

**Key insights**  
This is a clean-up change that improves API consistency. Developers should update any scripts using `--common-prefix-len` to use `--random-prefix-len` instead. The team should verify that the remaining parameter provides the necessary flexibility for all benchmarking scenarios that previously used the removed option.

---

## 12. [[ROCm][CI] Set VLLM_FLOAT32_MATMUL_PRECISION="tf32" For terratorch Tests In AMD CI](https://github.com/vllm-project/vllm/pull/31242)


### Base Information

- **PR Number:** #31242
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 19:21:51
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31242/files) (2):**
  - `.buildkite/test-amd.yaml`
  - `tests/models/test_terratorch.py`

### Summary

**What changed and why**  
The PR addresses two issues in AMD CI: it sets `VLLM_FLOAT32_MATMUL_PRECISION="tf32"` for terratorch-related tests to resolve a PyTorch TF32 API conflict error, and it fixes a missing `pooling_task` parameter in `test_terratorch.py` that was causing test failures. The environment variable workaround is temporary until a future PyTorch update.

**Technical impact**  
These changes ensure terratorch tests pass in AMD CI by aligning matmul precision settings and correcting an API call. The modifications are isolated to CI configuration and a single test file, so they do not affect production code or other CI pipelines.

**Potential risks**  
The temporary environment variable fix may be forgotten and left in place after the PyTorch update. Additionally, the `pooling_task="plugin"` hardcode assumes a specific use case, which could break if the underlying model or plugin behavior changes.

**Key insights**  
The PR highlights an integration issue between vLLM's recent TF32 API update and AMD's PyTorch version. Developers should track the TODO comments to remove the workaround after updating PyTorch. The test fix suggests `models/test_terratorch.py` may not have been actively monitored in CI, warranting a review of test coverage.

---

## 13. [[ROCm][CI][Bugfix] Fix Siglip2 rotary embedding dispatch and InternVL video test tolerance](https://github.com/vllm-project/vllm/pull/31235)


### Base Information

- **PR Number:** #31235
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-23 18:56:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31235/files) (2):**
  - `tests/models/multimodal/generation/test_common.py`
  - `vllm/model_executor/models/siglip2navit.py`

### Summary

**What changed and why**  
This PR fixes two ROCm-specific issues: a bug in Siglip2's rotary embedding dispatch logic that incorrectly selected implementations, and flaky test failures for InternVL video due to numerical precision differences on ROCm hardware. The changes ensure proper function selection for different platforms and increase test tolerance to account for platform-specific numerical variance.

**Technical impact**  
The Siglip2 fix corrects the rotary embedding implementation selection, ensuring CUDA uses `forward_cuda`, ROCm uses `forward_hip`, and other platforms use `forward_native`. The test change increases the `num_logprobs` parameter from 5 to 10 for ROCm only, aligning with similar accommodations made for other sensitive models and preventing spurious test failures due to minor output differences.

**Potential risks**  
The conditional logic for platform detection could become complex if more platforms are added. Increasing `num_logprobs` may mask deeper numerical instability issues rather than addressing root causes, though this pattern is established for other models. There is a minor risk that the test tolerance increase could allow genuine regressions to pass undetected.

**Key insights**  
The fix demonstrates the importance of thorough platform-specific testing, especially for numerical precision-sensitive operations. Developers should verify that all conditional platform checks use positive logic (e.g., `is_cuda()` rather than `not is_cuda()`) to avoid inversion errors. When adjusting test tolerances, follow existing patterns and limit changes to the specific platforms where issues are observed.

---

## 14. [[Bug] Fix `Number of dimensions of tensors must match.` for Deepseek V3.2](https://github.com/vllm-project/vllm/pull/31160)


### Base Information

- **PR Number:** #31160
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2025-12-23 18:41:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31160/files) (1):**
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
The fix addresses a tensor dimension mismatch error when serving DeepSeek-V3.2 with expert parallelism. The issue occurred during RoPE (Rotary Position Embedding) application in the MLA (Multi-head Latent Attention) indexer, where `q_pe` and `k_pe` tensors had inconsistent dimensions (4-D vs. 3-D) after compilation, causing a `torch.cat` failure. The solution reshapes these tensors back to token-flattened shapes and adjusts the squeeze dimension for `k_pe`.

**Technical impact**  
This change ensures tensor dimensions remain consistent after RoPE transformations during Torch Dynamo compilation, particularly when `is_neox_style=True` is used for the indexer. It maintains compatibility with the MLA attention mechanism and prevents runtime crashes during KV cache initialization, enabling successful model serving with expert parallelism.

**Potential risks**  
If the reshape dimensions (`-1, self.n_head, self.rope_dim`) do not match the actual tensor sizes post‑compilation, it could lead to shape errors or incorrect attention computations. Additionally, the fix assumes the RoPE implementation always introduces extra leading dimensions during compilation, which may not hold for all future model versions or compilation modes.

**Key insights**  
The root cause highlights a subtle interaction between RoPE style settings and compilation‑induced tensor shape changes. Developers should verify that `is_neox_style` configuration is consistent across attention components and consider adding shape assertions or dynamic reshaping logic to handle compilation variability more robustly.

---

## 15. [[P/D] Mooncake connector support more protocols](https://github.com/vllm-project/vllm/pull/30133)


### Base Information

- **PR Number:** #30133
- **Author:** [LCAIZJ](https://github.com/LCAIZJ)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 18:24:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30133/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`

### Summary

**What changed and why**  
The change makes the Mooncake connector's protocol configurable by reading a `mooncake_protocol` setting from the KV transfer configuration. This allows support for additional protocols (e.g., `ascend` for Huawei NPU hardware) beyond the default `rdma`, enabling compatibility with a wider range of hardware.

**Technical impact**  
This modification introduces a flexible protocol selection mechanism without altering the core connector logic. The connector now dynamically initializes the transfer engine with the specified protocol, maintaining backward compatibility by defaulting to `rdma` if no configuration is provided.

**Potential risks**  
If an unsupported protocol value is provided, the transfer engine initialization may fail, potentially crashing the service. There is also a risk of configuration errors if `kv_connector_extra_config` is malformed or missing, though the default fallback mitigates this.

**Key insights**  
Developers should ensure that any new protocol values (like `ascend`) are fully supported by the underlying transfer engine. Consider adding validation for allowed protocol values and documenting the configuration option in relevant user-facing documentation.

---

## 16. [[KVEvent] User request.block_hash for parent block_hash](https://github.com/vllm-project/vllm/pull/30544)


### Base Information

- **PR Number:** #30544
- **Author:** [heheda12345](https://github.com/heheda12345)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 18:23:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30544/files) (2):**
  - `tests/v1/core/test_prefix_caching.py`
  - `vllm/v1/core/block_pool.py`

### Summary

**What changed and why**  
The PR modifies how parent block hashes are determined in KV cache events when dealing with null blocks. Previously, the parent block hash was extracted directly from the physical block object, which fails when the parent is a null block (with no hash). Now it uses the logical parent hash from `request.block_hashes` instead, ensuring correct parent identification in scenarios like Mamba or sliding-window caching where null blocks appear in the block table.

**Technical impact**  
This change decouples the logical parent hash from the physical block representation, making the system more robust to null blocks in caching workflows. It affects `BlockStored` event generation in `cache_full_blocks()`, ensuring events always reference valid parent hashes even when the physical parent block lacks a hash. The test addition validates this behavior explicitly.

**Potential risks**  
If `request.block_hashes` is misaligned with the actual block sequence (e.g., due to incorrect indexing or hash computation), the parent hash could be wrong, leading to downstream inconsistencies in caching or prefetching logic. The removal of the `assert parent_block.block_hash is not None` check also eliminates a safeguard against non-null blocks without hashes.

**Key insights**  
Always use logical metadata (like `request.block_hashes`) over physical state when dealing with caching abstractions, as physical blocks may be placeholders. Ensure hash arrays remain synchronized with block allocations. Consider adding validation that `block_hashes[num_cached_blocks - 1]` corresponds to the logical parent when non-null.

---

## 17. [[Misc] Remove unused custom ops `copy_blocks` and `copy_blocks_mla`](https://github.com/vllm-project/vllm/pull/30967)


### Base Information

- **PR Number:** #30967
- **Author:** [lengrongfu](https://github.com/lengrongfu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 18:22:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30967/files) (6):**
  - `csrc/cache.h`
  - `csrc/cache_kernels.cu`
  - `csrc/torch_bindings.cpp`
  - `tests/kernels/attention/test_cache.py`
  - `vllm/_custom_ops.py`
  - `vllm/_ipex_ops.py`

### Summary

**What changed and why**  
Removed two unused custom operations (`copy_blocks` and `copy_blocks_mla`) along with their implementations, bindings, and tests. The changes eliminate dead code that is no longer referenced in the codebase, simplifying maintenance and reducing binary size.

**Technical impact**  
This cleanup reduces the attack surface and potential for confusion, as the removed functions were not being used. The remaining cache operations (`swap_blocks`, `reshape_and_cache`, etc.) are unaffected, so core functionality is unchanged.

**Potential risks**  
If any external or internal code still depends on these functions (e.g., via dynamic imports or undocumented use), it will break. The removal of comprehensive tests means any future reintroduction of similar functionality would require re-implementing validation.

**Key insights**  
Always verify that no dependencies exist before removing public APIs. Consider deprecating functions first if there’s uncertainty. The cleanup is positive but highlights the need for better code usage tracking (e.g., via static analysis or deprecation warnings).

---

## 18. [[Bugfix] Enable `dynamic_dims` for different embeds shape](https://github.com/vllm-project/vllm/pull/31223)


### Base Information

- **PR Number:** #31223
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 18:15:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31223/files) (3):**
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/qwen2_audio.py`

### Summary

**What changed and why**  
The PR fixes a bug where `list[torch.Tensor]` fields in embedding input schemas for three multimodal models (AudioFlamingo3, MiniCPMV, and Qwen2Audio) were not correctly enabling dynamic dimensions. The change adds `dynamic_dims` parameter to the `TensorShape` annotation for the variable-length dimension (`"naf"` or `"ns"`) in each schema, ensuring tensors within the list can have different shapes as intended.

**Technical impact**  
This enables proper handling of batched inputs with variable sequence lengths in the embedding tensors for these models. The `dynamic_dims` parameter informs the tensor processing system that the specified dimension can vary across tensors in the list, which is essential for multimodal inputs where audio or image embeddings may have different sizes per sample.

**Potential risks**  
If the dynamic dimension is incorrectly specified or if other dimensions in the tensor shape also need to be dynamic, this could lead to runtime shape mismatches. There's also a risk that similar issues exist in other model schemas that weren't updated in this PR, potentially causing inconsistent behavior across the codebase.

**Key insights**  
The fix is minimal and targeted, addressing a specific limitation in tensor shape validation. Developers should verify that all models using `list[torch.Tensor]` for embeddings have appropriate `dynamic_dims` configured. Consider adding a test to validate that variable-shaped tensors work correctly with these schemas to prevent regression.

---

## 19. [[Chore] Simplify logic of `_execute_mm_encoder`](https://github.com/vllm-project/vllm/pull/31222)


### Base Information

- **PR Number:** #31222
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 18:15:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31222/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The PR simplifies the multimodal encoder execution logic by removing the need to return `mm_positions` from the batching utility. The helper method `_batch_mm_kwargs_from_scheduler` is renamed to `_batch_mm_inputs_from_scheduler` and now returns only multimodal hashes and kwargs, eliminating the position tracking previously bundled with each hash.

**Technical impact**  
This change streamlines the data flow by decoupling position information from the batching process. The encoder execution now directly maps hashes to outputs without intermediate position tracking, which reduces complexity in the caching mechanism (`encoder_cache`). The video processing logic is also slightly refactored to handle outputs as a list before assignment.

**Potential risks**  
The removal of position tracking (`PlaceholderRange`) could break functionality if downstream components still rely on this information for multimodal feature placement. The video processing micro-batching logic now explicitly creates a list (`curr_group_outputs_lst`), which might introduce subtle behavioral changes if `embed_multimodal` returns non-list types in edge cases.

**Key insights**  
This is a clean-up PR that reduces unnecessary data passing, but developers should verify that no other parts of the codebase depend on the removed `mm_positions`/`PlaceholderRange` data. The explicit list construction for video outputs improves clarity but should be tested to ensure compatibility with all multimodal model interfaces.

---

## 20. [[CI] Add Qwen3-Next-FP8 to Blackwell model tests](https://github.com/vllm-project/vllm/pull/31049)


### Base Information

- **PR Number:** #31049
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 17:21:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31049/files) (5):**
  - `tests/evals/gsm8k/configs/Qwen3-Next-FP8-EP2.yaml`
  - `tests/evals/gsm8k/configs/models-blackwell.txt`
  - `tests/evals/gsm8k/test_gsm8k_correctness.py`
  - `tests/utils.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
Added Qwen3-Next-FP8 model testing configuration for Blackwell hardware, including a new YAML config file, model list entry, and enhanced debug logging. The changes enable FP8 precision testing with expert parallelism and specific environment variables for nightly testing.

**Technical impact**  
This extends the existing GSM8K evaluation suite to support a new FP8-quantized model variant with expert parallelism. The addition requires proper tensor parallel configuration (size 2) and enables FlashInfer optimizations via environment variable. The debug logging improvements help diagnose environment variable propagation during test execution.

**Potential risks**  
The model requires specific hardware (Blackwell) and may fail if run on unsupported systems. The `VLLM_USE_FLASHINFER_MOE_FP8` environment variable dependency could cause inconsistent behavior if not properly set. The accuracy threshold of 0.85 may need validation against actual model performance.

**Key insights**  
The changes follow established patterns for adding new model tests. Developers should verify the FP8 quantization works correctly with expert parallelism on target hardware. The enhanced logging will help debug environment-related issues during test failures. Ensure nightly test infrastructure properly handles the new model entry.

---

## 21. [[ROCm][Bugfix] Fix RuntimeError in MMEncoderAttention by replacing .view() with .reshape()](https://github.com/vllm-project/vllm/pull/31203)


### Base Information

- **PR Number:** #31203
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-23 13:48:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31203/files) (2):**
  - `tests/models/multimodal/conftest.py`
  - `vllm/attention/layers/mm_encoder_attention.py`

### Summary

**What changed and why**  
The PR fixes a `RuntimeError` in `MMEncoderAttention` by replacing `.view()` with `.reshape()` in two attention forward methods (`_forward_sdpa` and `_forward_fa`). This error occurred when attention operations produced non-contiguous tensors, which `.view()` cannot handle, while `.reshape()` safely handles both contiguous and non-contiguous cases.

**Technical impact**  
The change ensures compatibility with non-contiguous tensors generated by SDPA and FlashAttention operations, particularly on ROCm platforms. It maintains identical output values and tensor shapes, preserving functional correctness while improving robustness across different hardware and attention backends.

**Potential risks**  
Using `.reshape()` may introduce silent performance overhead in cases where a copy is required due to non-contiguity, though this is minimal. There is a low risk of unintended side effects if other parts of the codebase rely on tensor contiguity assumptions, but the change is isolated to reshaping operations.

**Key insights**  
Always prefer `.reshape()` over `.view()` when the tensor's memory layout is uncertain, especially after operations like attention that may transpose data. The accompanying test configuration update clarifies the ROCm SDP accuracy issue link, aiding future debugging. Developers should audit similar `.view()` usage in attention-related code for potential non-contiguity issues.

---

## 22. [[Core] Add a random suffix to frontend-provided request IDs](https://github.com/vllm-project/vllm/pull/27987)


### Base Information

- **PR Number:** #27987
- **Author:** [markmc](https://github.com/markmc)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2025-12-23 13:05:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/27987/files) (16):**
  - `tests/v1/engine/test_async_llm.py`
  - `tests/v1/engine/test_engine_core.py`
  - `tests/v1/engine/test_engine_core_client.py`
  - `tests/v1/engine/test_fast_incdec_prefix_err.py`
  - `tests/v1/engine/test_output_processor.py`
  - `tests/v1/engine/test_parallel_sampling.py`
  - `tests/v1/engine/test_process_multi_modal_uuids.py`
  - `tests/v1/engine/utils.py`
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/output_processor.py`
  - `vllm/v1/engine/parallel_sampling.py`

### Summary

**What changed and why**  
This PR addresses request ID collisions by appending an 8-character random suffix to client-provided request IDs, ensuring internal uniqueness while preserving the external ID for backward compatibility. The changes introduce an `external_req_id` field in `EngineCoreRequest` to store the original client ID, while the internal `request_id` becomes unique. This prevents collisions in concurrent scenarios like `vllm serve bench` and mitigates race conditions in abort and scheduling logic.

**Technical impact**  
The core change decouples external and internal request identifiers, requiring updates across the request lifecycle: input processing, output generation, abort handling, and parallel sampling. The `OutputProcessor` now maintains a mapping from external to internal IDs to support abort operations. This adds complexity to ID management but ensures request uniqueness without breaking existing client integrations, as `RequestOutput.request_id` still returns the external ID.

**Potential risks**  
- The 8-character suffix (32 bits of entropy) may still collide under extreme loads (>10k concurrent requests with the same external ID), though probability is low (~1.16%).  
- Increased memory overhead from maintaining the external→internal ID mapping, especially with high request volumes.  
- Edge cases in abort logic where multiple internal IDs map to one external ID could lead to unintended bulk aborts if not handled carefully by callers.

**Key insights**  
- Developers must use the new `internal` flag in `abort()` when referencing internal IDs (e.g., from `EngineCoreRequest.request_id`).  
- Test updates show the need to consistently provide both `request_id` and `external_req_id` in test fixtures.  
- The change is backward compatible for API consumers but requires internal adaptations in engine components and monitoring tools that rely on request IDs.

---

## 23. [[Mamba] - Consolidate Mambas Attention Logic](https://github.com/vllm-project/vllm/pull/28133)


### Base Information

- **PR Number:** #28133
- **Author:** [Josephasafg](https://github.com/Josephasafg)
- **Merged By:** [tdoublep](https://github.com/tdoublep)
- **Merged time:** 2025-12-23 12:57:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28133/files) (5):**
  - `vllm/model_executor/layers/mamba/short_conv.py`
  - `vllm/v1/attention/backends/mamba1_attn.py`
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/attention/backends/mamba_attn.py`
  - `vllm/v1/attention/backends/short_conv_attn.py`

### Summary

**What changed and why**  
This PR consolidates duplicate attention logic across Mamba1, Mamba2, and ShortConv attention backends into a shared base class (`BaseMambaAttentionMetadataBuilder`). The goal is to reduce maintenance overhead and eliminate code duplication, particularly for common metadata computation and prefix caching logic.

**Technical impact**  
The changes introduce a common metadata structure (`BaseMambaAttentionMetadata`) and move shared logic—such as handling prefill/decode splits, prefix caching indices, and CUDA graph optimizations—into the base builder. Mamba2 retains custom chunking logic, while Mamba1 and ShortConv become minimal subclasses. This improves code reuse and centralizes future updates.

**Potential risks**  
- The refactor may introduce subtle bugs if the shared logic does not correctly handle edge cases for all three backends (e.g., Mamba2’s chunking or prefix caching interactions).  
- Changes to tensor shapes or device placements in the base class could affect downstream kernels that rely on specific metadata formats.  
- The removal of duplicate code might obscure backend-specific nuances if not carefully documented.

**Key insights**  
- The consolidation significantly reduces code volume (~143 lines net reduction) and aligns with DRY principles.  
- Developers should verify that the shared `_compute_common_metadata` method correctly serves all backends, especially for prefix caching and CUDA graph scenarios.  
- Future modifications to Mamba attention should prioritize extending the base class rather than duplicating logic.

---

## 24. [docs: Add llm-d integration to the website](https://github.com/vllm-project/vllm/pull/31234)


### Base Information

- **PR Number:** #31234
- **Author:** [terrytangyuan](https://github.com/terrytangyuan)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-23 12:27:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31234/files) (3):**
  - `docs/deployment/integrations/kserve.md`
  - `docs/deployment/integrations/llm-d.md`
  - `docs/deployment/k8s.md`

### Summary

**What changed and why**  
This PR adds documentation for integrating vLLM with llm-d, a Kubernetes-native distributed inference serving stack. The changes include creating a new documentation page for llm-d integration and updating existing KServe documentation and Kubernetes deployment overview to reference this new option.

**Technical impact**  
The changes expand the documented deployment options for vLLM on Kubernetes by adding llm-d as another supported integration. This provides users with more choices for scalable distributed model serving, particularly emphasizing llm-d's focus on achieving state-of-the-art performance across various hardware accelerators.

**Potential risks**  
The documentation now references external guides that may change independently of vLLM's documentation. If the linked llm-d or KServe documentation URLs become outdated or change significantly, users might encounter broken links or inconsistent instructions.

**Key insights**  
The addition of llm-d integration demonstrates vLLM's growing ecosystem of deployment options. Developers should ensure all external documentation links are periodically verified for accuracy. The clear distinction between using llm-d directly versus through KServe's LLMInferenceService provides users with flexible deployment pathways.

---

## 25. [Use helper function instead of looping through attribute names](https://github.com/vllm-project/vllm/pull/29788)


### Base Information

- **PR Number:** #29788
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2025-12-23 09:31:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29788/files) (2):**
  - `vllm/config/model.py`
  - `vllm/config/utils.py`

### Summary

**What changed and why**  
The PR replaces manual attribute checking loops with a centralized `getattr_iter` helper function. In `model.py`, two methods (`get_inputs_embeds_size` and `get_total_num_kv_heads`) now use `getattr_iter` to search for configuration attributes, ensuring consistent fallback behavior. The helper function in `utils.py` was extended to support a `default_factory` callable for lazy default value generation.

**Technical impact**  
This change improves code maintainability by centralizing attribute lookup logic, reducing duplication, and ensuring uniform handling of missing attributes. The architecture now better supports lazy evaluation of default values through `default_factory`, which is particularly useful when defaults require computation (like calling `self.get_hidden_size()`).

**Potential risks**  
The `default_factory` parameter introduces a potential risk if callables have side effects or are expensive, as they're invoked whenever defaults are needed. There's also a subtle behavior change: the original code returned `None` for missing attributes before falling back, while `getattr_iter` now directly returns the factory result. This could mask issues if `None` was a meaningful intermediate state.

**Key insights**  
The refactoring successfully abstracts repetitive patterns into a reusable utility. Developers should ensure `default_factory` callables are pure and inexpensive. Consider adding type hints or validation to prevent misuse of the new parameter. This pattern could be extended to other similar attribute lookups in the codebase.

---

## 26. [Only patch `original_max_position_embeddings` for Transformers v4](https://github.com/vllm-project/vllm/pull/31214)


### Base Information

- **PR Number:** #31214
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 08:46:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31214/files) (1):**
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
The change modifies the `patch_rope_parameters` function to conditionally patch `original_max_position_embeddings` only for Transformers v4. This aligns with upstream changes in Transformers v5, which now automatically handles this field within `rope_parameters`.

**Technical impact**  
The update ensures compatibility with both Transformers v4 and v5 by avoiding duplicate patching in v5. The logic now checks for `original_max_position_embeddings` early and includes it in `rope_parameters` only under v4, preventing potential conflicts or redundant entries.

**Potential risks**  
If Transformers v5 is used but the `rope_parameters` field is missing or malformed, the function may not correctly handle edge cases where `original_max_position_embeddings` is needed. Additionally, any custom configurations relying on the old unconditional patching behavior could break when upgrading to v5.

**Key insights**  
Developers should ensure their Transformers version is correctly detected, as the logic bifurcates based on `Version("5.0.0.dev0")`. Testing with both v4 and v5 is recommended to validate compatibility. The change highlights the importance of tracking upstream library updates to avoid redundant code.

---

## 27. [[FIX] FP4 quantization kernel padding initialization bug](https://github.com/vllm-project/vllm/pull/31097)


### Base Information

- **PR Number:** #31097
- **Author:** [danielafrimi](https://github.com/danielafrimi)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 08:45:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31097/files) (1):**
  - `csrc/quantization/fp4/nvfp4_quant_kernels.cu`

### Summary

**What changed and why**  
The fix addresses a bug in FP4 quantization kernel padding initialization where separate loops for zeroing padded scale factor regions and processing actual data caused incorrect addressing. The solution unifies both operations into a single loop that iterates over all rows/columns (including padded regions), using zero-filled vectors for out-of-bounds areas and loading real data only for valid regions. This ensures proper scale factor computation while preventing buffer overruns.

**Technical impact**  
The kernel now correctly initializes all scale factor memory locations, including padded areas, by processing every address in a unified loop. Output writes are restricted to valid (non-padded) regions, maintaining correctness without altering the padded tensor layout. Performance benchmarks show negligible impact, with throughput and latency metrics remaining effectively unchanged.

**Potential risks**  
Although the unified loop simplifies logic, it introduces conditional branches (`if` checks for valid regions) that could affect GPU warp efficiency in edge cases. Additionally, the reliance on `num_padded_cols` calculation must consistently align with `numKTiles` rounding behavior; any mismatch in padding assumptions could lead to incorrect memory access patterns.

**Key insights**  
The fix prioritizes correctness over micro-optimizations, as performance remains stable. Developers should verify that padding calculations (e.g., `num_padded_cols`) are consistent across all related kernels and that conditional checks do not introduce divergence in warps processing mostly valid data. Future optimizations could explore branch reduction for padded regions.

---

## 28. [Fix edge case Mistral tool parser](https://github.com/vllm-project/vllm/pull/30724)


### Base Information

- **PR Number:** #30724
- **Author:** [joa-stdn](https://github.com/joa-stdn)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 06:19:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30724/files) (2):**
  - `tests/tool_parsers/test_mistral_tool_parser.py`
  - `vllm/tool_parsers/mistral_tool_parser.py`

### Summary

**What changed and why**  
The PR fixes edge cases in the Mistral tool parser where content before `[TOOL_CALLS]` contains curly braces `{`, which previously caused JSON parsing errors. It also ensures that malformed JSON in tool arguments returns partial content instead of failing entirely, aligning behavior with the API's handling of incorrect tool calls.

**Technical impact**  
The changes restructure the parsing logic to split content and tool calls more robustly, separating handling for pre-v11 and v11+ tokenizer versions. This improves resilience to malformed inputs and ensures consistent extraction of tool calls even when the preceding content includes special characters that interfere with JSON parsing.

**Potential risks**  
The regex-based fallback for malformed JSON could still be brittle for highly nested or complex tool calls. Additionally, the change assumes that only one `BOT` token appears for pre-v11 tokenizers; if multiple appear unexpectedly, it may raise an unhandled `ValueError`.

**Key insights**  
Developers should note that the parser now returns partial tool call data for malformed JSON instead of failing, which helps maintain conversation context. Ensure that tool call arguments avoid containing unmatched curly braces or quotes to prevent parsing issues. The updated logic better aligns with API behavior, reducing unexpected failures in tool-calling workflows.

---

## 29. [[Misc] Introduce `encode_*_url` utility function](https://github.com/vllm-project/vllm/pull/31208)


### Base Information

- **PR Number:** #31208
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 05:45:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31208/files) (14):**
  - `tests/entrypoints/openai/test_audio.py`
  - `tests/entrypoints/openai/test_video.py`
  - `tests/entrypoints/openai/test_vision.py`
  - `tests/entrypoints/pooling/embed/test_online_vision.py`
  - `tests/entrypoints/test_chat_utils.py`
  - `tests/models/multimodal/generation/test_keye.py`
  - `tests/models/multimodal/generation/test_vit_backend_functionality.py`
  - `tests/v1/ec_connector/integration/test_epd_correctness.py`
  - `tests/v1/entrypoints/openai/serving_responses/test_image.py`
  - `tests/v1/kv_connector/unit/test_example_connector.py`
  - `tests/v1/tpu/test_multimodal.py`
  - `vllm/multimodal/audio.py`
  - `vllm/multimodal/image.py`
  - `vllm/multimodal/utils.py`

### Summary

**What changed and why**  
This PR introduces new `encode_*_url` utility functions (`encode_audio_url`, `encode_image_url`, `encode_video_url`) that generate data URLs with proper mimetypes, replacing manual construction of `data:` URLs in tests and other code. It also adds warnings about future default format changes (image format will switch from JPEG to PNG in v0.15) and updates the underlying `encode_*_base64` functions to accept format parameters.

**Technical impact**  
The changes centralize URL generation logic, reducing code duplication and ensuring consistent mimetype handling across the codebase. Tests now use the new URL utilities, simplifying test fixtures and improving maintainability. The underlying media encoding functions now support configurable formats, with backward-compatible warnings for the impending default change.

**Potential risks**  
The warning about the default image format change may cause confusion if not properly communicated to downstream users. There is a risk that external code relying on the current default JPEG format could break after v0.15. Additionally, the mimetype detection via `mimetypes.types_map` may not cover all formats, potentially leading to incorrect mimetypes in data URLs.

**Key insights**  
Developers should update any manual `data:` URL construction to use the new `encode_*_url` functions for consistency. Be mindful of the upcoming default format change for images and explicitly specify `format="JPEG"` if lossy compression is required. Ensure that any custom media formats have proper mimetype mappings to avoid incorrect URL generation.

---

## 30. [adapt voxtral](https://github.com/vllm-project/vllm/pull/31095)


### Base Information

- **PR Number:** #31095
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 05:31:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31095/files) (12):**
  - `tests/models/multimodal/generation/test_voxtral.py`
  - `tests/models/registry.py`
  - `vllm/config/model.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_streaming.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/model_executor/models/whisper_utils.py`
  - `vllm/transformers_utils/configs/mistral.py`
  - `vllm/v1/attention/backends/utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR adds streaming support for the Voxtral model by introducing a new `VoxtralStreamingGeneration` class that processes audio inputs incrementally. Key changes include a new model class with specialized multimodal processing, modifications to the Whisper encoder to support causal convolutions and block-pooled attention, and updates to handle raw input tokens directly rather than embeddings.

**Technical impact**  
The changes extend the multimodal architecture to support streaming audio inputs, requiring modifications to the attention mechanism (via `WhisperAttentionWithBlockPooling`), input processing pipeline, and KV cache management. The new model class bypasses traditional embedding lookups, instead using precomputed audio embeddings and a time-based positional encoding. This impacts how tokens are processed and cached, particularly for the encoder's KV cache which now uses block pooling.

**Potential risks**  
The streaming model assumes a single audio input per step and lacks caching support, which could affect performance for multi-audio scenarios. The block-pooling attention mechanism may have compatibility issues with non-FlashAttention backends. Additionally, the reliance on raw input tokens introduces new code paths that must be carefully validated to avoid regressions in non-streaming models.

**Key insights**  
Developers should note the new `requires_raw_input_tokens` flag and ensure that the multimodal input pipeline correctly handles both streaming and non-streaming modes. The Whisper encoder updates (causal convolutions, positional embedding types) are critical for streaming but may affect other Whisper-based models. Testing should verify that the block-pooling attention works correctly across different hardware and attention backends.

---

## 31. [Add util function for checking nesting of rope parameters](https://github.com/vllm-project/vllm/pull/31146)


### Base Information

- **PR Number:** #31146
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 03:41:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31146/files) (3):**
  - `vllm/config/model.py`
  - `vllm/model_executor/models/transformers/utils.py`
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
This PR centralizes the logic for checking if RoPE parameters are nested by introducing a new utility function `is_rope_parameters_nested`. The change handles a Transformers v5 compatibility issue where the global variable name for allowed layer types changed from `ALLOWED_LAYER_TYPES` to `ALLOWED_ATTENTION_LAYER_TYPES`, while also removing duplicated code across multiple files.

**Technical impact**  
The changes improve code maintainability by consolidating the nested RoPE parameter detection into a single function. This ensures consistent behavior across the codebase and abstracts away the version-specific import logic for Transformers v4/v5 compatibility, reducing the risk of inconsistencies.

**Potential risks**  
If the `ALLOWED_ATTENTION_LAYER_TYPES` variable structure differs unexpectedly between Transformers versions or if `rope_parameters` is not a dictionary, the function may raise errors. Additionally, the empty dictionary check in `is_rope_parameters_nested` returns `False`, which might not align with all edge-case expectations for nested structures.

**Key insights**  
Developers should use `is_rope_parameters_nested` for all nested RoPE parameter checks to ensure compatibility and consistency. The utility function simplifies future updates to Transformers version handling and reduces code duplication, making the logic easier to test and maintain.

---

## 32. [[OpenAI] Add parameter metadata to validation errors](https://github.com/vllm-project/vllm/pull/30134)


### Base Information

- **PR Number:** #30134
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-23 03:30:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30134/files) (8):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/protocol.py`
  - `vllm/entrypoints/openai/serving_chat.py`
  - `vllm/entrypoints/openai/serving_completion.py`
  - `vllm/entrypoints/openai/serving_engine.py`
  - `vllm/entrypoints/openai/serving_responses.py`
  - `vllm/entrypoints/openai/speech_to_text.py`
  - `vllm/entrypoints/renderer.py`

### Summary

**What changed and why**  
This PR introduces a new `VLLMValidationError` exception class to provide detailed parameter metadata in OpenAI API error responses. The changes convert 30+ validation errors across all OpenAI endpoints from generic `ValueError` to `VLLMValidationError` with parameter and value information, improving API compatibility with OpenAI's error format.

**Technical impact**  
The changes enhance error handling consistency across the codebase by standardizing validation errors. Exception handlers in `api_server.py` and `serving_engine.py` are updated to extract parameter metadata from `VLLMValidationError` instances and include them in error responses via the `param` field, matching OpenAI API specifications.

**Potential risks**  
The extensive conversion of validation errors (30+ instances) increases the risk of missing edge cases where parameter metadata might not be properly extracted. There's also a risk that some validation errors might not be caught by the updated exception handlers, particularly in streaming scenarios where error propagation differs.

**Key insights**  
The implementation successfully addresses API compatibility requirements by providing structured error responses. Developers should ensure all validation paths use `VLLMValidationError` consistently and test error scenarios thoroughly, especially for streaming endpoints where error handling is more complex. The parameter extraction logic in exception handlers should be monitored for robustness.

---

## 33. [[Frontend] Support using chat template as custom score template for reranking models](https://github.com/vllm-project/vllm/pull/30550)


### Base Information

- **PR Number:** #30550
- **Author:** [jzakrzew](https://github.com/jzakrzew)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2025-12-23 03:19:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30550/files) (19):**
  - `docs/models/supported_models.md`
  - `docs/serving/openai_compatible_server.md`
  - `examples/pooling/score/offline_using_template.py`
  - `examples/pooling/score/online_using_template.py`
  - `examples/pooling/score/template/nemotron-rerank.jinja`
  - `tests/entrypoints/pooling/score/test_utils.py`
  - `tests/models/language/pooling_mteb_test/mteb_utils.py`
  - `tests/models/language/pooling_mteb_test/test_nemotron.py`
  - `tests/models/registry.py`
  - `tests/models/utils.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/run_batch.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/score_utils.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR enables custom prompt templates for scoring/reranking models via the `--chat-template` CLI argument or `chat_template` in `tokenizer_config.json`. The change decouples prompt templates from model-specific code, allowing any model requiring a custom score template (like `nvidia/llama-nemotron-rerank-1b-v2`) to work without modifying the model class. Previously, custom templates were tied to the `SupportsScoreTemplate` interface, which required architecture-specific changes.

**Technical impact**  
The implementation extends the existing chat template infrastructure to scoring workflows, introducing a new `score_template` parameter that flows through the scoring pipeline (`LLM.score()`, OpenAI API server, and batch processing). It adds a `LlamaBidirectionalModel` architecture for bidirectional attention models and updates configuration handling to support custom pooling types. The changes maintain backward compatibility by falling back to default tokenization when no template is provided.

**Potential risks**  
1. **Template inheritance issues**: Models may inherit inappropriate chat templates from their base LLM, potentially breaking scoring behavior if not explicitly overridden.  
2. **Inconsistent tokenization**: Edge cases exist where `add_special_tokens=False` is required for template-based prompts, creating inconsistency with non-template paths.  
3. **Scope confusion**: Mixing `chat_template` and `score_template` concepts could cause developer confusion, as noted in TODO item #6.

**Key insights**  
1. **Use semantic role selection**: The provided examples demonstrate robust template design using `selectattr("role", "eq", "query")` instead of positional indexing, which future-proofs against message ordering changes.  
2. **Validate template necessity**: The system correctly restricts custom templates to cross-encoder models only, preventing misapplication to embedding models.  
3. **Testing strategy**: The comprehensive test suite validates both template and non-template paths, ensuring the fallback mechanisms work correctly across different model types.

---

## 34. [[Bugfix] Fix MoE LoRA bin/pt loading](https://github.com/vllm-project/vllm/pull/31161)


### Base Information

- **PR Number:** #31161
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2025-12-23 03:09:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31161/files) (2):**
  - `vllm/lora/lora_model.py`
  - `vllm/lora/utils.py`

### Summary

**What changed and why**  
The PR fixes a bug where MoE model LoRA weights couldn't be loaded correctly from `adapter_model.bin` or `adapter_model.pt` files. It removes the previous complex validation logic that relied on target module regex matching and instead applies the same `check_unexpected_modules` function to bin/pt files as used for safetensors, ensuring consistent validation across all LoRA file types.

**Technical impact**  
This change simplifies the codebase by removing redundant validation logic (`is_regex_target_modules`) and unifying the module-checking approach. Bin/pt files now undergo the same validation as safetensors, improving consistency and fixing the MoE LoRA loading issue. The removal of regex-based target module validation reduces complexity but assumes `check_unexpected_modules` is sufficient for all cases.

**Potential risks**  
If `check_unexpected_modules` lacks robustness for regex-based target modules (previously handled by `is_regex_target_modules`), some valid LoRA configurations might be incorrectly rejected. The change also assumes that the removed validation was unnecessary, which could reintroduce issues if edge cases existed in regex-based LoRA definitions.

**Key insights**  
The fix prioritizes consistency and simplicity, but developers should verify that `check_unexpected_modules` adequately handles all expected LoRA module patterns, especially for MoE models. Consider adding tests for regex-based target modules to ensure no regression occurs. The removal of 60 lines of code is a significant simplification, but thorough testing is advised.

---

## 35. [Correct position of docstring of class attributes](https://github.com/vllm-project/vllm/pull/31209)


### Base Information

- **PR Number:** #31209
- **Author:** [wdhongtw](https://github.com/wdhongtw)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 02:08:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31209/files) (2):**
  - `vllm/forward_context.py`
  - `vllm/v1/kv_cache_interface.py`

### Summary

**What changed and why**  
The PR moves docstrings for class attributes to positions immediately following their corresponding attribute declarations. This ensures documentation tools can properly parse and associate docstrings with the correct attributes, addressing potential parsing issues.

**Technical impact**  
These changes improve code documentation clarity and maintainability without affecting runtime behavior. The restructured docstrings follow Python conventions where attribute docstrings should appear directly after the attribute they document, which is especially important for automated documentation generation tools.

**Potential risks**  
Minimal risk since these are documentation-only changes. However, if any tools or scripts rely on specific line numbers or exact string positions in these files, they might need updates. The changes could also temporarily affect merge conflicts if other PRs modify the same lines.

**Key insights**  
Always place attribute docstrings immediately after the attribute declaration for proper tooling support. While this PR doesn't require testing, developers should verify that their IDE/documentation tools correctly display the updated docstrings. Consider adopting linters or formatters that enforce consistent docstring placement to prevent similar issues.

---

## 36. [[ROCm][FEAT] Support AITER RMSNorm quantization fusion pass](https://github.com/vllm-project/vllm/pull/26575)


### Base Information

- **PR Number:** #26575
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-23 02:07:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/26575/files) (5):**
  - `tests/compile/test_fusion.py`
  - `vllm/_aiter_ops.py`
  - `vllm/compilation/matcher_utils.py`
  - `vllm/compilation/pass_manager.py`
  - `vllm/compilation/rocm_aiter_fusion.py`

### Summary

**What changed and why**  
This PR adds ROCm AITER fusion passes that combine RMSNorm operations with FP8 quantization. It introduces new fused kernels for both standalone RMSNorm+quant and RMSNorm-with-add+quant patterns, leveraging ROCm's AITER library for improved performance on AMD hardware. The changes include new test infrastructure, updated matcher utilities, and integration into the compilation pass manager.

**Technical impact**  
The implementation creates specialized fusion patterns for ROCm that replace sequences of RMSNorm and quantization operations with single fused kernels. This reduces kernel launch overhead and improves memory access patterns, leading to measurable throughput improvements as shown in the benchmark results (2.68k tok/s vs 2.63k tok/s). The architecture maintains compatibility with existing fusion infrastructure while adding ROCm-specific paths.

**Potential risks**  
The fusion patterns have specific constraints: per-tensor quantization isn't supported, and group quantization currently requires group size 128. There's also platform-specific logic that could break if FP8 dtype configurations change. The accuracy tests show minor variations (0.7612 vs 0.7718 exact match), though within acceptable tolerance.

**Key insights**  
Developers should note that this optimization is ROCm-specific and requires the AITER library. The fusion is controlled by compilation config flags (`fuse_norm_quant: true` with ROCm custom ops). When porting to new models, verify that quantization patterns match the supported group shapes (per-token or 128-group). The implementation demonstrates how to extend the fusion framework for hardware-specific optimizations while maintaining the existing abstraction layers.

---

