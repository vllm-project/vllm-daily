# vLLM Merged PR Report

**Report Date:** 2025-12-27 PST

**Total Merged PRs:** 7

---

## 1. [[ROCm][CI] Added perceptron lib in requirements for isaac multi-modal test](https://github.com/vllm-project/vllm/pull/31441)


### Base Information

- **PR Number:** #31441
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-27 20:15:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31441/files) (1):**
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
Added `perceptron==0.1.4` to the ROCm test requirements file to resolve dependency failures in the "Multi-Modal Models Test (Extended) 2" for the Isaac model. The change addresses missing package errors that caused six specific test cases to fail.

**Technical impact**  
This addition ensures the Isaac multi-modal model tests have the necessary library to execute successfully within the ROCm CI environment. It directly impacts test reliability by eliminating dependency-related failures, but does not alter production code or core system architecture.

**Potential risks**  
Introducing a new dependency could lead to version conflicts with other packages or transitive dependencies. If `perceptron` has its own dependencies, they may not be explicitly managed, potentially causing future CI instability. The specific version pin (`0.1.4`) might also become outdated or incompatible with future Isaac model updates.

**Key insights**  
The fix is narrowly scoped to restore CI test passes, but consider verifying if `perceptron` should be a core dependency or only a test-specific one. Document the rationale for this version choice and monitor for any upstream changes to the Isaac model that might affect this requirement.

---

## 2. [[BugFix] register quant scale tensors as buffer](https://github.com/vllm-project/vllm/pull/31395)


### Base Information

- **PR Number:** #31395
- **Author:** [BoyuanFeng](https://github.com/BoyuanFeng)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-27 19:20:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31395/files) (1):**
  - `vllm/attention/layer.py`

### Summary

**What changed and why**  
The PR fixes an illegal memory access (IMA) error in FP8 quantization tests by registering quantization scale tensors (`_k_scale`, `_v_scale`, `_q_scale`, `_prob_scale`) as PyTorch buffers. Previously, these tensors were created as regular attributes, so `model.to(device)` didn't move them to GPU, causing device mismatch when CUDA kernels accessed them.

**Technical impact**  
The changes ensure scale tensors are properly registered in the module's state dict, making them follow the device placement of the model. Two helper functions were added: `should_load_quant_weights()` determines when to load quantized weights, and `set_default_quant_scales()` centralizes scale initialization with optional buffer registration. Both attention layer classes now handle scale initialization in `process_weights_after_loading()` for non-quantized models.

**Potential risks**  
The fix assumes scale tensors should always be registered as buffers, which may not be optimal for all backends (some prefer CPU scales). The dual initialization path (register_buffer vs fill_) adds complexity and could lead to inconsistencies if not carefully maintained. There's also a risk of state dict bloat if scales are registered unnecessarily for non-quantized models.

**Key insights**  
Always register tensors that need device synchronization as PyTorch buffers. The solution elegantly handles edge cases where checkpoints don't contain scale tensors by reinitializing them after weight loading. Developers should be aware that `model.to(device)` only affects parameters and registered buffers, not arbitrary tensor attributes.

---

## 3. [add tip for VLLM_USE_PRECOMPILED arg to reduce docker build time](https://github.com/vllm-project/vllm/pull/31385)


### Base Information

- **PR Number:** #31385
- **Author:** [yitingdc](https://github.com/yitingdc)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-27 19:19:48
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31385/files) (1):**
  - `docs/deployment/docker.md`

### Summary

**What changed and why**  
Documentation update adding a note about the `VLLM_USE_PRECOMPILED` build argument to reduce Docker build times. The change addresses common issues where building from source is slow or encounters compilation errors in the C++/CUDA kernel stage.

**Technical impact**  
This change improves the developer experience by providing a documented optimization path. It does not alter any functional code—only the documentation—so there is no runtime or architectural impact. The note guides users to leverage pre-existing precompiled wheels from nightly builds.

**Potential risks**  
If users enable `VLLM_USE_PRECOMPILED` after modifying C++/CUDA code, they may inadvertently run with outdated or incompatible kernels, leading to runtime errors or incorrect behavior. The documentation could also become outdated if the nightly builds process or argument names change.

**Key insights**  
Always verify that no kernel changes have been made before using the precompiled option. This documentation should be kept synchronized with the actual build script arguments. Consider adding a warning in the Dockerfile or build script itself to alert users when this flag is used with local modifications.

---

## 4. [[MoE Refactor][10/N] Cleanup Fp8 Process Weights After Loading](https://github.com/vllm-project/vllm/pull/31169)


### Base Information

- **PR Number:** #31169
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-27 12:22:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31169/files) (1):**
  - `vllm/model_executor/layers/quantization/fp8.py`

### Summary

**What changed and why**  
The PR refactors the FP8 MoE weight processing logic by cleaning up `process_weights_after_loading`. It removes conditional branches for block vs. non-block quantization and extracts backend-specific weight transformations into a helper method (`_convert_weights_to_kernel_format`). This simplifies the code and makes the transformations reusable for online processing.

**Technical impact**  
The changes centralize weight conversion logic for different backends (DeepGemm, AITER, FlashInfer) into a single method, reducing code duplication. The removal of block/non-block branching improves maintainability, and the introduction of `weight_scale_name` standardizes parameter naming. This refactor aligns with broader MoE FP8 backend integration efforts.

**Potential risks**  
If the helper method is called incorrectly (e.g., mismatched backend or quantization type), it could lead to silent errors in weight formatting. The removal of explicit branching may obscure logic flow, making debugging harder. Additionally, any undiscovered edge cases in backend-specific transformations (e.g., DeepGemm block processing) could cause numerical inaccuracies.

**Key insights**  
Developers should ensure that `_convert_weights_to_kernel_format` is only called with compatible backend and quantization configurations. The refactor sets the stage for online weight processing, so future changes should maintain clear separation between loading-time and runtime transformations. Testing across all supported backends (as outlined in the test plan) is critical to validate correctness.

---

## 5. [[BugFix] Fix cache issue in compilation_config](https://github.com/vllm-project/vllm/pull/31376)


### Base Information

- **PR Number:** #31376
- **Author:** [BoyuanFeng](https://github.com/BoyuanFeng)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2025-12-27 06:30:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31376/files) (2):**
  - `tests/compile/test_config.py`
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The fix moves the `get_cached_compilation_config.cache_clear()` call from the `finally` block to the beginning of the `try` block in `set_current_vllm_config`. This ensures the cache is cleared immediately when entering the context manager, preventing stale cached configurations from being used within the new context.

**Technical impact**  
This change guarantees that any code executed inside the `set_current_vllm_config` context uses the updated compilation configuration. It resolves a race condition where a previously cached `old_config` could persist if accessed before the context switch, ensuring the cache is invalidated before the new configuration is applied.

**Potential risks**  
If the cache clearance fails or is skipped due to an early exception, the stale cache issue could resurface. Additionally, the fix assumes that clearing the cache at context entry is always appropriate; if there are nested context managers or concurrent accesses, further synchronization might be needed.

**Key insights**  
Cache invalidation should occur at the point of context entry, not exit, to prevent stale data from being used. Developers should be cautious when using `@lru_cache` on configuration accessors—explicit cache management is required when underlying state can change. The added test validates that the cache is properly refreshed, which is essential for maintaining correctness in dynamic configuration environments.

---

## 6. [[Chore]: Remove HF format Phi4-MM examples](https://github.com/vllm-project/vllm/pull/31405)


### Base Information

- **PR Number:** #31405
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-27 05:42:03
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31405/files) (3):**
  - `examples/offline_inference/audio_language.py`
  - `examples/offline_inference/vision_language.py`
  - `examples/offline_inference/vision_language_multi_image.py`

### Summary

**What changed and why**  
Removed three example functions (`run_phi4_multimodal`, `load_phi4_multimodal`) and their corresponding registry entries for the Hugging Face format of Phi-4-multimodal-instruct. This cleanup addresses an oversight from a previous PR (#30049) that missed removing these examples.

**Technical impact**  
The changes eliminate outdated or unsupported model examples from the codebase, reducing maintenance burden and potential confusion. No functional impact on the core system, as these were only example utilities for offline inference demos.

**Potential risks**  
Low risk since these are example files, but users who were referencing these specific model examples may need to update their scripts. The removal could break existing workflows that depend on the `phi4_multimodal` key in the example registries.

**Key insights**  
Ensure all related references are removed in a single PR to avoid leftover code. Consider adding a deprecation notice or migration guide if these examples were widely used. Verify that no other documentation or configuration files still reference the removed model examples.

---

## 7. [[CI/Build] Ignore max transformers version for more common tests](https://github.com/vllm-project/vllm/pull/31401)


### Base Information

- **PR Number:** #31401
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-27 05:06:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31401/files) (4):**
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/test_registry.py`
  - `tests/models/utils.py`

### Summary

**What changed and why**  
The PR modifies four test files to disable the maximum transformers version check by setting `check_max_version=False` and adding `check_version_reason="vllm"`. This change follows up on issue #30619 to expand test coverage by allowing vLLM-only tests to run even when the installed transformers version exceeds the maximum specified for a model.

**Technical impact**  
These changes relax version constraints for specific test suites, enabling more tests to execute in environments with newer transformers versions. This improves test coverage for vLLM-specific functionality without affecting production code or model compatibility checks in non-test contexts.

**Potential risks**  
Tests may pass with transformers versions that have breaking changes not yet supported by vLLM, potentially masking compatibility issues. There's also a risk that tests could become dependent on newer transformers features that aren't yet integrated into vLLM's model implementations.

**Key insights**  
This is a targeted change to improve CI test coverage, but developers should ensure that any transformers version dependencies are properly documented. Consider adding periodic audits to verify that skipped version checks don't hide critical incompatibilities between vLLM and newer transformers releases.

---

