# vLLM Merged PR Report

**Report Date:** 2025-12-31 PST

**Total Merged PRs:** 6

---

## 1. [[Audio] Improve Audio Inference Scripts (offline/online)](https://github.com/vllm-project/vllm/pull/29279)


### Base Information

- **PR Number:** #29279
- **Author:** [ekagra-ranjan](https://github.com/ekagra-ranjan)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-31 15:34:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29279/files) (2):**
  - `examples/offline_inference/audio_language.py`
  - `examples/online_serving/openai_transcription_client.py`

### Summary

**What changed and why**
The changes improve audio inference scripts for both offline and online scenarios. For offline inference, the script now varies audio inputs when batch size > 1 to prevent cache reuse issues. For online inference, two enhancements were made: adding direct endpoint calls without the OpenAI SDK, and supporting custom audio file inputs via a command-line argument.

**Technical impact**
The offline inference modification changes how batched requests are constructed - instead of repeating identical inputs, it now cycles through different audio assets to avoid vLLM engine deduplication. The online changes introduce a new HTTP-based streaming endpoint call and make the client more flexible by supporting arbitrary audio files, reducing dependency on pre-defined assets.

**Potential risks**
The offline change assumes audio_assets has sufficient length to support cycling through different inputs; if audio_assets has only one element, the cycling behavior becomes identical to the previous implementation. The direct HTTP streaming implementation lacks proper error handling for network failures or malformed responses. The model detection logic (`if "openai" in model`) is fragile and could incorrectly route requests.

**Key insights**
The offline fix addresses a critical performance optimization by preventing cache reuse in batch processing. Developers should ensure audio_assets contains diverse samples for effective batching. The HTTP streaming implementation provides a lightweight alternative to the OpenAI SDK but needs robust error handling. Consider using a more reliable model detection mechanism than string matching.

---

## 2. [[CI][Bugfix] Fix token counting in chunked prefill streaming test](https://github.com/vllm-project/vllm/pull/31565)


### Base Information

- **PR Number:** #31565
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2025-12-31 15:05:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31565/files) (1):**
  - `tests/entrypoints/openai/test_chunked_prompt.py`

### Summary

**What changed and why**  
The test was incorrectly assuming each streaming chunk contains exactly one token, causing flaky failures when vLLM batches multiple tokens into a single SSE chunk. The fix replaces the per-chunk token counter with actual token counting using the logprobs content length.

**Technical impact**  
This change ensures the test accurately measures token production regardless of batching behavior, making it robust across different hardware platforms (especially ROCm) and vLLM's internal optimizations. It aligns test logic with actual system behavior rather than relying on implementation-specific assumptions.

**Potential risks**  
If logprobs are disabled or structured differently in future API versions, the assertion on `chunk.choices[0].logprobs.content` could fail. The fix assumes logprobs content always exists when tokens are present, which may not hold if the test configuration changes.

**Key insights**  
Tests should validate behavior, not implementation details like chunk-token mapping. When dealing with streaming outputs, always use authoritative sources (like logprobs) rather than inferring counts from metadata. Consider adding a safeguard to handle missing logprobs gracefully in future test updates.

---

## 3. [[BugFix] Fix async scheduling for pooling models](https://github.com/vllm-project/vllm/pull/31584)


### Base Information

- **PR Number:** #31584
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-31 14:48:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31584/files) (7):**
  - `vllm/v1/executor/ray_utils.py`
  - `vllm/v1/outputs.py`
  - `vllm/v1/pool/metadata.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/worker_base.py`

### Summary

**What changed and why**  
This PR fixes a race condition in pooling models with async scheduling by moving CPU copy operations to a dedicated stream and introducing `AsyncGPUPoolingModelRunnerOutput` to handle non-blocking copies. It also simplifies the `ModelRunnerOutput` dataclass by providing default values and removes redundant field assignments.

**Technical impact**  
The changes enable true asynchronous execution for pooling models by decoupling GPU computation from CPU data transfer. The introduction of a dedicated copy stream allows overlapping computation and data movement, potentially improving throughput. The `ModelRunnerOutput` simplification reduces boilerplate and ensures consistent default values across the codebase.

**Potential risks**  
The async copy mechanism introduces new synchronization points via `torch.Event`, which could cause deadlocks if not properly managed. There's a risk of tensor lifetime issues if device tensors are released before copies complete. The removal of explicit field assignments in `ModelRunnerOutput` could lead to unintended `None` values if callers rely on previous defaults.

**Key insights**  
Developers should verify that the async copy stream is properly synchronized in all execution paths. The pooling cursor now expects a NumPy array instead of a Python list, which may affect callers. The `AsyncModelRunnerOutput` handling in `execute_model_ray` must correctly unwrap outputs to maintain compatibility with existing scheduling logic.

---

## 4. [[Bugfix] Fix BAGEL online serving for text and image understanding](https://github.com/vllm-project/vllm/pull/31546)


### Base Information

- **PR Number:** #31546
- **Author:** [Dylan1229](https://github.com/Dylan1229)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-31 14:46:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31546/files) (2):**
  - `vllm/model_executor/models/bagel.py`
  - `vllm/transformers_utils/processors/bagel.py`

### Summary

**What changed and why**  
This PR fixes two critical issues preventing the BAGEL multimodal model from working with vLLM's OpenAI-compatible API. First, `BagelProcessor.__call__` was incorrectly returning a `BatchEncoding` instead of the required `BatchFeature`, causing the multimodal embedding pipeline to fail. Second, `BagelForConditionalGeneration` lacked the `get_placeholder_str` class method needed to handle image placeholders in multimodal prompts.

**Technical impact**  
The changes ensure that the BAGEL model correctly processes both text-only and image-text multimodal inputs through vLLM's serving API. The processor now properly wraps outputs in `BatchFeature`, aligning with the multimodal pipeline's expectations, while the placeholder method enables image token replacement during prompt construction. This restores full functionality for online serving scenarios.

**Potential risks**  
The `get_placeholder_str` method currently only supports the "image" modality and will raise a `ValueError` for others, which could break future multimodal extensions (e.g., video or audio). Additionally, the processor's fallback to an empty `BatchFeature` when no inputs are provided might mask upstream issues if called unexpectedly with null inputs.

**Key insights**  
Developers should note that the BAGEL model now fully supports vLLM's multimodal API, but the placeholder implementation is intentionally limited to images. For consistency, consider adding a default return (e.g., `None`) for unsupported modalities instead of raising an error, and validate that the processor's empty-output case aligns with all usage patterns.

---

## 5. [Add GLM-ASR multimodal support](https://github.com/vllm-project/vllm/pull/31436)


### Base Information

- **PR Number:** #31436
- **Author:** [baonudesifeizhai](https://github.com/baonudesifeizhai)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2025-12-31 07:12:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31436/files) (8):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/audio_language.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/glmasr_utils.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
This PR adds support for the GLM-ASR-Nano-2512 automatic speech recognition (ASR) model to vLLM. The changes include a new model implementation (`glmasr.py`), utility functions (`glmasr_utils.py`), integration into the multimodal processing pipeline, and updates to documentation and tests. The purpose is to enable efficient inference for this speech-to-text model within vLLM's serving framework.

**Technical impact**  
The integration extends vLLM's multimodal capabilities to include another ASR architecture. It follows existing patterns (e.g., reusing `AudioFlamingo3` components) and adds GLM-ASR-specific logic for audio chunking and embedding projection. The model supports LoRA and tensor parallelism, aligning with vLLM's feature set. Changes to the config loader ensure `trust_remote_code` is properly passed for this model.

**Potential risks**  
The implementation assumes a 1:1 mapping between audio inputs and text tokens (`<\|pad\|>` placeholders), which could break if prompts deviate from this pattern. The audio chunking logic and convolution parameter defaults may not match all GLM-ASR variants. There is also a risk of performance regression if the audio processing pipeline introduces overhead for other multimodal models.

**Key insights**  
Developers should note that GLM-ASR requires exact placeholder matching and currently limits audio inputs to one per prompt in tests. The model relies on `trust_remote_code`, so environment compatibility must be verified. When extending support to other GLM-ASR versions, ensure convolution parameters and audio length defaults are configurable.

---

## 6. [[ROCm][CI] Update MiniCPM model test: MiniCPM3-4B to MiniCPM4.1-8B and simplify attention backend testing](https://github.com/vllm-project/vllm/pull/31551)


### Base Information

- **PR Number:** #31551
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-31 00:12:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31551/files) (2):**
  - `tests/models/language/generation/test_common.py`
  - `tests/models/registry.py`

### Summary

**What changed and why**  
This PR updates the ROCm CI's language model generation tests by replacing the outdated MiniCPM3-4B model with MiniCPM4.1-8B, which is compatible with recent `transformers` library versions. It also adds embedding scaling support for MiniCPM models to ensure correct behavior when using precomputed embeddings and removes explicit AITER kernel testing in favor of default backend dispatching.

**Technical impact**  
The model upgrade resolves a `DynamicCache` API incompatibility, preventing test failures. Embedding scaling fixes ensure parity between `vllm` and `vllm_from_embeds` outputs for MiniCPM models. Removing AITER-specific code simplifies the test suite and aligns testing with the default production data path, improving realism.

**Potential risks**  
The increased GPU memory requirement (32GB to 48GB) may limit test execution on hardware with less memory. Relying solely on default backend dispatching could reduce coverage for AITER-specific optimizations, though this is mitigated by testing the default Triton backend. The embedding scaling fix is model-specific and may need extension if other models exhibit similar behavior.

**Key insights**  
Always verify model compatibility with library versions to avoid runtime errors. When testing embeddings, ensure any internal scaling applied by the model is accounted for to maintain output consistency. Testing default code paths over explicit kernel forcing provides more representative coverage of real-world usage. Monitor GPU memory requirements when upgrading model sizes.

---

