# vLLM Merged PR Report

**Report Date:** 2025-12-26 PST

**Total Merged PRs:** 8

---

## 1. [Fix/get raw stream patch #30905](https://github.com/vllm-project/vllm/pull/30912)


### Base Information

- **PR Number:** #30912
- **Author:** [baonudesifeizhai](https://github.com/baonudesifeizhai)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-26 20:08:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30912/files) (3):**
  - `tests/compile/fullgraph/test_full_graph.py`
  - `tests/compile/test_config.py`
  - `vllm/env_override.py`

### Summary

**What changed and why**  
This PR adds a workaround for a TorchInductor autotune error that occurs when using `compile_sizes > 1` in the compilation config. The error (`RuntimeError: name 'get_raw_stream' is not defined`) happens because TorchInductor's autotune code generation references an undefined `get_raw_stream()` function. The fix patches this function into Python's `builtins` module by importing it from `torch._C._cuda_getCurrentRawStream`, but only for specific PyTorch versions (2.9.0 and 2.9.1) where the bug exists.

**Technical impact**  
The change conditionally modifies the global `builtins` module at import time (`vllm.env_override.py`), injecting the missing `get_raw_stream` function when needed. This allows TorchInductor's autotune to function correctly with multiple `compile_sizes`. The fix is version-specific, ensuring it doesn't affect newer PyTorch versions where the underlying issue may be resolved. Two new tests validate the patch's application and the autotune functionality.

**Potential risks**  
Patching `builtins` is a global modification that could have unintended side effects if other code relies on the `builtins` namespace. The patch only checks for CUDA availability via `hasattr(torch._C, "_cuda_getCurrentRawStream")`, which might fail in non-CUDA environments or with different PyTorch builds. The version check (`is_torch_equal`) is precise but could miss future patch releases (e.g., 2.9.2) if the bug persists.

**Key insights**  
This is a targeted, temporary workaround for a specific PyTorch version bug. Developers should be aware that the patch is applied globally and conditionally. The tests added are crucial for ensuring the patch works and doesn't interfere with other versions. Consider adding a comment about removing this workaround once support for the affected PyTorch versions is dropped or the upstream bug is fixed.

---

## 2. [[Core][Hybrid allocator + connector] Support hybrid allocator + kv cache connector](https://github.com/vllm-project/vllm/pull/30166)


### Base Information

- **PR Number:** #30166
- **Author:** [ivanium](https://github.com/ivanium)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2025-12-26 18:25:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30166/files) (8):**
  - `tests/v1/core/test_single_type_kv_cache_manager.py`
  - `vllm/v1/core/block_pool.py`
  - `vllm/v1/core/kv_cache_coordinator.py`
  - `vllm/v1/core/kv_cache_manager.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/core/single_type_kv_cache_manager.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/tpu_worker.py`

### Summary

**What changed and why**  
This PR modifies the hybrid KV cache manager to allocate KV cache only for tokens within the sliding window for sliding window layers, instead of allocating all tokens and later freeing those outside the window. This addresses memory pressure and data contention issues when using external KV cache connectors like LMCache, particularly in scenarios with long prefixes.

**Technical impact**  
The changes introduce a more precise allocation strategy that distinguishes between local computed tokens, external computed tokens (from connectors), and new tokens. The `allocate_slots` method now handles these separately, and the block calculation logic in `single_type_kv_cache_manager` has been updated to skip blocks outside the sliding window and allocate only for necessary tokens. This reduces unnecessary GPU memory usage and prevents conflicts during asynchronous KV cache transfers.

**Potential risks**  
- The logic for counting evictable blocks and skipped tokens is complex and could lead to incorrect block counts if edge cases (e.g., speculative decoding rejections) are not handled properly.  
- The changes assume that external connectors (currently only LMCache) are compatible; other connectors may require updates in follow-up PRs, risking temporary incompatibility.  
- The modified `touch` method in `block_pool.py` now accepts a single sequence instead of a tuple, which could break existing callers if not updated consistently.

**Key insights**  
- Developers should ensure that all KV cache connectors are updated to work with the new allocation logic, as currently only LMCache is supported.  
- The test suite has been expanded to cover evictable cached blocks, but additional integration tests with various connectors are recommended.  
- The performance benchmarks show negligible impact on latency and throughput, indicating the changes are efficient for typical workloads.

---

## 3. [[XPU][CI]skip test_preprocess_error_handling due to fork/spawn issue](https://github.com/vllm-project/vllm/pull/31381)


### Base Information

- **PR Number:** #31381
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2025-12-26 13:40:44
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31381/files) (1):**
  - `tests/v1/engine/test_preprocess_error_handling.py`

### Summary

**What changed and why**  
The PR modifies a test file to skip `test_preprocess_error_handling` on XPU devices, similar to an existing skip for ROCm. This is because the test relies on the 'fork' multiprocessing start method, which is not supported on XPU (and ROCm), where 'spawn' is used instead.

**Technical impact**  
This change prevents test failures on XPU CI runs by conditionally skipping the test when XPU is detected. It aligns XPU handling with the existing ROCm pattern, maintaining consistency in test behavior across unsupported platforms.

**Potential risks**  
The primary risk is that the underlying issue—lack of 'fork' support—remains unaddressed for XPU. If the test's functionality becomes critical for XPU in the future, a proper fix or alternative implementation will be needed rather than a skip.

**Key insights**  
This is a straightforward, low-risk change that ensures CI stability. Developers should note that XPU now follows the same test-skip pattern as ROCm for fork/spawn issues. Consider documenting this limitation in platform-specific notes if similar tests arise.

---

## 4. [CustomOp: Unify aiter impl into GroupedTopk](https://github.com/vllm-project/vllm/pull/31221)


### Base Information

- **PR Number:** #31221
- **Author:** [xinyu-intel](https://github.com/xinyu-intel)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2025-12-26 09:44:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31221/files) (2):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`

### Summary

**What changed and why**  
The changes unify the ROCm aiter implementation into the `GroupedTopk` class by adding a new `forward_hip` method and removing the conditional logic from the layer initialization. This follows up on a previous PR to consolidate implementation paths.

**Technical impact**  
The `GroupedTopk` class now handles ROCm-specific execution through `forward_hip`, which delegates to `rocm_aiter_grouped_topk` when enabled. This simplifies the main layer code by removing inline conditional checks and partial function construction, centralizing the dispatch logic within the `GroupedTopk` class.

**Potential risks**  
If the ROCm aiter ops are enabled but `rocm_aiter_grouped_topk` fails or returns unexpected outputs, the fallback to `forward_native` may not be triggered correctly. Additionally, the new `num_fused_shared_experts` parameter must be validated consistently across both native and ROCm paths to avoid assertion failures.

**Key insights**  
This refactor improves code maintainability by encapsulating platform-specific logic. Developers should ensure that the `forward_hip` method’s conditions and fallback behavior are thoroughly tested on ROCm hardware. The removal of `partial` usage reduces runtime overhead and clarifies the initialization flow.

---

## 5. [[Docs] Add profiler user docs for http request](https://github.com/vllm-project/vllm/pull/31370)


### Base Information

- **PR Number:** #31370
- **Author:** [lengrongfu](https://github.com/lengrongfu)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2025-12-26 07:48:15
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31370/files) (1):**
  - `docs/contributing/profiling.md`

### Summary

**What changed and why**  
Added documentation showing how to trigger the vLLM profiler via HTTP requests instead of just command-line interface. This addresses GitHub issue #31342 by providing users with instructions for profiling in HTTP-based serving scenarios.

**Technical impact**  
This documentation update expands the profiling capabilities documentation to cover HTTP API endpoints (`/start_profile` and `/stop_profile`), making profiling accessible for users who interact with vLLM through REST APIs rather than CLI tools.

**Potential risks**  
The documentation assumes the profiler endpoints are already implemented and stable. If these endpoints change or have different behavior in future versions, this documentation could become misleading. No validation of the actual curl command syntax or error handling is provided.

**Key insights**  
This is a straightforward documentation improvement that fills a gap in user guidance. Developers should ensure the actual implementation of `/start_profile` and `/stop_profile` endpoints matches this documentation. Consider adding notes about profiler overhead and recommended usage patterns for production environments.

---

## 6. [[Mistral common] Ensure all functions are imported from the top & only use public methods](https://github.com/vllm-project/vllm/pull/31138)


### Base Information

- **PR Number:** #31138
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-26 04:48:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31138/files) (5):**
  - `requirements/common.txt`
  - `requirements/nightly_torch_test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `vllm/tokenizers/mistral.py`

### Summary

**What changed and why**  
Updated mistral_common dependency from 1.8.5 to 1.8.8 across all requirement files and refactored the Mistral tokenizer implementation to use only public API methods. The changes ensure proper import organization (imports at module top) and eliminate reliance on private/internal methods.

**Technical impact**  
The tokenizer now uses stable public interfaces instead of private attributes like `_vocab`, `_control_tokens`, and `_all_special_tokens`. This improves maintainability and reduces breakage risk during library updates. The dependency bump suggests compatibility with new public APIs introduced in mistral_common 1.8.8.

**Potential risks**  
The removal of type assertions for tokenizer subclasses (Tekkenizer/SentencePieceTokenizer) could mask type errors. Changes to special token detection logic (`is_special()` method) may behave differently than previous implementation. The dependency version constraint may break existing installations if 1.8.8 introduces backward-incompatible changes.

**Key insights**  
Always prefer public APIs over private/internal methods for long-term stability. The simplified special token handling using `is_special()` is cleaner but requires thorough testing. Ensure all type checking remains robust despite removed assertions. Consider adding integration tests to verify tokenizer behavior across different mistral_common versions.

---

## 7. [[Core] Initialize LoRA support for tower and connector in multi-modal models](https://github.com/vllm-project/vllm/pull/26674)


### Base Information

- **PR Number:** #26674
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-26 04:48:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/26674/files) (20):**
  - `docs/features/lora.md`
  - `tests/lora/conftest.py`
  - `tests/lora/test_lora_manager.py`
  - `tests/lora/test_qwenvl.py`
  - `vllm/config/lora.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/lora/layers/__init__.py`
  - `vllm/lora/layers/base_linear.py`
  - `vllm/lora/layers/utils.py`
  - `vllm/lora/model_manager.py`
  - `vllm/lora/worker_manager.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/lora_model_runner_mixin.py`
  - `vllm/v1/worker/tpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces LoRA support for the tower (vision encoder) and connector components in multi-modal models like Qwen2/2.5/3 VL and Idefics3. Previously, a single global Punica wrapper was used, which couldn't handle the differing input structures between the language model, tower, and connector. The changes decouple Punica wrappers per component and add helper methods to calculate precise token counts for tower/connector inputs.

**Technical impact**  
The architecture now maintains separate Punica wrappers (mapped by component prefix) in `LoRAModelManager`, enabling independent LoRA operations for language, tower, and connector modules. New `get_num_mm_encoder_tokens` and `get_num_mm_connector_tokens` methods in model interfaces bridge token-count discrepancies. Multi-modal embedding caching is also adjusted to incorporate LoRA request identifiers when tower/connector LoRA is enabled.

**Potential risks**  
The feature is experimental and currently incompatible with the multi-modal processor cache (`mm_processor_cache_gb` must be 0). Incorrect token-count calculations in model-specific helper methods could lead to misaligned LoRA mappings or runtime errors. The changes assume consistent prefix mappings across multi-modal models, which may not hold for future integrations.

**Key insights**  
Developers must implement the two token-count helper methods in each multi-modal model class to enable tower/connector LoRA support. The `enable_tower_connector_lora` flag should be used cautiously due to the cache incompatibility. Testing shows successful integration across Qwen VL variants, but contributions are needed to extend support to other models.

---

## 8. [[Docs] Fix some snippets](https://github.com/vllm-project/vllm/pull/31378)


### Base Information

- **PR Number:** #31378
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-26 04:47:41
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31378/files) (15):**
  - `docs/cli/bench/latency.md`
  - `docs/cli/bench/serve.md`
  - `docs/cli/bench/sweep/plot.md`
  - `docs/cli/bench/sweep/plot_pareto.md`
  - `docs/cli/bench/sweep/serve.md`
  - `docs/cli/bench/sweep/serve_sla.md`
  - `docs/cli/bench/throughput.md`
  - `docs/cli/chat.md`
  - `docs/cli/complete.md`
  - `docs/cli/run-batch.md`
  - `docs/cli/serve.md`
  - `docs/configuration/engine_args.md`
  - `docs/mkdocs/hooks/generate_argparse.py`
  - `docs/mkdocs/hooks/generate_metrics.py`
  - `docs/usage/metrics.md`

### Summary

**What changed and why**  
This PR updates documentation include paths and generation scripts to correct file extensions and output directories. It changes references from `.md` to `.inc.md` for generated documentation snippets and updates the `generate_argparse.py` script to output files to the `docs/generated/argparse` directory, aligning with the existing include paths.

**Technical impact**  
The changes ensure documentation builds correctly by fixing broken include references. Updating the output directory in `generate_argparse.py` centralizes generated argparse documentation, improving consistency and maintainability. The file extension corrections prevent build failures due to missing files.

**Potential risks**  
If the `generate_argparse.py` script's output directory change isn't reflected in the build process or CI, generated files may not be placed correctly, causing documentation gaps. There's a minor risk of leftover `.md` files in the old `docs/argparse` directory causing confusion if not cleaned up.

**Key insights**  
Always verify that documentation generation scripts and their output locations are synchronized. Consider adding a cleanup step in the generation process to remove outdated files. These changes highlight the importance of consistent naming conventions for generated content to prevent build-time errors.

---

