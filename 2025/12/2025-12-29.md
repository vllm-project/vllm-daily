# vLLM Merged PR Report

**Report Date:** 2025-12-29 PST

**Total Merged PRs:** 24

---

## 1. [[Frontend] add continue_final_message parameter to /embeddings endpoint](https://github.com/vllm-project/vllm/pull/31497)


### Base Information

- **PR Number:** #31497
- **Author:** [kevin-pw](https://github.com/kevin-pw)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2025-12-29 23:21:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31497/files) (2):**
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/embed/serving.py`

### Summary

**What changed and why**  
This PR adds a `continue_final_message` parameter to the `/embeddings` endpoint's request protocol and integrates it into the preprocessing logic. The change enables users to generate embeddings with a prefilled assistant message, improving embedding quality for chat-based models by allowing the model to continue an open-ended final message rather than starting a new one.

**Technical impact**  
The modification extends the `EmbeddingChatRequest` schema to include the new parameter and propagates its value through the serving layer to the chat formatting logic. This aligns the embeddings endpoint with existing chat completion functionality, ensuring consistent behavior when generating embeddings from partially prefilled conversations.

**Potential risks**  
If `continue_final_message` and `add_generation_prompt` are used simultaneously, undefined behavior may occur since the description states they are mutually exclusive. Additionally, improper validation of the final message's role or content could lead to formatting errors or degraded embedding quality.

**Key insights**  
Developers should ensure that `continue_final_message` and `add_generation_prompt` are not both set to true in requests. The parameter is particularly valuable for retrieval tasks where prefilled assistant context improves semantic relevance. Consider adding explicit validation for conflicting parameters in future updates.

---

## 2. [[xpu] [bugfix] upgrade to latest oneccl in dockerfile](https://github.com/vllm-project/vllm/pull/31522)


### Base Information

- **PR Number:** #31522
- **Author:** [rogerxfeng8](https://github.com/rogerxfeng8)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2025-12-29 22:52:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31522/files) (1):**
  - `docker/Dockerfile.xpu`

### Summary

**What changed and why**  
Upgraded oneCCL from version 2021.15.6.9 to 2021.15.7.6 in the Dockerfile to include fixes for allreduce overflow issues. The changes also introduce a variable for the installer name, clean up the downloaded installer, and ensure the symbolic link `/opt/intel/oneapi/ccl/latest` points to the correct version.

**Technical impact**  
This update resolves a known bug in the collective communication library (oneCCL) that could cause overflow in allreduce operations, improving numerical stability and correctness for distributed training on XPU. The Docker image will now use a more stable and patched version of oneCCL, which may affect performance and reliability of multi-GPU/XPU communication.

**Potential risks**  
While the upgrade addresses a specific bug, it could introduce subtle behavioral changes in communication patterns or compatibility with other oneAPI components. The removal of the old installer and symlink update are safe but assume the new version's installation path (`2021.15`) remains consistent; if the directory structure changes in future releases, the symlink may break.

**Key insights**  
Always verify that the new oneCCL version maintains compatibility with the rest of the software stack, especially other oneAPI libraries. Consider adding a version check or integration test to ensure allreduce operations function correctly after the upgrade. The use of an `ARG` for the installer name is a good practice for maintainability.

---

## 3. [[Minor] Various small code cleanups/simplifications](https://github.com/vllm-project/vllm/pull/31508)


### Base Information

- **PR Number:** #31508
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-29 22:42:07
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31508/files) (14):**
  - `vllm/config/model.py`
  - `vllm/entrypoints/openai/serving_responses.py`
  - `vllm/entrypoints/renderer.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/output_processor.py`
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/request.py`
  - `vllm/v1/structured_output/__init__.py`
  - `vllm/v1/structured_output/request.py`
  - `vllm/v1/worker/gpu_input_batch.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR contains minor code cleanups and simplifications across 14 files, primarily focused on improving code clarity, reducing redundancy, and applying Python best practices. Changes include simplifying conditional logic, removing unused variables, using more efficient expressions, and improving type hints.

**Technical impact**  
The changes are mostly cosmetic and maintain the same functional behavior. They improve code readability and maintainability by eliminating unnecessary complexity, reducing line count by 26 lines overall. Some changes optimize small performance aspects (like using tuples instead of lists for iteration), but the core functionality remains unchanged.

**Potential risks**  
The logic reversal in `is_hybrid` property could be misinterpreted - while functionally equivalent, the new `return layer_types is None or not all(...)` requires careful reading. The removal of the `processer = None` assignment in async context might be intentional cleanup, but could potentially mask issues if the variable was meant to be used later. The change from `prompt_ids or []` to `prompt_ids or ()` uses an immutable tuple, which is safe but changes the object type.

**Key insights**  
These are quality-of-life improvements that follow Pythonic patterns. Developers should note the improved conditional expressions and removal of redundant code. The changes demonstrate good practices like using `itertools.chain` for efficient iteration and simplifying complex boolean logic. When reviewing similar cleanups, ensure logic equivalence is preserved, especially when inverting conditions.

---

## 4. [Add Loraconfig parameter to  get_punica_wrapper function](https://github.com/vllm-project/vllm/pull/31408)


### Base Information

- **PR Number:** #31408
- **Author:** [ZT-AIA](https://github.com/ZT-AIA)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-29 22:27:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31408/files) (3):**
  - `tests/lora/test_layers.py`
  - `vllm/lora/model_manager.py`
  - `vllm/lora/punica_wrapper/punica_gpu.py`

### Summary

**What changed and why**  
The PR adds a `lora_config` parameter to the `get_punica_wrapper` function to allow passing a full `LoRAConfig` object instead of just `max_loras`. This enables operators to access comprehensive LoRA configuration details beyond just the maximum number of LoRAs, supporting scenarios where different operators require different LoRA configurations. The change maintains backward compatibility by keeping `max_loras` extraction from `lora_config`.

**Technical impact**  
The modification centralizes LoRA configuration management by passing the complete `LoRAConfig` object through the initialization chain. This affects three key areas: test files now create `lora_config` before calling `get_punica_wrapper`, the model manager passes `lora_config` instead of `max_loras`, and the punica wrapper constructor now stores the full configuration object while maintaining the existing `max_loras` attribute for compatibility.

**Potential risks**  
The main risk is inconsistent usage patterns during transition—some callers might still pass `max_loras` while others use `lora_config`. The current implementation handles this via `**kwargs`, but future changes removing `max_loras` support could break existing code. Additionally, there's a risk of configuration mismatch if `lora_config` objects with different `max_loras` values are passed to different components.

**Key insights**  
This change represents a migration step toward a more structured configuration approach. Developers should consistently use `lora_config` parameter in new code and update existing calls when possible. The team should establish a clear deprecation timeline for the `max_loras` parameter and ensure all internal and external consumers are updated before its removal to prevent breaking changes.

---

## 5. [Migrate meetups & sponsors [2/N]](https://github.com/vllm-project/vllm/pull/31500)


### Base Information

- **PR Number:** #31500
- **Author:** [esmeetu](https://github.com/esmeetu)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2025-12-29 20:26:15
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31500/files) (3):**
  - `README.md`
  - `docs/community/meetups.md`
  - `docs/community/sponsors.md`

### Summary

**What changed and why**  
This PR removes detailed meetup and sponsor information from the README and documentation files, replacing it with links to the vLLM website (`vllm.ai/events` and `vllm.ai/#sponsors`). This is part 2 of a migration effort to centralize community content on the official website.

**Technical impact**  
The changes significantly reduce the size of the README.md (removing 90 lines) and documentation files by removing static lists that require manual updates. This shifts maintenance responsibility from the code repository to the external website, making the repository more focused on code.

**Potential risks**  
There's a risk of broken links if the website URLs change or become unreachable. Users who rely on the repository documentation offline will lose access to meetup and sponsor details. The migration also creates a dependency on external website availability for accessing community information.

**Key insights**  
This is a strategic move to separate dynamic community content from static code documentation. Developers should verify all new external links are correct and consider adding link health checks in CI/CD. Future documentation should follow this pattern of referencing external resources rather than duplicating content.

---

## 6. [feat(frontend): add --default-chat-template-kwargs CLI argument](https://github.com/vllm-project/vllm/pull/31343)


### Base Information

- **PR Number:** #31343
- **Author:** [effortprogrammer](https://github.com/effortprogrammer)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2025-12-29 19:38:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31343/files) (7):**
  - `docs/features/reasoning_outputs.md`
  - `tests/entrypoints/openai/test_cli_args.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/entrypoints/openai/run_batch.py`
  - `vllm/entrypoints/openai/serving_chat.py`
  - `vllm/entrypoints/openai/serving_engine.py`

### Summary

**What changed and why**  
This PR adds a `--default-chat-template-kwargs` CLI argument to vLLM's OpenAI-compatible server, allowing deployment-level configuration of chat template parameters. This addresses the need to control reasoning model behavior (like enabling/disabling thinking mode) at the server level rather than requiring every client request to specify these parameters. Request-level `chat_template_kwargs` still override these defaults when provided.

**Technical impact**  
The changes introduce a new configuration layer that sits between server defaults and request-specific parameters. The implementation follows a clean override hierarchy: request parameters > server defaults > model defaults. This affects the chat template rendering pipeline across the serving stack, from CLI argument parsing through to the preprocessing logic in `serving_engine.py`.

**Potential risks**  
The JSON parsing of the CLI argument could fail with malformed input, though this is mitigated by validation in the argument parser. There's a risk of parameter name conflicts if different models use different key names for similar functionality (e.g., `enable_thinking` vs `thinking`). The documentation shows examples for specific models, but users must ensure they use the correct parameter names for their chosen model.

**Key insights**  
This is a well-architected feature that provides important deployment flexibility for reasoning models. Developers should note that the server defaults apply to all requests unless explicitly overridden, making this suitable for environment-specific configurations. The comprehensive test coverage and documentation updates are excellent practices that should be maintained for similar features.

---

## 7. [[Core] Deduplicate generate/encode logic in `AsyncLLM`](https://github.com/vllm-project/vllm/pull/31510)


### Base Information

- **PR Number:** #31510
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2025-12-29 18:42:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31510/files) (1):**
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
The changes deduplicate common logic by moving validation, pause handling, and output handler initialization from the `generate()` and `encode()` methods into the shared `add_request()` method. This ensures consistency when `add_request()` is called directly and reduces code duplication.

**Technical impact**  
This refactoring centralizes request preprocessing logic, making the codebase more maintainable. The `add_request()` method now fully handles request validation, pause state management, and output handler initialization, which simplifies the higher-level `generate()` and `encode()` methods.

**Potential risks**  
The validation for `prompt_text` usage with `EngineCoreRequest` is stricter (now raising an error instead of an assertion), which could break existing code that incorrectly mixes these parameters. Additionally, moving pause state handling into `add_request()` means all request paths now respect the pause condition, which could affect performance or behavior if not anticipated.

**Key insights**  
Developers should ensure that `prompt_text` is only provided when using `EngineCoreRequest`. The centralized logic improves robustness for direct `add_request()` usage, but all callers must now be aware of the integrated pause and validation steps. This change aligns with better separation of concerns and reduces the risk of inconsistencies across request types.

---

## 8. [fix: update kimi k2 tool parser logic](https://github.com/vllm-project/vllm/pull/31207)


### Base Information

- **PR Number:** #31207
- **Author:** [wangln19](https://github.com/wangln19)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2025-12-29 18:01:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31207/files) (2):**
  - `tests/tool_parsers/test_kimi_k2_tool_parser.py`
  - `vllm/tool_parsers/kimi_k2_tool_parser.py`

### Summary

**What changed and why**  
The fix addresses streaming content leakage in the Kimi-K2 tool parser where tool call markers (e.g., `<\|tool_call_begin\|>`) were incorrectly included in the `content` field during streaming. This occurred in two scenarios: when exiting a tool section, buffered tool data was returned as content, and when regex matching failed, delta text was returned without checking if the parser was inside a tool section.

**Technical impact**  
The changes ensure that tool call markers and internal tool data are properly suppressed during streaming, preventing contamination of the reasoning content returned to users. This maintains clean separation between tool call metadata and user-facing content, which is critical for integrations like AWS Bedrock that rely on clean output.

**Potential risks**  
If the parser incorrectly identifies tool sections (e.g., due to malformed or partial markers), legitimate content could be suppressed. Edge cases where tool sections are nested or markers split across multiple chunks may still pose challenges. Additionally, the fix assumes tool sections are well-formed; malformed streams might lead to unexpected behavior.

**Key insights**  
Developers should verify that the parser correctly handles all marker variants and split-marker scenarios. The added tests provide good coverage, but integration testing with real streaming data is recommended. Ensure that any future changes to tool parsing logic maintain the strict separation between tool metadata and user content.

---

## 9. [[Prefix Cache] Include lora_name in BlockStored event for deterministic KV-cache reconstruction](https://github.com/vllm-project/vllm/pull/27577)


### Base Information

- **PR Number:** #27577
- **Author:** [sagearc](https://github.com/sagearc)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2025-12-29 16:17:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/27577/files) (8):**
  - `examples/online_serving/kv_events_subscriber.py`
  - `tests/v1/engine/test_engine_core_client.py`
  - `tests/v1/kv_connector/unit/test_lmcache_connector.py`
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `vllm/distributed/kv_events.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/v1/core/block_pool.py`

### Summary

**What changed and why**  
The PR adds a new `lora_name` field to the `BlockStored` event class across multiple files, replacing the previous reliance on `lora_id` for KV-cache reconstruction. This change aligns with a deterministic hashing scheme (introduced in #27211) to ensure consistent block hashes across runs, enabling reliable cache lookups, routing, and sharing.

**Technical impact**  
The addition of `lora_name` enhances the determinism of KV-cache key generation by using a stable identifier (name) instead of a potentially variable numeric ID. This affects event serialization, caching logic, and downstream consumers (e.g., connectors, subscribers) that now receive both `lora_id` (deprecated) and `lora_name` in events.

**Potential risks**  
- Backward compatibility is maintained via retained `lora_id`, but new fields may cause issues if downstream systems expect only the old schema.  
- Inconsistent `lora_name` values (e.g., null/empty strings) could lead to hash collisions or misrouting if not validated.  
- The change is additive, but any missed updates in event-handling code could result in uninitialized `lora_name` fields.

**Key insights**  
- The PR prioritizes deterministic caching over numeric IDs, which is critical for distributed caching scenarios.  
- Developers should update all event consumers to use `lora_name` for new implementations while phasing out `lora_id`.  
- Ensure thorough testing of cache reconstruction with varied LoRA configurations to validate deterministic behavior.

---

## 10. [[CI]Test Group 'NixlConnector PD accuracy tests' is fixed](https://github.com/vllm-project/vllm/pull/31460)


### Base Information

- **PR Number:** #31460
- **Author:** [qli88](https://github.com/qli88)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-29 15:48:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31460/files) (5):**
  - `.buildkite/test-amd.yaml`
  - `docker/Dockerfile.rocm_base`
  - `docs/features/nixl_connector_usage.md`
  - `requirements/kv_connectors_rocm.txt`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR fixes the 'NixlConnector PD accuracy tests' CI group for ROCm platforms by replacing NIXL with RIXL, the ROCm-compatible equivalent. The changes add RIXL and UCX dependencies to the ROCm Dockerfile, create a separate requirements file for ROCm, update documentation, and modify the connector code to conditionally import RIXL on ROCm systems.

**Technical impact**  
The Dockerfile now includes a multi-stage build for RIXL, UCX, and etcd-cpp-apiv3, ensuring ROCm platforms have the necessary libraries. The nixl_connector.py dynamically imports RIXL instead of NIXL when on ROCm, maintaining backward compatibility for NVIDIA platforms. CI tests are updated to use the ROCm-specific requirements and attention backend.

**Potential risks**  
Using a forked RIXL repository (`vcave/RIXL`) instead of the upstream ROCm version introduces maintenance risk if the fork diverges or becomes outdated. The added complexity in the Dockerfile increases build time and potential for build failures. Conditional imports could lead to runtime errors if platform detection fails or if RIXL/NIXL APIs differ subtly.

**Key insights**  
Developers should monitor the TODO comment about switching to the upstream RIXL repo once patches are merged. The separation of requirements files (kv_connectors.txt vs. kv_connectors_rocm.txt) is a good practice for platform-specific dependencies. Ensure all future ROCm CI tests set `VLLM_ATTENTION_BACKEND=ROCM_ATTN` appropriately.

---

## 11. [[CI/ROCm] Fixing "V1 Test attention (H100)" test group.](https://github.com/vllm-project/vllm/pull/31187)


### Base Information

- **PR Number:** #31187
- **Author:** [Alexei-V-Ivanov-AMD](https://github.com/Alexei-V-Ivanov-AMD)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-29 13:53:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31187/files) (3):**
  - `tests/v1/attention/test_attention_backends.py`
  - `tests/v1/attention/test_rocm_attention_backends_selection.py`
  - `tests/v1/attention/test_sparse_mla_backends.py`

### Summary

**What changed and why**  
The changes fix CI test failures for ROCm platform by excluding unsupported backends and configurations. Specifically, Flash Attention backend is excluded from ROCm tests, and ROCm-specific attention backend selection logic is updated to use a new `AttentionSelectorConfig` parameter. Additionally, sparse MLA tests with fp8 data type are skipped on ROCm.

**Technical impact**  
These changes modify test behavior based on platform detection, creating divergent test paths for ROCm vs other platforms. The introduction of `AttentionSelectorConfig` centralizes attention configuration parameters, improving parameter passing consistency. Test coverage for ROCm is maintained while avoiding unsupported features.

**Potential risks**  
Platform-specific logic increases test maintenance overhead and could lead to divergence between platform implementations. The duplicate `AttentionSelectorConfig` instantiation in `test_mla_backend_selection` appears to be a copy-paste error that creates dead code. Skipping tests rather than implementing proper support may hide underlying compatibility issues.

**Key insights**  
The changes demonstrate proper platform-aware test design but reveal architectural gaps in ROCm feature support. Developers should verify the duplicate configuration code is corrected and consider whether skipped tests indicate missing ROCm implementations that need addressing. The `AttentionSelectorConfig` pattern improves parameter management and should be adopted consistently.

---

## 12. [[Bugfix][ROCm] Fix Static Quant Issue](https://github.com/vllm-project/vllm/pull/31502)


### Base Information

- **PR Number:** #31502
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-29 13:27:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31502/files) (2):**
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`

### Summary

**What changed and why**  
This PR fixes a regression in ROCm's FP8 static quantization for Mixture of Experts (MoE) layers. A recent refactoring incorrectly assigned tensor values to parameters instead of using `replace_parameter`, breaking static quantization. The changes correct this by properly using `replace_parameter` and updating assertions to match the quant config's expected scales.

**Technical impact**  
The fix ensures that static quantization scales are correctly handled during weight loading, particularly for per-tensor kernels that require single activation scales. It also aligns the ROCm fused MoE layer's interface with the quantization configuration, maintaining consistency across platforms.

**Potential risks**  
The assertion change from `a2_scale is None` to `a2_scale == self.quant_config.a2_scale` assumes the quant config's scale is always available, which could fail if the config is improperly initialized. Additionally, the TODO comment indicates the ROCm implementation still uses a temporary workaround for static quantization, suggesting future refactoring may be needed.

**Key insights**  
Developers should ensure quantization scales are managed via `replace_parameter` to maintain parameter tracking. The ROCm fused MoE layer currently relies on `self.quant_config.a_scales`, which may need updates as quantization integrations evolve. Testing with static quant models like Mixtral FP8 is critical for validation.

---

## 13. [[MoE Refactor][12/N] Marlin Fp8 MoE Pure Function](https://github.com/vllm-project/vllm/pull/31499)


### Base Information

- **PR Number:** #31499
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-29 13:27:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31499/files) (4):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py`

### Summary

**What changed and why**  
The PR refactors `prepare_moe_fp8_layer_for_marlin` from a function that modifies layer attributes in-place to a pure function that returns transformed tensors. This change ensures consistency with other quantization methods and enables compatibility with `replace_parameter` semantics needed for quantized weight reloading. Additionally, it removes the unused `size_k_first` option and `w13_bias` permutation logic.

**Technical impact**  
The refactor changes how Marlin FP8 MoE layers are prepared across three quantization implementations (compressed_tensors, fp8.py, and quark). Instead of directly modifying layer attributes, the function now returns processed tensors that callers must assign back to the layer. This enables proper parameter replacement via `replace_parameter` in most cases, though quark still uses direct assignment with a TODO comment.

**Potential risks**  
The quark implementation doesn't use `replace_parameter` yet, creating inconsistency and potential issues with weight reloading in RL scenarios. Removing the `size_k_first` option and bias permutation could break backward compatibility if future quantization methods require these features. The error type change from `RuntimeError` to `NotImplementedError` for unsupported W8A8 might affect error handling.

**Key insights**  
Developers should ensure all quantization backends consistently use `replace_parameter` for proper weight reloading support. The removed functionality (`size_k_first`, bias permutation) should be documented as deprecated, with clear guidance on how to re-add it if needed. The pure function approach improves testability and maintainability but requires careful integration across all call sites.

---

## 14. [[Core] Enable async scheduling by default](https://github.com/vllm-project/vllm/pull/27614)


### Base Information

- **PR Number:** #27614
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2025-12-29 12:20:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/27614/files) (2):**
  - `vllm/config/scheduler.py`
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The changes enable asynchronous scheduling by default in vLLM. The `async_scheduling` configuration parameter now defaults to `None`, which triggers automatic detection and enabling unless incompatible features are present (e.g., pipeline parallelism or unsupported speculative decoding methods). This aims to improve GPU utilization, latency, and throughput for most use cases.

**Technical impact**  
This modifies the scheduler's default behavior across the codebase. When `async_scheduling` is `None`, the system now automatically enables it unless specific incompatibilities are detected (pipeline parallelism, non-EAGLE speculative decoding, or unsupported executors). The validation logic has been refined to provide more granular warnings and automatically disable NCCL for DP synchronization when async scheduling is active.

**Potential risks**  
Users relying on pipeline parallelism or non-EAGLE speculative decoding may experience unexpected behavior if they were previously using async scheduling explicitly. The automatic disabling of NCCL for DP synchronization could affect performance or correctness in distributed setups. Edge cases where async scheduling is implicitly disabled may lead to inconsistent performance if users are unaware of the automatic fallback.

**Key insights**  
Developers should be aware that async scheduling is now the default; explicit configuration may be needed to disable it for incompatible features. The improved warning messages help diagnose why async scheduling is disabled. The `logger.info_once` call ensures users are informed of the final scheduling state without log spam.

---

## 15. [implements register kv caches in lmcache connector](https://github.com/vllm-project/vllm/pull/31397)


### Base Information

- **PR Number:** #31397
- **Author:** [chunxiaozheng](https://github.com/chunxiaozheng)
- **Merged By:** [ApostaC](https://github.com/ApostaC)
- **Merged time:** 2025-12-29 11:13:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31397/files) (2):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py`

### Summary

**What changed and why**  
A new `register_kv_caches` method was added to the LMCache connector and its adapter to allow pre-registration of KV caches. This enables initialization of KV caches in the connector before inference, which is particularly useful for systems like NIXL that require pre-registration.

**Technical impact**  
The changes extend the KV connector interface with optional KV cache registration, maintaining backward compatibility through a feature check. The adapter stores the caches and passes them to the underlying LMCache engine via `post_init`, ensuring the engine is properly initialized with pre-existing KV data.

**Potential risks**  
If the LMCache engine lacks the `register_kv_caches` method, only a warning is logged, which might lead to silent failures in pre-registration scenarios. The adapter's assertion (`len(self.kv_caches) == 0`) could fail if `register_kv_caches` is called multiple times unexpectedly.

**Key insights**  
Developers should ensure the LMCache engine supports this feature to avoid runtime warnings. The TODO note suggests `_init_kv_caches_from_forward_context` may become obsolete, so future refactoring should consolidate KV cache initialization paths.

---

## 16. [Optimize QKNorm for MiniMax-M2/M2.1](https://github.com/vllm-project/vllm/pull/31493)


### Base Information

- **PR Number:** #31493
- **Author:** [rogeryoungh](https://github.com/rogeryoungh)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2025-12-29 08:30:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31493/files) (2):**
  - `vllm/model_executor/layers/mamba/linear_attn.py`
  - `vllm/model_executor/models/minimax_m2.py`

### Summary

**What changed and why**  
The PR introduces a fused QKNorm operation for tensor-parallel mode in the MiniMax-M2/M2.1 model. It adds a static `forward_qk` method to `MiniMaxText01RMSNormTP` that processes query (q) and key (k) tensors together, consolidating their variance calculations into a single `all_reduce` communication call. This replaces separate normalization calls for q and k, reducing communication overhead in distributed training/inference.

**Technical impact**  
This optimization reduces the number of `all_reduce` operations from two to one when computing RMSNorm for query and key tensors in tensor-parallel configurations. The implementation computes variances in float32 precision for numerical stability, concatenates them for a combined all_reduce, then applies normalization with separate scaling weights. This improves communication efficiency without altering the mathematical correctness of the normalization.

**Potential risks**  
The implementation requires contiguous tensor inputs (enforced via `.contiguous()` calls), which may introduce minor overhead if tensors aren't already contiguous. The float32 conversion for variance calculation adds precision handling but could affect performance on hardware optimized for lower precision. There's also a risk if the tensor-parallel world size isn't properly synchronized between q_norm and k_norm instances.

**Key insights**  
The benchmark shows meaningful throughput improvements (64.450 → 80.840 at concurrency=1) with maintained accuracy, confirming the optimization's effectiveness. Developers should ensure this fused approach is only used when both q and k norms share the same tensor-parallel configuration. Consider extending this pattern to other models with similar normalization patterns in attention layers.

---

## 17. [[Feature] Add offline FastAPI documentation support for air-gapped environments](https://github.com/vllm-project/vllm/pull/30184)


### Base Information

- **PR Number:** #30184
- **Author:** [rickychen-infinirc](https://github.com/rickychen-infinirc)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2025-12-29 08:22:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30184/files) (8):**
  - `pyproject.toml`
  - `setup.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/entrypoints/serve/__init__.py`
  - `vllm/entrypoints/serve/instrumentator/offline_docs.py`
  - `vllm/entrypoints/serve/instrumentator/static/swagger-ui-bundle.js`
  - `vllm/entrypoints/serve/instrumentator/static/swagger-ui.css`

### Summary

**What changed and why**  
This PR adds support for offline FastAPI documentation (`/docs`) in air-gapped environments by integrating the `fastapi-offline` package. It introduces a new CLI flag `--enable-offline-docs` to bundle static assets locally, ensuring the documentation route remains accessible without internet connectivity.

**Technical impact**  
The changes modify the FastAPI app initialization to conditionally disable default docs when offline mode is enabled, and add a new router that mounts vendored static files and serves a custom Swagger UI. This maintains backward compatibility by falling back to standard FastAPI if the static assets are missing, with appropriate warnings.

**Potential risks**  
If the static directory is missing or incorrectly packaged, the offline docs will silently fail (with a warning but no fallback to online docs). The conditional logic in `api_server.py` could become complex if more documentation-related flags are added. There's also a risk of version mismatch between vendored Swagger UI assets and FastAPI's expected API.

**Key insights**  
Ensure the static assets are correctly included in the package distribution via `setup.py`. Consider adding a runtime check that validates the presence of required static files. The implementation cleanly separates concerns by using a dedicated router, but the flag handling in `api_server.py` should be reviewed for clarity as the number of documentation options grows.

---

## 18. [Replace `nn.ConvNd` with vLLM's `ConvNdLayer` for Transformers modeling backend](https://github.com/vllm-project/vllm/pull/31498)


### Base Information

- **PR Number:** #31498
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2025-12-29 08:20:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31498/files) (2):**
  - `vllm/model_executor/models/transformers/base.py`
  - `vllm/model_executor/models/transformers/utils.py`

### Summary

**What changed and why**  
This PR extends the existing pattern of replacing PyTorch modules with vLLM's custom layers to include convolutional layers (`nn.Conv2d` and `nn.Conv3d`). It adds a new `replace_conv_class` utility function and integrates it into the model loading process for the Transformers backend, enabling potential performance improvements on specialized hardware.

**Technical impact**  
The changes allow models loaded via the Transformers backend to automatically substitute standard PyTorch convolutional layers with vLLM's optimized `Conv2dLayer` and `Conv3dLayer`. This aligns with previous replacements for linear and RMSNorm layers, creating a more consistent and potentially faster modeling backend.

**Potential risks**  
The replacement only occurs when `padding_mode` is `"zeros"`; other padding modes will retain the original PyTorch conv layer, which could lead to inconsistent behavior. Additionally, the new conv layers must exactly replicate the functionality of their PyTorch counterparts to avoid subtle numerical differences or training/inference discrepancies.

**Key insights**  
Developers should verify that all target models use zero-padding conv layers to benefit from this optimization. The change is backward-compatible due to the fallback mechanism, but performance gains will be hardware-dependent. Future work may need to extend support to other padding modes or conv variants.

---

## 19. [Migrate doc to website: Hardware Plugins (1/N)](https://github.com/vllm-project/vllm/pull/31496)


### Base Information

- **PR Number:** #31496
- **Author:** [esmeetu](https://github.com/esmeetu)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2025-12-29 07:55:20
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31496/files) (1):**
  - `docs/getting_started/installation/README.md`

### Summary

**What changed and why**  
The PR removes the detailed hardware plugin table from the repository's installation documentation and replaces it with a reference to the vllm.ai website. This change aims to centralize third-party hardware information on the website, keeping the repository focused on core development and simplifying maintenance.

**Technical impact**  
This reduces documentation duplication and shifts the maintenance burden for hardware plugin listings from the code repository to the external website. The repository's documentation now serves as a lightweight pointer rather than containing detailed, versioned hardware information.

**Potential risks**  
Users without internet access may be unable to view hardware plugin details. There's also a risk of the website becoming outdated if the update process isn't streamlined, potentially leading to user confusion or installation errors.

**Key insights**  
Centralizing documentation on the website improves maintainability but introduces a dependency on external resources. Developers should ensure the website is regularly updated and consider adding a local fallback or cached version for offline scenarios. The new contact channels (Slack/Email) for adding hardware should be clearly monitored to avoid bottlenecks.

---

## 20. [[Docs] Use relative `md` links instead of absolute `html` links for cross referencing](https://github.com/vllm-project/vllm/pull/31494)


### Base Information

- **PR Number:** #31494
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2025-12-29 05:33:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31494/files) (5):**
  - `docs/deployment/docker.md`
  - `docs/design/debug_vllm_compile.md`
  - `docs/features/custom_logitsprocs.md`
  - `docs/getting_started/installation/cpu.md`
  - `docs/mkdocs/hooks/url_schemes.py`

### Summary

**What changed and why**  
Updated documentation links from absolute HTML URLs to relative Markdown paths to prevent stale links and ensure cross-references always point to the correct version of vLLM documentation. Modified a Python hook to properly handle fragment identifiers (#anchors) in relative links.

**Technical impact**  
Changes improve documentation maintainability by making internal references version-agnostic and reducing dependency on external URL structures. The hook modification ensures relative links with anchors (like `file.md#section`) are correctly transformed to GitHub URLs when pointing outside the docs directory.

**Potential risks**  
The hook regex change could potentially break if paths contain spaces or unusual characters. Relative paths might become invalid if files are reorganized without updating all references. The broken link in `custom_logitsprocs.md` (changed to `[vllm.logits_process]`) appears to reference an undefined link target.

**Key insights**  
Always use relative paths for internal documentation links to avoid versioning issues. Verify that all link references are properly defined when converting absolute URLs. Consider adding link validation to CI/CD pipelines to catch broken references early. The hook improvement correctly preserves fragment identifiers, which is essential for deep linking.

---

## 21. [[Bugfix] Preserve tool call id/type/name in streaming finish chunk](https://github.com/vllm-project/vllm/pull/31438)


### Base Information

- **PR Number:** #31438
- **Author:** [amittell](https://github.com/amittell)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2025-12-29 05:10:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31438/files) (2):**
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `vllm/entrypoints/openai/serving_chat.py`

### Summary

**What changed and why**  
The fix addresses a bug where streaming tool call responses lost the `id`, `type`, and `function.name` fields in the final delta chunk. Previously, the finish chunk code created a new `DeltaMessage` with only `index` and `arguments`, discarding critical metadata. The solution introduces a helper method `_create_remaining_args_delta` that preserves these fields from the original delta message.

**Technical impact**  
This change ensures that streaming tool call responses maintain full compliance with the OpenAI API specification, where `id`, `type`, and `function.name` are required for proper client-side tool call handling. The helper method centralizes the logic for constructing finish chunks, improving code maintainability and consistency across streaming scenarios.

**Potential risks**  
If the original delta message contains multiple tool calls, the helper uses `index` to match the correct one, which assumes indices are unique and correctly aligned. Edge cases where no matching tool call is found (e.g., due to index mismatch) are handled gracefully by setting fields to `None`, but this could lead to incomplete responses if not anticipated by clients.

**Key insights**  
The addition of comprehensive unit tests validates the fix across various scenarios, including missing fields and multiple tool calls. Developers should ensure that any future modifications to tool call streaming preserve these metadata fields, and consider adding validation for index consistency in multi-tool call scenarios.

---

## 22. [[CI] fix test_chat_truncation_content_not_null test](https://github.com/vllm-project/vllm/pull/31488)


### Base Information

- **PR Number:** #31488
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-29 04:47:08
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31488/files) (1):**
  - `tests/entrypoints/openai/test_response_api_with_harmony.py`

### Summary

**What changed and why**  
The test was failing because generated reasoning content exceeded the original `max_tokens=250` limit. The fix increases `max_tokens` to 350 and modifies the prompt to explicitly request a longer response, ensuring the test can complete without truncation.

**Technical impact**  
This change only affects a single test case's parameters and prompt, ensuring it passes by providing sufficient token budget for the expected output. It does not impact production code, system behavior, or other tests.

**Potential risks**  
If the model's response generation becomes even longer in the future, this test could fail again for the same reason. The hardcoded token limit may need further adjustment if response patterns change.

**Key insights**  
When testing LLM outputs with length constraints, ensure `max_tokens` is sufficiently large to accommodate expected responses. Consider making tests more robust by checking response structure rather than relying on specific token limits, or using dynamic token calculation based on prompt characteristics.

---

## 23. [[Bugfix][Frontend] Fix Jina reranker multimodal input compatibility](https://github.com/vllm-project/vllm/pull/31445)


### Base Information

- **PR Number:** #31445
- **Author:** [twjww](https://github.com/twjww)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-29 01:13:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31445/files) (2):**
  - `tests/models/multimodal/pooling/test_jinavl_reranker.py`
  - `vllm/entrypoints/score_utils.py`

### Summary

**What changed and why**  
This PR fixes Jina reranker multimodal input compatibility by expanding the supported content types in `ScoreContentPartParam` to include text inputs alongside images. Previously, vLLM only accepted `image_url` for the Jina reranker, but now it properly handles mixed text+image inputs as per Jina's official multimodal API.

**Technical impact**  
The changes enable the Jina reranker model to process queries and documents containing any combination of text and images, aligning vLLM's implementation with the upstream model's capabilities. The test suite has been significantly refactored to validate various multimodal scenarios (text+image, text+text, image+text, image+image, and mixed documents).

**Potential risks**  
The refactored test file introduces complex logic for handling different content types, which could increase maintenance overhead. There's also a risk of regression if the normalization logic for image URLs/base64 encoding doesn't perfectly match the original model's expectations.

**Key insights**  
Developers should note that the `ScoreContentPartParam` type alias now explicitly includes text parts, making the API more flexible. The test improvements provide comprehensive coverage but may need updates if Jina's API evolves. Ensure any new multimodal models follow similar patterns for consistency.

---

## 24. [[ROCm][GPTQ][Bugfix] Fix GPTQ GEMM kernel output zeroing race condition](https://github.com/vllm-project/vllm/pull/30719)


### Base Information

- **PR Number:** #30719
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-29 01:13:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30719/files) (1):**
  - `csrc/quantization/gptq/q_gemm.cu`

### Summary

**What changed and why**  
The fix removes in-kernel zeroing logic from GPTQ GEMM kernels and pre-zeros the output tensor using `torch::zeros()` instead of `torch::empty()`. This addresses a race condition where multiple thread blocks along the k-dimension performed atomic additions while only the first block (`blockIdx.z == 0`) attempted to zero outputs, causing overwrites due to lack of cross-block synchronization.

**Technical impact**  
Eliminates the race condition by ensuring the output tensor is fully zeroed before kernel execution, guaranteeing correct accumulation via `atomicAdd` across all thread blocks. This restores numerical accuracy for cases where `input_size > 128` (triggering multiple k-blocks) and simplifies kernel logic by removing redundant zeroing code.

**Potential risks**  
Pre-zeroing adds a global memory write pass, which may slightly increase kernel launch overhead, though this is negligible compared to the GEMM computation. Developers must ensure no other kernels or code paths rely on the previous behavior where `torch::empty()` was used, as uninitialized memory could have been implicitly expected elsewhere.

**Key insights**  
Always use explicit synchronization or pre-initialization when multiple thread blocks perform atomic operations on shared output memory. The fix highlights the limitation of `__syncthreads()`—it only synchronizes within a block, not across blocks. For similar patterns, consider moving initialization outside the kernel or using CUDA’s grid-wide synchronization primitives if available.

---

