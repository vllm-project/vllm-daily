# vLLM Merged PR Report

**Report Date:** 2025-12-30 PST

**Total Merged PRs:** 14

---

## 1. [[Bug] Fix log issue with `\n`](https://github.com/vllm-project/vllm/pull/31390)


### Base Information

- **PR Number:** #31390
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-30 21:16:55
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31390/files) (1):**
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`

### Summary

**What changed and why**  
The change fixes a formatting issue in a warning log message where a line break (`\n`) was unintentionally introduced due to Python's implicit string concatenation across lines. The original code used a backslash for line continuation, which created unwanted whitespace in the output. The updated version properly concatenates strings without introducing extra spaces or line breaks.

**Technical impact**  
This is a minor cosmetic fix that improves log readability by ensuring the warning message appears as a single, clean line. It does not affect the kernel's functionality, performance, or any core logic—only the presentation of the log output.

**Potential risks**  
There are no functional risks, as the change is purely related to string formatting. However, developers should be cautious when using backslashes for line continuation in f-strings or log messages, as unintended whitespace can propagate to output.

**Key insights**  
Always use explicit string concatenation or parentheses for multi-line strings in logs to avoid hidden formatting issues. Consider adopting a linter or code formatter to catch similar whitespace problems in log statements across the codebase.

---

## 2. [Add get_expert_mapping to NemotronHModel (for LoRA support)](https://github.com/vllm-project/vllm/pull/31539)


### Base Information

- **PR Number:** #31539
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-30 21:09:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31539/files) (1):**
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
The PR extracts the expert parameter mapping logic from `load_weights` into a new method `get_expert_mapping`. This change is required to enable basic LoRA support by exposing the mapping of MoE expert parameters, which LoRA needs to identify target layers for adaptation.

**Technical impact**  
This refactor separates concerns: `load_weights` remains focused on loading model weights, while `get_expert_mapping` provides a reusable interface for external systems (like LoRA) to query expert mappings. The core behavior of the Nemotron-H model remains unchanged, as verified by testing with the NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model.

**Potential risks**  
If `get_expert_mapping` is called before MoE-related attributes (like `num_redundant_experts`) are initialized, it could raise `AttributeError`. Additionally, the method currently returns an empty list for non-MoE models, which may need to be documented to avoid confusion for LoRA integration.

**Key insights**  
This is a preparatory step for LoRA support; developers should ensure that `get_expert_mapping` is only called after full model initialization. The clear separation of mapping logic will simplify future extensions, such as adding LoRA to all linear layers in subsequent PRs.

---

## 3. [[CI] [Critical] [CUDA] Fix duplicated test name](https://github.com/vllm-project/vllm/pull/31562)


### Base Information

- **PR Number:** #31562
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-30 21:01:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31562/files) (1):**
  - `.buildkite/test-pipeline.yaml`

### Summary

**What changed and why**  
The PR fixes a duplicated test name in the CI pipeline configuration by updating a label from "NixlConnector PD accuracy tests (Distributed)" to "DP EP NixlConnector PD accuracy tests (Distributed)". This resolves a naming conflict that likely caused issues in CI reporting or job identification.

**Technical impact**  
This change ensures each CI step has a unique label, preventing potential conflicts in Buildkite's test pipeline visualization, logging, or job tracking. It maintains the same test functionality while improving clarity and avoiding duplicate identifiers.

**Potential risks**  
The risk is minimal as only a label string was modified. However, if any external systems or scripts rely on the exact previous label name for parsing or triggering actions, they may need updates to match the new label.

**Key insights**  
Always use distinct, descriptive labels for CI steps to avoid ambiguity in test reporting. When renaming CI job labels, verify that no downstream processes depend on the old naming convention. This fix highlights the importance of meticulous naming in pipeline configurations.

---

## 4. [[Core] Remove unused `num_tokens` parameter from `_init_model_kwargs`](https://github.com/vllm-project/vllm/pull/31517)


### Base Information

- **PR Number:** #31517
- **Author:** [maang-h](https://github.com/maang-h)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-30 20:47:23
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31517/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The `num_tokens` parameter was removed from the `GPUModelRunner._init_model_kwargs()` method signature and all its call sites. This parameter was passed to the method but never used within its implementation, constituting dead code.

**Technical impact**  
This is a pure refactoring change that simplifies the method's interface by removing an unused parameter. It reduces cognitive load for developers and eliminates a potential source of confusion, as callers no longer need to provide a value that has no effect. The system's behavior remains identical.

**Potential risks**  
The risk is minimal since the parameter was never utilized. However, there is a theoretical risk if the parameter was intended for future use or if external systems rely on the method's signature via reflection or dynamic calls. The change also touches multiple call sites, so a missed update could cause runtime errors.

**Key insights**  
This cleanup improves code maintainability and follows good software hygiene by removing dead code. Developers should verify that no downstream code (e.g., subclasses or external integrations) overrides or depends on the old signature. Future changes to this method should ensure any new parameters are actually used.

---

## 5. [[BugFix] Fix NUMA node validation in CPU platform](https://github.com/vllm-project/vllm/pull/31520)


### Base Information

- **PR Number:** #31520
- **Author:** [SameerAsal](https://github.com/SameerAsal)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2025-12-30 20:06:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31520/files) (1):**
  - `vllm/platforms/cpu.py`

### Summary

**What changed and why**  
Fixed a bug in NUMA node validation where NUMA node IDs from the `CPU_VISIBLE_MEMORY_NODES` environment variable were incorrectly compared against a list of allowed CPU core IDs. This caused invalid NUMA nodes to pass validation, leading to CPU binding failures with out-of-range errors. The fix ensures NUMA node IDs are validated against the correct list of available NUMA nodes.

**Technical impact**  
This correction ensures the CPU platform correctly interprets and applies NUMA affinity constraints when the `CPU_VISIBLE_MEMORY_NODES` environment variable is set. It restores proper thread-to-core binding on NUMA architectures, which is critical for performance and stability in data-parallel CPU execution.

**Potential risks**  
The change introduces a call to `sorted(list(set(visible_nodes)))`, which deduplicates and sorts the input. While this improves robustness, it could silently change the order of nodes specified by the user if order was semantically important (though this is unlikely). There is also a minor risk if `allowed_numa_nodes` is not properly initialized or defined elsewhere in the function's scope.

**Key insights**  
The root cause was a conceptual confusion between CPU IDs and NUMA node IDs—a critical distinction in systems programming. Developers should carefully review code that interfaces with hardware abstraction layers (like `libnuma`) to ensure correct semantics. The fix is minimal and targeted, but the surrounding code should be examined for similar confusions.

---

## 6. [[Mics] add pcp basic support to MoE model](https://github.com/vllm-project/vllm/pull/31003)


### Base Information

- **PR Number:** #31003
- **Author:** [pisceskkk](https://github.com/pisceskkk)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2025-12-30 20:01:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31003/files) (1):**
  - `vllm/model_executor/layers/fused_moe/config.py`

### Summary

**What changed and why**  
Two properties were added to expose PCP (presumably "Pipeline Context Parallelism") configuration from the underlying `moe_parallel_config`. This change completes missing PCP support that was omitted in a previous PR (#28718), ensuring the MoE layer can access PCP size and rank information.

**Technical impact**  
The additions enable the MoE layer to integrate with PCP parallelism strategies, aligning it with other parallelism dimensions (TP, DP, EP). This ensures consistent configuration access patterns across all parallelism types within the fused MoE module.

**Potential risks**  
If the underlying `moe_parallel_config` does not properly initialize or define `pcp_size` and `pcp_rank`, accessing these properties could raise `AttributeError`. Additionally, any dependent code assuming PCP support is now present may fail if the configuration is not correctly set up elsewhere.

**Key insights**  
This is a straightforward completion of a previously incomplete feature. Developers should verify that `moe_parallel_config` reliably provides PCP attributes and that all related PCP initialization logic is properly implemented. Consider adding defensive checks or documentation if PCP is an optional feature.

---

## 7. [[Fix] Align fused moe lora_b shape with peft](https://github.com/vllm-project/vllm/pull/31534)


### Base Information

- **PR Number:** #31534
- **Author:** [linitra24](https://github.com/linitra24)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2025-12-30 17:44:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31534/files) (3):**
  - `docs/models/supported_models.md`
  - `tests/lora/test_gptoss_tp.py`
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
This PR fixes a shape mismatch in LoRA B weight loading for MoE layers. vLLM incorrectly assumed LoRA B weights had shape `(output_dim, num_experts, rank)`, while PEFT defines them as `(output_dim, rank, num_experts)`. The changes reshape and permute tensors to align with PEFT's expected format, ensuring compatibility with PEFT-trained adapters.

**Technical impact**  
The fix ensures vLLM's fused MoE layers correctly interpret LoRA weights from PEFT adapters, enabling output consistency with Transformers/PEFT. This resolves a critical interoperability issue where vLLM previously produced divergent results when loading standard PEFT adapters for MoE models.

**Potential risks**  
The changes assume all PEFT adapters for MoE models follow the `(output_dim, rank, num_experts)` shape convention. If any custom or non-standard adapters use a different layout, they may break. Additionally, the tensor reshaping logic could introduce subtle bugs if the input tensor dimensions are malformed or unexpected.

**Key insights**  
Developers should verify that any existing MoE LoRA adapters are compatible with the new shape assumption. The updated test expectations confirm the fix restores functional correctness, but edge cases around tensor dimensions should be validated. This change is essential for users relying on PEFT-trained adapters with vLLM's MoE support.

---

## 8. [Add docker buildx bake configuration](https://github.com/vllm-project/vllm/pull/31477)


### Base Information

- **PR Number:** #31477
- **Author:** [amrmahdi](https://github.com/amrmahdi)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2025-12-30 17:08:54
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31477/files) (1):**
  - `docker/docker-bake.hcl`

### Summary

**What changed and why**  
Added a `docker/docker-bake.hcl` configuration file to standardize Docker builds using Docker Buildx Bake. This enables consistent build definitions across regular CI image builds and AMI cache warm-up processes, supporting faster CI builds through improved layer caching.

**Technical impact**  
Introduces a declarative build configuration that centralizes variables (e.g., CUDA arch lists, parallelism settings), common build arguments, and OCI labels. It establishes base targets (`test`, `openai`) that CI can extend via overlays (e.g., `ci.hcl`), ensuring cache key parity and maximizing layer reuse across different build contexts.

**Potential risks**  
If the `COMMIT` variable is not properly passed during builds, OCI labels may lack revision metadata, potentially affecting traceability. Overlay files (like `ci.hcl`) must maintain compatibility with this base configuration to avoid build failures. Incorrect variable defaults (e.g., `TORCH_CUDA_ARCH_LIST`) could lead to suboptimal builds for specific hardware.

**Key insights**  
This configuration is foundational for caching optimizations in CI; ensure all CI scripts correctly reference and extend it. Developers should verify that any future Dockerfile changes align with the defined variables and targets. The `--print` flag is useful for debugging resolved configurations before execution.

---

## 9. [[Bugfix]Fix pooling model always disabled due to incorrect PP rank check](https://github.com/vllm-project/vllm/pull/31505)


### Base Information

- **PR Number:** #31505
- **Author:** [vintipandey](https://github.com/vintipandey)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2025-12-30 11:27:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31505/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The change fixes a bug where pooling models were incorrectly disabled due to a faulty pipeline parallel (PP) rank check. The `broadcast_pp_output` flag was always set to `True` because `len(get_pp_group().ranks) > 0` always evaluates to true (since there is at least one rank). This triggered an assertion that prevents pooling models from running. The fix updates the condition to `len(get_pp_group().ranks) > 1`, ensuring broadcasting only occurs when multiple PP ranks are present.

**Technical impact**  
This correction restores proper functionality for pooling models (e.g., models with classification heads) when using a single pipeline parallel rank. The `broadcast_pp_output` flag now accurately reflects whether output broadcasting across PP ranks is necessary, aligning with the intended behavior for single-rank PP configurations.

**Potential risks**  
If the condition `len(get_pp_group().ranks) > 1` is incorrectly evaluated in edge cases (e.g., dynamic rank changes or misconfigured PP groups), pooling models might still fail. Additionally, the fix assumes that `get_pp_group().ranks` reliably represents the current PP group size; any upstream issues with rank initialization could reintroduce similar bugs.

**Key insights**  
Developers should verify that PP group initialization consistently provides accurate rank counts. This fix highlights the importance of validating conditions involving distributed state, especially when transitioning between single-rank and multi-rank PP setups. Consider adding a comment explaining why `> 1` is used instead of `> 0` to prevent regression.

---

## 10. [[BugFix]  add select_gemm_impl on CompressedTensorsWNA16MoEMethod to support LoRA](https://github.com/vllm-project/vllm/pull/31453)


### Base Information

- **PR Number:** #31453
- **Author:** [JartX](https://github.com/JartX)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-30 11:20:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31453/files) (1):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`

### Summary

**What changed and why**  
Added a `select_gemm_impl` method to `CompressedTensorsWNA16MoEMethod` to support LoRA integration. Previously, the missing implementation caused a `NotImplementedError` when LoRA was enabled for MoE layers, preventing the system from selecting the appropriate GEMM implementation.

**Technical impact**  
This change enables LoRA functionality for compressed MoE models by providing the required method to choose between Triton-based or fallback GEMM implementations. It ensures the quantized MoE layer can properly handle LoRA weights and integrates with the existing fused MoE infrastructure.

**Potential risks**  
If Triton is not available and LoRA is enabled, the method will raise a `NotImplementedError`, potentially crashing the system. Additionally, the method currently only handles the LoRA-enabled case; other scenarios (non-LoRA) still raise `NotImplementedError`, which may need future expansion.

**Key insights**  
Developers must ensure Triton is installed when using LoRA with compressed MoE models. Consider extending the method to handle non-LoRA cases or providing a clearer error message for missing Triton. Verify that `layer.w13_weight_packed` and `layer.w2_weight_packed` attributes exist and are correctly initialized.

---

## 11. [[Model] Add support for openPangu moe model](https://github.com/vllm-project/vllm/pull/28775)


### Base Information

- **PR Number:** #28775
- **Author:** [yt0428](https://github.com/yt0428)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-30 08:11:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28775/files) (11):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/attention/backends/registry.py`
  - `vllm/attention/layer.py`
  - `vllm/attention/layers/static_sink_attention.py`
  - `vllm/attention/ops/triton_reshape_and_cache_flash.py`
  - `vllm/model_executor/models/openpangu.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/v1/attention/backends/flash_attn_diffkv.py`
  - `vllm/v1/core/single_type_kv_cache_manager.py`
  - `vllm/v1/kv_cache_interface.py`

### Summary

**What changed and why**  
This PR adds support for the openPangu MoE model, which introduces two novel features: different key/value head sizes and static sink tokens in attention. The implementation includes a new attention backend (`FlashAttentionDiffKVBackend`) for handling varying head dimensions, a `StaticSinkAttention` layer to manage learned sink tokens, and updates to KV cache management and model definitions to accommodate these features.

**Technical impact**  
The changes extend vLLM's attention system to support models with asymmetric key/value head dimensions, requiring modifications to KV cache shapes, reshape kernels, and attention backends. The sink token implementation reserves dedicated KV cache blocks, modifying block tables and sequence lengths during metadata construction. This introduces new abstractions like `SinkFullAttentionSpec` and `StaticSinkAttention` that integrate with existing attention and caching infrastructure.

**Potential risks**  
1. The sink token implementation modifies block tables and sequence lengths in-place, which could affect other parts of the scheduler or attention metadata if not carefully synchronized.  
2. The different head size support adds complexity to KV cache management and may introduce edge cases with FP8 quantization or mixed backends.  
3. The static sink approach assumes sink tokens are fixed and stored once; dynamic updates or model variants with variable sink lengths are not supported.  
4. New Triton kernels and backend logic require thorough testing across hardware and dtype configurations.

**Key insights**  
1. The separation of diff-KV and sink token features into distinct components improves modularity, but further refactoring could align sink token handling more closely with existing KV cache interfaces.  
2. Developers should ensure the `head_size_v` parameter is consistently propagated through attention layers, specs, and backends to avoid shape mismatches.  
3. The sink token block reservation mechanism should be reviewed for potential fragmentation or interference with block allocation policies.  
4. Consider adding validation to prevent sink length misconfiguration (e.g., non-divisible by block size) and to handle edge cases like zero-length sinks gracefully.

---

## 12. [[CPU] Disable async schedule on CPU](https://github.com/vllm-project/vllm/pull/31525)


### Base Information

- **PR Number:** #31525
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2025-12-30 04:34:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31525/files) (1):**
  - `vllm/platforms/cpu.py`

### Summary

**What changed and why**  
The change explicitly disables async scheduling (`async_scheduling = False`) for CPU platforms. This is done because asynchronous scheduling is unnecessary for CPU execution, likely due to CPU's different concurrency and memory access patterns compared to GPU.

**Technical impact**  
This modification ensures the scheduler configuration aligns with CPU platform constraints, potentially simplifying scheduling logic and reducing overhead. It may also prevent unintended behavior or performance degradation that could arise from enabling async scheduling on CPU.

**Potential risks**  
If async scheduling was previously enabled by default for CPU, this change could affect any existing code that implicitly relied on its behavior. Additionally, if future CPU optimizations could benefit from async scheduling, this hardcoded disable might need revisiting.

**Key insights**  
Developers should verify that no CPU-specific features depend on async scheduling. This change is likely a cleanup, but it's important to ensure it doesn't break any edge cases in CPU-only deployments or hybrid CPU/GPU environments.

---

## 13. [[CI][NIXL] Split DPEP tests](https://github.com/vllm-project/vllm/pull/31491)


### Base Information

- **PR Number:** #31491
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-30 04:26:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31491/files) (2):**
  - `.buildkite/test-pipeline.yaml`
  - `tests/v1/kv_connector/nixl_integration/config_sweep_accuracy_test.sh`

### Summary

**What changed and why**  
The PR splits NixlConnector accuracy tests into separate TP (Tensor Parallelism) and DPEP (Data Parallelism with Expert Parallelism) test steps in the CI pipeline. This separation allows better utilization of parallel resources and enables DPEP tests to scale independently with GPU allocation for future scenarios.

**Technical impact**  
The pipeline now runs TP and DPEP tests as distinct CI jobs, with DPEP tests configured to use 4 GPUs. The test script has been refactored to dynamically select configurations based on the `DP_EP` environment variable, improving modularity and reducing redundancy in test execution.

**Potential risks**  
If the `DP_EP` environment variable is inadvertently set or leaked between CI steps, it could cause the wrong test configurations to run. The DPEP test step is marked as "TODO," which may indicate incomplete implementation or pending validation. Additionally, hardcoded GPU counts (e.g., `num_gpus: 4`) could limit flexibility in resource-constrained environments.

**Key insights**  
This change enhances CI efficiency by enabling parallel execution of TP and DPEP tests. Developers should ensure the "TODO" is addressed and consider parameterizing GPU allocation to adapt to different CI environments. The dynamic configuration selection in the script is a clean pattern that can be reused for other test splits.

---

## 14. [[ROCm][Bugfix] Fix accuracy issue on fmoe when `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS` enabled](https://github.com/vllm-project/vllm/pull/31523)


### Base Information

- **PR Number:** #31523
- **Author:** [ganyi1996ppo](https://github.com/ganyi1996ppo)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2025-12-30 01:21:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31523/files) (1):**
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
This PR fixes an accuracy regression in Mixture of Experts (MOE) models when the `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS` environment variable is enabled. The issue was introduced by a previous PR (#31221) that enabled shared experts fusion but did not ensure the required `grouped_topk` custom operation was properly activated in the compilation configuration. The fix adds logic to automatically enable `+grouped_topk` when the relevant AITER features are active.

**Technical impact**  
The changes modify the `check_and_update_config` method in the ROCm platform configuration to conditionally append `+grouped_topk` to the `custom_ops` list. This ensures the correct kernel dispatch path is taken for fused MOE operations with shared experts, directly impacting model accuracy for all ROCm-based MOE models using this feature. The system now properly validates and overrides user-provided `-grouped_topk` settings when the fusion feature is enabled.

**Potential risks**  
There is a minor risk if users explicitly disabled `grouped_topk` for performance or compatibility reasons, as the code now overrides `-grouped_topk` when shared experts fusion is enabled. The warning message helps mitigate this. Additionally, the change assumes `use_aiter_fused_moe` is a sufficient proxy for needing `grouped_topk`; if other configurations require this op without that flag, they might be missed.

**Key insights**  
The fix highlights the importance of ensuring all required custom ops are enabled when activating advanced kernel fusion features. Developers should verify that feature flags and their dependencies are always synchronized. The test results show a dramatic accuracy recovery (from ~1% to ~95% on GSM8K), confirming this was a critical dispatch issue. Future similar features should include comprehensive dependency checks at implementation time.

---

