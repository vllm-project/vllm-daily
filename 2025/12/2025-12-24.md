# vLLM Merged PR Report

**Report Date:** 2025-12-24 PST

**Total Merged PRs:** 15

---

## 1. [[Frontend] add FunctionGemma tool parser support](https://github.com/vllm-project/vllm/pull/31218)


### Base Information

- **PR Number:** #31218
- **Author:** [gateremark](https://github.com/gateremark)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2025-12-24 23:29:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31218/files) (5):**
  - `docs/features/tool_calling.md`
  - `examples/tool_chat_template_functiongemma.jinja`
  - `tests/tool_parsers/test_functiongemma_tool_parser.py`
  - `vllm/tool_parsers/__init__.py`
  - `vllm/tool_parsers/functiongemma_tool_parser.py`

### Summary

**What changed and why**  
This PR adds support for Google's FunctionGemma model (`google/functiongemma-270m-it`) to vLLM's tool-calling framework. It introduces a new parser to handle FunctionGemma's unique output format, which uses `<start_function_call>` and `<end_function_call>` tags with `<escape>`-delimited arguments. A corresponding chat template and comprehensive unit tests are also included.

**Technical impact**  
The changes extend vLLM's tool parser registry with a new `functiongemma` option, enabling seamless integration of FunctionGemma for function-calling tasks. The parser correctly extracts tool calls from the model's specialized syntax and supports both batch and streaming modes. The addition is modular and follows existing patterns, minimizing impact on other parsers.

**Potential risks**  
The parser relies on regex patterns that may fail if the model outputs malformed sequences or nested escape tags. Streaming logic assumes tool calls are well-formed and could buffer incomplete tokens incorrectly. Since FunctionGemma requires fine-tuning for optimal performance, users might expect base-model behavior that differs from documented examples.

**Key insights**  
Developers must use the provided chat template (`examples/tool_chat_template_functiongemma.jinja`) and specify `--tool-call-parser functiongemma` for correct operation. The implementation handles edge cases like partial streaming and argument parsing, but thorough testing with actual fine-tuned models is recommended. Ensure the model's output adheres strictly to the expected format to avoid parsing errors.

---

## 2. [[Doc] Add tool call parser documentation for GPT-OSS models](https://github.com/vllm-project/vllm/pull/31212)


### Base Information

- **PR Number:** #31212
- **Author:** [amithkk](https://github.com/amithkk)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2025-12-24 21:29:10
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31212/files) (1):**
  - `docs/features/tool_calling.md`

### Summary

**What changed and why**  
This change adds documentation for the `openai` tool calling parser format, specifically for OpenAI OSS models (GPT-OSS-20b and GPT-OSS-120b). The update extends the existing tool calling documentation by including these models and specifying the required command-line flag, addressing issue #31211.

**Technical impact**  
The modification is purely documentation-focused and has no impact on the codebase or runtime behavior. It enhances the user guide by providing clear instructions for configuring tool calling with supported OpenAI OSS models, improving discoverability and usability.

**Potential risks**  
There are minimal risks as this is a documentation-only change. However, if the underlying implementation of the `openai` tool call parser differs from what's documented, users could encounter configuration errors. Ensuring the documentation stays synchronized with any future parser updates is important.

**Key insights**  
This update fills a documentation gap for OpenAI OSS models, aiding users in leveraging tool calling features. Developers should verify that the documented models and flags align with the actual implementation in the codebase. Future enhancements to the parser or supported models should prompt corresponding documentation updates.

---

## 3. [[Bugfix] Fix eagle dp tests on A100](https://github.com/vllm-project/vllm/pull/31241)


### Base Information

- **PR Number:** #31241
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2025-12-24 16:05:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31241/files) (1):**
  - `tests/v1/distributed/test_eagle_dp.py`

### Summary

**What changed and why**  
The change enables batch invariance mode (`VLLM_BATCH_INVARIANT=1`) and sets the attention backend to `FLASH_ATTN` in a distributed test for Eagle DP. This ensures that the target model's batch processing (which verifies multiple draft tokens in one forward pass) produces identical outputs to the non-Eagle model's sequential generation (batch size 1), addressing a test failure on A100 hardware.

**Technical impact**  
Enabling batch invariance guarantees consistent token generation across different batch sizes, which is critical for Eagle's draft-and-verify mechanism. The explicit attention backend selection ensures compatibility with the batch invariance requirement, preventing backend-related inconsistencies in distributed environments.

**Potential risks**  
The test now depends on specific environment variables and attention backends, which may reduce portability across different hardware or configurations. If `FLASH_ATTN` is unsupported on some systems, the test could fail or require fallback handling.

**Key insights**  
Batch invariance is essential for validating Eagle's correctness when comparing batched and sequential generation. Developers should ensure that any similar comparative tests explicitly account for batch invariance and attention backend requirements to maintain consistency across hardware platforms.

---

## 4. [[Perf] Add skip_clone to SamplingParams for internal request handling](https://github.com/vllm-project/vllm/pull/31041)


### Base Information

- **PR Number:** #31041
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-24 14:35:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31041/files) (5):**
  - `vllm/entrypoints/api_server.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/protocol.py`
  - `vllm/entrypoints/openai/speech_to_text.py`
  - `vllm/sampling_params.py`

### Summary

**What changed and why**  
This PR adds a `skip_clone` flag to `SamplingParams` to avoid expensive `copy.deepcopy()` operations when the parameters are created fresh per request and dedicated to a single request lifecycle. The flag is set in all `to_sampling_params()` methods across OpenAI protocol classes, the demo API server, and beam search implementations, enabling performance optimization by returning `self` instead of performing a deep copy.

**Technical impact**  
The changes reduce CPU overhead in request processing by eliminating unnecessary deep copies of `SamplingParams`. When `skip_clone=True`, `clone()` uses `copy.copy()` (shallow copy) instead of `copy.deepcopy()`, which is particularly beneficial for high-throughput scenarios. The optimization is applied consistently across internal request-handling paths, ensuring performance gains without altering external API behavior.

**Potential risks**  
If `skip_clone` is incorrectly set on a shared or mutable `SamplingParams` instance, it could lead to unintended side effects or data corruption across requests. The current implementation relies on careful manual flag assignment, which introduces a maintenance burden and risk of human error. Additionally, the shallow copy approach may not fully isolate nested mutable objects if they exist in future extensions.

**Key insights**  
This optimization effectively addresses a known performance bottleneck, as evidenced by the profiling results showing the clone operation completely removed. Developers should ensure `skip_clone=True` is only used for short-lived, request-dedicated instances. Consider future refactoring to make this behavior automatic or safer, such as by enforcing immutability or using a factory pattern to avoid manual flag management.

---

## 5. [[Chore][1/2] Drop `v0.14` deprecations](https://github.com/vllm-project/vllm/pull/31285)


### Base Information

- **PR Number:** #31285
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-24 09:54:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31285/files) (22):**
  - `docs/api/README.md`
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `vllm/entrypoints/context.py`
  - `vllm/entrypoints/openai/parser/responses_parser.py`
  - `vllm/entrypoints/openai/serving_models.py`
  - `vllm/entrypoints/openai/tool_parsers/__init__.py`
  - `vllm/lora/request.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/multimodal/__init__.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/utils.py`
  - `vllm/tokenizers/__init__.py`
  - `vllm/tokenizers/registry.py`
  - `vllm/transformers_utils/tokenizer.py`
  - `vllm/transformers_utils/tokenizer_base.py`
  - `vllm/utils/__init__.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR removes various deprecated code scheduled for removal in v0.14. Changes include deleting deprecated classes (e.g., `MultiModalKwargs`, `TokenizerBase`), removing deprecated attributes (e.g., `lora_local_path`, `merge_by_field_config`), and updating imports to use new module paths (e.g., `vllm.tokenizers` instead of `vllm.transformers_utils.tokenizer`).

**Technical impact**  
The cleanup reduces code complexity and maintenance overhead by eliminating legacy interfaces. It enforces migration to updated APIs, such as using `lora_path` over `lora_local_path` and `TokenizerLike` over `AnyTokenizer`. The removal of deprecated multimodal fields and parameters simplifies the multimodal batching logic.

**Potential risks**  
External code still relying on the removed deprecated interfaces will break. For example, any imports from `vllm.entrypoints.openai.tool_parsers` or usage of `decode_tokens`/`encode_tokens` will fail. The `get_tokenizer` import is kept temporarily for compatibility but may be removed later.

**Key insights**  
Developers must update their codebases to use the new APIs before upgrading to v0.14. Pay special attention to LoRA requests (`lora_path`), tokenizer imports (`vllm.tokenizers`), and multimodal configurations (removed `merge_by_field_config`). The PR is part of a broader cleanup effort, with further removals likely in subsequent releases.

---

## 6. [[Bugfix] Remove dead `block_quant_to_tensor_quant` function](https://github.com/vllm-project/vllm/pull/31294)


### Base Information

- **PR Number:** #31294
- **Author:** [yurekami](https://github.com/yurekami)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2025-12-24 09:22:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31294/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`

### Summary

**What changed and why**  
Removed the unused `block_quant_to_tensor_quant` function and its import of `group_broadcast`. This function was identified as dead code (never called) and contained a bug where it incorrectly passed a tensor to `group_broadcast`, which expects a `GroupShape` tuple.

**Technical impact**  
This change simplifies the codebase by eliminating unused code, reducing maintenance burden and potential confusion. It also fixes a latent type error that would have caused runtime failures if the function were ever called, improving code reliability.

**Potential risks**  
The removal is low-risk since the function was never used. However, if any external or undocumented code paths relied on this function, they would now break. The bug fix aspect is purely preventative, as the erroneous code path was unreachable.

**Key insights**  
Regularly auditing for and removing dead code is a good practice that improves code clarity and reduces technical debt. The discovery of a bug in unused code highlights the value of such cleanups, as it prevents future issues if the code were to be activated. Ensure no tests or documentation indirectly reference the removed function.

---

## 7. [[cli] complete vllm cli help message](https://github.com/vllm-project/vllm/pull/31226)


### Base Information

- **PR Number:** #31226
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-24 07:45:47
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31226/files) (2):**
  - `vllm/entrypoints/cli/benchmark/main.py`
  - `vllm/entrypoints/cli/serve.py`

### Summary

**What changed and why**  
This PR adds missing help messages for the `serve` and `bench` subcommands in the vLLM CLI. Previously, these subcommands appeared in the main help output without descriptions, causing confusion. The changes populate the `help` parameter in their respective argument parser initializations.

**Technical impact**  
The modifications improve CLI usability by providing clear, concise descriptions for all subcommands. This aligns with standard CLI conventions and enhances the developer experience without affecting core functionality or performance. The changes are minimal and focused solely on the help text output.

**Potential risks**  
There is minimal risk as these are purely documentation changes within the CLI interface. However, if the `help` text contains inaccuracies or becomes outdated, it could mislead users. Additionally, any future refactoring of the subcommand initialization logic must preserve these help strings.

**Key insights**  
Always ensure CLI tools follow consistent help documentation practices to improve usability. Consider adding similar help text for any future subcommands. Verify that the descriptions accurately reflect each subcommand's purpose and align with the project's documentation.

---

## 8. [[Bugfix][Hardware][AMD] Fix FP8 dtype in silu_mul quantization](https://github.com/vllm-project/vllm/pull/31179)


### Base Information

- **PR Number:** #31179
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2025-12-24 07:37:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31179/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`

### Summary

**What changed and why**  
The fix replaces hardcoded `torch.float8_e4m3fn` dtype references with platform-aware logic to determine the correct FP8 dtype and min/max values. This ensures compatibility with ROCm platforms that use `torch.float8_e4m3fnuz`, preventing dtype mismatches and quantization accuracy issues.

**Technical impact**  
The changes align the `silu_mul_per_token_group_quant_fp8_colmajor` function with the existing pattern used in `per_token_group_quant_fp8`, promoting consistency across FP8 quantization utilities. This enables correct tensor dtype assignment and appropriate clamping bounds (224.0 vs. 240.0) based on the platform's FP8 variant.

**Potential risks**  
If `current_platform.fp8_dtype()` or `is_fp8_fnuz()` are not properly implemented for all supported platforms, runtime errors or incorrect quantization could occur. Additionally, the hardcoded value 224.0 assumes specific behavior for `fnuz` dtypes; future dtype variants may require further adjustments.

**Key insights**  
This fix highlights the importance of avoiding hardcoded dtype assumptions in cross-platform code. Developers should reuse established platform-abstraction patterns (like those in `fp8_utils.py`) to maintain consistency and reduce similar issues in other quantization functions.

---

## 9. [[CI] Reorganization pooling_mteb_test](https://github.com/vllm-project/vllm/pull/31265)


### Base Information

- **PR Number:** #31265
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-24 07:36:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31265/files) (18):**
  - `tests/entrypoints/pooling/embed/test_correctness_mteb.py`
  - `tests/entrypoints/pooling/score/test_correctness_mteb.py`
  - `tests/entrypoints/pooling/score/test_utils.py`
  - `tests/models/language/pooling_mteb_test/mteb_embed_utils.py`
  - `tests/models/language/pooling_mteb_test/mteb_score_utils.py`
  - `tests/models/language/pooling_mteb_test/test_baai.py`
  - `tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py`
  - `tests/models/language/pooling_mteb_test/test_cross_encoder.py`
  - `tests/models/language/pooling_mteb_test/test_gte.py`
  - `tests/models/language/pooling_mteb_test/test_intfloat.py`
  - `tests/models/language/pooling_mteb_test/test_jina.py`
  - `tests/models/language/pooling_mteb_test/test_mxbai_rerank.py`
  - `tests/models/language/pooling_mteb_test/test_nemotron.py`
  - `tests/models/language/pooling_mteb_test/test_nomic.py`
  - `tests/models/language/pooling_mteb_test/test_qwen3_reranker.py`
  - `tests/models/language/pooling_mteb_test/test_snowflake_arctic_embed.py`
  - `tests/models/language/pooling_mteb_test/test_st_projector.py`
  - `tests/models/utils.py`

### Summary

**What changed and why**  
This PR reorganizes pooling MTEB tests by removing confusing `CLSPoolingModelInfo` and `LASTPoolingModelInfo` classes, and splitting the monolithic `mteb_utils.py` into separate `mteb_embed_utils.py` and `mteb_score_utils.py` files. The changes aim to improve code clarity and align with recent testing enhancements that now validate specific model attributes like `pooling_type`, `attn_type`, and caching support flags.

**Technical impact**  
The refactoring creates a cleaner separation between embedding and reranking test utilities, making the codebase more maintainable. All model configuration tests now directly validate `pooling_type`, `attn_type`, `is_prefix_caching_supported`, and `is_chunked_prefill_supported` attributes instead of relying on derived `default_pooling_type`. This provides more granular and explicit testing of model characteristics.

**Potential risks**  
The removal of specialized model info classes (`CLSPoolingModelInfo`, `LASTPoolingModelInfo`) could break any external code that imports them. Additionally, the split utilities might introduce import errors if not all dependencies are properly migrated between the two new files. The extensive modifications across 18 files increase the risk of subtle integration issues.

**Key insights**  
This refactoring represents a significant improvement in test organization and clarity. Developers should note that all model tests now require explicit specification of pooling/attention types and caching support flags. The comment update in `test_utils.py` provides important context about template handling limitations that affect both embedding and reranking models.

---

## 10. [[PERF] Add interleaved memory allocation to NUMA module](https://github.com/vllm-project/vllm/pull/30800)


### Base Information

- **PR Number:** #30800
- **Author:** [skaraban3807](https://github.com/skaraban3807)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2025-12-24 05:47:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30800/files) (1):**
  - `csrc/cpu/utils.cpp`

### Summary

**What changed and why**  
This PR introduces NUMA-aware interleaved memory allocation for systems with multiple NUMA nodes. The change replaces strict memory binding to a single node with interleaved allocation across all available nodes when multiple nodes are detected, aiming to distribute memory traffic evenly and improve performance for memory-bound workloads.

**Technical impact**  
The modification enhances memory locality and bandwidth utilization by dynamically selecting between `numa_set_interleave_mask()` (for multi-node configurations) and `numa_set_membind()` (for single-node setups). This optimizes memory access patterns, particularly on AMD EPYC systems with NPS4-NUMA configurations, and removes the previous warning about CPUs spanning different NUMA nodes.

**Potential risks**  
If `numa_set_interleave_mask()` fails silently (only a warning is logged), the system may fall back to default allocation policies, potentially degrading performance. Additionally, the change assumes that interleaving is always beneficial for multi-node systems, which may not hold for all workloads or hardware configurations.

**Key insights**  
The PR demonstrates a significant performance uplift (~30%) for memory-bound workloads on multi-node NUMA systems. Developers should validate that the interleaving policy aligns with their specific workload characteristics and monitor for any regressions in non-memory-bound scenarios. Consider adding a configuration flag to allow users to toggle interleaving behavior if needed.

---

## 11. [[Chore] Bump `lm-eval` version](https://github.com/vllm-project/vllm/pull/31264)


### Base Information

- **PR Number:** #31264
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-24 05:39:13
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31264/files) (14):**
  - `.buildkite/lm-eval-harness/run-lm-eval-chartqa-vllm-vlm-baseline.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-mmlupro-vllm-baseline.sh`
  - `.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh`
  - `.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh`
  - `docs/features/quantization/fp8.md`
  - `docs/features/quantization/int4.md`
  - `docs/features/quantization/int8.md`
  - `docs/features/quantization/quark.md`
  - `requirements/nightly_torch_test.txt`
  - `requirements/rocm-test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`

### Summary

**What changed and why**  
The PR updates the `lm-eval` dependency from a pinned Git commit to version 0.4.9.2 or higher. This change replaces a specific, potentially unstable commit reference with an official release tag, relaxing the upper version bound to allow future updates and ensure compatibility with the latest stable release.

**Technical impact**  
This transition moves the codebase from a development snapshot to a stable release, which may include bug fixes, performance improvements, and new features from the upstream library. The change affects installation scripts, documentation, and dependency files, ensuring consistent versioning across testing, CI pipelines, and user documentation.

**Potential risks**  
The new version could introduce breaking changes or regressions not present in the previously pinned commit. Since the version bound is relaxed (`>=0.4.9.2`), future automatic updates might inadvertently pull in incompatible versions, though this risk is mitigated by the lower bound. Additionally, the `[api]` extra is now consistently specified, which may affect installations if the extra includes new dependencies.

**Key insights**  
Always prefer official releases over direct Git commits for better stability and maintainability. Ensure thorough testing of evaluation pipelines post-update to catch any behavioral changes. Consider pinning to a specific version (e.g., `==0.4.9.2`) in critical environments to prevent unexpected updates while still benefiting from the stable release.

---

## 12. [[Chore] Remove unused `noqa`s](https://github.com/vllm-project/vllm/pull/31263)


### Base Information

- **PR Number:** #31263
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-24 05:38:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31263/files) (17):**
  - `tests/conftest.py`
  - `tests/entrypoints/openai/test_async_tokenization.py`
  - `tests/entrypoints/openai/test_chat.py`
  - `tests/entrypoints/openai/test_chat_with_tool_reasoning.py`
  - `tests/entrypoints/openai/test_completion_with_function_calling.py`
  - `tests/entrypoints/openai/test_default_mm_loras.py`
  - `tests/entrypoints/openai/test_enable_force_include_usage.py`
  - `tests/entrypoints/openai/test_messages.py`
  - `tests/entrypoints/openai/test_return_tokens_as_ids.py`
  - `tests/models/multimodal/generation/test_qwen2_vl.py`
  - `tests/v1/kv_connector/unit/test_example_connector.py`
  - `vllm/distributed/ec_transfer/ec_connector/example_connector.py`
  - `vllm/entrypoints/serve/elastic_ep/api_router.py`
  - `vllm/model_executor/model_loader/bitsandbytes_loader.py`
  - `vllm/model_executor/model_loader/runai_streamer_loader.py`
  - `vllm/v1/worker/ec_connector_model_runner_mixin.py`
  - `vllm/v1/worker/kv_connector_model_runner_mixin.py`

### Summary

**What changed and why**  
This PR removes unused `# noqa` comments and fixes a typo in a docstring. The changes are purely cosmetic code cleanup to eliminate unnecessary linting directives that are no longer needed, likely after code changes or linting rule updates.

**Technical impact**  
These changes have no functional impact on the codebase. They improve code cleanliness by removing obsolete linting suppressions and fixing a minor documentation error ("BitAndBytes" â†’ "BitsAndBytes"). The system behavior remains unchanged.

**Potential risks**  
The main risk is if any of the removed `# noqa` comments were actually suppressing legitimate linting errors that should be addressed differently. However, since these appear to be standard imports and function redefinitions, the risk is minimal. The string formatting change in `example_connector.py` could potentially affect log message formatting if the original tuple was intentional.

**Key insights**  
This cleanup improves code maintainability by removing visual clutter. Developers should ensure linting passes after these changes and consider whether any underlying issues were being masked. The PR demonstrates good hygiene but should be accompanied by verification that no new linting warnings appear.

---

## 13. [[Bugfix] Fix `max_model_len="auto"` handling](https://github.com/vllm-project/vllm/pull/31260)


### Base Information

- **PR Number:** #31260
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-24 03:15:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31260/files) (3):**
  - `vllm/config/model.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/v1/core/kv_cache_utils.py`

### Summary

**What changed and why**  
This PR fixes an incompatibility between two previous changes (#29431 and #30695) that prohibited `max_model_len=-1`, which is the integer representation for "auto" detection. The changes update the field validation to allow `-1`, create a dedicated parser for `max_model_len` that accepts both `-1` and `"auto"`, and refactor KV cache memory checking logic to eliminate code duplication.

**Technical impact**  
The `max_model_len` field now accepts values `>= -1` instead of `> 0`, restoring the "auto" functionality. A new `human_readable_int_or_auto` function handles parsing for `max_model_len` specifically, while other integer fields continue using the original parser. The KV cache memory validation logic has been consolidated into a reusable helper function, reducing code duplication across two similar validation paths.

**Potential risks**  
The validation change from `gt=0` to `ge=-1` could theoretically allow other negative values besides `-1`, though the parser restricts input to only `-1` or `"auto"`. The refactored memory checking logic introduces lambda functions that might have minor performance implications, though these are likely negligible given the initialization-time context.

**Key insights**  
This fix demonstrates the importance of maintaining backward compatibility when introducing validation changes. The solution elegantly separates concerns by creating a specialized parser for the `max_model_len` field while keeping general integer parsing unchanged. The refactoring of duplicate error handling code improves maintainability and ensures consistent error messages across different code paths.

---

## 14. [[Model] Introduce verify_and_update_model_config for VerifyAndUpdateConfig.](https://github.com/vllm-project/vllm/pull/31131)


### Base Information

- **PR Number:** #31131
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2025-12-24 01:54:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31131/files) (3):**
  - `vllm/config/model.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/llama.py`

### Summary

**What changed and why**  
This PR introduces a new `verify_and_update_model_config` method in the `VerifyAndUpdateConfig` class and calls it earlier in the `ModelConfig` initialization. The change ensures that model-specific configuration updates (e.g., setting pooling types, causal flags) occur before `_set_default_chunked_prefill_and_prefix_caching_args`, fixing a timing issue where logs didn't match actual behavior and avoiding non-obvious fallback logic.

**Technical impact**  
The refactor moves model-specific configuration logic from `verify_and_update_config` (which operates on the full `VllmConfig`) to `verify_and_update_model_config` (which operates on `ModelConfig`). This allows earlier validation and updates directly within `ModelConfig.__post_init__`, ensuring configuration is correct before downstream components like chunked prefill and prefix caching are initialized. The architecture-specific config classes now inherit from `VerifyAndUpdateConfig` and implement the new method.

**Potential risks**  
If any existing model relies on the old `verify_and_update_config` timing or accesses `VllmConfig` properties not available in `ModelConfig`, it could break. The removal of `@attn_type("encoder_only")` decorators from `LlamaBidirectional` classes might affect attention type resolution if not handled elsewhere. Additionally, the `config_updated` flag check in the new method may not prevent all re-entrancy issues if called from multiple paths.

**Key insights**  
Developers should update any custom model config classes to implement `verify_and_update_model_config` instead of `verify_and_update_config`. Ensure that all necessary `VllmConfig` dependencies are accessible via `ModelConfig`. Verify that attention type inference for bidirectional models remains correct after decorator removal. This change improves clarity and correctness but requires careful testing of affected models (e.g., embedding and reranking models).

---

## 15. [[Bugfix][ROCm] Fix load issue on deepseek quark quantization when shared expert enabled](https://github.com/vllm-project/vllm/pull/31261)


### Base Information

- **PR Number:** #31261
- **Author:** [ganyi1996ppo](https://github.com/ganyi1996ppo)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-24 00:47:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31261/files) (1):**
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
The PR fixes a bug in the DeepSeek V2 model loader when loading shared experts with quark quantization. The issue occurred because the code only processed the `weight` parameter for shared experts but not the `weight_scale` parameter used in quark quantization, causing an IndexError when trying to access shape dimensions on 1D tensors.

**Technical impact**  
This change modifies the weight loading logic to properly handle both 1D (scale) and 2D (weight) tensors when splitting shared expert parameters across multiple chunks. The fix ensures compatibility with quark quantization's scale parameters and maintains correct tensor slicing behavior for both weight and scale tensors.

**Potential risks**  
The change assumes that 1D tensors (scales) should always be split along dimension 0, which may not hold for all quantization schemes. There's also a risk that the `ndim` check could mask other shape-related issues if tensors with unexpected dimensions are passed to this code path.

**Key insights**  
The fix correctly distinguishes between weight and scale tensors using `ndim` checks, but developers should verify this logic works for all quantization variants. Consider adding explicit tensor dimension validation and potentially separating scale and weight handling into distinct code paths for clarity and maintainability.

---

