# vLLM Merged PR Report

**Report Date:** 2025-12-25 PST

**Total Merged PRs:** 12

---

## 1. [[Misc] Fix Qwen2-MoE shared_expert_gate](https://github.com/vllm-project/vllm/pull/31339)


### Base Information

- **PR Number:** #31339
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-25 21:10:39
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31339/files) (2):**
  - `vllm/lora/request.py`
  - `vllm/model_executor/models/qwen2_moe.py`

### Summary

**What changed and why**  
The PR fixes Qwen2-MoE's shared_expert_gate implementation by replacing `torch.nn.Linear` with `ReplicatedLinear` to align with vLLM's distributed architecture and prevent LoRA weight loading failures. Additionally, it removes an unused `long_lora_max_len` field from the `LoRARequest` class.

**Technical impact**  
Using `ReplicatedLinear` ensures proper weight distribution across tensor parallel groups, maintaining consistency with other linear layers in the model. The change to `self.expert_gate(x)[0]` suggests the gate output may have extra dimensions that need indexing. Removing the unused field simplifies the `LoRARequest` data structure.

**Potential risks**  
If `self.expert_gate(x)` returns a tensor with unexpected shape, the `[0]` indexing could introduce subtle bugs or dimension mismatches. The shared_expert_gate change assumes `ReplicatedLinear` is properly configured for this specific use case without bias.

**Key insights**  
Always use vLLM's distributed layer implementations (`ReplicatedLinear`) instead of PyTorch native layers to ensure compatibility with the engine's parallelization and LoRA systems. Verify tensor shapes when adding indexing operations to avoid runtime errors. Clean up unused code fields to reduce technical debt.

---

## 2. [[CI] Fix flaky vision beam search test with flexible semantic validation](https://github.com/vllm-project/vllm/pull/31324)


### Base Information

- **PR Number:** #31324
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-25 20:39:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31324/files) (1):**
  - `tests/entrypoints/openai/test_vision.py`

### Summary

**What changed and why**  
The test `test_single_chat_session_image_base64encoded_beamsearch` was refactored to replace exact string matching with semantic term validation. This addresses intermittent failures caused by minor wording variations in model outputs across platforms and attention backends, making the test more robust and platform-agnostic.

**Technical impact**  
The changes eliminate platform-specific expected results (`EXPECTED_MM_BEAM_SEARCH_RES_ROCM`) and introduce a flexible validation system using `REQUIRED_BEAM_SEARCH_TERMS` with AND/OR logic. This decouples the test from exact phrasing, reduces maintenance overhead, and ensures compatibility with different hardware and software configurations.

**Potential risks**  
Semantic validation may be too permissive if term groups are poorly defined, potentially allowing incorrect outputs to pass. The warning-based debug output could clutter logs if not managed. Additionally, the test now assumes beam search always produces two distinct non-empty outputs, which may not hold if the model behavior changes.

**Key insights**  
The shift to semantic validation is a best practice for LLM output testing, as it accommodates natural language variation. Developers should ensure term groups are precise and comprehensive to maintain test rigor. Consider adding bounds on output length or additional validation to catch edge cases beyond semantic correctness.

---

## 3. [Support LoRA and GPTQModel for PLaMo 2/3](https://github.com/vllm-project/vllm/pull/31322)


### Base Information

- **PR Number:** #31322
- **Author:** [Alnusjaponica](https://github.com/Alnusjaponica)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2025-12-25 19:41:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31322/files) (3):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/plamo3.py`

### Summary

**What changed and why**  
This PR fixes GPTQ quantization and adds LoRA support for PLaMo 2/3 models. The changes correct the `packed_modules_mapping` configuration to prevent incorrect layer splitting in GPTQ models, add LoRA interface support, and include necessary workarounds for GPTQ weight loading compatibility.

**Technical impact**  
The modifications ensure PLaMo models work correctly with GPTQ quantization by preventing unwanted splitting of fused layers (`qkv_proj`, `gate_up_proj`). Adding `SupportsLoRA` interface enables LoRA functionality, while the weight loading adjustments maintain compatibility with existing GPTQ model loading mechanisms. The documentation updates reflect the new LoRA support status.

**Potential risks**  
The changes to `packed_modules_mapping` could affect other quantization methods if they rely on different splitting logic. The LoRA implementation requires tensor contiguity in `_project_ssm_parameters`, which might impact performance. There's also a risk that the GPTQ weight loading workaround could mask underlying issues with rotary embedding weights.

**Key insights**  
Developers should verify that the fused layer configurations align with the actual model architecture. The LoRA support adds new target modules that need proper testing. The GPTQ compatibility fix follows established patterns from `AutoWeightsLoader`, ensuring consistency across the codebase. Future changes to quantization utilities should consider PLaMo's unique fused layer structure.

---

## 4. [[benchmark] use model card root instead of id](https://github.com/vllm-project/vllm/pull/31329)


### Base Information

- **PR Number:** #31329
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-25 18:55:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31329/files) (1):**
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
The PR modifies the benchmark utility to use the `root` field from the `/v1/models` endpoint instead of the `id` field when inferring the model name. This addresses an issue where using a custom `served_model_name` causes the benchmark to incorrectly use the custom ID to fetch the tokenizer from Hugging Face, resulting in a "not found" error.

**Technical impact**  
The `get_first_model_from_server` function now returns a tuple containing both the custom model ID (`id`) and the canonical Hugging Face model ID (`root`). The benchmark logic is updated to use `root` as `model_id` for tokenizer loading, while preserving the custom `model_name` for API interactions. This ensures compatibility with custom model names in vLLM serving.

**Potential risks**  
If the `/v1/models` endpoint response structure changes or lacks a `root` field, the benchmark may fail. Additionally, the change assumes the first model in the list is the intended target, which could be problematic in multi-model serving scenarios. The typo fix in help text ("ttfts") is minor but should be verified.

**Key insights**  
Always use canonical Hugging Face model IDs (`root`) for tokenizer initialization, not custom serving names (`id`). This change maintains backward compatibility while fixing the HF tokenizer fetch issue. Developers should ensure their model configurations include proper `root` fields when using custom `served_model_name`.

---

## 5. [[CI/Build] Ignore max transformers version skipping for initialization tests](https://github.com/vllm-project/vllm/pull/30619)


### Base Information

- **PR Number:** #30619
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-25 18:50:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30619/files) (2):**
  - `tests/models/registry.py`
  - `tests/models/test_initialization.py`

### Summary

**What changed and why**  
The PR modifies version checking logic to differentiate between vLLM and Hugging Face (HF) model compatibility issues. Previously, `max_transformers_version` would skip all tests for certain models due to HF-specific incompatibilities, but initialization tests don't use HF models. Now, version skip reasons are categorized by runner type (`"vllm"` or `"hf"`), allowing selective test skipping.

**Technical impact**  
The `transformers_version_reason` field is now a dictionary mapping runner types to skip reasons, enabling granular control over test execution. The `check_transformers_version` method accepts a `check_version_reason` parameter to determine which runner's compatibility issues should be enforced. Initialization tests now ignore max version checks for HF-specific issues, ensuring vLLM initialization tests run when only HF is broken.

**Potential risks**  
If the `check_version_reason` parameter is incorrectly set or omitted in other test suites, tests may be skipped unnecessarily or run when they shouldn't. The logic assumes clear separation between vLLM and HF runner failures, which might not hold if a version issue affects both runners. There's also a risk of inconsistent dictionary key usage across model entries.

**Key insights**  
Developers should ensure all test suites using `check_transformers_version` explicitly specify the appropriate `check_version_reason`. When adding new model entries, carefully assign skip reasons to the correct runner type (`"vllm"` for vLLM issues, `"hf"` for HF-specific issues). This change improves test precision but requires careful maintenance of the reason dictionaries.

---

## 6. [Feature/isaac 0.1](https://github.com/vllm-project/vllm/pull/28367)


### Base Information

- **PR Number:** #28367
- **Author:** [oscardev256](https://github.com/oscardev256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-25 18:49:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28367/files) (11):**
  - `docs/models/supported_models.md`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/models/multimodal/generation/test_common.py`
  - `tests/models/multimodal/generation/vlm_utils/model_utils.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/isaac.py`

### Summary

**What changed and why**  
This PR adds support for the Isaac-0.1 multimodal model to vLLM. It introduces a new model class `IsaacForConditionalGeneration` with custom vision processing (including pixel-shuffle operations for variable-length sequences) and integrates it into the vLLM framework. The changes include model implementation, configuration, testing utilities, and documentation updates.

**Technical impact**  
The integration extends vLLM's multimodal capabilities by adding a new model architecture that handles both text and images. The implementation includes specialized vision embeddings with pixel-shuffle support for variable-resolution images and patches the Hugging Face runner to ensure compatibility with vLLM's inference pipeline. This affects the model registry, configuration system, and testing infrastructure.

**Potential risks**  
The pixel-shuffle operations assume spatial dimensions are divisible by the scale factor, which could fail for certain image sizes. The patched HF runner modifies the model's forward pass to return hidden states, which may introduce subtle differences from the original implementation. There is also a risk of device mismatches when moving processor outputs, especially in distributed or mixed-precision scenarios.

**Key insights**  
Developers should note that Isaac uses a custom vision token `<image>` and requires the `perceptron` package for testing. The model supports variable-length image sequences via pixel-shuffle, but image preprocessing must ensure compatible dimensions. The integration follows vLLM's multimodal patterns, but the extensive patching of the HF runner indicates potential complexity in maintaining long-term compatibility.

---

## 7. [[BugFix] Fix async scheduling + reasoning with struct output](https://github.com/vllm-project/vllm/pull/31332)


### Base Information

- **PR Number:** #31332
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2025-12-25 15:01:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31332/files) (3):**
  - `tests/v1/entrypoints/llm/test_struct_output_generate.py`
  - `tests/v1/structured_output/test_reasoning_structured_output.py`
  - `vllm/v1/structured_output/__init__.py`

### Summary

**What changed and why**  
This PR fixes a bug in async scheduling when using structured output with reasoning. The issue was incorrect token accounting in `StructuredOutputManager.should_advance()` for async cases where output placeholders affect the token index calculation. The fix adjusts the delta calculation to account for `num_output_placeholders` when determining where reasoning ends.

**Technical impact**  
The changes ensure that reasoning end detection works correctly in both synchronous and async scheduling modes by properly offsetting the token index used for streaming checks. This maintains consistency in structured output generation behavior across different scheduling configurations and prevents premature or delayed advancement decisions.

**Potential risks**  
The fix assumes `num_output_placeholders` is properly maintained in async scheduling scenarios. If this field isn't correctly populated in all async cases, the calculation could still be incorrect. Additionally, the test expansion only covers one model with async scheduling enabled, potentially missing edge cases with other models or configurations.

**Key insights**  
The core issue was that async scheduling introduces output placeholders that shift token indices, requiring adjustment in reasoning boundary detection. Developers should verify that `num_output_placeholders` is consistently tracked across all scheduling modes. Consider adding more comprehensive async scheduling tests across different model types to ensure robustness.

---

## 8. [[Hybrid] Mamba2 prefix cache blocks freeing for running requests](https://github.com/vllm-project/vllm/pull/28047)


### Base Information

- **PR Number:** #28047
- **Author:** [s3woz](https://github.com/s3woz)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2025-12-25 12:54:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28047/files) (1):**
  - `vllm/v1/core/single_type_kv_cache_manager.py`

### Summary

**What changed and why**  
This PR adds a `get_num_skipped_tokens` method to the `MambaManager` class in the KV cache manager. The method returns `num_computed_tokens - 1`, indicating that Mamba2 models only need to retain the prefix cache block for the most recent computed token, allowing older blocks from running requests to be marked as evictable. This optimizes memory usage under tight memory conditions, especially for long prompts.

**Technical impact**  
The change enables the prefix cache eviction policy to free unnecessary Mamba2 state blocks during request processing, reducing memory pressure. This improves latency significantly for long prompts (up to 96% in tested scenarios) and slightly increases throughput for short prompts by allowing more efficient memory reuse across requests.

**Potential risks**  
If the assumption that Mamba2 only needs the last computed token’s state is incorrect for certain model configurations or edge cases, it could lead to premature eviction of needed cache blocks, potentially causing correctness issues. Additionally, the change is specific to Mamba2 hybrid models and may not generalize to other architectures without similar logic.

**Key insights**  
This is a targeted optimization that leverages Mamba2’s state retention characteristics to improve memory efficiency. Developers should verify that the `num_computed_tokens - 1` logic holds for all Mamba2 variants and consider adding safeguards or configuration flags if model-specific deviations exist. The performance gains are substantial, making this a high-value change for deployments with long-context workloads.

---

## 9. [[Model][Ernie4.5-VL] Support video metadata for timestamp rendering](https://github.com/vllm-project/vllm/pull/31274)


### Base Information

- **PR Number:** #31274
- **Author:** [Tiiiktak](https://github.com/Tiiiktak)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2025-12-25 06:07:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31274/files) (2):**
  - `tests/models/multimodal/processing/test_common.py`
  - `vllm/model_executor/models/ernie45_vl.py`

### Summary

**What changed and why**  
This PR adds video metadata support for ERNIE-4.5-VL to enable timestamp rendering. The processor now forwards video inputs as (frames, metadata) tuples to HuggingFace processors that support metadata, with backward compatibility maintained via a feature flag. The model parser and dummy video generation are updated to handle metadata-aware inputs and enforce a minimum of 2 frames.

**Technical impact**  
The changes modify the multimodal processing pipeline to conditionally pass video metadata, affecting how ERNIE-4.5-VL processes video inputs. The system now distinguishes between processors that support metadata (enabling timestamp rendering) and those that don't (falling back to raw frames). The dummy video generator ensures compliance with the model's frame requirements.

**Potential risks**  
If the HuggingFace processor lacks the `supports_video_metadata` attribute or returns an unexpected value, the fallback logic may not trigger correctly, potentially causing runtime errors. The warning for unsupported metadata could be missed in logs, leading to silent degradation of timestamp functionality. Override validations in dummy video generation might not handle all edge cases (e.g., zero frames).

**Key insights**  
Developers should ensure that any HuggingFace processor used with ERNIE-4.5-VL implements the `supports_video_metadata` attribute for proper feature detection. The fallback mechanism is critical for backward compatibility, but testing with both metadata-aware and legacy processors is recommended. The enforced minimum of 2 frames in dummy data generation aligns with model requirements and should be considered in all video input preparations.

---

## 10. [use the same stream for cuda graph catpure and replay for NCCL](https://github.com/vllm-project/vllm/pull/29207)


### Base Information

- **PR Number:** #29207
- **Author:** [Amir-19](https://github.com/Amir-19)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2025-12-25 03:10:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29207/files) (4):**
  - `benchmarks/kernels/benchmark_device_communicators.py`
  - `tests/utils_/test_torch_utils.py`
  - `vllm/compilation/cuda_graph.py`
  - `vllm/utils/torch_utils.py`

### Summary

**What changed and why**  
The changes ensure CUDA graph capture and replay use the same stream as warm-up iterations to prevent NCCL 2.28+ crashes during window registration. Previously, separate streams caused new memory allocations during capture, triggering NCCL registration failures.

**Technical impact**  
These modifications unify stream usage across CUDA graph operations by explicitly passing the current stream to `torch.cuda.graph()` calls. This affects CUDA graph capture in both benchmarking utilities and production code, and modifies `current_stream()` to create dedicated streams for both ROCm and CUDA platforms instead of default streams.

**Potential risks**  
The changes assume NCCL's memory allocation is stream-tied, which could introduce subtle synchronization issues if other parts of the system rely on default stream behavior. The modified `current_stream()` logic now applies to CUDA universally, which might affect performance or compatibility in non-NCCL scenarios.

**Key insights**  
Developers should verify that all CUDA graph operations consistently use `current_stream()` for capture. The fix addresses a specific NCCL 2.28+ incompatibility but reinforces the importance of stream management in CUDA graph workflows. Consider adding validation to ensure stream consistency across warm-up and capture phases.

---

## 11. [[Doc] Add troubleshooting for Triton PTX error about undefined gpu-name](https://github.com/vllm-project/vllm/pull/31338)


### Base Information

- **PR Number:** #31338
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2025-12-25 02:26:34
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31338/files) (1):**
  - `docs/usage/troubleshooting.md`

### Summary

**What changed and why**  
A new troubleshooting section was added to the documentation to address a specific Triton PTX compilation error (`ptxas fatal: Value 'sm_110a' is not defined for option 'gpu-name'`). This occurs when using Triton kernels with CUDA 13 on incompatible hardware, and the fix involves manually setting the `TRITON_PTXAS_PATH` environment variable to use the system's CUDA toolkit `ptxas`.

**Technical impact**  
This change enhances the troubleshooting documentation by providing a clear solution for a niche but critical error that can block users from running vLLM on certain GPU configurations. It does not affect the codebase or runtime behavior—only the user-facing documentation.

**Potential risks**  
The instructions assume the user has a CUDA toolkit installed at `/usr/local/cuda`, which may not be true for all environments (e.g., custom installations or containerized setups). Additionally, the error might manifest under other conditions beyond CUDA 13, but the documentation currently focuses on that specific scenario.

**Key insights**  
Developers should ensure their CUDA toolkit version matches the Triton and driver requirements. For environments where the default path differs, the documentation could be extended to suggest locating `ptxas` via `which ptxas` or checking `$CUDA_HOME`. Consider linking this to a broader compatibility matrix in future updates.

---

## 12. [[Benchmark Suite] improve cpu Benchmark Suite tests and comparison report for 0.12.0](https://github.com/vllm-project/vllm/pull/30994)


### Base Information

- **PR Number:** #30994
- **Author:** [louie-tsai](https://github.com/louie-tsai)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2025-12-25 00:51:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30994/files) (4):**
  - `.buildkite/performance-benchmarks/README.md`
  - `.buildkite/performance-benchmarks/scripts/compare-json-results.py`
  - `.buildkite/performance-benchmarks/tests/serving-tests-cpu.json`
  - `docs/benchmarking/dashboard.md`

### Summary

**What changed and why**  
This PR enhances the CPU benchmark suite for vLLM 0.12.0 by improving test coverage and report readability. Key changes include adding INT4 quantized model tests, removing redundant columns from comparison reports, grouping related metrics (Throughput, TTFT, TPOT) for easier analysis, and adding a new sizing table based on SLA thresholds.

**Technical impact**  
The benchmark comparison script (`compare-json-results.py`) has been significantly refactored with better data alignment logic, improved styling, and new functionality for SLA-based capacity planning. Documentation has been updated to reflect these changes and moved to a more accessible location. The test suite now includes INT4 quantized models with varying tensor parallelism configurations.

**Potential risks**  
The refactored comparison script introduces more complex data alignment logic that could potentially misalign metrics if key columns differ across files. The new SLA-based sizing calculations assume specific threshold values (TTFT ≤ 3000ms, TPOT ≤ 100ms) that may not be appropriate for all use cases. Removing the `enforce_eager` and `load_format` parameters from test configurations could affect reproducibility on certain hardware.

**Key insights**  
The new sizing table provides actionable capacity planning data by identifying maximum concurrency levels that meet both TTFT and TPOT SLAs. Developers should verify that the default SLA thresholds align with their application requirements. The improved grouping of related metrics will make performance comparisons more intuitive, especially when evaluating different TP/PP configurations or model quantizations.

---

