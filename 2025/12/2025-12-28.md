# vLLM Merged PR Report

**Report Date:** 2025-12-28 PST

**Total Merged PRs:** 7

---

## 1. [[ROCm][CI] Skip DeepGemm-dependent test on ROCm platform](https://github.com/vllm-project/vllm/pull/31462)


### Base Information

- **PR Number:** #31462
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2025-12-28 23:31:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31462/files) (1):**
  - `tests/kernels/moe/test_silu_mul_per_token_group_quant_fp8_colmajor.py`

### Summary

**What changed and why**  
Added a `pytest.mark.skipif` decorator to skip the test `test_silu_mul_fp8_quant_deep_gemm` on ROCm platforms because DeepGemm is a CUDA-specific feature unsupported by ROCm, preventing test failures.

**Technical impact**  
This change ensures CI stability on ROCm by excluding a test that depends on unsupported functionality, while preserving test coverage for CUDA platforms where DeepGemm is available.

**Potential risks**  
Skipping the test may hide future compatibility issues if DeepGemm support is added to ROCm, and it reduces test coverage for ROCm-specific implementations of similar functionality.

**Key insights**  
Consider documenting this skip in a central location for platform-specific test exclusions and monitor ROCm feature development to re-enable the test if DeepGemm becomes supported.

---

## 2. [[CI/Build][CPU] Update CPU CI test cases](https://github.com/vllm-project/vllm/pull/31466)


### Base Information

- **PR Number:** #31466
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-28 22:17:52
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31466/files) (1):**
  - `.buildkite/scripts/hardware_ci/run-cpu-test.sh`

### Summary

**What changed and why**  
The PR updates a single test file path in the CPU CI script, changing from `test_qwen2vl.py` to `test_qwenvl.py`. This appears to be a correction to align the test execution with the actual filename of a LoRA test for the QwenVL model.

**Technical impact**  
This change ensures the CPU CI pipeline runs the correct test file, preventing potential test failures due to a missing or misnamed file. It maintains CI reliability but does not affect production code, build processes, or architectural components.

**Potential risks**  
If the renamed test file (`test_qwenvl.py`) does not exist or contains breaking changes, the CI could fail. Additionally, there is a risk that other references to the old filename (`test_qwen2vl.py`) might remain elsewhere in the codebase, causing inconsistencies.

**Key insights**  
Always verify that file renaming changes are comprehensive across the codebase. Consider adding a check in the CI script to validate test file existence before execution to catch such issues early. Ensure the PR description includes test results to confirm the change works as intended.

---

## 3. [Add Fused MoE Triton kernels for GLM-4.5-Air, GLM-4.5v, GLM-4.6v on 2x RTX Pro 6000](https://github.com/vllm-project/vllm/pull/31407)


### Base Information

- **PR Number:** #31407
- **Author:** [mratsim](https://github.com/mratsim)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-28 08:38:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31407/files) (2):**
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=704,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition,dtype=fp8_w8a8.json`
  - `vllm/model_executor/layers/fused_moe/configs/E=129,N=704,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition,dtype=fp8_w8a8.json`

### Summary

**What changed and why**  
Two new configuration files were added for optimized Fused MoE (Mixture of Experts) Triton kernels targeting the NVIDIA RTX Pro 6000 (Blackwell Workstation Edition) with FP8 data type. The configurations are tailored for GLM-4.5-Air (E=129 experts) and other GLM variants (E=128 experts) to improve inference performance on this specific hardware.

**Technical impact**  
These additions extend vLLM's kernel auto-tuning capabilities by providing pre-optimized parameters (block sizes, warp counts, staging) for the given GPU and model configurations. This enables efficient execution of MoE layers without requiring on-the-fly tuning, reducing latency and improving throughput for the specified GLM models.

**Potential risks**  
The configurations are hardware-specific and may not generalize well to other GPU models or architectures. There is also a risk of performance degradation if used with different data types (non-FP8) or expert counts, and the lack of validation across diverse batch sizes or input sequences could introduce edge-case inefficiencies.

**Key insights**  
Developers should ensure these configurations are only applied to the exact GPU and model combinations specified. Consider adding validation checks to prevent accidental misuse, and document the tuning methodology (SGLang tuner) for future reference. Expanding test coverage to include more batch sizes and model variants would further mitigate risks.

---

## 4. [[Model] Add tuned triton fused_moe configs for Qwen3Moe on B200](https://github.com/vllm-project/vllm/pull/31448)


### Base Information

- **PR Number:** #31448
- **Author:** [Jzz1943](https://github.com/Jzz1943)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-28 08:38:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31448/files) (1):**
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_B200.json`

### Summary

**What changed and why**  
A new JSON configuration file has been added to provide tuned Triton kernel parameters for the fused MoE (Mixture of Experts) operation targeting the Qwen3-Omni-30B-A3B-Instruct model on NVIDIA B200 GPUs. The configuration is specifically optimized for 128 experts, bf16 data type, and a single GPU, aiming to improve inference performance through hardware-aware kernel tuning.

**Technical impact**  
This addition enhances the performance of MoE layers for the specified model and hardware by providing pre-tuned kernel parameters (e.g., block sizes, warp counts, staging) across various problem sizes (from 1 to 4096). It integrates into vLLM's existing fused MoE infrastructure, allowing the system to automatically select optimal kernel configurations during inference, which can significantly reduce latency and improve throughput for MoE-based models on B200.

**Potential risks**  
The configuration is highly specific to the B200 GPU and the exact model architecture; using it on other hardware (e.g., older NVIDIA GPUs) or with different model variants may lead to suboptimal performance or errors. Additionally, the tuning assumes a particular workload shape (E=128, N=768), so deviations in batch size or expert count might not benefit from these settings. There is also a risk of regression if the configuration file is inadvertently applied to incompatible kernels.

**Key insights**  
Developers should ensure this configuration is only loaded for B200 deployments with the matching model and dtype. Consider adding validation to prevent misuse across platforms. For future work, similar tuning could be extended to other GPU architectures or expert counts to maintain performance portability. Always verify performance gains with benchmarks before deploying to production.

---

## 5. [[BugFix] Re-fix async multimodal cpu tensor race condition](https://github.com/vllm-project/vllm/pull/31373)


### Base Information

- **PR Number:** #31373
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-28 03:05:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31373/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The PR fixes a previously reverted race condition in multimodal async scheduling by ensuring the `_preprocess` method is properly synchronized. The change combines the `record_function_or_nullcontext` and `self.synchronize_input_prep()` context managers into a single `with` block, guaranteeing that CPU tensor preprocessing occurs within the synchronization barrier.

**Technical impact**  
This restores correct synchronization for reused CPU tensors in `GPUModelRunner.execute_model`, preventing data races in multimodal models during async execution. The refactoring simplifies the context manager structure while maintaining the same synchronization guarantees, ensuring tensor operations are safely coordinated across asynchronous tasks.

**Potential risks**  
If the `synchronize_input_prep()` context manager has side effects or dependencies outside the preprocessing scope, combining it with the tracing context could inadvertently affect performance or error handling. Additionally, any future modifications to the context managers must preserve their combined execution order to avoid reintroducing the race condition.

**Key insights**  
Developers should verify that the combined context managers do not introduce overhead or unintended interactions. The fix highlights the importance of maintaining synchronization boundaries when refactoring async code, and future changes to this method should ensure the `_preprocess` logic remains within the synchronization barrier.

---

## 6. [[ROCm] Migrate xgrammar to upstream release](https://github.com/vllm-project/vllm/pull/31327)


### Base Information

- **PR Number:** #31327
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-28 00:08:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31327/files) (2):**
  - `requirements/common.txt`
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
The PR migrates the `xgrammar` dependency from a custom AMD fork to the upstream PyPI release. Specifically, it updates `requirements/common.txt` to version `0.1.29` and removes the custom fork entry from `requirements/rocm-test.txt`. This aligns with best practices to reduce maintenance overhead and dependency on custom forks.

**Technical impact**  
This change simplifies dependency management by using a stable upstream release, ensuring consistency across different platforms (x86_64, ARM, etc.). It eliminates the need to track a specific Git commit, reducing potential build complexities and improving reproducibility. The update also bumps the version from `0.1.27` to `0.1.29`, which may include upstream bug fixes or features.

**Potential risks**  
The version jump (`0.1.27` â†’ `0.1.29`) could introduce breaking changes or regressions not covered by the limited test suite. The custom fork might have contained ROCm-specific patches that are absent in the upstream release, potentially affecting functionality on AMD hardware. Additionally, the change in `requirements/common.txt` applies globally, not just to ROCm, which could impact other platforms if not thoroughly tested.

**Key insights**  
Developers should verify that the upstream `xgrammar==0.1.29` is fully compatible with all ROCm workflows, especially for async scheduling and structured output generation. It's recommended to run broader integration tests across all supported platforms to ensure no regressions. Future dependency updates should include checking for any missing patches from the forked version.

---

## 7. [[ROCm][CI] Add TorchCodec source build for transcription tests](https://github.com/vllm-project/vllm/pull/31323)


### Base Information

- **PR Number:** #31323
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-28 00:06:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31323/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `docker/Dockerfile.rocm`
  - `tools/install_torchcodec_rocm.sh`

### Summary

**What changed and why**  
This PR adds support for building TorchCodec from source within ROCm CI environments to enable audio/video transcription tests on AMD GPUs. The changes address a dependency issue where the HuggingFace `datasets` package requires TorchCodec for audio decoding, but no pre-built ROCm-compatible wheels exist, and existing wheels have compiler flag incompatibilities.

**Technical impact**  
The modifications enable transcription tests in the ROCm CI pipeline by installing TorchCodec directly into the Docker image and executing the installation script before test runs. This ensures the `datasets` package can decode audio files, allowing the transcription correctness tests to execute on AMD hardware. The changes are isolated to the ROCm build/test infrastructure and do not affect the core vLLM codebase.

**Potential risks**  
Building from source introduces variability in build success across different environments and could fail if system dependencies (FFmpeg dev libraries) are missing or incompatible. The script's dependency on specific environment variables (`TORCHCODEC_CMAKE_BUILD_DIR`, `TORCHCODEC_DISABLE_COMPILE_WARNING_AS_ERROR`) may cause silent failures if not properly set. There is also a risk of version drift if the cloned `main` branch of TorchCodec introduces breaking changes.

**Key insights**  
The solution correctly addresses a missing dependency for a specific test suite without modifying production code. Developers should ensure the `tools/install_torchcodec_rocm.sh` script remains robust by adding more explicit error handling and logging. Consider pinning the TorchCodec repository to a specific commit hash instead of using the `main` branch to prevent unexpected build failures due to upstream changes.

---

