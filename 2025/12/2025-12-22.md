# vLLM Merged PR Report

**Report Date:** 2025-12-22 PST

**Total Merged PRs:** 24

---

## 1. [[Bugfix] Fix Jais2ForCausalLM](https://github.com/vllm-project/vllm/pull/31198)


### Base Information

- **PR Number:** #31198
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2025-12-22 23:44:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31198/files) (1):**
  - `vllm/model_executor/models/jais2.py`

### Summary

**What changed and why**  
This PR fixes a bug in the Jais2ForCausalLM implementation by removing incorrect LoRA (Low-Rank Adaptation) logic that was improperly modifying the model's vocabulary size and embedding layers. The changes address issue #31148 where LoRA integration was incorrectly implemented for this model.

**Technical impact**  
The changes simplify the model architecture by removing LoRA-specific vocabulary padding and size adjustments. The vocabulary size now consistently uses the base model's `config.vocab_size` without LoRA modifications, and the rotary embedding initialization no longer includes redundant parameters. This makes the implementation cleaner and more aligned with standard causal language model patterns in vLLM.

**Potential risks**  
Removing LoRA support entirely could break existing workflows that depend on LoRA fine-tuning for Jais2 models. The changes also modify the LogitsProcessor initialization signature, which might affect compatibility with other parts of the codebase that expect the previous parameter structure. There's a risk that this fix oversimplifies the solution if LoRA support is actually needed for this model.

**Key insights**  
The PR indicates that LoRA integration was incorrectly implemented for Jais2 models and needed complete removal rather than fixing. Developers should verify if LoRA support is actually required for Jais2 use cases, and if so, a proper implementation should be designed separately. The cleanup improves code maintainability but may require updating any dependent code that expected the previous LoRA-modified vocabulary behavior.

---

## 2. [[XPU] decrease IGC_ForceOCLSIMDWidth for speculative decoding triton-xpu kernel compilation](https://github.com/vllm-project/vllm/pull/30538)


### Base Information

- **PR Number:** #30538
- **Author:** [yma11](https://github.com/yma11)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2025-12-22 21:22:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30538/files) (3):**
  - `docker/Dockerfile.xpu`
  - `docs/features/README.md`
  - `vllm/platforms/xpu.py`

### Summary

**What changed and why**  
This PR addresses a compilation error in speculative decoding on XPU by decreasing the `IGC_ForceOCLSIMDWidth` environment variable from its default to 16 when speculative decoding is enabled. This reduces the Triton kernel compilation scratch space to stay within hardware limits (max 262144 bytes). Additionally, it updates the Dockerfile to use a different Intel graphics repository and adds the `arctic-inference` package for suffix-method speculative decoding support.

**Technical impact**  
The change directly affects Triton kernel compilation behavior for XPU speculative decoding by limiting SIMD width, which reduces scratch memory usage and avoids a hard compilation failure. The Dockerfile modifications ensure the necessary dependencies are available for building and running speculative decoding workloads on Intel GPUs. The documentation update reflects that speculative decoding is now fully supported on XPU.

**Potential risks**  
Reducing SIMD width could potentially impact kernel performance by limiting parallelism, though it's necessary to avoid compilation failures. The change to `kobuk-team/intel-graphics-staging` may introduce less stable graphics drivers. The environment variable is set globally when speculative decoding is enabled, which might affect other kernels if not properly isolated.

**Key insights**  
This is a workaround for a hardware limitation rather than an optimal solution—future improvements should optimize kernel memory usage. Developers should monitor performance of speculative decoding kernels after this change. The Dockerfile changes indicate that suffix-method speculative decoding requires `arctic-inference` as a dependency, which is now explicitly installed.

---

## 3. [[Chore] Update more locations to use `attention_config.backend`](https://github.com/vllm-project/vllm/pull/31153)


### Base Information

- **PR Number:** #31153
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 19:19:50
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31153/files) (2):**
  - `benchmarks/benchmark_batch_invariance.py`
  - `tests/compile/distributed/test_fusions_e2e.py`

### Summary

**What changed and why**  
The PR updates two test files to replace direct environment variable usage (`VLLM_ATTENTION_BACKEND`) with the new `attention_config` parameter in model initialization. This follows up on a previous change (#26315) to standardize attention backend configuration.

**Technical impact**  
These changes migrate test infrastructure from environment-based configuration to API-based configuration, improving code consistency and test isolation. The `attention_config` parameter provides a cleaner, more explicit way to specify attention backends during model initialization.

**Potential risks**  
If other tests or scripts still rely on the `VLLM_ATTENTION_BACKEND` environment variable without proper fallback mechanisms, they may fail. The changes assume the `attention_config` parameter is fully supported across all tested scenarios.

**Key insights**  
The migration from environment variables to explicit API parameters represents a positive architectural shift toward more maintainable and predictable configuration. Developers should audit other test files for similar environment variable usage and complete the migration. Ensure backward compatibility is maintained during this transition period.

---

## 4. [[Feature] Batch invariant: Lora](https://github.com/vllm-project/vllm/pull/30097)


### Base Information

- **PR Number:** #30097
- **Author:** [quanliu1991](https://github.com/quanliu1991)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2025-12-22 18:32:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30097/files) (1):**
  - `vllm/lora/ops/triton_ops/utils.py`

### Summary

**What changed and why**  
This PR extracts LoRA-related changes from a larger PR (#30018) to separate concerns. The changes modify LoRA operation configuration logic to handle batch-invariant cases differently by disabling user-defined configurations and adjusting kernel parameters for improved performance.

**Technical impact**  
The modifications affect LoRA kernel configuration selection and tuning parameters. When batch-invariant mode is active, the system bypasses user-defined configurations and uses optimized default parameters (specifically setting split_k=1), which can improve performance for certain batch sizes and hardware configurations.

**Potential risks**  
The conditional logic based on `is_batch_invariant` introduces a new code path that may behave differently across platforms. The hardcoded split_k value (1) for batch-invariant cases might not be optimal for all hardware configurations or batch sizes, potentially affecting performance in edge cases.

**Key insights**  
This change demonstrates proper separation of concerns by isolating LoRA optimizations from other features. Developers should verify that the batch-invariant detection (`vllm_is_batch_invariant()`) works correctly across all deployment scenarios and consider making the split_k value configurable rather than hardcoded for future flexibility.

---

## 5. [Revert "[SM100] Enable fp8 compute for prefill MLA (#30746)"](https://github.com/vllm-project/vllm/pull/31197)


### Base Information

- **PR Number:** #31197
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 18:15:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31197/files) (3):**
  - `tests/v1/attention/test_mla_backends.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/v1/attention/backends/mla/common.py`

### Summary

**What changed and why**  
This PR reverts commit b10f41c, which enabled FP8 compute for prefill MLA (Multi-Head Latent Attention). The revert is temporary, pending the merge of PR #30993, to address integration issues or dependencies that need to be resolved before FP8 support can be safely enabled.

**Technical impact**  
The changes remove FP8-specific logic from the MLA attention backends, including FP8 dtype handling, FP8-compute paths in prefill operations, and FP8-related parameters in `MLAAttentionSpec`. This reverts the system to using standard data types (e.g., float16, bfloat16) for prefill computations, ensuring stability while the FP8 feature is finalized.

**Potential risks**  
Reverting may temporarily reduce performance for prefill operations that benefited from FP8 compute, particularly on hardware with FP8 acceleration. There is also a risk of reintroducing the same issues if PR #30993 is merged without thorough testing, as the FP8 feature will need to be re-enabled later.

**Key insights**  
Developers should treat this as a temporary rollback to maintain system stability. The FP8 feature should be re-enabled only after PR #30993 is merged and validated. Ensure all FP8-related tests are updated or disabled during the revert to avoid CI failures.

---

## 6. [[ci] Fix Pytorch compilation test oom in 2.10](https://github.com/vllm-project/vllm/pull/31194)


### Base Information

- **PR Number:** #31194
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2025-12-22 17:56:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31194/files) (1):**
  - `tests/compile/test_dynamic_shapes_compilation.py`

### Summary

**What changed and why**  
Added `max_model_len=1024` parameter to the model initialization in a PyTorch compilation test. This reduces the KV cache memory requirement from 16 GiB to 0.44 GiB to prevent OOM errors in CI runners with limited memory (~20 GiB).

**Technical impact**  
The change ensures the test can run within CI memory constraints by limiting the maximum sequence length. It does not affect production code or other tests, as it only modifies a single test configuration for a specific CI environment.

**Potential risks**  
If the test relies on longer sequences to validate dynamic shape compilation behavior, artificially limiting `max_model_len` could mask issues that only appear with larger KV caches. Additionally, this fix assumes the OOM is solely due to KV cache size, but other memory factors could still cause failures.

**Key insights**  
This is a pragmatic fix for CI resource limitations, but developers should ensure the test still validates the intended compilation behavior. Consider adding a comment explaining why `max_model_len` is reduced, and monitor for any future CI memory issues that may require broader adjustments.

---

## 7. [[AMD][CI] fix v1/engine test_preprocess_error_handling](https://github.com/vllm-project/vllm/pull/31192)


### Base Information

- **PR Number:** #31192
- **Author:** [divakar-amd](https://github.com/divakar-amd)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2025-12-22 17:28:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31192/files) (1):**
  - `tests/v1/engine/test_preprocess_error_handling.py`

### Summary

**What changed and why**  
A conditional skip was added to `test_preprocess_error_handling.py` for ROCm platforms. The test relies on the `fork` multiprocessing start method, but ROCm uses `spawn`, causing incompatibility. The change prevents test failures on AMD hardware by skipping execution when `current_platform.is_rocm()` returns true.

**Technical impact**  
This change ensures the test suite passes on ROCm platforms without altering test logic or system behavior. It maintains test coverage for non-ROCm environments while avoiding runtime errors due to multiprocessing method mismatches. The modification is minimal and localized, affecting only this specific test.

**Potential risks**  
Skipping the test on ROCm means error-handling behavior for preprocessing is not validated on AMD platforms, potentially masking platform-specific issues. If the test is ever refactored to support `spawn`, the skip condition may become obsolete and need removal. There is also a risk that similar tests elsewhere in the codebase might require analogous fixes.

**Key insights**  
Developers should verify whether other tests depend on `fork` and consider adding skips or adjustments for ROCm compatibility. The skip reason should be documented clearly, as done here, to aid future maintenance. If possible, explore making the test platform-agnostic by adapting it to work with `spawn`, ensuring consistent validation across all supported hardware.

---

## 8. [[MoE Refactor][7/N] AITER MK](https://github.com/vllm-project/vllm/pull/31102)


### Base Information

- **PR Number:** #31102
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2025-12-22 15:42:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31102/files) (4):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`

### Summary

**What changed and why**  
This PR continues refactoring Mixture of Experts (MoE) to use modular kernels (MKs), specifically converting the ROCm AITER backend into an MK implementation. It also fixes a bug where FP8 Triton was broken on ROCm by adding support for the `torch.float8_e4m3fnuz` data type.

**Technical impact**  
The changes integrate AITER into the modular kernel framework by creating `AiterExperts` class that inherits from `mk.FusedMoEPermuteExpertsUnpermute`. This allows AITER to be called through the same unified interface as other backends (Triton, DeepGEMM, Marlin). The `defer_input_quant` parameter is added to handle AITER's single-kernel quantization approach, and weight shuffling logic is now properly gated by backend type.

**Potential risks**  
The `defer_input_quant=True` parameter creates a divergence in quantization flow between backends. The `AiterExperts` class doesn't support chunking, which may limit scalability. There's also a risk of regression since the AITER implementation path has been significantly restructured, though test results show successful execution.

**Key insights**  
Developers should note that AITER now uses the same modular kernel infrastructure as other backends, improving code consistency. The FP8 data type support expansion (`torch.float8_e4m3fnuz`) is critical for ROCm compatibility. When adding new MoE backends, ensure proper handling of quantization timing differences and workspace management requirements.

---

## 9. [[CI Failure] Disable mosaicml/mpt-7b and databricks/dbrx-instruct tests](https://github.com/vllm-project/vllm/pull/31182)


### Base Information

- **PR Number:** #31182
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 15:40:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31182/files) (3):**
  - `docs/serving/integrations/langchain.md`
  - `tests/models/registry.py`
  - `tests/tokenizers_/test_detokenize.py`

### Summary

**What changed and why**  
This PR disables tests for two deleted Hugging Face models (`mosaicml/mpt-7b` and `databricks/dbrx-instruct`) by marking them as unavailable online in the test registry and removing references in documentation and tokenizer tests. The changes prevent CI failures caused by attempting to fetch models that no longer exist.

**Technical impact**  
The modifications ensure CI stability by preventing network requests to deleted model repositories. The test suite will skip online availability checks for these models, while the documentation example is updated to use an alternative, available model (`Qwen/Qwen3-4B`).

**Potential risks**  
If the disabled tests were critical for validating model-specific behavior, their removal could mask regressions for those model architectures. Additionally, the documentation change assumes `Qwen/Qwen3-4B` is a suitable replacement for all demonstration purposes, which may not hold for all use cases.

**Key insights**  
This is a temporary fix to unblock CI; a long-term solution should involve replacing these models with active alternatives in the test suite. Developers should monitor for similar issues with other external model dependencies and consider adding automated checks for model availability.

---

## 10. [[Perf] Remove blocking copy in GDN Attention](https://github.com/vllm-project/vllm/pull/31167)


### Base Information

- **PR Number:** #31167
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 14:25:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31167/files) (1):**
  - `vllm/v1/attention/backends/gdn_attn.py`

### Summary

**What changed and why**  
The change replaces a blocking device transfer with a non-blocking copy by adding `non_blocking=True` to a tensor movement operation in GDN attention. This modification enables better async scheduling for Qwen3-Next by eliminating a synchronization point that was preventing full pipeline parallelism.

**Technical impact**  
This single-line change allows the CPU-to-GPU tensor copy to occur asynchronously, enabling overlap between computation and data transfer. This improves GPU utilization and pipeline efficiency, particularly for models like Qwen3-Next that benefit from non-blocking operations in multi-GPU setups.

**Potential risks**  
The non-blocking transfer assumes proper synchronization later in the kernel execution; if downstream operations depend on `context_lens_tensor` before the transfer completes, it could cause data races or incorrect results. There's also a risk if the tensor is used immediately after this line without proper CUDA stream synchronization.

**Key insights**  
This is a low-risk optimization that follows standard CUDA best practices for overlapping compute and transfer. Developers should verify that no immediate operations depend on `context_lens_tensor` before async completion and consider applying similar patterns to other blocking copies in attention mechanisms. The 1% speedup at batch size 1 suggests meaningful throughput improvements for inference workloads.

---

## 11. [[Bug] Fix `'CutlassMLAImpl' object has no attribute '_workspace_buffer'`](https://github.com/vllm-project/vllm/pull/31173)


### Base Information

- **PR Number:** #31173
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 14:24:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31173/files) (1):**
  - `vllm/v1/attention/backends/mla/common.py`

### Summary

**What changed and why**  
The fix addresses an `AttributeError` where `CutlassMLAImpl` objects lacked the `_workspace_buffer` attribute when using TRT-LLM ragged attention for DeepSeek models. The changes store the workspace buffer in the `PrefillMetadata` object during metadata construction and pass it through the metadata instead of accessing it directly from the implementation instance.

**Technical impact**  
This change decouples the workspace buffer from the backend implementation, making the buffer accessible via metadata during prefill operations. It ensures compatibility with the TRT-LLM ragged attention flow, particularly for DeepSeek models with expert parallelism enabled, and maintains the existing buffer initialization and usage patterns.

**Potential risks**  
If the workspace buffer is not properly initialized or passed in the metadata, it could lead to null pointer issues or incorrect attention computations. The use of `assert` statements for validation may cause runtime errors in production if metadata is malformed. Additionally, changes to buffer initialization (e.g., `fill_(0)`) could affect performance if called excessively.

**Key insights**  
The fix highlights the importance of ensuring all required resources are propagated through metadata in distributed or parallel execution contexts. Developers should verify that workspace buffers are consistently initialized and passed across all code paths using TRT-LLM ragged attention. Consider adding defensive checks or fallbacks for missing metadata attributes to improve robustness.

---

## 12. [[SpecDecode] Simplified alternative padded-speculation acceptance rate fix](https://github.com/vllm-project/vllm/pull/29845)


### Base Information

- **PR Number:** #29845
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 13:06:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29845/files) (8):**
  - `tests/v1/spec_decode/test_eagle.py`
  - `vllm/v1/attention/backends/mla/common.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/spec_decode/utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces an alternative fix for padded-speculation acceptance rate calculation in speculative decoding. The core change modifies how rejected tokens are handled during draft proposal preparation by computing rejection counts on GPU and adjusting sequence lengths accordingly, avoiding CPU synchronization overhead.

**Technical impact**  
The changes optimize speculative decoding performance by eliminating GPU→CPU synchronization when handling rejected tokens. Key modifications include: 1) Adding GPU-side rejection token tracking in `eagle.py`, 2) Removing CPU sequence length dependencies in MLA attention backends, and 3) Computing maximum sequence length analytically for DCP configurations to minimize workspace allocation.

**Potential risks**  
The CPU-side sequence length invalidation (`_seq_lens_cpu = None`) could cause issues if other code paths rely on these cached values. The DCP max sequence length calculation assumes uniform token distribution across partitions, which may over-allocate memory in imbalanced cases. Kernel changes require careful validation across different hardware and model configurations.

**Key insights**  
Benchmarks show significant throughput improvements (up to 4.5% for K=10), demonstrating the value of reducing synchronization overhead. Developers should verify that all attention backends properly handle the new `max_seq_len` parameter and that no hidden CPU dependencies exist in other code paths. The fix maintains backward compatibility while optimizing a critical performance path.

---

## 13. [[Doc] Add vllm-metal to hardware plugin documentation](https://github.com/vllm-project/vllm/pull/31174)


### Base Information

- **PR Number:** #31174
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-22 12:06:29
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31174/files) (2):**
  - `docs/getting_started/installation/README.md`
  - `docs/getting_started/installation/cpu.apple.inc.md`

### Summary

**What changed and why**  
Added documentation for the vllm-metal hardware plugin in two locations: the main hardware plugin table now includes Apple Silicon (Metal) support, and a tip box was added to the macOS installation guide pointing users to the GPU-accelerated Metal plugin.

**Technical impact**  
These changes improve discoverability of Apple Silicon GPU support by formally listing vllm-metal alongside other hardware backends and providing clear guidance for macOS users seeking GPU acceleration. The documentation now reflects community-driven Metal support as an experimental option.

**Potential risks**  
The plugin is marked as community-maintained, which appropriately sets expectations about support levels. However, users might assume full feature parity with mainline vLLM. The experimental nature should be emphasized to prevent production dependency issues.

**Key insights**  
This documentation update follows established patterns for third-party hardware plugins. Developers should ensure any future macOS-specific features or limitations are documented alongside this reference. Consider adding compatibility notes or version requirements as the plugin matures.

---

## 14. [[SM100] Enable fp8 compute for prefill MLA](https://github.com/vllm-project/vllm/pull/30746)


### Base Information

- **PR Number:** #30746
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [pavanimajety](https://github.com/pavanimajety)
- **Merged time:** 2025-12-22 11:15:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30746/files) (3):**
  - `tests/v1/attention/test_mla_backends.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/v1/attention/backends/mla/common.py`

### Summary

**What changed and why**  
This PR enables FP8 compute for prefill MLA (Multi-Layer Attention) by adding hooks to run FP8 prefill for FlashInfer Cutlass FMHA and TRT-LLM ragged prefill kernels on SM100+ GPUs. The changes are triggered when `--kv-cache-dtype fp8` is enabled, similar to the decode path, and include modifications to support FP8 data types in prefill metadata and kernel execution.

**Technical impact**  
The changes extend FP8 support from decode to prefill for MLA models, allowing reduced memory usage and potentially faster computation for prefill stages. The implementation adds FP8 data type handling in metadata, conditionally converts Q/K/V tensors to FP8 before kernel calls, and updates test configurations to use the new `MLAAttentionSpec` with cache dtype. This aligns prefill with existing decode FP8 support.

**Potential risks**  
- FlashAttention and Cudnn prefill backends do not support FP8 attention (assertions guard this), which could cause confusion or runtime errors if misconfigured.  
- The FP8 conversion adds tensor dtype casts that may introduce overhead or numerical differences if not carefully validated.  
- The dependency on an external FlashInfer PR (#2047) means this feature may not work until that PR is merged and integrated.

**Key insights**  
- Developers should ensure FP8 is only used with supported backends (FlashInfer and TRT-LLM ragged) to avoid assertion failures.  
- Accuracy evaluations (e.g., Math500, MMLU Pro) are critical to validate numerical stability with FP8 prefill.  
- Future optimizations (like fusing RoPE + quant ops) are planned to reduce overhead, so this is an incremental step.

---

## 15. [[MoE Refactor][9/N] Use modular kernel for unquantized Triton MoE](https://github.com/vllm-project/vllm/pull/31052)


### Base Information

- **PR Number:** #31052
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2025-12-22 09:34:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31052/files) (2):**
  - `tests/kernels/moe/test_moe.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`

### Summary

**What changed and why**  
The changes migrate unquantized Triton MoE operations to use the new modular kernel architecture (`TritonExperts`). This refactor replaces the direct call to `fused_experts` with a `FusedMoEModularKernel` instance, centralizing kernel selection and configuration. A test file is updated to initialize the workspace manager and call a post-load weight processing method.

**Technical impact**  
This introduces a more modular and maintainable kernel abstraction, decoupling kernel implementation from the MoE layer logic. The `FusedMoEModularKernel` now handles kernel dispatch, which simplifies future extensions (e.g., adding new backends or quantization). The test update ensures proper environment setup and weight processing for the new kernel path.

**Potential risks**  
The test includes a FIXME comment indicating that `self.kernel` assignment in `FusedMoE.__init__` may be incomplete or misaligned, which could lead to initialization issues. There’s also a risk if `process_weights_after_loading` is called incorrectly or if the modular kernel’s interface doesn’t fully match the previous `fused_experts` signature (e.g., the removed `quant_config` parameter).

**Key insights**  
Developers should verify that `FusedMoEModularKernel` correctly initializes and that the removed `quant_config` parameter is handled internally by `TritonExperts`. The FIXME must be addressed to ensure robust kernel assignment. This refactor aligns with a cleaner separation of concerns but requires thorough validation of all MoE configurations, especially for edge cases like expert parallelism.

---

## 16. [[ROCm][CI/Build] Fix triton version to one that has triton_kernels required for gpt-oss to run](https://github.com/vllm-project/vllm/pull/31159)


### Base Information

- **PR Number:** #31159
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2025-12-22 09:19:27
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31159/files) (1):**
  - `docker/Dockerfile.rocm_base`

### Summary

**What changed and why**  
The change updates the Triton commit hash from `a272dfa8` to `57c693b6` in the ROCm base Dockerfile. This fixes an `ImportError` related to missing `GFX950MXScaleLayout` in Triton kernels when running GPT-OSS on gfx950 hardware, ensuring compatibility with the updated `rocm/vllm-dev:base` image.

**Technical impact**  
This modifies the Triton version used in the build environment, directly affecting kernel availability for specific AMD GPU architectures (gfx950). The change aligns the Triton dependency with the required feature set, preventing runtime import failures in downstream applications like GPT-OSS.

**Potential risks**  
The new commit may introduce unintended behavioral changes or regressions in Triton kernels for other GPU architectures. If the commit is not thoroughly tested across all supported ROCm targets, it could lead to new compatibility issues or performance variations.

**Key insights**  
This is a targeted fix for a specific hardware/software compatibility issue. Developers should verify that the new Triton commit is stable across all intended ROCm deployments and consider adding integration tests for gfx950 to catch similar issues early. The change highlights the importance of version pinning for Triton in heterogeneous GPU environments.

---

## 17. [[UX] improve profiler error message](https://github.com/vllm-project/vllm/pull/31125)


### Base Information

- **PR Number:** #31125
- **Author:** [BoyuanFeng](https://github.com/BoyuanFeng)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 08:46:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31125/files) (1):**
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The PR improves an error message in the GPU worker's `profile()` method. Previously, the error simply stated "Profiling is not enabled." Now it provides actionable guidance by suggesting the user set `--profiler-config` with a concrete example.

**Technical impact**  
This change enhances user experience by making error messages more informative and actionable. It does not affect system behavior, performance, or architecture—only the error output when profiling is attempted without proper configuration.

**Potential risks**  
The example path (`YOUR_DIR_PATH_TO_DUMP_TRACE`) uses a placeholder that users must replace, which could still cause confusion if not understood. Additionally, the message assumes the user is using the torch profiler, though other profiler types might be supported.

**Key insights**  
Always provide actionable error messages with clear next steps. Consider whether the example should be dynamically generated based on available profiler options or if a link to documentation would be beneficial. Ensure placeholder text (like `YOUR_DIR_PATH_TO_DUMP_TRACE`) is intuitively replaceable.

---

## 18. [[ROCm] [Critical]: Remove unused variable](https://github.com/vllm-project/vllm/pull/31156)


### Base Information

- **PR Number:** #31156
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 08:28:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31156/files) (1):**
  - `csrc/fused_qknorm_rope_kernel.cu`

### Summary

**What changed and why**  
Removed an unused variable `rotary_dim` that was declared but never referenced in the code. This fixes a ROCm build failure because the ROCm compiler flags include `-Werror=unused-variable`, which treats unused variables as errors.

**Technical impact**  
The change eliminates a compiler error without affecting runtime behavior, as the variable was not used. It ensures the ROCm build passes while maintaining functional correctness of the fused QK normalization and rotary position embedding kernel.

**Potential risks**  
Minimal risk since the variable was entirely unused. However, developers should verify that `rotary_dim` wasn't intended for future use or debugging. The removal could mask a deeper issue if the variable was meant to be utilized but was omitted due to an oversight.

**Key insights**  
Always clean up unused variables to prevent build failures, especially in strict compilation environments. Consider enabling similar warnings in non-ROCm builds to catch such issues early. Review related code to ensure no missing logic depended on `rotary_dim`.

---

## 19. [[AMD][CI] Add "V1 Test e2e + engine" to mi325_8 Agent Pool](https://github.com/vllm-project/vllm/pull/31040)


### Base Information

- **PR Number:** #31040
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2025-12-22 07:41:56
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31040/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The PR moves the "V1 Test e2e + engine" CI job from the `mi325_4` agent pool to the `mi325_8` pool. This change aims to resolve intermittent "agent lost" failures observed when this test group ran concurrently with other resource-intensive tests on the same 8-GPU node, causing resource contention.

**Technical impact**  
By scheduling the test on an 8-GPU machine (instead of sharing a node), the job will have exclusive access to the node's resources, eliminating interference from other concurrent tests. This should stabilize the test execution and reduce flaky failures without altering the test logic or runtime requirements.

**Potential risks**  
Using 8-GPU machines for a 4-GPU test may reduce overall CI resource utilization efficiency, potentially increasing queue times or costs if the pool is limited. Additionally, if the root cause is more complex (e.g., driver or hardware issues), this change might not fully resolve future failures.

**Key insights**  
This is a pragmatic fix for resource contention, but consider monitoring for similar issues in other test groups. If the problem persists, further investigation into the underlying cause (e.g., GPU memory or ROCm driver stability) may be needed. Ensure the `mi325_8` pool has sufficient capacity to avoid bottlenecks.

---

## 20. [[CI][Bugfix] Fix `entrypoints/openai/test_audio.py`](https://github.com/vllm-project/vllm/pull/31151)


### Base Information

- **PR Number:** #31151
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 07:21:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31151/files) (1):**
  - `tests/entrypoints/openai/test_audio.py`

### Summary

**What changed and why**  
The change modifies a single test case in the OpenAI audio API test suite by updating the text prompt from "What's happening in this audio?" to "What's a short title for this audio?". This appears to address an issue related to test behavior, likely aligning with a fix for the referenced GitHub issue #30878.

**Technical impact**  
This is a minimal change that only affects a single test's input data. It should not impact the core functionality of the audio processing or chat streaming features, but ensures the test uses a more appropriate or less ambiguous prompt for the audio context, potentially improving test reliability or specificity.

**Potential risks**  
The risk is very low since this is purely a test change. However, if the test was originally designed to validate a specific response format or behavior tied to the old prompt, the new prompt might inadvertently alter the expected test outcomes or coverage.

**Key insights**  
Developers should verify that the updated prompt still adequately tests the intended functionality (e.g., audio input handling in streaming chat). This change highlights the importance of precise test data to avoid ambiguity in test assertions, especially for multimodal inputs like audio.

---

## 21. [[BugFix] skip language model in Encoder](https://github.com/vllm-project/vllm/pull/30242)


### Base Information

- **PR Number:** #30242
- **Author:** [Bounty-hunter](https://github.com/Bounty-hunter)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 05:25:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30242/files) (8):**
  - `examples/online_serving/disaggregated_encoder/README.md`
  - `vllm/config/model.py`
  - `vllm/model_executor/model_loader/utils.py`
  - `vllm/model_executor/models/adapters.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR adds support for vision-language (VL) encoder-only models by introducing a new `--convert "mm_encoder_only"` option. The change allows skipping the language model initialization in VL models to reduce GPU memory usage for encoder instances in disaggregated serving setups, addressing the memory issue described in GitHub issue #30231.

**Technical impact**  
The implementation creates a new model adapter (`as_mm_encoder_only_model`) that modifies VL model classes to exclude their language model components during initialization and weight loading. This reduces memory footprint by preventing the loading of unnecessary LM parameters while maintaining the visual encoder functionality. The changes affect model loading logic, VL model interfaces, and GPU model runner execution paths.

**Potential risks**  
The approach relies on monkey-patching class initialization methods (`__init__` and `AutoWeightsLoader.__init__`), which could lead to subtle bugs if not properly reverted in exception scenarios. There's also a risk that models not implementing the required `get_language_model_spec()` method will fail at runtime. The dummy run bypass for encoder-only models might mask initialization issues that would normally be caught during warmup.

**Key insights**  
VL models must implement the `get_language_model_spec()` interface to use this feature. The memory savings are significant (demonstrated by reduced GPU memory usage in test results). Developers should ensure proper error handling around the temporary class modifications and consider adding validation for the encoder-only execution path to catch potential issues early.

---

## 22. [[CI] add polling for precompiled wheel in python_only_compile.sh, fix index generation for releases](https://github.com/vllm-project/vllm/pull/30781)


### Base Information

- **PR Number:** #30781
- **Author:** [Harry-Chen](https://github.com/Harry-Chen)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2025-12-22 05:24:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30781/files) (3):**
  - `.buildkite/scripts/generate-nightly-index.py`
  - `.buildkite/scripts/upload-wheels.sh`
  - `tests/standalone_tests/python_only_compile.sh`

### Summary

**What changed and why**  
This PR addresses two CI-related issues: 1) It adds polling logic in `python_only_compile.sh` to wait for a compatible precompiled wheel (matching the current architecture) before running tests, resolving a timing issue where CI tests run before the release pipeline completes. 2) It fixes index generation for releases by adding a `--wheel-dir` parameter to ensure generated indices point to the correct wheel paths in S3.

**Technical impact**  
The polling enhancement makes the test script more robust by verifying both JSON validity and architecture compatibility, reducing false failures. The index generation fix corrects path mappings in release artifacts, ensuring wheels are discoverable via the proper S3 directory structure. Both changes improve CI reliability and artifact consistency.

**Potential risks**  
The extended polling interval (5 minutes vs 3) and additional checks could slightly increase CI runtime if wheels are delayed. The architecture check assumes `platform.machine()` output is contained within the `platform_tag` string, which may not hold for all wheel naming conventions. The wheel directory fix relies on correct parameter passing in the release pipeline.

**Key insights**  
Developers should verify the architecture matching logic aligns with actual wheel naming patterns. The polling approach is a practical workaround for pipeline dependencies; consider implementing proper inter-pipeline dependencies in Buildkite when feasible. Ensure the `--wheel-dir` parameter is consistently used across all release workflows to maintain correct index paths.

---

## 23. [[gpt-oss] Fix harmony parser in streaming responses](https://github.com/vllm-project/vllm/pull/30205)


### Base Information

- **PR Number:** #30205
- **Author:** [AlonKejzman](https://github.com/AlonKejzman)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2025-12-22 04:56:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30205/files) (1):**
  - `vllm/entrypoints/openai/serving_chat.py`

### Summary

**What changed and why**  
The fix addresses a bug where final streaming tokens were missed in speculative decoding when using the harmony parser. Specifically, it handles edge cases where multiple tokens (including the final one) are generated simultaneously, resulting in text deltas but empty channel states.

**Technical impact**  
This change ensures the OpenAI chat completion streaming generator correctly identifies final tokens in batch generation scenarios. It modifies the channel assignment logic to mark tokens with content but no channel as "final", preventing loss of terminal output in streaming responses.

**Potential risks**  
The hardcoded "final" channel string could conflict if the harmony parser uses this as a legitimate channel value. There's also a risk of incorrectly marking non-final tokens if the parser's state management has other edge cases not covered by this fix.

**Key insights**  
The fix demonstrates careful handling of state mismatches between token generation and parser state tracking. Developers should verify this doesn't interfere with other parser states and consider adding tests for multi-token generation edge cases. The solution is minimal but addresses a critical streaming completeness issue.

---

## 24. [[Model] Fix bagel failed to run](https://github.com/vllm-project/vllm/pull/31132)


### Base Information

- **PR Number:** #31132
- **Author:** [Potabk](https://github.com/Potabk)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2025-12-22 02:15:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31132/files) (1):**
  - `vllm/model_executor/models/bagel.py`

### Summary

**What changed and why**  
The PR renames the `get_multimodal_embeddings` method to `embed_multimodal` in the Bagel model implementation. This change aligns the method name with the expected interface used by `modelrunner`, resolving a mismatch that caused the model to fail during execution.

**Technical impact**  
This is a straightforward interface correction that ensures the Bagel model conforms to the expected multimodal embedding API contract. The change is minimal and localized, affecting only the method signature while preserving the underlying functionality and return type (`MultiModalEmbeddings`).

**Potential risks**  
If any other components directly call `get_multimodal_embeddings` by name (e.g., via reflection or dynamic imports), they will break. Additionally, the PR description lacks test details, so it's unclear whether the fix has been validated beyond resolving the immediate interface mismatch.

**Key insights**  
Always verify that model implementations adhere to defined interfaces, especially when integrating new models. The fix is correct but highlights the need for better test coverage in the PR—consider adding a unit test or integration test to confirm the model runs successfully after the change.

---

