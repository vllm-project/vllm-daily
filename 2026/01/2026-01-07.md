# vLLM Merged PR Report

**Report Date:** 2026-01-07 PST

**Total Merged PRs:** 46

---

## 1. [[ROCm]Skip test_torchao.py::test_pre_quantized_model on CDNA3 arch](https://github.com/vllm-project/vllm/pull/31905)


### Base Information

- **PR Number:** #31905
- **Author:** [ZhiweiYan-96](https://github.com/ZhiweiYan-96)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 23:47:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31905/files) (1):**
  - `tests/quantization/test_torchao.py`

### Summary

**What changed and why**  
A skip condition was added to `test_pre_quantized_model` to exclude execution on ROCm CDNA3 architectures (e.g., MI325, MI300X). This is necessary because CDNA3 only supports the `fp8_e4m3_fnuz` data type, while the test uses checkpoints with `fp8_e4m3`, which would cause failures.

**Technical impact**  
The change prevents test failures on unsupported hardware by conditionally skipping the test. It leverages existing platform detection utilities (`current_platform.is_rocm()` and `current_platform.is_fp8_fnuz()`) to identify CDNA3 architectures, ensuring the test suite remains robust across different ROCm generations.

**Potential risks**  
If the platform detection logic is inaccurate or changes, the test may be incorrectly skipped or executed. Additionally, this skip might mask broader compatibility issues with `fp8_e4m3` on CDNA3, potentially delaying fixes if future checkpoints or models rely on similar data types.

**Key insights**  
This is a targeted, hardware-specific workaround that maintains test suite stability. Developers should ensure platform detection is reliable and consider documenting CDNA3’s `fp8_e4m3_fnuz` limitation in relevant quantization or hardware support guides. Future updates to test checkpoints should align with supported data types per architecture.

---

## 2. [[docker] A follow-up patch to fix #30913: `[docker] install cuda13 version of lmcache and nixl`](https://github.com/vllm-project/vllm/pull/31775)


### Base Information

- **PR Number:** #31775
- **Author:** [wangshangsam](https://github.com/wangshangsam)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-07 23:47:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31775/files) (1):**
  - `docker/Dockerfile`

### Summary

**What changed and why**  
Two lines were added to the Dockerfile: an `ARG` declaration for `torch_cuda_arch_list` with a default value, and an `ENV` declaration that sets the `TORCH_CUDA_ARCH_LIST` environment variable from that argument. This fixes a build error that occurred when building CUDA 13 images because the `lmcache` installation requires this environment variable, which was inadvertently removed in a previous change.

**Technical impact**  
This change restores the `TORCH_CUDA_ARCH_LIST` environment variable during the Docker build process, specifically for the step that installs `kv_connectors`. It ensures that PyTorch extensions (like those in `lmcache`) are compiled with the correct CUDA architecture support, allowing the Docker image to build successfully for CUDA 13 targets.

**Potential risks**  
The default architecture list (`'7.0 7.5 8.0 8.9 9.0 10.0 12.0'`) may not be optimal for all build scenarios, especially for newer GPU architectures. If users override the `torch_cuda_arch_list` build argument incorrectly, it could lead to incompatible binaries or extended build times. There is also a minor risk of environment variable pollution if this `ENV` is set too broadly in the Dockerfile, though it's currently scoped to the relevant `RUN` instruction.

**Key insights**  
Always verify that required build-time environment variables are preserved when refactoring Dockerfiles. Consider documenting the purpose of `torch_cuda_arch_list` and how to override it for specific hardware. For future changes, ensure that any removal of `ARG` or `ENV` directives is evaluated for downstream dependencies in multi-stage builds.

---

## 3. [fix(rocm): Add get_supported_kernel_block_sizes() to ROCM_ATTN](https://github.com/vllm-project/vllm/pull/31712)


### Base Information

- **PR Number:** #31712
- **Author:** [rabi](https://github.com/rabi)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 23:46:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31712/files) (1):**
  - `vllm/v1/attention/backends/rocm_attn.py`

### Summary

**What changed and why**  
Added a `get_supported_kernel_block_sizes()` method to the `RocmAttentionBackend` class that explicitly returns `[16, 32]`. This corrects the kernel's previous default behavior of claiming support for any block size, which caused `select_common_block_size()` to select incompatible large block sizes (like 256) for certain models, leading to kernel failures.

**Technical impact**  
This change properly constrains the ROCm attention kernel to its actual hardware-supported block sizes due to shared memory (LDS) limitations on AMD GPUs. It ensures the block size selection mechanism works correctly, allowing hybrid models like Nemotron-H to use a compatible block size (32) instead of an unsupported one.

**Potential risks**  
If future ROCm kernel development adds support for additional block sizes, this hardcoded list will need to be updated. There is also a risk if other parts of the system assume more flexible block size support, but this is mitigated by aligning the code with the documented hardware constraints in the CUDA kernel macro.

**Key insights**  
The fix is minimal and targeted, directly addressing a mismatch between advertised and actual kernel capabilities. Developers should be aware that ROCm attention kernels have specific block size limitations (16 and 32) due to AMD GPU architecture. This pattern of explicitly declaring backend constraints is crucial for correct system-wide resource selection.

---

## 4. [[Model] Enable LoRA support for tower and connector in GLM4-V](https://github.com/vllm-project/vllm/pull/31652)


### Base Information

- **PR Number:** #31652
- **Author:** [Zyyeric](https://github.com/Zyyeric)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 23:45:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31652/files) (1):**
  - `vllm/model_executor/models/glm4_1v.py`

### Summary

**What changed and why**  
Two new methods were added to the GLM4-V model class to support LoRA (Low-Rank Adaptation) for its vision tower and connector components. These methods calculate token counts before and after spatial merging, ensuring LoRA layers are correctly sized when processing vision tokens across different stages of the multimodal pipeline.

**Technical impact**  
The changes enable LoRA to dynamically adjust input dimensions based on the spatial merge configuration. This ensures compatibility between the vision tower's patch-level tokens and the connector's merged tokens, maintaining proper tensor shapes during LoRA fine-tuning without breaking the spatial merge operation.

**Potential risks**  
If `merge_size` is zero or not properly initialized, division by zero or incorrect token counts could occur. Additionally, integer division in `get_num_mm_connector_tokens` may truncate results if `num_vision_tokens` is not perfectly divisible by `merge_size**2`, potentially leading to mismatched tensor dimensions.

**Key insights**  
Always validate `merge_size` and ensure `num_vision_tokens` is a multiple of `merge_size**2` to prevent dimension mismatches. Consider adding assertions or error handling for edge cases. These methods are critical for LoRA support but should be tested with varying spatial merge configurations.

---

## 5. [[Bugfix] Remove the num_hidden_layers override for glm4_moe](https://github.com/vllm-project/vllm/pull/31745)


### Base Information

- **PR Number:** #31745
- **Author:** [andyl98](https://github.com/andyl98)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 23:45:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31745/files) (1):**
  - `vllm/config/speculative.py`

### Summary

**What changed and why**  
Removed the `"num_hidden_layers": 0` override for the `Glm4MoeMTPModel` configuration. This fixes a mismatch where the override incorrectly set the number of hidden layers to zero, while the actual model checkpoint weights are indexed based on the base model's true layer count (e.g., `model.layers.92`).

**Technical impact**  
The correction ensures the model configuration accurately reflects the underlying architecture, allowing proper layer indexing and weight loading. This prevents potential initialization errors or runtime failures when the model attempts to access layers that the configuration claimed didn't exist.

**Potential risks**  
If any downstream code or logic depended on the `num_hidden_layers` being zero for this model type, it could now break. There is also a risk that the checkpoint weights themselves might have been saved with a different layer expectation, though the description suggests the checkpoint is correct.

**Key insights**  
Always verify configuration overrides against the actual checkpoint structure. This fix highlights the importance of aligning configuration parameters with the physical model architecture to ensure successful weight loading and model execution. Developers should audit other similar overrides for consistency.

---

## 6. [[Fix] Enable mm_processor_cache with vision LoRA](https://github.com/vllm-project/vllm/pull/31927)


### Base Information

- **PR Number:** #31927
- **Author:** [prashanth058](https://github.com/prashanth058)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 23:31:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31927/files) (5):**
  - `tests/multimodal/test_cache.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/multimodal/cache.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The changes enable the multi-modal processor cache to work with vision LoRA adapters. Previously, the cache was disabled when `enable_tower_connector_lora` was active due to a conflict: the encoder cache uses a LoRA-prefixed identifier, but the processor cache should be shared across different LoRAs. The solution introduces a separate `mm_hash` field in `MultiModalFeatureSpec` to store the base hash (without LoRA prefix), allowing the processor cache to use this shared hash while the encoder cache continues using the prefixed identifier.

**Technical impact**  
This modification decouples the cache key strategies for the processor and encoder caches. The processor cache now uses `mm_hash` (or falls back to `identifier` for backward compatibility) to enable cache sharing across different LoRA adapters for the same base multimodal content. The encoder cache remains isolated per LoRA via the prefixed `identifier`. This improves efficiency by reusing processed features across LoRAs and removes the previous restriction that forced `mm_processor_cache_gb` to be zero when LoRA was enabled.

**Potential risks**  
If `mm_hash` is not correctly populated or conflicts arise (e.g., hash collisions), cache hits/misses could be incorrect, leading to data corruption or performance degradation. The fallback to `identifier` when `mm_hash` is `None` must be handled carefully to maintain backward compatibility. Additionally, the test coverage focuses on a specific scenario; edge cases involving multiple modalities or dynamic hash generation may need further validation.

**Key insights**  
Developers should ensure that `mm_hash` is consistently derived from the base content (without LoRA prefixes) across all code paths. The removal of the compatibility check in `arg_utils.py` is safe only if the cache logic correctly handles LoRA scenarios. When extending this feature, verify that both caches (processor and encoder) are independently validated for correctness under various LoRA configurations.

---

## 7. [[BugFix] Fix spec decoding edge case bugs](https://github.com/vllm-project/vllm/pull/31944)


### Base Information

- **PR Number:** #31944
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 23:31:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31944/files) (3):**
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/worker/gpu_input_batch.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR fixes two edge case bugs in speculative decoding. First, it clears `spec_token_ids` when a request is preempted to prevent stale draft tokens from causing issues upon resumption. Second, it addresses cases where requests are temporarily excluded from batches (but not preempted) by ensuring speculative tokens are properly handled when requests resume, via a new method `update_req_spec_token_ids`.

**Technical impact**  
The changes centralize speculative token handling into `update_req_spec_token_ids`, reducing code duplication and improving maintainability. The logic for updating `num_computed_tokens` is now correctly gated by `self.use_async_scheduling`, preventing misbehavior in synchronous modes. Additionally, the fix ensures draft token counts are only set for requests that have completed prompt processing, aligning with speculative decoding requirements.

**Potential risks**  
If `update_req_spec_token_ids` is called incorrectly (e.g., with invalid indices), it could lead to data corruption in `token_ids_cpu`. The removal of the `else` clause in `num_decode_draft_tokens` assignment might inadvertently leave some entries uninitialized if not all request indices are covered, potentially causing undefined behavior in downstream logic.

**Key insights**  
Developers should ensure `update_req_spec_token_ids` is only invoked for valid request indices and that `num_decode_draft_tokens` is properly initialized for all requests. The consolidation of speculative token management is a positive step, but thorough testing of preemption and resumption scenarios—especially with structured output—is critical to validate these fixes.

---

## 8. [[grpc] Support gRPC server entrypoint](https://github.com/vllm-project/vllm/pull/30190)


### Base Information

- **PR Number:** #30190
- **Author:** [CatherineSue](https://github.com/CatherineSue)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-07 23:24:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30190/files) (12):**
  - `.gitignore`
  - `mkdocs.yaml`
  - `pyproject.toml`
  - `requirements/build.txt`
  - `requirements/common.txt`
  - `requirements/test.txt`
  - `setup.py`
  - `tests/entrypoints/test_grpc_server.py`
  - `vllm/entrypoints/grpc_server.py`
  - `vllm/grpc/__init__.py`
  - `vllm/grpc/compile_protos.py`
  - `vllm/grpc/vllm_engine.proto`

### Summary

**What changed and why**  
This PR adds gRPC server support to vLLM, enabling binary protocol communication via gRPC/Protobuf instead of HTTP/JSON. The primary motivation is to improve integration with upstream applications and routing layers (like `sgl-model-gateway`), reduce serialization overhead, leverage HTTP/2 multiplexing, and bypass Python GIL bottlenecks by moving tokenization to Rust.

**Technical impact**  
The changes introduce a new gRPC server entrypoint (`grpc_server.py`) with a corresponding protocol definition (`vllm_engine.proto`). The build process now automatically compiles Protobuf files during package installation, and the server integrates with the existing `AsyncLLM` engine. This expands vLLM's serving capabilities beyond HTTP/REST, offering a more efficient binary interface for high-concurrency scenarios.

**Potential risks**  
Generated Protobuf files are excluded from linting and documentation, which could hide issues in auto-generated code. The `Embed` RPC is marked as unimplemented, which may confuse users expecting full feature parity. There's also a risk of version mismatches with `grpcio-tools` and `protobuf` dependencies, especially given the pinned versions in requirements files.

**Key insights**  
The gRPC server demonstrates significant performance gains (37% higher throughput, 26% lower p99 TTFT) in high-concurrency tests. Developers should note that the server returns token IDs instead of text, requiring clients to handle detokenization. Ensure `grpcio-tools` is installed for local development, and consider that the generated Protobuf files are intentionally ignored by tooling to avoid false positives.

---

## 9. [[chore] Update FA commit](https://github.com/vllm-project/vllm/pull/30460)


### Base Information

- **PR Number:** #30460
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-07 23:24:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30460/files) (1):**
  - `cmake/external_projects/vllm_flash_attn.cmake`

### Summary

**What changed and why**  
Updated the Git commit hash for the `vllm-flash-attn` dependency from `86f8f157cf82aa2342743752b97788922dd7de43` to `188be16520ceefdc625fdf71365585d2ee348fe2`. This change incorporates two upstream FlashAttention pull requests (PRs 110 and 112) into the vLLM build, likely to include bug fixes, performance improvements, or new features from the upstream repository.

**Technical impact**  
The change modifies the specific version of the external FlashAttention library that vLLM will compile and link against. This could affect kernel behavior, performance characteristics, or compatibility with certain hardware or operations, depending on the nature of the upstream PRs. The build system will now fetch and compile the newer commit during the next build.

**Potential risks**  
If the new FlashAttention commit introduces breaking changes, regressions, or untested edge cases, it could impact vLLM's stability or correctness. There is also a risk if the new commit has not been thoroughly validated within the vLLM context, as external dependency updates can sometimes cause subtle integration issues.

**Key insights**  
Always verify that the upstream changes (PRs 110 and 112) are compatible with vLLM's use cases and have been tested in relevant environments. Consider running integration tests or benchmarks to confirm no performance degradation or functional regressions. Keeping external dependencies updated is good practice, but should be paired with appropriate validation.

---

## 10. [[platform] add dp_metadata arg to set_additional_forward_context](https://github.com/vllm-project/vllm/pull/31942)


### Base Information

- **PR Number:** #31942
- **Author:** [Ronald1995](https://github.com/Ronald1995)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 22:56:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31942/files) (1):**
  - `vllm/forward_context.py`

### Summary

**What changed and why**  
The change adds the `dp_metadata` argument to the `set_additional_forward_context` function call within `set_forward_context`. This prevents other platforms from needing to redundantly call `coordinate_batch_across_dp` when they require `num_tokens_across_dp`, as they can now access it directly from the provided `dp_metadata`.

**Technical impact**  
This modification enhances the API by providing a complete set of distributed processing metadata to downstream platform-specific context setters. It eliminates unnecessary inter-process communication overhead by allowing platforms to reuse the already-computed `dp_metadata` instead of recalculating it.

**Potential risks**  
If `set_additional_forward_context` implementations in other platforms were already handling the missing `dp_metadata` gracefully (e.g., with a default argument), this change is low-risk. However, if any implementation strictly expected only the previous set of arguments, this could cause a `TypeError` due to the new positional argument.

**Key insights**  
This is a straightforward, backward-compatible enhancement that improves efficiency. Developers integrating with this API should update their `set_additional_forward_context` implementations to accept the new `dp_metadata` parameter, even if unused, to maintain compatibility. The change aligns with good practice by providing all necessary context to avoid redundant computations.

---

## 11. [[Model] Enable LoRA support for tower and connector in DotsOCR](https://github.com/vllm-project/vllm/pull/31825)


### Base Information

- **PR Number:** #31825
- **Author:** [ShaanveerS](https://github.com/ShaanveerS)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-07 22:50:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31825/files) (2):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/dots_ocr.py`

### Summary

**What changed and why**  
Two methods were added to `DotsOCRForCausalLM` to enable LoRA support for the multimodal vision tower and connector. These methods calculate token counts before and after spatial merging, addressing the discrepancy between vision tower tokens and language model image tokens introduced by the `spatial_merge_size` parameter.

**Technical impact**  
The changes allow dynamic LoRA adapters to correctly scale based on the actual token dimensions in the vision pipeline. This ensures LoRA parameters align with the transformer layers they modify, maintaining model integrity when spatial merging is applied.

**Potential risks**  
If `spatial_merge_size` is not a perfect square or if `num_vision_tokens` is not divisible by `merge_size**2`, the division in `get_num_mm_connector_tokens` could produce non-integer results, leading to token count mismatches. Additionally, the methods assume `vision_tower` has a `spatial_merge_size` attribute, which may not be present in all configurations.

**Key insights**  
Developers should validate that `spatial_merge_size` is defined and that token counts are compatible with the merge operation. Consider adding integer validation or rounding logic to prevent silent errors. The update to the documentation correctly reflects the new LoRA support status for the model.

---

## 12. [[ROCm][CI] v1 cpu offloading attention backend fix](https://github.com/vllm-project/vllm/pull/31833)


### Base Information

- **PR Number:** #31833
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 22:37:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31833/files) (1):**
  - `tests/v1/kv_offload/test_cpu_offloading.py`

### Summary

**What changed and why**  
This PR fixes a regression on ROCm platforms by adjusting the attention backend list in a test file. The issue stemmed from a previous PR (#30687) that introduced `FLASH_ATTN` as a default backend, which is unsupported on ROCm. The change ensures ROCm uses only `TRITON_ATTN`, while CUDA platforms retain all three backends (`FLASH_ATTN`, `FLASHINFER`, `TRITON_ATTN`).

**Technical impact**  
The modification localizes the fix to test configuration, preventing test failures on ROCm due to unsupported backends. It maintains existing test coverage for CUDA and correctly isolates ROCm to a compatible backend, ensuring CI stability without affecting production code paths.

**Potential risks**  
If other platforms (e.g., CPU-only or future accelerators) are added without updating this logic, they may inherit an empty `ATTN_BACKENDS` list, potentially skipping relevant tests. Additionally, hardcoded backend lists could become outdated if support changes, requiring manual updates.

**Key insights**  
This is a targeted, correct fix for a CI regression, but consider extracting backend compatibility logic into a shared utility to avoid duplication and simplify maintenance. Verify that all test scenarios for ROCm are adequately covered with `TRITON_ATTN` alone, and monitor for similar issues in other test files.

---

## 13. [[Doc] Add Claude code usage example](https://github.com/vllm-project/vllm/pull/31188)


### Base Information

- **PR Number:** #31188
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-07 21:50:23
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31188/files) (2):**
  - `docs/assets/deployment/claude-code-example.png`
  - `docs/serving/integrations/claude_code.md`

### Summary

**What changed and why**  
Added new documentation (`docs/serving/integrations/claude_code.md`) with a usage example for integrating Claude Code with vLLM. The purpose is to guide users on configuring Claude Code to use vLLM-served models as a backend, enabling local/private coding assistance with open-weight models.

**Technical impact**  
This documentation extends vLLM's integration support by providing clear instructions for leveraging its Anthropic Messages API compatibility. It enables users to replace Anthropic's API with vLLM for Claude Code, broadening the tool's applicability to custom models with tool-calling capabilities.

**Potential risks**  
The guide assumes users have a model with robust tool-calling support, which may not be available for all vLLM-served models. Incorrect `--tool-call-parser` configuration could lead to failed tool calls. Additionally, the note about model names not containing `/` may cause confusion if not carefully followed.

**Key insights**  
Ensure the model explicitly supports tool calling and test with simple prompts before complex tasks. Developers should verify the `--served-model-name` aligns with environment variables and consult the Tool Calling documentation for model-specific parser flags. This integration enhances vLLM's versatility but requires precise configuration.

---

## 14. [[CI][BugFix][AMD] Actually skip tests marked @pytest.mark.skip_v1](https://github.com/vllm-project/vllm/pull/31873)


### Base Information

- **PR Number:** #31873
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 21:06:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31873/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The change modifies the AMD CI configuration to add a pytest marker filter (`-m 'not skip_v1'`) when running sampler tests. This ensures tests decorated with `@pytest.mark.skip_v1` are skipped, addressing intermittent failures in AMD CI that occur because these tests were not being filtered out as intended.

**Technical impact**  
This update aligns the AMD CI behavior with the intended test-skipping logic for v1-specific issues, preventing unreliable beam search tests from executing. It also consolidates two separate pytest commands into one, simplifying the test execution step while maintaining coverage for non-skipped tests.

**Potential risks**  
If the `skip_v1` marker is applied inconsistently across tests, some tests might be incorrectly skipped or executed. Additionally, removing the `VLLM_USE_FLASHINFER_SAMPLER=1` variant could reduce test coverage for the FlashInfer sampler integration unless it is covered elsewhere.

**Key insights**  
The fix is a temporary workaround; a permanent solution should address the root cause of the unreliable beam search behavior. Developers should verify that the `skip_v1` marker is correctly applied to all relevant tests and ensure FlashInfer sampler testing is not inadvertently omitted from CI pipelines.

---

## 15. [[ROCm][CI] Add rocm support for run-multi-node-test.sh](https://github.com/vllm-project/vllm/pull/31922)


### Base Information

- **PR Number:** #31922
- **Author:** [charlifu](https://github.com/charlifu)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 20:36:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31922/files) (1):**
  - `.buildkite/scripts/run-multi-node-test.sh`

### Summary

**What changed and why**  
The PR adds ROCm support to the `run-multi-node-test.sh` script by detecting ROCm environments and using appropriate device flags for Docker containers. Previously, the script only supported CUDA via the `--gpus` flag.

**Technical impact**  
The script now conditionally sets `GPU_DEVICES` to either NVIDIA's `--gpus "device=..."` syntax or ROCm's `--device /dev/kfd --device /dev/dri -e HIP_VISIBLE_DEVICES=...` format. This enables multi-node testing on both NVIDIA and AMD GPU platforms within CI pipelines.

**Potential risks**  
The detection logic uses multiple indicators (device files, directories, commands, environment variables), which could lead to false positives if partial ROCm installations exist. The script assumes all nodes in a multi-node test have identical GPU architectures (either all ROCm or all CUDA), which may not hold in heterogeneous environments.

**Key insights**  
The implementation cleanly separates CUDA and ROCm device configuration paths. Developers should ensure ROCm Docker images properly expose `/dev/kfd` and `/dev/dri` devices. Consider adding validation to confirm all nodes in a test share the same GPU platform type to prevent configuration mismatches.

---

## 16. [[ROCm][CI] Fix attention backend test flakiness from uninitialized KV cache memory](https://github.com/vllm-project/vllm/pull/31928)


### Base Information

- **PR Number:** #31928
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 20:35:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31928/files) (1):**
  - `tests/v1/attention/test_attention_backends.py`

### Summary

**What changed and why**  
The change replaces `torch.empty()` with `torch.zeros()` in the `create_and_prepopulate_kv_cache` function to initialize KV cache memory. This addresses intermittent test failures caused by reading uninitialized memory, which was particularly problematic on ROCm where uninitialized GPU memory values are less predictable.

**Technical impact**  
Initializing the KV cache to zeros ensures deterministic behavior across all test runs by eliminating reliance on random, uninitialized memory values. This improves test reliability on both ROCm and CUDA backends, though the fix is most critical for ROCm due to its different memory initialization characteristics.

**Potential risks**  
Using `torch.zeros()` introduces a slight performance overhead compared to `torch.empty()` due to the zero-initialization step, but this is negligible in a test context. There is a minimal risk if any test inadvertently depends on uninitialized memory values, though such tests would be flawed and should be corrected.

**Key insights**  
Always initialize tensors when their contents will be read, especially in tests where deterministic behavior is required. For production code, consider whether lazy initialization is acceptable or if explicit initialization is needed for correctness. This fix highlights the importance of platform-specific considerations in memory handling.

---

## 17. [[ROCm][LoRA] Fix MoE accuracy regression by preserving float32 router weight scaling](https://github.com/vllm-project/vllm/pull/31931)


### Base Information

- **PR Number:** #31931
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 20:17:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31931/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The fix reorders operations in the fused MoE kernel to perform router weight multiplication (`MUL_ROUTED_WEIGHT`) before precision conversion to float32, while keeping bias addition after dequantization. This restores the original float32 scaling for router weights, correcting an MoE accuracy regression on ROCm caused by precision loss when the multiplication was performed in bf16/fp16.

**Technical impact**  
This change ensures numerical stability by maintaining float32 precision for router weight scaling, which is critical for deterministic expert routing. The architecture now correctly separates concerns: weight scaling occurs in high precision, dequantization handles precision conversion, and bias addition remains post-dequantization as intended.

**Potential risks**  
The fix assumes router weights should always be scaled in float32, which may not hold for all hardware or model configurations. There is a minor risk of performance impact due to the extra precision conversion step, though likely negligible. The change also tightly couples the operation order to ROCm's numerical behavior, which could complicate future optimizations.

**Key insights**  
Always verify mixed-precision operation ordering across different hardware backends, as numerical differences can cause non-deterministic behavior. When fixing regressions, preserve the original intent of related changes (like bias placement). Consider adding backend-specific numerical validation tests to catch similar issues early.

---

## 18. [[BugFix] Fix flakiness in test_eagle_dp for PyTorch 2.10](https://github.com/vllm-project/vllm/pull/31915)


### Base Information

- **PR Number:** #31915
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-07 20:04:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31915/files) (1):**
  - `tests/v1/distributed/test_eagle_dp.py`

### Summary

**What changed and why**  
The change reduces the number of expected tokens from 100 to 20 in the `test_run_eagle_dp` test to address flakiness. This adjustment is a temporary mitigation for intermittent test failures in PyTorch 2.10 (and potentially 2.9), as documented in the linked GitHub issue.

**Technical impact**  
This modification decreases the test's execution time and resource consumption, which may reduce the likelihood of race conditions or timing-related failures. However, it does not resolve the underlying cause of the flakiness, only making the symptom less frequent.

**Potential risks**  
The test may still exhibit flakiness under certain conditions, as the root issue remains unaddressed. Additionally, reducing the token count might inadvertently mask other subtle bugs that only manifest with longer sequences or higher loads.

**Key insights**  
This is a tactical fix rather than a permanent solution; developers should prioritize investigating the root cause outlined in issue #31913. Consider adding a comment or TODO to track the need for a more robust fix, and ensure the reduced token count still provides adequate test coverage for the data parallel functionality.

---

## 19. [[MoE Refactor][16/N] Apply Refactor to NVFP4](https://github.com/vllm-project/vllm/pull/31692)


### Base Information

- **PR Number:** #31692
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-07 19:46:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31692/files) (15):**
  - `benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py`
  - `docs/design/moe_kernel_features.md`
  - `tests/kernels/moe/test_nvfp4_moe.py`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py`

### Summary

**What changed and why**  
This PR refactors NVFP4 MoE integration by centralizing kernel selection logic into a new oracle (`nvfp4.py`), removing direct kernel calls (`cutlass_moe_fp4`, `flashinfer_cutedsl_moe_fp4`), and unifying weight processing between compressed-tensors and modelopt quantization methods. The changes transition NVFP4 MoE to use the modular kernel abstraction (`FusedMoEModularKernel`) with a standardized prepare/finalize pattern.

**Technical impact**  
The refactor simplifies NVFP4 MoE backend selection through a dedicated oracle that picks between FlashInfer (CUTLASS/TRTLLM/CUTEDSL), vLLM CUTLASS, and Marlin backends based on platform capabilities and environment flags. It consolidates duplicate weight‑processing logic into shared utilities (`prepare_nvfp4_moe_layer_for_fi_or_cutlass`, `prepare_nvfp4_moe_layer_for_marlin`), reducing code duplication and improving maintainability. The removal of standalone kernel functions pushes all callers to use the modular kernel interface, aligning NVFP4 with the broader MoE refactor direction.

**Potential risks**  
- The removal of `cutlass_moe_fp4` and `flashinfer_cutedsl_moe_fp4` may break any external code that directly imports these functions.  
- Centralized oracle logic introduces a single point of failure; misconfiguration or missing backend detection could lead to runtime errors.  
- Weight‑processing changes (e.g., repacking, scale swizzling) must be carefully validated across different backends to ensure numerical correctness.  
- The `use_global_sf` property migration (from FlashInfer‑specific checks to a generic attribute) may affect expert‑parallel scaling factor handling if not correctly implemented for all backends.

**Key insights**  
- Developers should adopt the `FusedMoEModularKernel` interface for NVFP4 MoE operations instead of calling legacy kernel functions directly.  
- Backend selection is now controlled via `VLLM_USE_FLASHINFER_MOE_FP4` and automatic capability detection; ensure environment variables are set appropriately for desired kernels.  
- Weight‑processing utilities are now shared between compressed‑tensors and modelopt—any future changes must be applied in both paths.  
- Verify that the new oracle correctly handles all supported hardware configurations and expert‑parallel scenarios, especially for global scaling factor support.

---

## 20. [[CI] Skip Qwen-VL in multimodal processing tests due to flaky external dependency](https://github.com/vllm-project/vllm/pull/31932)


### Base Information

- **PR Number:** #31932
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-07 18:58:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31932/files) (1):**
  - `tests/models/multimodal/processing/test_common.py`

### Summary

**What changed and why**  
Two Qwen-VL model variants are now skipped in multimodal processing tests due to intermittent CI failures. The tokenizer for these models attempts to download a font file from external servers in China, which frequently times out or refuses connections in CI environments, causing test flakiness.

**Technical impact**  
This change reduces CI flakiness by excluding models with unreliable external dependencies from the test suite. It maintains test coverage for other multimodal models while preventing sporadic failures that disrupt CI pipelines and developer workflows.

**Potential risks**  
Skipping these tests means any future regressions specific to Qwen-VL tokenizer processing may go undetected. Additionally, if other models develop similar external dependencies, the same issue could recur, requiring further test modifications.

**Key insights**  
External network dependencies in CI tests should be minimized or mocked to ensure reliability. Consider implementing a more robust solution, such as mocking network calls or caching required resources, to eventually restore test coverage for these models while maintaining CI stability.

---

## 21. [fix(rocm): add early return in get_flash_attn_version for ROCm](https://github.com/vllm-project/vllm/pull/31286)


### Base Information

- **PR Number:** #31286
- **Author:** [rabi](https://github.com/rabi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 18:28:07
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31286/files) (1):**
  - `vllm/attention/utils/fa_utils.py`

### Summary

**What changed and why**  
Added an early return of `None` in `get_flash_attn_version()` when the platform is ROCm. This prevents unnecessary attempts to import the CUDA-specific `vllm_flash_attn` module, which would cause "libcudart.so.12 not found" errors on ROCm systems.

**Technical impact**  
The change ensures ROCm platforms bypass CUDA-specific version checks entirely, aligning with ROCm's separate flash attention implementation. This maintains functional parity while eliminating spurious dependency errors, improving cross-platform compatibility.

**Potential risks**  
If ROCm's flash attention implementation relies on version detection elsewhere, returning `None` could inadvertently disable optimizations or cause version mismatches. Additionally, any future changes to ROCm's flash attention versioning may require updates to this logic.

**Key insights**  
This is a targeted fix that cleanly separates CUDA and ROCm code paths. Developers should verify that downstream code correctly handles `None` returns for ROCm. Consider adding a comment or constant to document ROCm's flash attention version strategy for future maintenance.

---

## 22. [feat(moe): Add is_act_and_mul=False support for Triton MoE kernels](https://github.com/vllm-project/vllm/pull/31645)


### Base Information

- **PR Number:** #31645
- **Author:** [rabi](https://github.com/rabi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 18:27:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31645/files) (7):**
  - `tests/kernels/moe/test_triton_moe_no_act_mul.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`

### Summary

**What changed and why**  
Added support for non-fused activations (e.g., `relu2_no_mul`, `silu_no_mul`, `gelu_no_mul`) in Triton MoE kernels to accommodate models like Nemotron-H that use non-SwiGLU activations. This includes a new `is_act_and_mul` flag in `FusedMoEQuantConfig`, updated workspace size calculations, and activation implementations for both fused and non-fused paths.

**Technical impact**  
The changes extend the MoE kernel architecture to handle two activation modes: fused (SwiGLU-style, where activation output is half the input size) and non-fused (where activation output matches input size). This affects memory allocation (workspace shapes), kernel execution paths, and platform support—now enabling ROCm when AITER is disabled. The quantization config and layer initialization are updated to propagate the `is_act_and_mul` setting.

**Potential risks**  
- Incorrect workspace size calculations could lead to out-of-bounds memory access or allocation errors, especially when switching between fused and non-fused modes.  
- The non-fused activation implementations (e.g., `gelu_no_mul`) rely on PyTorch ops with `out=` and in-place operations, which may have subtle numerical differences or performance implications compared to fused kernels.  
- Platform restrictions (CUDA-only or ROCm without AITER) may cause runtime errors if unsupported configurations are attempted.

**Key insights**  
- Ensure `is_act_and_mul` is correctly set in quant configs for models using non-SwiGLU activations to avoid shape mismatches.  
- Verify workspace shape logic thoroughly, as intermediate sizes now depend on `is_act_and_mul`.  
- The addition of comprehensive tests covering various shapes, dtypes, and activations is a strong practice—maintain and expand these for edge cases.

---

## 23. [[0/N][Attention] Fix miscellaneous pre-commit issues](https://github.com/vllm-project/vllm/pull/31924)


### Base Information

- **PR Number:** #31924
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-07 17:15:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31924/files) (8):**
  - `vllm/attention/layers/static_sink_attention.py`
  - `vllm/attention/ops/flashmla.py`
  - `vllm/attention/ops/paged_attn.py`
  - `vllm/attention/ops/prefix_prefill.py`
  - `vllm/attention/ops/rocm_aiter_mla_sparse.py`
  - `vllm/attention/ops/triton_decode_attention.py`
  - `vllm/attention/ops/triton_prefill_attention.py`
  - `vllm/attention/utils/fa_utils.py`

### Summary

**What changed and why**  
This PR fixes various pre-commit issues across attention-related modules, primarily addressing type hints, device capability checks, import aliasing, and minor code style improvements. The changes ensure consistency, correct type annotations, and proper handling of GPU architecture families (e.g., Hopper and Blackwell) for FlashMLA support.

**Technical impact**  
The updates standardize device capability checks using `is_device_capability_family()` for better maintainability and future compatibility. Import aliasing (`ops = _custom_ops`) improves clarity and platform-specific handling. Code simplifications (e.g., ternary operators) enhance readability without altering functionality.

**Potential risks**  
Incorrect device family checks could inadvertently enable or disable features on unsupported hardware. The lambda function rename (`grid` to `grid_fn`) may affect readability but is functionally equivalent. Type ignore comments (`# type: ignore[arg-type]`) could mask underlying type issues if not validated.

**Key insights**  
Use centralized device capability methods for robustness. Ensure type hints are accurate rather than suppressed. Review platform-specific import patterns to avoid duplication. These fixes are minor but important for code hygiene and pre-commit compliance.

---

## 24. [[MoE Refactor][15/N] Apply Refactor to Fp8](https://github.com/vllm-project/vllm/pull/31415)


### Base Information

- **PR Number:** #31415
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-07 16:42:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31415/files) (38):**
  - `.buildkite/test-pipeline.yaml`
  - `benchmarks/kernels/benchmark_cutlass_moe_fp8.py`
  - `benchmarks/kernels/benchmark_grouped_gemm_cutlass.py`
  - `benchmarks/kernels/benchmark_moe.py`
  - `docs/design/moe_kernel_features.md`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Llama-4-Scout-Fp8-ModelOpt-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ht.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ht.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-CT-vllm-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/config-b200.txt`
  - `tests/evals/gsm8k/configs/moe-refactor/config-h100.txt`
  - `tests/evals/gsm8k/test_gsm8k_correctness.py`
  - `tests/kernels/moe/test_cutlass_moe.py`
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/quantization/test_fp8.py`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/fallback.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/oracle/__init__.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py`

### Summary

**What changed and why**  
This PR continues the MoE refactor by applying modular kernel abstractions to FP8 MoE implementations. Key changes include removing the direct `cutlass_moe_fp8` entrypoint in favor of `CutlassExpertsFp8` managed through the modular kernel interface, factoring out kernel selection logic into a new `oracle/fp8.py` module, and consolidating weight‑processing utilities for reuse across FP8 and compressed‑tensors backends. The changes also update benchmarks and tests to use the new modular kernel API.

**Technical impact**  
The refactor centralizes FP8 MoE backend selection and configuration, reducing code duplication and improving maintainability. By moving stride management into `CutlassExpertsFp8`, the kernel interface becomes cleaner and more consistent with other expert implementations. The introduction of `FallbackExperts` enables runtime shape‑based fallbacks (e.g., Triton for small batches on SM100). This modularization paves the way for easier integration of new backends and better support for expert‑parallel and data‑parallel scenarios.

**Potential risks**  
Removing the direct `cutlass_moe_fp8` function may break external code that relies on the old API, though internal usage has been updated. The new fallback logic introduces conditional dispatch that must be thoroughly tested across different GPU architectures and batch sizes. There is also a risk of regression in performance or correctness if the modular kernel abstraction does not perfectly match the previous low‑level kernel behavior, especially for edge cases like expert‑parallel mapping or custom quantization strategies.

**Key insights**  
Developers should adopt the modular kernel pattern (`FusedMoEModularKernel` with `PrepareAndFinalize` and `Experts` components) for new MoE kernels. The new `oracle/fp8.py` provides a single point for backend selection, replacing scattered environment‑variable checks. When updating existing code, ensure that stride tensors are no longer passed explicitly—they are now managed internally by `CutlassExpertsFp8`. Integration tests have been expanded to cover data‑parallel and expert‑parallel configurations, which should be run to validate correctness under distributed settings.

---

## 25. [Add back missing DeepEP LL params](https://github.com/vllm-project/vllm/pull/31911)


### Base Information

- **PR Number:** #31911
- **Author:** [elvircrn](https://github.com/elvircrn)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-07 14:47:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31911/files) (1):**
  - `vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py`

### Summary

**What changed and why**  
Two parameters (`round_scale` and `use_ue8m0`) were added to the `prepare_async` call in the DeepEP low-latency (LL) path. This addresses a regression introduced in a prior commit (f72a817) that caused missing parameters, leading to degraded performance on the GSM8K benchmark.

**Technical impact**  
The changes restore proper configuration for DeepEP’s low-latency backend, aligning its behavior with the high-throughput (HT) case. This ensures consistent performance between the two backends, as evidenced by the improved exact_match scores (from ~0.78 to ~0.86–0.88).

**Potential risks**  
If the added parameters are incorrectly set (e.g., mismatched with FP8 or UE8M0 dispatch flags), it could reintroduce performance regressions or numerical instability. The fix assumes the underlying DeepEP kernel correctly handles these flags; any latent bugs in that kernel may now surface.

**Key insights**  
This is a targeted fix for a regression, and the test results confirm effectiveness. Developers should verify that `round_scale` and `use_ue8m0` are consistently applied across all DeepEP dispatch paths to prevent similar issues. Consider adding unit tests to catch parameter mismatches early.

---

## 26. [[BugFix] Fix bad words with speculative decoding](https://github.com/vllm-project/vllm/pull/31908)


### Base Information

- **PR Number:** #31908
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-07 12:46:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31908/files) (2):**
  - `tests/v1/sample/test_rejection_sampler.py`
  - `vllm/v1/sample/ops/bad_words.py`

### Summary

**What changed and why**  
The fix corrects a bug in the bad words filtering logic during speculative decoding. The original code incorrectly assumed that requests with `bad_words` constraints were always the first ones in the batch, leading to misaligned logit indexing. The updated implementation iterates through all requests, correctly matching each request's index to its bad words list.

**Technical impact**  
This change ensures that bad words constraints are applied to the correct logit positions for all requests in a batch, regardless of their order. The test update validates the fix by applying bad words to non-consecutive requests (0 and 2) and confirms that token rejection works correctly when the bad word matches the prefix.

**Potential risks**  
The implementation remains inefficient as noted, which could impact performance if bad words usage becomes more common. The early exit optimization (`if not remaining: break`) assumes `bad_words_token_ids` is a contiguous dictionary from the start, which may not hold if indices are sparse or unordered.

**Key insights**  
Developers should note that the fix is a minimal correctness patch; a more efficient redesign is planned for the future model runner. The test now better captures edge cases with non-consecutive bad words requests. Ensure any future changes to batch request ordering or bad words data structures maintain this corrected indexing logic.

---

## 27. [[EPLB] Optimize EPLB with numpy](https://github.com/vllm-project/vllm/pull/29499)


### Base Information

- **PR Number:** #29499
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-07 12:21:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29499/files) (8):**
  - `tests/distributed/test_eplb_algo.py`
  - `tests/distributed/test_eplb_execute.py`
  - `vllm/config/parallel.py`
  - `vllm/distributed/eplb/async_worker.py`
  - `vllm/distributed/eplb/eplb_state.py`
  - `vllm/distributed/eplb/policy/abstract.py`
  - `vllm/distributed/eplb/policy/default.py`
  - `vllm/distributed/eplb/rebalance_execute.py`

### Summary

**What changed and why**  
This PR optimizes the EPLB (Expert Parallel Load Balancing) algorithm by implementing key primitives in NumPy to reduce GPU-CPU transfers and adding a `preserve_intragpu_slots` post-processing step. It also introduces a `log_balancedness_interval` configuration to control logging frequency and fixes a bug in EPLB config validation.

**Technical impact**  
The changes improve performance by 20-25% per primitive through NumPy-based implementations of `move_to_buffer`, `move_from_buffer`, and `get_ep_ranks_with_expert`, reducing GPU-CPU transfers. The new `preserve_intragpu_slots` function minimizes unnecessary GPU memory copies by keeping intra-GPU expert assignments in their original slots. The configurable logging interval reduces collective communication overhead when balancedness logging is enabled.

**Potential risks**  
The NumPy implementations assume CPU-based processing, which may introduce synchronization overhead if not carefully managed with async workflows. The `preserve_intragpu_slots` logic adds complexity and must be thoroughly validated across edge cases, especially with duplicate experts and varying GPU counts. Changes to data structures (e.g., using `RecvMetadata` instead of separate fields) could break compatibility with existing async worker integrations if not fully updated.

**Key insights**  
Developers should verify that all async worker references to `experts_recv_loc` are updated to use `recv_metadata`. The performance gains are significant, but ensure thorough testing of the new `preserve_intragpu_slots` with diverse expert distributions. The config validation now enforces that async mode only works with the default policy, preventing runtime errors.

---

## 28. [[Kernel] Support bias type in grouped_topk kernel](https://github.com/vllm-project/vllm/pull/31781)


### Base Information

- **PR Number:** #31781
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-07 12:16:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31781/files) (3):**
  - `csrc/moe/grouped_topk_kernels.cu`
  - `tests/kernels/moe/test_grouped_topk.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The PR adds support for separate bias data types in the `grouped_topk` kernel by introducing a template parameter `BiasT` distinct from the input type `T`. This eliminates the need for explicit dtype casting of bias tensors (typically fp32) to match input tensors (e.g., bfloat16/half), which previously incurred overhead from a `torch.compile` fused copy operation.

**Technical impact**  
The kernel now accepts bias tensors of any supported floating-point type (float, half, bfloat16) independent of the input tensor type, reducing computational overhead by avoiding implicit casting. The changes propagate through the kernel call hierarchy, including device functions and launch wrappers, and are exposed via updated template instantiations that cover all type combinations.

**Potential risks**  
Introducing a new template parameter increases kernel binary size due to additional type combinations. There is a risk of precision loss when casting bias values (e.g., from fp32 to lower precision) if not carefully managed, though the existing `static_cast<T>` ensures explicit conversion. The modifications could affect other code paths that rely on the kernel’s original signature.

**Key insights**  
The optimization yields a measurable 5% throughput improvement by removing a casting kernel launch. Developers should ensure bias tensors are passed in their native dtype without manual conversion. The expanded template instantiations may slightly increase compilation time and binary size, but the performance gain justifies this trade-off for typical MoE workloads.

---

## 29. [[refactor] refactor memory constants usage](https://github.com/vllm-project/vllm/pull/31865)


### Base Information

- **PR Number:** #31865
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 10:37:31
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31865/files) (9):**
  - `tests/basic_correctness/test_cumem.py`
  - `vllm/config/cache.py`
  - `vllm/multimodal/cache.py`
  - `vllm/platforms/cpu.py`
  - `vllm/utils/mem_utils.py`
  - `vllm/v1/core/kv_cache_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR refactors memory constant usage by replacing duplicate f-string formatting with a centralized `format_gib()` function. The changes standardize how memory values are displayed in GiB units across the codebase, improving consistency and maintainability.

**Technical impact**  
The refactoring introduces a new `format_gib()` function that returns formatted strings instead of floats, and adds a `format_mib()` function for MiB formatting. This affects logging statements and error messages throughout the system, ensuring consistent memory display formatting. The swap space calculation now uses `math.ceil()` for more conservative memory allocation.

**Potential risks**  
The change from returning floats to strings in `format_gib()` could break any code that performs arithmetic operations on the return value. The `math.ceil()` addition to swap space calculation might slightly increase memory allocation requirements in edge cases. There's also a risk of inconsistent formatting if some calls still use direct division by `GiB_bytes`.

**Key insights**  
Developers should use the new `format_gib()` and `format_mib()` functions for all memory display needs rather than manual calculations. The refactoring improves code maintainability but requires careful review to ensure no functionality depends on the numeric return type. Future memory formatting should consistently use these utility functions.

---

## 30. [[Perf] Fuse stride preparation for NVFP4 cutlass_moe](https://github.com/vllm-project/vllm/pull/31837)


### Base Information

- **PR Number:** #31837
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-07 10:31:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31837/files) (1):**
  - `csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu`

### Summary

**What changed and why**  
The PR fuses stride initialization into the existing `__get_group_gemm_starts` kernel to eliminate three separate `torch::full` kernel launches. Previously, stride tensors were created via `torch::full`, which launched independent vectorized elementwise kernels. Now, stride values are computed once and written directly within the pointer setup kernel, reducing kernel launch overhead.

**Technical impact**  
This change reduces GPU kernel launch overhead by consolidating work into a single kernel that already runs per-expert. The stride tensors are now allocated as empty tensors and populated in-place, avoiding additional synchronization points. This improves latency by ~5% in benchmarks, with smaller throughput gains, while maintaining identical functional behavior.

**Potential risks**  
The modified kernel now has additional parameters and writes to more memory locations, which could theoretically increase register pressure or warp divergence. However, since stride values are constant per expert and the kernel remains memory-bound, the risk is minimal. Edge cases could arise if `num_experts` exceeds the kernel's grid/block configuration, but the existing guard clause prevents out-of-bounds writes.

**Key insights**  
Kernel fusion is an effective optimization for reducing launch overhead in performance-critical paths. Developers should profile CUDA kernels to identify similar opportunities where small, constant-value initializations can be merged into existing computational kernels. The change demonstrates that even simple fusions can yield measurable latency improvements in MoE workloads.

---

## 31. [[Doc] Fix: Correct vLLM announcing blog post link in docs](https://github.com/vllm-project/vllm/pull/31868)


### Base Information

- **PR Number:** #31868
- **Author:** [Ayobami-00](https://github.com/Ayobami-00)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-07 10:06:42
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31868/files) (1):**
  - `docs/README.md`

### Summary

**What changed and why**  
The PR fixes a broken documentation link in `docs/README.md` by updating the URL for the "vLLM announcing blog post" from `https://vllm.ai` (the main website) to the correct blog post at `https://blog.vllm.ai/2023/06/20/vllm.html`. This ensures users can directly access the introductory content on PagedAttention.

**Technical impact**  
This change has no impact on code functionality, system behavior, or architecture. It solely improves documentation accuracy and user experience by directing readers to the intended resource, which is critical for effective onboarding and learning.

**Potential risks**  
The risk is minimal as this is a documentation-only change. However, if the new URL becomes outdated or inaccessible in the future, the link would break again. No testing or validation beyond manual verification is required.

**Key insights**  
Documentation accuracy is essential for user trust and effective knowledge sharing. While this fix is straightforward, it highlights the importance of regularly auditing documentation links. Developers should consider implementing automated link checking in CI/CD to prevent similar issues.

---

## 32. [Enable quantized attention in NemotronH models](https://github.com/vllm-project/vllm/pull/31898)


### Base Information

- **PR Number:** #31898
- **Author:** [roikoren755](https://github.com/roikoren755)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-07 09:37:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31898/files) (2):**
  - `vllm/model_executor/model_loader/weight_utils.py`
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
This PR enables quantized attention support for NemotronH models by fixing two issues: the KV cache scale remapping logic didn't recognize NemotronH's weight naming pattern, and the attention layer wasn't receiving the quantization configuration. The changes ensure quantized attention and KV cache work correctly with NemotronH's architecture.

**Technical impact**  
The modification in `weight_utils.py` adds a regex pattern to remap NemotronH's key/value scale parameter names (e.g., `.mixer.k_proj.k_scale` → `.mixer.attn.k_scale`) to match the expected attention layer structure. In `nemotron_h.py`, the `quant_config` is now passed to the attention layer initialization, enabling quantization-aware computation. This aligns NemotronH with other models that support quantized attention.

**Potential risks**  
If the regex pattern is too broad or conflicts with other model architectures, it could incorrectly remap scale parameters for non-NemotronH models. Additionally, any undiscovered edge cases in NemotronH's attention implementation (e.g., multi-query or grouped-query attention variants) might not be fully handled by the quantization logic, potentially leading to runtime errors or performance degradation.

**Key insights**  
Developers should verify that the regex pattern precisely matches NemotronH's weight naming convention across all released variants. It's also critical to test quantized inference end-to-end with NemotronH to ensure accuracy and performance meet expectations. Consider adding a unit test for the remapping logic to prevent regressions.

---

## 33. [UX: add vLLM env info in '/server_info'](https://github.com/vllm-project/vllm/pull/31899)


### Base Information

- **PR Number:** #31899
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 09:13:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31899/files) (1):**
  - `vllm/entrypoints/serve/instrumentator/server_info.py`

### Summary

**What changed and why**  
The PR adds vLLM environment variable information to the `/server_info` endpoint response. A new helper function `_get_vllm_env_vars()` collects all environment variables prefixed with `VLLM_` (excluding those containing "KEY"), normalizes their values, and returns them as a dictionary. This data is included in the JSON response under the key `"vllm_env"`.

**Technical impact**  
This change enhances the observability of the vLLM server by exposing runtime environment configurations alongside existing server configuration. It allows users and developers to inspect environment-specific settings (e.g., feature flags, performance tunables) without needing direct server access, aiding in debugging and configuration validation.

**Potential risks**  
The function may inadvertently expose sensitive environment variables if any `VLLM_*KEY*` variables are not adequately filtered. Additionally, the normalization of values (`normalize_value`) could introduce serialization issues for complex or custom types. The increased response size due to many environment variables might affect network performance for frequent requests.

**Key insights**  
Ensure the filtering logic for `"KEY"` is robust and consider adding a denylist for other sensitive patterns. Validate that `normalize_value` handles all possible value types returned by `getattr(envs, key)`. For production, consider making this endpoint optional or rate-limited to avoid performance overhead.

---

## 34. [[KVConnector]: Enable Cross-layers KV cache layout for MultiConnector](https://github.com/vllm-project/vllm/pull/30761)


### Base Information

- **PR Number:** #30761
- **Author:** [kfirtoledo](https://github.com/kfirtoledo)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-07 08:59:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30761/files) (4):**
  - `tests/v1/kv_connector/unit/test_multi_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/base.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`

### Summary

**What changed and why**  
The changes enable MultiConnector to support cross-layer KV cache layout by implementing the `prefer_cross_layer_blocks` property across connector classes. This property indicates whether a connector prefers KV blocks that hold data for all layers to speed up transfers. MultiConnector aggregates this property across all underlying connectors, returning true only if all connectors support it.

**Technical impact**  
The KVConnectorBase_V1 base class now defines `prefer_cross_layer_blocks` as a property with a default False value, replacing the previous class variable. This allows dynamic evaluation per connector instance. MultiConnector implements aggregation logic and a new `register_cross_layers_kv_cache` method to propagate cross-layer cache registration to all child connectors, ensuring consistent behavior.

**Potential risks**  
If any underlying connector does not support cross-layer blocks, MultiConnector will return False, which may disable optimizations unexpectedly. The new property-based approach could introduce subtle bugs if subclasses incorrectly override the property. Additionally, the `register_cross_layers_kv_cache` method assumes all connectors implement it, which may cause AttributeError if a connector lacks this method.

**Key insights**  
Developers should ensure all connector implementations properly define the `prefer_cross_layer_blocks` property and implement `register_cross_layers_kv_cache` if needed. The aggregation logic in MultiConnector is conservative (requires all connectors to support cross-layer blocks), which is safe but may limit performance gains. Testing should verify mixed connector scenarios work as expected.

---

## 35. [[Bugfix]: prevent leaking tokens in crash log](https://github.com/vllm-project/vllm/pull/30751)


### Base Information

- **PR Number:** #30751
- **Author:** [dr75](https://github.com/dr75)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 08:15:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30751/files) (1):**
  - `vllm/v1/core/sched/output.py`

### Summary

**What changed and why**  
Added `anon_repr()` methods to `NewRequestData` and `CachedRequestData` classes to anonymize token data in logs. Instead of exposing actual token IDs (which could contain sensitive information), the methods now log only the lengths of token arrays. This prevents token leakage in crash logs when the engine crashes with preempted requests.

**Technical impact**  
The change modifies the string representation of scheduler state objects used during logging. `CachedRequestData.__repr__` now delegates to `anon_repr()`, ensuring all log outputs automatically use the anonymized format. This improves security without affecting runtime logic or data structures.

**Potential risks**  
If other code relies on the original `__repr__` output for debugging or serialization, it may break. The change could also obscure debugging details when actual token values are needed for diagnosing model issues. Additionally, the `NewRequestData.anon_repr()` includes a new `prefill_token_ids_len` field, which must be consistently handled.

**Key insights**  
This is a security-focused change that appropriately prioritizes data protection. Developers should verify that no external tools depend on the full token IDs in logs. Consider adding a debug-mode flag to optionally restore original token output for internal troubleshooting while keeping production logs anonymized.

---

## 36. [[Refactor] Clean up pooler modules](https://github.com/vllm-project/vllm/pull/31897)


### Base Information

- **PR Number:** #31897
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 08:07:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31897/files) (7):**
  - `vllm/model_executor/layers/pooler.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/gritlm.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/v1/outputs.py`
  - `vllm/v1/pool/metadata.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This refactor clarifies type distinctions between pooling methods that return single tokens versus multiple tokens. It introduces explicit type aliases (`TokenPoolingMethodOutput`, `TokensPoolingMethodOutput`), restructures the inheritance hierarchy with `TokenPoolerHead` and `TokensPoolerHead`, and removes the redundant `forward_all` method. The changes ensure consistent type annotations and resolve type-checker errors.

**Technical impact**  
The refactor enforces a clearer contract between pooling methods and their heads, improving type safety and maintainability. The `hidden_states` parameter is now strictly `torch.Tensor` (not a list), simplifying internal logic. Model-specific poolers (BERT, GritLM, ModernBERT) are updated to align with the new abstract base classes, ensuring uniform implementation patterns.

**Potential risks**  
Introducing new abstract classes (`TokenPoolerHead`, `TokensPoolerHead`) may break custom pooler implementations that don’t inherit from them. The removal of `forward_all` could affect any external code relying on that method. Changes to `hidden_states` type could cause issues if any caller still passes a list.

**Key insights**  
Developers should update custom poolers to inherit from the appropriate new base classes and ensure `hidden_states` is always a tensor. The refactor improves code clarity but requires careful validation of all pooling-related code paths, especially for edge cases like partial prefill or unfinished chunks.

---

## 37. [[Perf][Kernels] Enable FlashInfer DeepGEMM swapAB on SM90 (for W8A8 Linear Op)](https://github.com/vllm-project/vllm/pull/29213)


### Base Information

- **PR Number:** #29213
- **Author:** [katec846](https://github.com/katec846)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-07 07:53:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29213/files) (4):**
  - `tests/kernels/quantization/test_block_fp8.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR introduces FlashInfer's DeepGEMM swapAB kernel for FP8 block-scale GEMM operations on SM90+ GPUs, specifically targeting small-batch (M<32) linear layers in W8A8 quantized models. The change addresses performance gaps in low-concurrency scenarios by leveraging TensorRT-LLM's optimized kernel, while maintaining accuracy by falling back to standard DeepGEMM for larger batches (M≥32).

**Technical impact**  
The implementation adds a new conditional execution path in the FP8 linear layer that selects between FlashInfer's swapAB kernel (for M<32) and DeepGEMM (for M≥32) using `torch.cond` to preserve torch.compile compatibility. This creates a hybrid GEMM strategy that improves throughput by 5-15% in low-concurrency benchmarks while maintaining numerical accuracy through batch-size-dependent kernel selection.

**Potential risks**  
The accuracy preservation relies on the M<32 threshold; if this boundary changes or model characteristics shift, accuracy regressions could occur. The conditional logic adds complexity to the quantization pipeline, and the dependency on FlashInfer's specific kernel availability creates a new system requirement. Edge cases around tensor dimension alignment (K%128==0, N%64==0) could lead to silent fallbacks or errors.

**Key insights**  
Developers should enable this feature via `VLLM_BLOCKSCALE_FP8_GEMM_FLASHINFER=1` for SM90+ systems. The implementation demonstrates how to safely integrate performance-critical kernels with accuracy constraints using torch.cond for compilation compatibility. Future work should monitor accuracy across diverse batch sizes and consider extending the FlashInfer kernel to larger M values once accuracy issues are resolved.

---

## 38. [[OpenAI] Extend VLLMValidationError to additional validation parameters](https://github.com/vllm-project/vllm/pull/31870)


### Base Information

- **PR Number:** #31870
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 06:45:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31870/files) (9):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/protocol.py`
  - `vllm/entrypoints/openai/serving_completion.py`
  - `vllm/entrypoints/openai/serving_engine.py`
  - `vllm/entrypoints/openai/serving_responses.py`
  - `vllm/entrypoints/renderer.py`
  - `vllm/exceptions.py`
  - `vllm/sampling_params.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The PR extends `VLLMValidationError` to populate the `param` field for additional validation scenarios. It moves the exception definition to a shared `exceptions.py` module and updates validation logic across multiple files to use this enhanced error type with parameter metadata.

**Technical impact**  
These changes improve error reporting consistency by ensuring validation errors include parameter names and values in API responses. The refactoring centralizes the exception definition, reducing circular import issues and enabling duck-typing for error handling in `serving_engine.py`.

**Potential risks**  
If not all validation paths are updated, some errors may lack parameter metadata, leading to inconsistent API responses. The duck-typing approach in `serving_engine.py` could fail if exception attributes are missing. Changes to error message formatting might affect clients parsing error strings.

**Key insights**  
Developers should ensure all new validation logic uses `VLLMValidationError` with appropriate `parameter` and `value` arguments. The centralized exception module simplifies future maintenance, but care is needed to avoid breaking existing error handling in other parts of the codebase.

---

## 39. [[Chore] Migrate V0 attention utils](https://github.com/vllm-project/vllm/pull/31891)


### Base Information

- **PR Number:** #31891
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 05:44:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31891/files) (10):**
  - `tests/kernels/mamba/test_causal_conv1d.py`
  - `tests/kernels/mamba/test_mamba_ssm.py`
  - `vllm/attention/backends/utils.py`
  - `vllm/model_executor/layers/mamba/ops/causal_conv1d.py`
  - `vllm/model_executor/layers/mamba/ops/mamba_ssm.py`
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/mla/common.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/worker/gpu/block_table.py`

### Summary

**What changed and why**  
This PR migrates V0 attention utilities to the V1 namespace as part of codebase refactoring. Specifically, it moves `PAD_SLOT_ID` from `vllm.attention.backends.utils` to `vllm.v1.attention.backends.utils` and relocates `get_mla_dims` function (along with its `MLADims` dataclass) from the same V0 module to `vllm.v1.attention.backends.mla.common`.

**Technical impact**  
The changes consolidate attention-related utilities under the V1 architecture, removing deprecated V0 code. All imports across test files, model layers, and attention backends have been updated to reference the new locations. The `get_mla_dims` function now resides within the MLA-specific module, improving logical organization.

**Potential risks**  
There's a risk of missing import updates in other files not included in this PR, which could cause runtime errors. The relocation of `get_mla_dims` changes its public API path, potentially breaking external code that directly imports it. Additionally, any hidden dependencies on the removed V0 module could surface as import failures.

**Key insights**  
This is a straightforward migration that follows established patterns for V0-to-V1 transitions. Developers should verify all attention-related imports in their codebase are updated. The PR successfully reduces technical debt by eliminating V0 code, but thorough testing is recommended to ensure no regressions in Mamba or MLA attention functionality.

---

## 40. [[Refactor] GLM-ASR Modeling](https://github.com/vllm-project/vllm/pull/31779)


### Base Information

- **PR Number:** #31779
- **Author:** [JaredforReal](https://github.com/JaredforReal)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-07 05:08:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31779/files) (2):**
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/glmasr_utils.py`

### Summary

**What changed and why**  
The PR refactors GLM-ASR modeling by implementing a native vLLM audio encoder (`GlmAsrEncoder`) with optimized components (QKVParallelLinear, rotary embeddings, flash attention) and streamlining the multimodal processing pipeline. It replaces the previous dependency on `AudioFlamingo3` with direct inheritance from vLLM's base classes (`BaseMultiModalProcessor`, `BaseProcessingInfo`, `BaseMultiModalDataParse`), reducing complexity and improving maintainability.

**Technical impact**  
These changes enhance performance through tensor parallelism, quantization support, and optimized attention mechanisms (SDPA/flash attention). The architecture now aligns with vLLM's native patterns, enabling better distributed inference and reducing overhead from external dependencies. The refactored utility functions also improve clarity in audio feature length calculations.

**Potential risks**  
- The removal of `AudioFlamingo3` dependencies may introduce subtle behavioral differences if the previous implementation had unique handling not captured in the base classes.  
- The new rotary embedding computation (on-demand cos/sin) could affect performance if sequence lengths vary widely, though caching is optimized.  
- Grouped query attention (GQA) handling must be validated to ensure correct k/v repetition across different tensor parallelism configurations.

**Key insights**  
- The refactor significantly modernizes the codebase by leveraging vLLM's optimized layers and eliminating legacy abstractions.  
- Developers should verify that the new encoder maintains full functional parity with the original, especially for edge cases in audio sequence processing.  
- The changes set a precedent for future multimodal model integrations, promoting cleaner architecture and better performance.

---

## 41. [[ROCm][AITER] fix wrong argument passed to  AITER `flash_attn_varlen_func`](https://github.com/vllm-project/vllm/pull/31880)


### Base Information

- **PR Number:** #31880
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-07 03:25:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31880/files) (2):**
  - `vllm/v1/attention/backends/mla/aiter_triton_mla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`

### Summary

**What changed and why**  
The PR fixes a parameter naming error introduced in a previous change (#31465). The argument `return_softmax_lse` was incorrectly passed to the AITER package's `flash_attn_varlen_func` function, which expects the parameter name `return_lse`. The correction ensures compatibility with the external AITER library.

**Technical impact**  
This change restores proper functionality for ROCm AITER attention backends (`ROCM_AITER_MLA` and `ROCM_AITER_TRITON_MLA`). Without the fix, the attention computation would fail due to an unrecognized argument, breaking inference for models using these backends on ROCm platforms.

**Potential risks**  
The fix is minimal and targeted, but any mismatch in parameter names could still cause runtime errors if the AITER library's API changes in the future. Additionally, the fix assumes the `return_lse` parameter behaves identically to `return_softmax_lse`; any semantic differences could affect downstream logic.

**Key insights**  
Always verify parameter names against external library APIs when integrating or updating dependencies. The PR includes comprehensive test results showing restored performance, but consider adding unit tests that validate parameter passing for these attention backends to prevent regression.

---

## 42. [[Bugfix][MTP] Fix GLM4 MoE fp8 loading with MTP on](https://github.com/vllm-project/vllm/pull/31757)


### Base Information

- **PR Number:** #31757
- **Author:** [andyl98](https://github.com/andyl98)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-07 01:18:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31757/files) (1):**
  - `vllm/model_executor/models/glm4_moe_mtp.py`

### Summary

**What changed and why**  
This PR fixes a bug preventing FP8 checkpoint loading for GLM-4 MoE models with model tensor parallelism (MTP) enabled. The issue occurs because the checkpoint contains FP8 scale parameters for certain components (the shared LM head and individual MoE experts) that vLLM's current implementation does not quantize or expects in a different fused format. The change adds a conditional skip during weight loading to ignore these unmatched scale parameters, avoiding a `KeyError`.

**Technical impact**  
The modification allows FP8-quantized GLM-4 MoE checkpoints to load successfully under MTP by making the weight loader more tolerant of parameter mismatches. It does not alter the model's architecture or quantization logic; it simply prevents a crash when the checkpoint includes extra scale tensors that the loaded model does not define. This enables the use of FP8 KV cache and speculative decoding with these models.

**Potential risks**  
Skipping scale parameters could mask underlying configuration mismatches between the checkpoint and the model definition. If a scale parameter is legitimately required but missing from `params_dict`, the model might load but produce incorrect numerical results during inference. Additionally, the fix is applied broadly to any parameter ending in `.weight_scale`, which could inadvertently skip valid scales in future model variants if not carefully reviewed.

**Key insights**  
The core issue is a discrepancy between checkpoint contents and model parameter expectations in vLLM's FP8 implementation. Developers should ensure the quantization logic for `ParallelLMHead` and `FusedMoE` layers is consistent with checkpoint generation. Consider logging a warning when scales are skipped to aid debugging. This fix is a pragmatic workaround; a more robust long-term solution would involve aligning the parameter structures.

---

## 43. [[Misc] Improve error messages for unsupported types and parameters](https://github.com/vllm-project/vllm/pull/30593)


### Base Information

- **PR Number:** #30593
- **Author:** [BlankRH](https://github.com/BlankRH)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-07 01:00:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30593/files) (11):**
  - `benchmarks/cutlass_benchmarks/sparse_benchmarks.py`
  - `vllm/attention/ops/chunked_prefill_paged_decode.py`
  - `vllm/config/lora.py`
  - `vllm/distributed/device_communicators/pynccl_wrapper.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py`
  - `vllm/model_executor/layers/quantization/auto_round.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/minimax_text_01.py`

### Summary

**What changed and why**  
This PR improves error messages for unsupported types and invalid parameters across multiple files. The changes enhance developer experience by making error messages more explicit—they now include parameter names, invalid values, and expected constraints—without altering runtime behavior.

**Technical impact**  
The changes are purely cosmetic and affect only error messaging. They improve debuggability by providing clearer, more actionable error information when invalid configurations are encountered, which helps developers quickly identify and fix issues.

**Potential risks**  
There is minimal risk since these are non-functional changes to error messages. However, there is a slight chance that some downstream error-handling logic might rely on the exact string format of these messages, though this is unlikely given the nature of the changes.

**Key insights**  
These improvements significantly enhance the developer experience by making error messages self-documenting. Developers should ensure that any automated error parsing or testing that depends on error message content is updated to match the new formats.

---

## 44. [[Model] Cleanup: Remove redundant manual definition of `make_empty_intermediate_tensors` in GLM-4-MoE](https://github.com/vllm-project/vllm/pull/31869)


### Base Information

- **PR Number:** #31869
- **Author:** [maang-h](https://github.com/maang-h)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-07 00:18:29
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31869/files) (1):**
  - `vllm/model_executor/models/glm4_moe.py`

### Summary

**What changed and why**  
Removed a manually defined `make_empty_intermediate_tensors` method from the `Glm4MoeModel` class because this method is already dynamically created and assigned via `make_empty_intermediate_tensors_factory` in the `__init__` constructor, making the manual implementation redundant.

**Technical impact**  
This cleanup eliminates duplicate code, ensuring consistency by relying solely on the factory-generated method. It reduces maintenance overhead and aligns the class with the established pattern used elsewhere in the codebase for creating intermediate tensors.

**Potential risks**  
If the factory method (`make_empty_intermediate_tensors_factory`) fails to properly assign the method at initialization, the class could lose this functionality entirely. Additionally, any external code that directly referenced the manual method (though unlikely) would break.

**Key insights**  
This is a straightforward code cleanup that improves maintainability. Developers should verify that the factory-assigned method behaves identically to the removed manual implementation and ensure no hidden dependencies exist. Future changes to intermediate tensor creation should focus on the factory mechanism.

---

## 45. [[XPU]fallback to TRITON_ATTN on xpu when use float32 dtype](https://github.com/vllm-project/vllm/pull/31762)


### Base Information

- **PR Number:** #31762
- **Author:** [1643661061leo](https://github.com/1643661061leo)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-01-07 00:10:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31762/files) (1):**
  - `vllm/platforms/xpu.py`

### Summary

**What changed and why**  
Added a fallback mechanism in the XPU platform's attention backend selection logic. When Flash Attention is requested with float32 dtype on XPU, it now automatically switches to Triton Attention because the XPU kernel doesn't support FP32 for Flash Attention.

**Technical impact**  
This change ensures compatibility for FP32 inference on XPU by redirecting unsupported dtype configurations to a working backend. It maintains the existing selection flow for other dtypes and backends, preventing runtime failures while logging a clear warning about the fallback.

**Potential risks**  
If Triton Attention has performance or functional limitations compared to Flash Attention for FP32 on XPU, this could impact inference speed or correctness. The warning may be missed in logs, leading to confusion about backend selection. No validation is added for Triton Attention's own FP32 support on XPU.

**Key insights**  
Always verify that fallback backends fully support the required dtype and operations. Consider adding a runtime performance comparison to document the impact of this fallback. Future kernel updates should re-enable Flash Attention for FP32 on XPU to restore optimal performance.

---

## 46. [[Refactor][TPU] Remove torch_xla path and use tpu-inference](https://github.com/vllm-project/vllm/pull/30808)


### Base Information

- **PR Number:** #30808
- **Author:** [weiyu0824](https://github.com/weiyu0824)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-07 00:07:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30808/files) (46):**
  - `docs/design/moe_kernel_features.md`
  - `tests/tpu/__init__.py`
  - `tests/tpu/lora/__init__.py`
  - `tests/tpu/lora/test_lora.py`
  - `tests/tpu/test_compilation.py`
  - `tests/tpu/test_custom_dispatcher.py`
  - `tests/tpu/test_moe_pallas.py`
  - `tests/tpu/test_quantization_accuracy.py`
  - `tests/v1/tpu/__init__.py`
  - `tests/v1/tpu/test_basic.py`
  - `tests/v1/tpu/test_kv_cache_update_kernel.py`
  - `tests/v1/tpu/test_mha_attn.py`
  - `tests/v1/tpu/test_multimodal.py`
  - `tests/v1/tpu/test_pallas.py`
  - `tests/v1/tpu/test_perf.py`
  - `tests/v1/tpu/test_sampler.py`
  - `tests/v1/tpu/test_spmd_model_weight_loading.py`
  - `tests/v1/tpu/test_topk_topp_sampler.py`
  - `tests/v1/tpu/test_tpu_int8.py`
  - `tests/v1/tpu/test_tpu_qkv_linear.py`
  - `tests/v1/tpu/worker/__init__.py`
  - `tests/v1/tpu/worker/test_tpu_model_runner.py`
  - `vllm/attention/backends/registry.py`
  - `vllm/attention/layers/mm_encoder_attention.py`
  - `vllm/distributed/device_communicators/tpu_communicator.py`
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`
  - `vllm/distributed/tpu_distributed_utils.py`
  - `vllm/lora/ops/xla_ops/__init__.py`
  - `vllm/lora/ops/xla_ops/lora_ops.py`
  - `vllm/lora/punica_wrapper/punica_tpu.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/moe_pallas.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py`
  - `vllm/model_executor/layers/quantization/tpu_int8.py`
  - `vllm/model_executor/model_loader/default_loader.py`
  - `vllm/model_executor/model_loader/tpu.py`
  - `vllm/platforms/tpu.py`
  - `vllm/usage/usage_lib.py`
  - `vllm/v1/attention/backends/pallas.py`
  - `vllm/v1/worker/tpu_model_runner.py`
  - `vllm/v1/worker/tpu_worker.py`

### Summary

**What changed and why**  
This PR removes all torch_xla-related code paths from vLLM, as the torch_xla backend is now deprecated. Users who want to run vLLM on TPU must now install and use the separate `tpu-inference` package. The changes delete TPU-specific implementations (e.g., Pallas attention backend, TPU model loader, LoRA ops, quantization kernels) and migrate all TPU-related tests to the tpu-inference repository.

**Technical impact**  
The codebase is significantly simplified by removing ~6,785 lines of TPU-specific code. This eliminates maintenance overhead for the deprecated torch_xla backend and consolidates TPU support under a single, dedicated package. The removal affects core components like attention backends, model runners, distributed utilities, and quantization layers, but non-TPU backends remain unaffected.

**Potential risks**  
- Existing users relying on the torch_xla backend will need to migrate to tpu-inference, which may require changes to their setup and configuration.  
- The removal of TPU-specific tests and documentation updates (pending) could lead to gaps in user guidance.  
- Edge cases in the tpu-inference integration might surface, especially if the migration is not fully seamless for all previously supported features (e.g., LoRA, quantization).

**Key insights**  
- Developers should update their TPU installation and configuration to use tpu-inference, as the torch_xla path will no longer work.  
- The PR demonstrates a clean architectural shift by externalizing TPU support, aligning with vLLM’s focus on maintaining a lean core codebase.  
- Ensure that pending documentation updates (README, Dockerfile) are completed to avoid user confusion.

---

