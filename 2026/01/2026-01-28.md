# vLLM Merged PR Report

**Report Date:** 2026-01-28 PST

**Total Merged PRs:** 28

---

## 1. [[Release] [ROCm] Remove old build step](https://github.com/vllm-project/vllm/pull/33316)


### Base Information

- **PR Number:** #33316
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-01-28 23:35:51
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33316/files) (1):**
  - `.buildkite/release-pipeline.yaml`

### Summary

**What changed and why**  
Removed an obsolete ROCm Docker image build step from the release pipeline configuration. This step was a legacy manual block that was missed during a previous rebase of PR #33156. The change also includes minor label updates for clarity on two remaining ROCm-related build steps.

**Technical impact**  
This simplifies the CI/CD pipeline by eliminating a redundant, manual build step that was no longer required. The pipeline execution graph will now be cleaner and more efficient, as it removes a blocking step that had no dependencies and was not part of the automated flow.

**Potential risks**  
The primary risk is minimal, as this is a cleanup of an unused step. However, there is a slight risk if the removed step was still referenced elsewhere or if its manual approval was serving an unaccounted-for governance purpose. The changes to the labels are purely cosmetic and carry no functional risk.

**Key insights**  
This is a straightforward pipeline cleanup. Developers should verify that no other pipeline configurations, documentation, or release processes implicitly depended on the existence of this manual block. The successful test plan confirms the removal was correct by checking the pipeline graph.

---

## 2. [[Misc][Build] Lazy load cv2 in nemotron_parse.py](https://github.com/vllm-project/vllm/pull/33189)


### Base Information

- **PR Number:** #33189
- **Author:** [kiersten-stokes](https://github.com/kiersten-stokes)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-28 22:55:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33189/files) (1):**
  - `vllm/model_executor/models/nemotron_parse.py`

### Summary

**What changed and why**  
The PR modifies `nemotron_parse.py` to lazy-load the `cv2` module by moving its import statements from the top-level scope into the methods `_create_transforms` and `_resize_with_aspect_ratio`. This change prevents import errors in environments where OpenCV dependencies (like `libxcb.so.1`) are missing, unless the specific functionality requiring `cv2` is actually invoked.

**Technical impact**  
This reduces the module's startup-time dependencies and avoids hard failures when `cv2` is not globally available. The lazy loading ensures that `cv2` is only imported when image processing functions are called, which may be optional depending on runtime usage. This aligns with the broader pattern of deferring heavy or optional imports to improve compatibility and startup performance.

**Potential risks**  
Repeated imports of `cv2` within the same module could occur if both methods are called, though Python's import caching mitigates overhead. There is a risk that developers might forget to import `cv2` in other future methods that also require it, leading to `NameError` at runtime. Additionally, any changes to `cv2`'s API or global state assumptions could be less obvious due to the decentralized imports.

**Key insights**  
Lazy loading is a pragmatic solution for optional dependencies, but consider documenting this pattern in the module or adding a centralized helper to manage such imports. Ensure all code paths that rely on `cv2` handle its import consistently. For broader impact, evaluate if similar issues exist with other optional imports in the codebase to maintain consistency.

---

## 3. [[Release] [CI] Optim release pipeline](https://github.com/vllm-project/vllm/pull/33156)


### Base Information

- **PR Number:** #33156
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-28 22:45:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33156/files) (5):**
  - `.buildkite/release-pipeline.yaml`
  - `.buildkite/scripts/annotate-release.sh`
  - `.buildkite/scripts/annotate-rocm-release.sh`
  - `docker/Dockerfile.rocm`
  - `tools/vllm-rocm/generate-rocm-wheels-root-index.sh`

### Summary

**What changed and why**  
This PR unifies the ROCm Docker image and wheel release pipelines, adding a new Docker image build step and a root index generation script. It centralizes release annotations, fixes a regex import bug, and ensures sccache is removed from final images to make them suitable for local development.

**Technical impact**  
The changes create a more consistent release workflow by integrating Docker image builds into the existing ROCm pipeline. The new root index generation enables simpler installation via a stable URL (`--extra-index-url https://wheels.vllm.ai/rocm/`). The pipeline now produces both a base image (with build dependencies) and a final release image (with sccache cleaned).

**Potential risks**  
The sccache cleanup in the Dockerfile may not be fully exhaustive if wrappers or environment variables are set elsewhere. The root index script assumes semantic versioning and could fail if version directories are malformed. Changes to image tagging (`vllm-openai-rocm` vs. `vllm-openai:rocm`) might affect downstream consumers expecting the old naming scheme.

**Key insights**  
Developers should verify that the new Docker images correctly build kernels JIT without sccache. The root index generation is a manual release step, so ensure it's triggered appropriately. Review the updated annotation scripts to confirm all image tags and installation instructions are accurate for users.

---

## 4. [Fix tool call indexing double-counting](https://github.com/vllm-project/vllm/pull/33141)


### Base Information

- **PR Number:** #33141
- **Author:** [wangln19](https://github.com/wangln19)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-28 21:57:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33141/files) (1):**
  - `vllm/entrypoints/openai/chat_completion/serving.py`

### Summary

**What changed and why**  
The fix addresses a bug where tool call IDs were generated with non-sequential indices due to double-counting. Previously, `history_tool_call_cnt` was incremented inside the loop while also being added to the loop's `idx`, causing skipped indices. The changes ensure sequential IDs by using `history_tool_call_cnt` directly as the index and incrementing it appropriately.

**Technical impact**  
This correction ensures tool call IDs are strictly sequential across multiple choices and tool calls, which is critical for downstream systems that rely on accurate indexing, such as the `kimi_k2` ID format. The fix maintains consistency between streaming and non-streaming response generation paths.

**Potential risks**  
If `history_tool_call_cnt` is not properly initialized or managed elsewhere, sequential integrity could still break. The fix assumes the counter is correctly maintained across the entire request lifecycle, and any future modifications to related logic must preserve this increment pattern.

**Key insights**  
Developers should verify that `history_tool_call_cnt` is only incremented once per tool call, as shown by the added increment in the streaming generator. This pattern ensures index uniqueness and prevents regressions. Consider adding unit tests that validate sequential ID generation across both streaming and non-streaming endpoints.

---

## 5. [[Refactor] Define MM data parser in processing info instead of processor itself](https://github.com/vllm-project/vllm/pull/33260)


### Base Information

- **PR Number:** #33260
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-28 21:55:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33260/files) (34):**
  - `tests/model_executor/test_qwen3_omni.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/funaudiochat.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/keye_vl1_5.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/midashenglm.py`
  - `vllm/model_executor/models/minicpmo.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/opencua.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/processor.py`

### Summary

**What changed and why**  
This refactor moves the definition of `MultiModalDataParser` from individual processor classes (`_get_data_parser`) to the corresponding `ProcessingInfo` classes (`get_data_parser`). The goal is to separate data-parsing logic from processor classes, centralize parser configuration, and ensure consistent use of `_get_expected_hidden_size` for embedding validation.

**Technical impact**  
The changes decouple data parsing from processor logic, promoting a cleaner separation of concerns. Processors now rely on `self.data_parser` (obtained via `info.get_data_parser()`), reducing code duplication. The `_get_expected_hidden_size` method is consistently applied across models to validate embedding dimensions when `mm_embeds` is enabled, improving robustness.

**Potential risks**  
The deprecation warning for `_get_data_parser` may cause confusion if not all processors are updated. There is a risk of inconsistent parser initialization if `get_data_parser` is not properly implemented in all `ProcessingInfo` subclasses. The test modification (`test_qwen3_omni.py`) suggests some models may require manual `_get_expected_hidden_size` mocking.

**Key insights**  
Developers should ensure all new models implement `get_data_parser` in their `ProcessingInfo` class. The refactor simplifies adding custom parsers by centralizing configuration. Verify that `_get_expected_hidden_size` is correctly derived from model configs to prevent embedding shape mismatches.

---

## 6. [[ez] Delete more torch version checks <= 2.8](https://github.com/vllm-project/vllm/pull/33288)


### Base Information

- **PR Number:** #33288
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-28 21:28:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33288/files) (2):**
  - `vllm/compilation/compiler_interface.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/pytorch.py`

### Summary

**What changed and why**  
This PR removes version-specific code for PyTorch versions ≤2.8, consolidating two separate code paths into a single implementation. The changes eliminate checks for `torch.__version__` and replace them with a unified approach that no longer distinguishes between older PyTorch versions.

**Technical impact**  
The codebase is simplified by removing conditional logic that handled PyTorch 2.5 and 2.6+ differently. The compiler interface now uses a consistent method for hijacking `compile_fx_inner` and loading compiled graphs, while the quantization kernel drops a version check that is no longer needed. This reduces maintenance overhead and aligns with supporting newer PyTorch versions.

**Potential risks**  
If any environments still use PyTorch <2.6, the removed version-specific logic could cause failures, as the consolidated code assumes newer PyTorch APIs. The removal of the Torch version check in the quantization kernel might allow unsupported versions to attempt execution, potentially leading to runtime errors or incorrect behavior.

**Key insights**  
Developers should ensure the minimum supported PyTorch version is clearly documented and enforced elsewhere. The simplification improves readability but requires validation that the unified code path works correctly across all targeted PyTorch versions (≥2.6). Consider adding a runtime version check at module initialization if backward compatibility is a concern.

---

## 7. [[Misc] Add orozery to CODEOWNERS (core, kv_transfer, kv_offload)](https://github.com/vllm-project/vllm/pull/33227)


### Base Information

- **PR Number:** #33227
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-28 20:24:20
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33227/files) (1):**
  - `.github/CODEOWNERS`

### Summary

**What changed and why**  
The changes add the GitHub user `@orozery` as a code owner for three key areas: `kv_transfer`, `v1/core`, and `kv_offload` (including related test directories). This update likely reflects `@orozery`'s increased involvement or expertise in these components, ensuring they receive review notifications for future modifications.

**Technical impact**  
These updates modify the CODEOWNERS file, which automatically assigns reviewers to pull requests affecting specified paths. This will streamline the review process for changes in distributed key-value transfer, core v1 logic, and offloading components by involving `@orozery` in relevant code reviews.

**Potential risks**  
If `@orozery` is not adequately familiar with the codebase or becomes unavailable, it could slow down review cycles. Additionally, overlapping ownership with existing maintainers might lead to confusion about primary review responsibilities if not clearly communicated.

**Key insights**  
This is a routine maintenance update to align code ownership with current team expertise. Teams should ensure new code owners are onboarded to review expectations and that ownership boundaries remain clear to avoid review bottlenecks.

---

## 8. [[Bugfix] Register fp8 cutlass_group_gemm as supported for only SM90+SM100](https://github.com/vllm-project/vllm/pull/33285)


### Base Information

- **PR Number:** #33285
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-28 18:40:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33285/files) (1):**
  - `vllm/_custom_ops.py`

### Summary

**What changed and why**  
The change adds a capability check to restrict the fp8 cutlass_group_gemm operation to only SM90 and SM100 architectures. This fixes an issue where users on SM120 (and other unsupported architectures) would incorrectly default to this kernel backend and encounter unsupported errors, when they should instead fall back to the Triton kernel.

**Technical impact**  
This modification ensures the fp8 oracle properly selects compatible kernels by explicitly filtering out unsupported GPU architectures before delegating to the underlying C++ implementation. The system will now correctly route SM120+ devices to alternative backends like TRITON, maintaining functionality across different hardware generations.

**Potential risks**  
The hardcoded range check (90-109) may require updates when future architectures (SM110+) gain support. There's a slight risk of false negatives if the underlying C++ implementation could support additional architectures beyond what's currently documented, though the test results validate the intended behavior.

**Key insights**  
Always validate kernel availability against actual hardware implementation capabilities. The fix demonstrates proper layering: Python wrapper handles architectural constraints while preserving the underlying C++ check. Developers should consider making version ranges configurable or documented for future architecture support.

---

## 9. [[UX] Remove noisy CT UnquantizedLinearMethod warn](https://github.com/vllm-project/vllm/pull/33273)


### Base Information

- **PR Number:** #33273
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-28 16:09:30
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33273/files) (1):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`

### Summary

**What changed and why**  
The change removes a warning log statement when the Compressed Tensors backend encounters a non-quantized linear method. Instead of logging a warning, it now silently falls back to the `UnquantizedLinearMethod` with only a comment indicating the fallback behavior.

**Technical impact**  
This reduces log noise in environments where falling back to an unquantized method is expected or acceptable. The functional behavior remains unchanged—the system still defaults to `UnquantizedLinearMethod` when quantization is not applied.

**Potential risks**  
Removing the warning could obscure debugging information if the fallback is unintended or problematic. Developers might miss visibility into cases where quantization was expected but not applied, potentially leading to unnoticed performance degradation.

**Key insights**  
While reducing log clutter is beneficial, consider whether this warning should be retained in debug modes or logged at a lower severity level. Ensure there are other monitoring mechanisms to detect unexpected quantization scheme mismatches in production environments.

---

## 10. [[ModelRunner V2] Misc code simplification and cleanup](https://github.com/vllm-project/vllm/pull/33266)


### Base Information

- **PR Number:** #33266
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-28 14:41:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33266/files) (3):**
  - `vllm/v1/worker/gpu/async_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR performs code simplification and cleanup for ModelRunner V2, removing obsolete code fragments and streamlining logic. Key changes include eliminating unused async scheduling components, simplifying data structure initialization with dictionary comprehensions, and improving iteration patterns with `zip()` and `enumerate()`. A log message is added to indicate when V2 Model Runner is active.

**Technical impact**  
The changes reduce code complexity and improve maintainability by removing dead code (e.g., `async_barrier`, unused event attributes) and consolidating operations (e.g., replacing manual loops with comprehensions). Performance remains unaffected as the modifications primarily involve cleanup and readability improvements, with no alterations to core algorithms or data flow.

**Potential risks**  
Removing `async_barrier` could impact any external code relying on this utility, though it appears internal. The simplification of `computed_prefill` updates in `postprocess` changes the logic from element-wise min operations to in-place addition and min, which must be verified for correctness in edge cases (e.g., overflow or prefill length boundaries). The removal of `self.copy_stream` in `AsyncGPUToCPUOutput` may affect stream management if not handled elsewhere.

**Key insights**  
The cleanup enhances code clarity and reduces technical debt, aligning with ModelRunner V2's evolution. Developers should ensure the `computed_prefill` update logic is thoroughly tested, especially for speculative decoding scenarios. The added log aids in debugging V2 adoption. Future work should continue eliminating legacy patterns and consider deprecating `async_barrier` if unused.

---

## 11. [[7/N][Attention][Docs] Add documentation for attention backends](https://github.com/vllm-project/vllm/pull/32477)


### Base Information

- **PR Number:** #32477
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-28 14:20:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32477/files) (4):**
  - `.pre-commit-config.yaml`
  - `docs/configuration/optimization.md`
  - `docs/design/attention_backends.md`
  - `tools/pre_commit/generate_attention_backend_docs.py`

### Summary

**What changed and why**  
This PR adds comprehensive documentation for vLLM's attention backends, including auto-generated feature support tables and priority lists. It introduces a new pre-commit hook to ensure the documentation stays synchronized with the codebase, and updates the optimization guide to reference the new documentation.

**Technical impact**  
The changes create a centralized, auto-generated reference (`attention_backends.md`) that details backend capabilities, selection logic, and configuration options. This improves maintainability by automating documentation updates and provides users with clear guidance on backend compatibility and performance tuning. The pre-commit hook enforces consistency between the code and documentation.

**Potential risks**  
The AST-based parsing in the generation script may fail if the backend class structures change significantly (e.g., renamed validation methods). The documentation could become outdated if the pre-commit hook is bypassed or if relevant files are modified without triggering regeneration. Additionally, the tables are complex and may be difficult for users to interpret without careful reading.

**Key insights**  
Developers should run the generation script after modifying any attention backend code to keep documentation current. The structured approach to backend selection (manual vs. automatic) is clearly documented, which will help users debug compatibility issues. Consider adding integration tests to verify the generation script works against actual backend implementations.

---

## 12. [[UX] Enable nested configs in config yaml files](https://github.com/vllm-project/vllm/pull/33193)


### Base Information

- **PR Number:** #33193
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-28 13:54:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33193/files) (2):**
  - `tests/utils_/test_argparse_utils.py`
  - `vllm/utils/argparse_utils.py`

### Summary

**What changed and why**  
The PR enables nested YAML configurations in config files by modifying `load_config_file` to detect dictionary values and serialize them as JSON strings. This eliminates the need for users to manually write JSON strings within YAML, allowing cleaner, more readable configuration files.

**Technical impact**  
The change maintains backward compatibility by preserving flat key-value parsing while adding support for nested structures. Nested dictionaries are automatically converted to JSON strings, which are then processed by existing JSON argument parsing logic, ensuring no disruption to current argument handling.

**Potential risks**  
If nested values contain non-serializable objects (e.g., custom classes), JSON serialization may fail. Additionally, any existing configs relying on string literals that resemble dictionaries could be incorrectly serialized, though this is unlikely given the previous JSON-in-YAML format.

**Key insights**  
Developers should ensure nested configs use only JSON-serializable types. The update simplifies configuration management but requires thorough testing of edge cases, particularly with complex nested structures or mixed flat/nested configurations.

---

## 13. [[Bugfix] Add missing encoder only guard for do_kv_cache_update](https://github.com/vllm-project/vllm/pull/33269)


### Base Information

- **PR Number:** #33269
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-28 13:25:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33269/files) (1):**
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
Added an early return guard in `do_kv_cache_update` to skip KV cache operations for encoder-only and encoder attention types. This prevents unnecessary cache updates since encoder models process input directly without caching key-value pairs.

**Technical impact**  
This change ensures the KV cache update logic only executes for decoder and cross-attention scenarios, aligning with the architectural assumption that encoder attention doesn't require caching. It prevents potential runtime errors or incorrect behavior when encoder models inadvertently trigger cache updates.

**Potential risks**  
If the `attn_type` classification is incorrect or changes dynamically, valid cache updates might be incorrectly skipped. The change assumes encoder attention types are properly identified and don't require any cache-related operations whatsoever.

**Key insights**  
This is a defensive programming fix that clarifies the separation between encoder and decoder attention mechanisms. Developers should verify that all attention type classifications are accurate and consistent across the codebase, particularly when adding new attention variants.

---

## 14. [[ez] Remove checks for torch version <= 2.8](https://github.com/vllm-project/vllm/pull/33209)


### Base Information

- **PR Number:** #33209
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-28 13:03:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33209/files) (11):**
  - `vllm/_aiter_ops.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/compiler_interface.py`
  - `vllm/compilation/decorators.py`
  - `vllm/compilation/inductor_pass.py`
  - `vllm/config/compilation.py`
  - `vllm/distributed/utils.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/torchao.py`
  - `vllm/utils/torch_utils.py`

### Summary

**What changed and why**  
This PR removes version checks for PyTorch ≤ 2.8 across multiple files, assuming vLLM now requires PyTorch 2.8+. The changes eliminate conditional logic and compatibility code for older PyTorch versions, simplifying the codebase.

**Technical impact**  
The codebase now relies on PyTorch 2.8+ features and APIs, such as `torch._inductor.custom_graph_pass.CustomGraphPass`, `ProcessGroup` constructor changes, and `standalone_compile` availability. This reduces maintenance overhead and removes fallback paths for older versions.

**Potential risks**  
Users with PyTorch < 2.8 will encounter errors or missing imports. The removal of `torch.Tag.needs_fixed_stride_order` tags for custom ops in older versions may affect tensor stride handling if any compatibility shims were required. Additionally, environment variable overrides for TorchAO quantization are removed, which could impact caching behavior.

**Key insights**  
Developers must ensure PyTorch ≥ 2.8 is installed. The PR simplifies code but enforces a stricter dependency. Reviewers should verify that all removed conditionals were indeed obsolete and that no runtime behavior changes affect performance or correctness on supported PyTorch versions.

---

## 15. [Use aiter triton fused_add_rmsnorm_pad for gpt-oss](https://github.com/vllm-project/vllm/pull/30976)


### Base Information

- **PR Number:** #30976
- **Author:** [Rohan138](https://github.com/Rohan138)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-28 12:47:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30976/files) (9):**
  - `tests/compile/test_fuse_act_padding.py`
  - `tests/compile/test_fusion.py`
  - `vllm/_aiter_ops.py`
  - `vllm/compilation/pass_manager.py`
  - `vllm/compilation/rocm_aiter_fusion.py`
  - `vllm/config/compilation.py`
  - `vllm/config/vllm.py`
  - `vllm/model_executor/layers/utils.py`
  - `vllm/model_executor/models/gpt_oss.py`

### Summary

**What changed and why**  
This PR introduces a fused operation that combines RMSNorm with residual addition and padding for GPT-OSS models on ROCm. The change eliminates a separate padding operation after the router GEMM, improving performance by reducing kernel launches. The implementation adds a new AITER Triton kernel (`fused_add_rmsnorm_pad`) and integrates it into the compilation pipeline via a new fusion pass.

**Technical impact**  
The changes add a new custom op (`rocm_aiter_triton_add_rmsnorm_pad`) and a corresponding fusion pass that replaces the sequence of RMSNorm+add followed by padding with a single fused kernel. This reduces memory operations and kernel overhead, particularly benefiting GPT-OSS models with hidden size 2880. The fusion is conditionally enabled based on ROCm/AITER availability and model configuration.

**Potential risks**  
The fusion is tightly coupled to specific model parameters (hidden size 2880, padding multiples 128/256) and ROCm hardware, which may limit applicability. Incorrect padding calculations could lead to shape mismatches or silent errors. The dependency on AITER Triton kernels introduces external library reliance, and the fusion pass may not handle edge cases like dynamic shapes or varying epsilon values beyond the hardcoded set.

**Key insights**  
Developers should ensure the fusion is only enabled under supported conditions (ROCm with AITER, hidden size 2880). The padding logic assumes fixed model dimensions; any changes to GPT-OSS architecture may require updates. The fusion pass should be extended to support more flexible configurations if similar optimizations are needed for other models.

---

## 16. [[Feature] Fully support for async scheduling + PP, 30.8% E2E throughput improvement, 31.8% TPOT improvement](https://github.com/vllm-project/vllm/pull/32618)


### Base Information

- **PR Number:** #32618
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-28 12:30:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32618/files) (4):**
  - `tests/test_config.py`
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR enables full support for async scheduling with pipeline parallelism (PP) by modifying how sampled token IDs are communicated between PP stages. Instead of relying on the scheduler to pass token IDs back through CPU, it implements a GPU broadcast from the last PP stage to earlier stages, allowing multiple in-flight scheduling steps per request.

**Technical impact**  
The changes remove the restriction that prevented scheduling additional steps while a request had output placeholders in PP mode. This enables true async scheduling with PP, resulting in significant performance improvements: 30.8% end-to-end throughput and 31.8% TPOT improvement. The architecture now uses direct GPU communication for token IDs in async mode while maintaining the existing CPU path for non-async PP.

**Potential risks**  
The broadcast mechanism assumes sampled_token_ids have shape [num_reqs, 1], which could break with different sampling configurations. There's a risk of synchronization issues if broadcast timing mismatches between stages. The TODO note indicates the new_token_ids field may still need investigation for complete async PP support.

**Key insights**  
Developers should verify that all sampling methods produce the expected tensor shape for broadcast. The performance gains are substantial, but careful testing is needed for edge cases in mixed scheduling modes. The changes maintain backward compatibility for non-async PP while enabling a more efficient communication path for async scheduling.

---

## 17. [[CI] Change GPU key to device key for B200 test](https://github.com/vllm-project/vllm/pull/33275)


### Base Information

- **PR Number:** #33275
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-01-28 11:14:29
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33275/files) (1):**
  - `.buildkite/test_areas/lm_eval.yaml`

### Summary

**What changed and why**  
The change replaces the `gpu` key with `device` key in a Buildkite pipeline configuration for a B200 integration test. This appears to be an update to align with a pipeline generator that now uses a more generic `device` parameter instead of the specific `gpu` parameter.

**Technical impact**  
This modification standardizes hardware specification across the CI/CD pipeline configuration. The `device` key likely supports a broader range of hardware types (not just GPUs), making the configuration more flexible and future-proof for different accelerator types.

**Potential risks**  
If the pipeline generator or Buildkite runner doesn't properly handle the `device` key, the B200 test job may fail to execute or may not be scheduled on the correct hardware. There's also a risk if other pipeline configurations still use `gpu` inconsistently, potentially causing configuration drift.

**Key insights**  
This is a straightforward configuration standardization that improves abstraction for hardware specification. Developers should verify that all similar configurations are updated consistently and ensure the pipeline generator properly maps `device` values to actual hardware resources. The optional flag suggests this is a temporary test, so monitoring its execution is particularly important.

---

## 18. [[Perf] Optimize `moe_permute` kernel, 40%~300% kernel performance improvement](https://github.com/vllm-project/vllm/pull/32892)


### Base Information

- **PR Number:** #32892
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-28 10:15:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32892/files) (3):**
  - `csrc/moe/moe_permute_unpermute_op.cu`
  - `csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.h`
  - `csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.inl`

### Summary

**What changed and why**  
The optimization introduces `aligned_expert_first_token_offset` to precompute aligned prefix offsets for experts, moving alignment calculations from the kernel's shared memory to a separate `getMIndices` kernel. This reduces per-thread computations and eliminates shared memory usage in the main permute kernel.

**Technical impact**  
Kernel performance improves by 40–300% across batch sizes by removing shared memory loads and serialized alignment loops. The architecture now separates alignment computation (`getMIndices`) from data permutation, enabling better parallelism and reducing memory bandwidth contention in the critical path.

**Potential risks**  
If `align_block_size` is provided but `aligned_expert_first_token_offset_ptr` remains null due to allocation failure, the kernel may access invalid memory. Additionally, the `getMIndices` kernel adds a slight overhead for small batch sizes, though benchmarks show net gains.

**Key insights**  
Precomputing alignment offsets significantly reduces kernel complexity and shared memory pressure. Developers should ensure `align_expert_first_token_offset` is properly allocated when `align_block_size` is set. The change is backward-compatible but requires validation of the new tensor lifecycle in all execution paths.

---

## 19. [[CI] Whisper tests `enforce_eager=False`](https://github.com/vllm-project/vllm/pull/33098)


### Base Information

- **PR Number:** #33098
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-28 09:36:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33098/files) (1):**
  - `tests/entrypoints/openai/test_transcription_validation_whisper.py`

### Summary

**What changed and why**  
The change removes the `--enforce-eager` flag from the Whisper transcription test configuration. This ensures CI tests exercise the compiled execution path (using torch.compile) instead of the eager mode, aligning test coverage with the default production configuration where Whisper optimizations are enabled.

**Technical impact**  
Tests will now run with the model's compiled graph execution, which better reflects real-world usage where Whisper leverages torch.compile for performance. This provides more meaningful validation of the optimization features that are part of the default `vllm serve` configuration.

**Potential risks**  
If torch.compile or graph execution has stability issues specific to Whisper, test failures could occur that weren't previously caught. There's also a slight increase in test execution time due to compilation overhead, though this is minimal for Whisper as noted.

**Key insights**  
This change improves test fidelity by matching production defaults. Developers should ensure all Whisper-related tests pass with graph execution enabled and monitor for any compilation-specific failures. Consider adding a separate test case for eager mode if needed for debugging compilation issues.

---

## 20. [[lora/moe] Avoid extra intermediate buffer & Python slicing in expand phase when split_k == 1](https://github.com/vllm-project/vllm/pull/32774)


### Base Information

- **PR Number:** #32774
- **Author:** [cwazai](https://github.com/cwazai)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-28 08:22:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32774/files) (1):**
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`

### Summary

**What changed and why**  
This PR introduces an `ADD_TO_C` mode to the fused MoE LoRA kernel, allowing the expand-stage GEMM to accumulate results directly into the final output tensor when `split_k == 1`. This eliminates the need for an intermediate buffer (`b_intermediate_cache1`) and a Python loop that previously performed slice-wise addition, reducing memory overhead and CPU-GPU synchronization.

**Technical impact**  
The changes optimize memory usage and computational efficiency by avoiding an extra buffer allocation and Python-side operations. When `split_k == 1` (the default for expand), the kernel writes directly to the output slice, improving performance. For `split_k > 1`, the original path is preserved to maintain correctness for parallel reductions, ensuring backward compatibility.

**Potential risks**  
If the `ADD_INPUTS` logic is incorrectly applied or the kernel's memory access patterns are misaligned, it could lead to data corruption or race conditions. The fallback path for `split_k > 1` must be thoroughly validated to ensure it handles edge cases, such as varying tensor shapes or concurrent executions, without introducing regressions.

**Key insights**  
The optimization significantly reduces latency and improves throughput, as shown by benchmark improvements in token generation and request handling. Developers should ensure `split_k == 1` remains the default for expand operations to maximize benefits. Future work could explore extending direct accumulation to other `split_k` values while maintaining atomic operation safety.

---

## 21. [[Benchmark] Add startup benchmarking to buildkite run](https://github.com/vllm-project/vllm/pull/33183)


### Base Information

- **PR Number:** #33183
- **Author:** [desertfire](https://github.com/desertfire)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-28 08:03:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33183/files) (1):**
  - `.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh`

### Summary

**What changed and why**  
Refactored the benchmark script to consolidate latency and throughput test execution into a generic `run_benchmark_tests` function, and added support for startup benchmarking. This enables code reuse and prepares for collecting compilation time metrics needed by the torch.compile dashboard.

**Technical impact**  
The architecture now uses a unified test runner that accepts a test type parameter (latency, throughput, or startup), reducing code duplication by ~60 lines. This makes it easier to add new benchmark types in the future. The startup tests are integrated into the main execution flow alongside existing benchmarks.

**Potential risks**  
The refactored function assumes consistent JSON structure across all test types—any deviation in test file format could cause failures. The dynamic command construction (`vllm bench $test_type`) relies on the vllm tool supporting the "startup" subcommand, which must be verified. Test name validation now uses pattern matching with the test type prefix, which could break if test naming conventions change.

**Key insights**  
This is a well-executed refactoring that follows DRY principles and cleanly extends functionality. Developers should ensure startup-test.json files follow the established pattern and that the vllm CLI supports the new benchmark type. The reduced code footprint improves maintainability for future benchmark additions.

---

## 22. [[Quantization][Deprecation] Remove Marlin 24](https://github.com/vllm-project/vllm/pull/32688)


### Base Information

- **PR Number:** #32688
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-01-28 07:55:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32688/files) (20):**
  - `CMakeLists.txt`
  - `benchmarks/kernels/benchmark_marlin.py`
  - `csrc/quantization/marlin/sparse/LICENSE`
  - `csrc/quantization/marlin/sparse/common/base.h`
  - `csrc/quantization/marlin/sparse/common/mem.h`
  - `csrc/quantization/marlin/sparse/common/mma.h`
  - `csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu`
  - `csrc/torch_bindings.cpp`
  - `tests/compile/fullgraph/test_full_graph.py`
  - `tests/kernels/quantization/test_marlin_gemm.py`
  - `tests/models/quantization/test_gptq_marlin_24.py`
  - `tests/quantization/test_compressed_tensors.py`
  - `vllm/_custom_ops.py`
  - `vllm/config/model.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin_24.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_test_24.py`

### Summary

**What changed and why**  
This PR removes the Marlin 24 (sparse) quantization implementation from the codebase. The change follows a deprecation notice in version 0.14 and now fully removes the feature in version 0.15. The removal includes kernel code, configuration files, tests, benchmarks, and associated utilities.

**Technical impact**  
Eliminating Marlin 24 reduces code complexity and maintenance overhead. Users relying on this quantization method will need to migrate to alternative quantization schemes (e.g., standard Marlin or other supported methods). The build system, import paths, and quantization registry are updated to reflect the removal, ensuring no accidental usage.

**Potential risks**  
- Existing models serialized in Marlin 24 format will become incompatible unless converted.  
- Users who depended on Marlin 24 for specific performance or compatibility needs may experience disruption.  
- The removal of extensive test coverage could hide regressions in related quantization code if not adequately covered by other tests.

**Key insights**  
- This is a clean, comprehensive removal that aligns with the deprecation policy.  
- Developers should verify that alternative quantization methods meet performance requirements for affected use cases.  
- Ensure documentation is updated to guide users toward supported quantization options.

---

## 23. [[Misc] Provide a DeepSeek ReasoningParser with thinking enabled by default](https://github.com/vllm-project/vllm/pull/33221)


### Base Information

- **PR Number:** #33221
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-28 05:16:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33221/files) (6):**
  - `tests/reasoning/test_holo2_reasoning_parser.py`
  - `vllm/reasoning/__init__.py`
  - `vllm/reasoning/deepseek_v3_reasoning_parser.py`
  - `vllm/reasoning/glm4_moe_reasoning_parser.py`
  - `vllm/reasoning/holo2_reasoning_parser.py`
  - `vllm/reasoning/kimi_k2_reasoning_parser.py`

### Summary

**What changed and why**  
This PR consolidates multiple reasoning parsers into a unified `DeepSeekV3ReasoningWithThinkingParser` that defaults to thinking mode. It removes three parser classes (`Glm4MoeModelReasoningParser`, `Holo2ReasoningParser`, `KimiK2ReasoningParser`) and updates the registry to point these model types to the new parser. The change eliminates code duplication while maintaining the same reasoning behavior with thinking enabled by default.

**Technical impact**  
The architecture is simplified by reducing three similar parser implementations to a single class. The `DeepSeekV3ReasoningWithThinkingParser` inherits from `DeepSeekV3ReasoningParser` and automatically sets `thinking=True` and `enable_thinking=True` in chat template kwargs when not explicitly provided. This affects how models registered under "glm45", "holo2", and "kimi_k2" handle reasoning extraction.

**Potential risks**  
If any of the removed parsers had subtle behavioral differences beyond default thinking mode, those differences could be lost. The test changes show Holo2 tests now use the new parser, but comprehensive testing is needed for all affected model types. The automatic setting of chat template kwargs could interfere with explicit user configurations if not carefully handled.

**Key insights**  
This is a good refactoring that reduces maintenance overhead, but developers should verify that all affected models behave identically with the new parser. The inheritance approach cleanly separates the "default thinking" behavior from the core parsing logic. Future parser implementations should follow this pattern of composing behavior rather than duplicating entire classes.

---

## 24. [Revert "Enable Cross layers KV cache layout at NIXL Connector (#30207)"](https://github.com/vllm-project/vllm/pull/33241)


### Base Information

- **PR Number:** #33241
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-28 04:36:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33241/files) (5):**
  - `docs/features/nixl_connector_usage.md`
  - `tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh`
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR fully reverts the cross-layers KV cache layout feature introduced in #30207 and its follow-up #33052. The changes remove all code, configuration options, and tests related to enabling contiguous physical memory blocks across attention layers in the NIXL connector.

**Technical impact**  
The removal simplifies the KV cache memory layout logic by eliminating the cross-layers optimization path. This reduces code complexity in the NIXL connector, KV topology calculations, and attention backend integrations. The system now consistently uses per-layer KV cache registration, which may increase the number of buffers transferred but ensures broader compatibility across attention backends.

**Potential risks**  
Reverting this feature could negatively impact performance for workloads that benefited from reduced buffer transfers, particularly on FlashAttention and FlashInfer backends. There is also a risk of breaking any existing configurations that had enabled `enable_cross_layers_blocks`, though the feature was experimental. The removal of associated tests reduces coverage for edge cases involving cross-layer memory layouts.

**Key insights**  
This revert indicates the cross-layers feature was either unstable, not providing expected benefits, or incompatible with other system components. Developers should update any deployment scripts that referenced `enable_cross_layers_blocks` or `kv_connector_extra_config` settings. Future optimizations should consider a more robust implementation that maintains compatibility across all supported attention backends.

---

## 25. [[Quantization][Deprecation] Remove BitBlas](https://github.com/vllm-project/vllm/pull/32683)


### Base Information

- **PR Number:** #32683
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-01-28 03:06:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32683/files) (15):**
  - `benchmarks/kernels/benchmark_bitblas.py`
  - `docs/features/quantization/README.md`
  - `docs/features/quantization/bitblas.md`
  - `tests/models/quantization/test_bitblas.py`
  - `tests/models/quantization/test_gptq_bitblas.py`
  - `vllm/config/model.py`
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/bitblas.py`
  - `vllm/model_executor/layers/quantization/gptq.py`
  - `vllm/model_executor/layers/quantization/gptq_bitblas.py`
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py`
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py`
  - `vllm/model_executor/layers/quantization/utils/bitblas_utils.py`
  - `vllm/model_executor/parameter.py`

### Summary

**What changed and why**  
This PR removes BitBLAS quantization support from vLLM as part of a deprecation plan for version 0.15. The changes delete all BitBLAS-related code, including configuration files, linear method implementations, kernel definitions, utility functions, benchmarks, tests, and documentation references.

**Technical impact**  
Users relying on BitBLAS quantization will no longer be able to use it after this change. The removal affects the quantization method registry, weight loading logic, and parameter sharding adjustments. Existing models quantized with BitBLAS will need to be converted to other supported quantization formats (e.g., GPTQ Marlin) for continued use.

**Potential risks**  
- Breaking changes for users with BitBLAS-quantized models; they must migrate to alternative quantization methods.
- Loss of performance optimizations that BitBLAS provided for specific hardware or precision combinations.
- Possible gaps in quantization coverage if BitBLAS supported unique configurations not available in other backends.

**Key insights**  
- This is a clean removal with no transitional deprecation warnings, so users must update their workflows promptly.
- Developers should verify that alternative quantization methods (e.g., GPTQ Marlin) meet performance and compatibility requirements.
- The deletion of extensive utility code reduces maintenance burden but may affect future extensibility for similar low-precision kernels.

---

## 26. [[CI] Update job dependency syntax for Intel and AMD jobs](https://github.com/vllm-project/vllm/pull/33240)


### Base Information

- **PR Number:** #33240
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-28 01:33:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33240/files) (5):**
  - `.buildkite/hardware_tests/amd.yaml`
  - `.buildkite/hardware_tests/intel.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `.buildkite/test_areas/models_basic.yaml`
  - `.buildkite/test_areas/models_multimodal.yaml`

### Summary

**What changed and why**  
Updated Buildkite pipeline configurations to use modern YAML syntax for job dependencies. Changed from `~` (null/empty) to `[]` (empty array) for jobs with no dependencies, and from string to list format for jobs with dependencies.

**Technical impact**  
These changes standardize dependency declarations across the CI pipeline, improving consistency and maintainability. The list format (`[]` or `- dependency-name`) is the preferred Buildkite syntax and provides better support for multiple dependencies.

**Potential risks**  
Minimal risk as this is primarily a syntax update. However, any misalignment in YAML indentation could cause parsing errors. The AMD/Intel GPU test changes remove explicit null dependencies, which should be functionally equivalent but rely on Buildkite's default behavior.

**Key insights**  
This cleanup improves pipeline readability and aligns with Buildkite best practices. Developers should verify all modified pipelines execute correctly in CI. Future dependency additions should use the list format for consistency.

---

## 27. [[CI] Update job dependency for hardware and CPU jobs](https://github.com/vllm-project/vllm/pull/33237)


### Base Information

- **PR Number:** #33237
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-28 01:10:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33237/files) (5):**
  - `.buildkite/hardware_tests/amd.yaml`
  - `.buildkite/hardware_tests/intel.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `.buildkite/test_areas/models_basic.yaml`
  - `.buildkite/test_areas/models_multimodal.yaml`

### Summary

**What changed and why**  
The changes modify Buildkite CI pipeline dependencies to fix CPU tests and enable hardware job execution per commit. Hardware jobs (AMD/Intel GPU) now run independently (`depends_on: ~`), while CPU test jobs now depend on the `image-build-cpu` job.

**Technical impact**  
This decouples hardware test jobs from the main pipeline dependency chain, allowing them to run immediately on commit. CPU test jobs now properly wait for the CPU image build to complete, ensuring they run with the correct environment. This improves pipeline efficiency by enabling parallel execution where appropriate.

**Potential risks**  
If `image-build-cpu` fails, all dependent CPU test jobs will be blocked, potentially hiding test failures. The hardware jobs running independently could consume resources even when other critical jobs fail. There's a risk of race conditions if hardware jobs depend on artifacts from earlier pipeline stages that no longer exist.

**Key insights**  
The dependency restructuring correctly separates hardware and CPU test workflows. Developers should monitor pipeline execution times and failure rates to ensure the new dependency graph works as intended. Consider adding conditional execution for hardware jobs based on code changes to optimize resource usage. Ensure `image-build-cpu` has appropriate retry logic since it's now a critical path dependency.

---

## 28. [[CI] Enable mypy import following for `vllm/compilation`](https://github.com/vllm-project/vllm/pull/33199)


### Base Information

- **PR Number:** #33199
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-28 00:59:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33199/files) (5):**
  - `pyproject.toml`
  - `tools/pre_commit/mypy.py`
  - `vllm/compilation/inductor_pass.py`
  - `vllm/config/compilation.py`
  - `vllm/model_executor/layers/rotary_embedding/__init__.py`

### Summary

**What changed and why**  
This PR simplifies the mypy configuration by enabling import following for the `vllm/compilation` module, removing its special strict checking treatment. The changes revert strict mypy overrides and `__all__` exports that were previously needed because import following was disabled for this module, aligning with a broader effort to standardize type checking.

**Technical impact**  
The `vllm/compilation` module is now treated like other code directories in the mypy check—it's included in the main file group and no longer has separate strict rules. This reduces configuration complexity and moves toward a unified mypy setup, though it may relax some type-checking rigor previously enforced on that module.

**Potential risks**  
Removing strict checks (`disallow_untyped_defs`, `disallow_incomplete_defs`, `warn_return_any`) could allow less precise type annotations to go unnoticed in `vllm/compilation`. The reverted `__all__` statements might affect explicit public API declarations, though this is likely safe if the modules are only imported internally.

**Key insights**  
This is a cleanup step toward eliminating custom mypy configurations. Developers should ensure that `vllm/compilation` now maintains type safety without strict enforcement. Future work should verify that the module's types remain robust and that any removed `__all__` statements do not impact external usage.

---

