# vLLM Merged PR Report

**Report Date:** 2026-01-14 PST

**Total Merged PRs:** 24

---

## 1. [[Refactor] [10/N] to simplify the vLLM openai completion serving architecture](https://github.com/vllm-project/vllm/pull/32369)


### Base Information

- **PR Number:** #32369
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 23:41:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32369/files) (43):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_cli_args.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_gptoss_structural_tags_integration.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/entrypoints/openai/test_serving_engine.py`
  - `tests/entrypoints/openai/test_serving_models.py`
  - `tests/v1/engine/test_async_llm.py`
  - `vllm/entrypoints/anthropic/__init__.py`
  - `vllm/entrypoints/anthropic/api_router.py`
  - `vllm/entrypoints/anthropic/serving.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/entrypoints/openai/completion/__init__.py`
  - `vllm/entrypoints/openai/completion/api_router.py`
  - `vllm/entrypoints/openai/completion/protocol.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/models/__init__.py`
  - `vllm/entrypoints/openai/models/api_router.py`
  - `vllm/entrypoints/openai/models/protocol.py`
  - `vllm/entrypoints/openai/models/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/openai/run_batch.py`
  - `vllm/entrypoints/openai/translations/api_router.py`
  - `vllm/entrypoints/openai/translations/serving.py`
  - `vllm/entrypoints/openai/translations/speech_to_text.py`
  - `vllm/entrypoints/pooling/classify/serving.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/sagemaker/routes.py`
  - `vllm/entrypoints/serve/disagg/api_router.py`
  - `vllm/entrypoints/serve/disagg/protocol.py`
  - `vllm/entrypoints/serve/disagg/serving.py`
  - `vllm/entrypoints/serve/elastic_ep/api_router.py`
  - `vllm/entrypoints/serve/lora/api_router.py`
  - `vllm/entrypoints/serve/tokenize/api_router.py`
  - `vllm/entrypoints/serve/tokenize/serving.py`
  - `vllm/entrypoints/utils.py`

### Summary

**What changed and why**  
This PR refactors the vLLM OpenAI completion serving architecture to simplify the codebase by modularizing endpoints into separate API routers. The main changes include extracting completion, models, and Anthropic endpoints into dedicated router modules, moving protocol definitions to appropriate packages, and cleaning up the main API server file.

**Technical impact**  
The architecture shifts from a monolithic API server to a modular design where each endpoint group (completion, models, Anthropic) has its own router, protocol definitions, and serving logic. This improves code organization, separation of concerns, and maintainability. The main API server now acts as a coordinator that registers these modular routers rather than containing all endpoint logic directly.

**Potential risks**  
The refactor involves significant file movements and import changes across 43 files, which increases the risk of import errors or broken dependencies. Changes to error handling in translation endpoints could affect error response formats. The extraction of completion protocol logic from engine/protocol.py to a new module may break custom extensions that relied on the previous structure.

**Key insights**  
This is part 10 of a larger refactoring effort to simplify the serving architecture. Developers should verify all imports are updated correctly, particularly in test files and custom extensions. The modular approach will make future endpoint additions cleaner but requires careful attention to dependency management between the new modules.

---

## 2. [[ROCm][CI] Pin transformers 4.57.3 to fix jina test failures](https://github.com/vllm-project/vllm/pull/32350)


### Base Information

- **PR Number:** #32350
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-14 23:19:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32350/files) (1):**
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
The PR pins the `transformers` dependency to version `4.57.3` in the ROCm test requirements file. This change addresses test failures in Jina multimodal model tests that occurred after an upgrade to `transformers>=4.57.5`, which introduced breaking changes in tokenizer attribute access and method signatures.

**Technical impact**  
This change ensures test stability by locking the dependency to a known working version, preventing CI failures due to upstream regressions. It temporarily avoids the need for code modifications in the Jina model implementation until the upstream issue is resolved.

**Potential risks**  
Pinning may delay necessary updates if the upstream fix takes time, potentially causing version drift with other dependencies. There is also a risk that other parts of the codebase might rely on newer `transformers` features not available in 4.57.3.

**Key insights**  
This is a temporary workaround; developers should monitor the upstream issue and plan to update once a fix is released. Consider adding a comment linking to the issue for visibility. Ensure comprehensive testing when unpinning to verify compatibility with all model integrations.

---

## 3. [[Bugfix] Fix stale `common_attn_metadata.max_seq_len` in speculative decoding with Eagle](https://github.com/vllm-project/vllm/pull/32312)


### Base Information

- **PR Number:** #32312
- **Author:** [ofirzaf](https://github.com/ofirzaf)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-14 22:39:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32312/files) (1):**
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
The fix addresses a stale `common_attn_metadata.max_seq_len` value during speculative decoding with the Eagle model. When sequences exceed the maximum model length, their `seq_lens` are capped at 1, but the `max_seq_len` field was not updated accordingly, potentially causing incorrect bounds for attention operations.

**Technical impact**  
This change ensures `max_seq_len` is incremented on each decoding step (capped at `self.max_model_len`) to maintain an accurate upper bound for sequence lengths. This prevents potential out-of-bounds errors or incorrect tensor allocations in attention computations that rely on this metadata.

**Potential risks**  
The unconditional increment could theoretically cause `max_seq_len` to drift from the actual maximum of `seq_lens` if many sequences are capped at 1. However, the explicit cap at `self.max_model_len` prevents unbounded growth. The comment acknowledges the hacky nature of the CPU-side shadow update, indicating technical debt.

**Key insights**  
This is a defensive fix for a subtle metadata synchronization issue. Developers should note that `max_seq_len` is treated as a conservative upper bound rather than the exact maximum sequence length. The code comment suggests the `seq_lens_cpu` shadow mechanism should eventually be refactored.

---

## 4. [[BugFix] Fix DeepSeek-V3.1 + DeepGEMM incompatible scale shapes](https://github.com/vllm-project/vllm/pull/32361)


### Base Information

- **PR Number:** #32361
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-14 22:32:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32361/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`

### Summary

**What changed and why**  
A condition was added to exclude DeepGEMM-quantized layers from a specific dequantization path. This prevents an assertion error that occurs when DeepGEMM-transformed scale shapes are incompatible with the `scaled_dequantize` function, reverting to the pre-PR#29867 behavior for DeepGEMM.

**Technical impact**  
DeepGEMM-quantized layers will now bypass the `scaled_dequantize` path, avoiding shape mismatch errors. This maintains compatibility for DeepSeek-V3.1 models using DeepGEMM while preserving the new dequantization logic for other FP8 quantization methods (like Marlin).

**Potential risks**  
If DeepGEMM's scale transformation logic changes in the future, this exclusion might need reevaluation. Additionally, other quantization methods with similar scale layout incompatibilities could trigger similar errors unless explicitly handled.

**Key insights**  
The fix is a targeted workaround for a specific integration issue. Developers should ensure any new quantization methods are tested with both `scaled_dequantize` and alternative dequantization paths. Consider adding a more generic mechanism to detect incompatible scale layouts rather than hardcoding method checks.

---

## 5. [[code clean] remove duplicate check](https://github.com/vllm-project/vllm/pull/32376)


### Base Information

- **PR Number:** #32376
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 21:29:34
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32376/files) (3):**
  - `vllm/multimodal/registry.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
The changes remove duplicate checks for `model_config.skip_tokenizer_init` in three locations. The logic for handling this configuration flag has been centralized inside the `cached_tokenizer_from_config` function, making external checks redundant.

**Technical impact**  
This refactoring simplifies the codebase by eliminating redundant conditional logic. The tokenizer initialization behavior remains unchanged since `cached_tokenizer_from_config` already handles the `skip_tokenizer_init` flag internally. This improves maintainability by having a single source of truth for tokenizer initialization logic.

**Potential risks**  
If `cached_tokenizer_from_config` doesn't properly handle the `skip_tokenizer_init` case (returning `None` when appropriate), these changes could break tokenizer initialization for configurations that require skipping tokenizer setup. The PR lacks test coverage, which increases risk.

**Key insights**  
This is a clean code improvement that reduces duplication. Developers should verify that `cached_tokenizer_from_config` correctly returns `None` when `skip_tokenizer_init=True`. Future changes to tokenizer initialization logic should be made only in `cached_tokenizer_from_config` to maintain consistency.

---

## 6. [[CI][AMD][Quantization][BugFix] Fix fp8 max in quant_utils.py and update test_fp8_quant.::test_static_fp8_quant_group_2d to use correct fp8 dtype and adjust atol/rtol](https://github.com/vllm-project/vllm/pull/32201)


### Base Information

- **PR Number:** #32201
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-14 21:04:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32201/files) (2):**
  - `tests/kernels/quantization/test_fp8_quant.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`

### Summary

**What changed and why**  
The PR fixes a platform-specific bug in FP8 quantization scaling and updates a related test. In `quant_utils.py`, the `scaled_quantize` function now uses a platform-aware FP8 maximum value from `get_fp8_min_max()` instead of a generic `finfo.max`. The test `test_static_fp8_quant_group_2d` is updated to use the correct FP8 dtype for the current platform and relaxes the absolute tolerance (`atol`) to `1e-5` to accommodate FP8 precision differences.

**Technical impact**  
These changes ensure consistent FP8 quantization behavior across different hardware platforms (e.g., ROCm vs. CUDA) by using platform-specific FP8 limits. The test adjustments align validation with realistic FP8 numerical tolerances, preventing spurious failures while maintaining correctness.

**Potential risks**  
If `get_fp8_min_max()` returns incorrect values for a platform, scaling may be off, leading to quantization errors. Relaxing `atol` to `1e-5` could mask genuine numerical issues if set too high, though FP8’s limited precision justifies some tolerance. Developers must ensure `current_platform.fp8_dtype()` is consistently used in all FP8 tests.

**Key insights**  
Always use platform-specific constants for low-precision data types like FP8 to avoid cross-platform discrepancies. Tests for quantization should account for hardware-dependent numerical tolerances. Verify that `get_fp8_min_max()` is robust across all supported platforms to maintain quantization accuracy.

---

## 7. [[ROCm][CI] Disable async scheduling on ROCm for test_structured_output[meta-llama/Meta-Llama-3.1-8B-Instruct-xgrammar-auto-speculative_config9]](https://github.com/vllm-project/vllm/pull/32355)


### Base Information

- **PR Number:** #32355
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-14 20:53:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32355/files) (1):**
  - `tests/v1/entrypoints/llm/test_struct_output_generate.py`

### Summary

**What changed and why**  
The change disables async scheduling specifically for ROCm platforms in the `test_structured_output` test case. This is a temporary workaround to unblock AMD CI, which started failing after async scheduling was enabled by default with speculative decoding in PR #31998. The test passes when `async_scheduling=False` is set.

**Technical impact**  
This modification adds a platform-specific argument (`async_scheduling=False`) only for ROCm, leaving other platforms unaffected. It ensures the test runs synchronously on AMD hardware, avoiding the underlying issue while maintaining existing behavior elsewhere. The change is minimal and localized to a single test configuration.

**Potential risks**  
The workaround masks an underlying compatibility issue between async scheduling and ROCm for this specific test/model combination. There is a risk that similar issues could surface in other tests or production scenarios on ROCm. Additionally, if not addressed, this temporary fix might become permanent, delaying the root cause investigation.

**Key insights**  
Developers should treat this as a temporary mitigation and prioritize investigating the root cause of async scheduling failures on ROCm. The issue may relate to interactions between speculative decoding, structured output, and AMD hardware. Consider adding a TODO comment or tracking issue to ensure the underlying problem is resolved.

---

## 8. [[Bugfix] Add CpuCommunicator.dispatch and combine to fix DP+MoE inference](https://github.com/vllm-project/vllm/pull/31867)


### Base Information

- **PR Number:** #31867
- **Author:** [kzwrime](https://github.com/kzwrime)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-14 20:50:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31867/files) (2):**
  - `vllm/distributed/device_communicators/base_device_communicator.py`
  - `vllm/distributed/device_communicators/cpu_communicator.py`

### Summary

**What changed and why**  
Added `dispatch` and `combine` methods to `CpuCommunicator` and updated the base class return type annotation to support MoE inference with data parallelism (DP) on CPU. The fix addresses a missing implementation that prevented correct MoE model inference when DP > 1, likely due to triggered sequence parallelism.

**Technical impact**  
Enables CPU-based MoE inference with data parallelism by integrating the existing `NaiveAll2AllManager` for tensor dispatch/combine operations. The base class return type is extended to optionally include extra tensors, aligning CPU behavior with GPU implementations and ensuring consistency across platforms.

**Potential risks**  
The fallback to "naive" all2all backend on CPU may impact performance for large-scale deployments. The `assert self.all2all_manager is not None` statements assume initialization via `use_all2all`; if not set, runtime errors could occur. Edge cases with `extra_tensors` handling in dispatch require careful validation.

**Key insights**  
CPU MoE support now matches GPU functionality, but performance considerations for production workloads remain. Developers should verify `use_all2all` is properly configured in CPU DP scenarios. The fix is minimal and leverages existing infrastructure, reducing maintenance overhead.

---

## 9. [[Misc] Remove redundant line](https://github.com/vllm-project/vllm/pull/32366)


### Base Information

- **PR Number:** #32366
- **Author:** [Potabk](https://github.com/Potabk)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 20:29:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32366/files) (1):**
  - `vllm/attention/layer.py`

### Summary

**What changed and why**  
Removed a redundant conditional assignment line (`output_shape = output_shape if output_shape is not None else query.shape`) because `output_shape` is always defined before this line executes. The change eliminates dead code that could cause confusion.

**Technical impact**  
This is a minor cleanup with no functional impact—the logic remains unchanged since the condition would never evaluate to `True`. The code becomes slightly more readable by removing unnecessary branching.

**Potential risks**  
Low risk: the line was never executed, so removal shouldn’t affect behavior. However, ensure no other code paths or future modifications inadvertently rely on this line’s fallback logic (though unlikely given the context).

**Key insights**  
Always validate that removed dead code isn’t referenced elsewhere. This cleanup improves code clarity, but consider adding a comment if the `output_shape` initialization logic is non-obvious to future developers.

---

## 10. [Support configure skip_special_tokens in openai response api](https://github.com/vllm-project/vllm/pull/32345)


### Base Information

- **PR Number:** #32345
- **Author:** [842974287](https://github.com/842974287)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-14 20:07:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32345/files) (1):**
  - `vllm/entrypoints/openai/responses/protocol.py`

### Summary

**What changed and why**  
Added a `skip_special_tokens` field to the `ResponsesRequest` model with a default value of `True`, and passed it through to the `to_sampling_params` method. This allows OpenAI API users to configure whether special tokens are skipped during token decoding, whereas previously this behavior was fixed.

**Technical impact**  
The change makes the decoding behavior more flexible by exposing an existing sampling parameter (`skip_special_tokens`) through the OpenAI-compatible API. It maintains backward compatibility by defaulting to `True`, preserving the current behavior unless explicitly overridden.

**Potential risks**  
If users set `skip_special_tokens=False`, special tokens (e.g., BOS, EOS, padding) may appear in generated text, which could disrupt downstream processing that assumes clean output. No validation is added for the parameter, but this is consistent with other boolean fields in the protocol.

**Key insights**  
This is a straightforward, low-risk enhancement that aligns with the existing parameter pattern. Developers should document this new option in the OpenAI API reference and consider whether any client SDKs need updates to expose the parameter.

---

## 11. [Fix optional parameter parsing in MiniMax M2 tool parser #32278](https://github.com/vllm-project/vllm/pull/32342)


### Base Information

- **PR Number:** #32342
- **Author:** [baonudesifeizhai](https://github.com/baonudesifeizhai)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-14 20:05:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32342/files) (1):**
  - `vllm/tool_parsers/minimax_m2_tool_parser.py`

### Summary

**What changed and why**  
The PR fixes optional parameter parsing in the MiniMax M2 tool parser by modifying how null values are handled. Previously, the code had redundant null checks that could cause incorrect parsing when "null" was listed as an allowed type but the actual value wasn't null. The change simplifies the logic to first check if the value itself indicates null (e.g., "null", "none", "nil") before proceeding to type conversion.

**Technical impact**  
This change ensures that optional parameters with `"type": ["string", "null"]` are correctly parsed. When a user provides a non-null value like `"/tmp/foo"`, it will now be properly interpreted as a string instead of incorrectly returning `null`. This aligns the parser's behavior with the expected JSON schema semantics for optional parameters.

**Potential risks**  
The removal of the redundant null check could theoretically affect edge cases where `"null"` is explicitly listed in `param_types` but the value is a different null-like string (e.g., "None"). However, the updated logic still covers common null representations. Additionally, the change might impact other tool parsers if they share similar code patterns, but the scope appears limited to MiniMax M2.

**Key insights**  
Developers should verify that the parser correctly handles all null-like strings ("null", "none", "nil") across different locales or model outputs. The fix highlights the importance of separating value interpretation from type validation in schema-based parsing. Consider adding unit tests for mixed-type arrays (e.g., `["string", "null"]`) to prevent regressions.

---

## 12. [[CI/Build][Hardware][AMD] Fix v1/shutdown](https://github.com/vllm-project/vllm/pull/31997)


### Base Information

- **PR Number:** #31997
- **Author:** [rjrock](https://github.com/rjrock)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 20:01:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31997/files) (3):**
  - `tests/v1/shutdown/conftest.py`
  - `tests/v1/shutdown/test_forward_error.py`
  - `tests/v1/shutdown/test_startup_error.py`

### Summary

**What changed and why**  
Added a `rocm_sitecustomize_factory` fixture and related fixtures (`rocm_evil_forward`, `rocm_evil_method`) to propagate monkeypatched methods to subprocesses on ROCm platforms. This ensures that forward-error and startup-error shutdown tests work correctly when ROCm uses spawn (instead of fork) for subprocess initialization.

**Technical impact**  
The changes enable cross-platform compatibility by dynamically injecting `sitecustomize.py` into spawned subprocesses' Python path, allowing monkeypatches to be applied in distributed test scenarios. This maintains test integrity on AMD hardware without altering the core test logic or assertions.

**Potential risks**  
If the `PYTHONPATH` environment variable manipulation conflicts with existing Python path configurations, it could affect other tests or subprocess behavior. Additionally, the fixture’s reliance on `inspect.getsource()` may break if the source code of patched functions becomes unavailable (e.g., in optimized or compiled environments).

**Key insights**  
Developers should ensure that any future tests relying on subprocess monkeypatching on ROCm use the provided fixture pattern. Consider centralizing similar cross-platform patching logic to avoid duplication and monitor for environment variable side effects in CI.

---

## 13. [[compile] raise on compile_size implicit padding](https://github.com/vllm-project/vllm/pull/32343)


### Base Information

- **PR Number:** #32343
- **Author:** [dolpm](https://github.com/dolpm)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-14 12:46:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32343/files) (2):**
  - `tests/compile/test_config.py`
  - `vllm/config/compilation.py`

### Summary

**What changed and why**  
Added validation in `CompilationConfig.post_init_cudagraph_sizes()` to raise a `ValueError` when `compile_sizes` contains values that would be padded due to CUDA graph capture sizes. This ensures that all provided compile sizes are actually compiled, preventing silent failures where padded sizes would never trigger compilation—critical for a subsequent change that saves compiled artifacts to disk.

**Technical impact**  
The change enforces that `compile_sizes` must align with `cudagraph_capture_sizes` (i.e., compile sizes must not be padded). Validation is skipped when CUDA graphs are disabled (`cudagraph_mode == CUDAGraphMode.NONE`), maintaining backward compatibility for non-CUDA-graph use cases. This tightens the contract between compile sizes and CUDA graph padding behavior.

**Potential risks**  
If users have existing configurations with `compile_sizes` that would be padded, their code will now raise an error, requiring updates to align with `cudagraph_capture_sizes`. Edge cases where `max_cudagraph_capture_size` is zero but `cudagraph_capture_sizes` is non-empty are handled, but any mismatch between these settings could cause confusion.

**Key insights**  
Developers must ensure `compile_sizes` are subset of `cudagraph_capture_sizes` when using CUDA graphs. The test suite comprehensively covers valid/invalid cases, including disabled CUDA graph modes. Consider documenting this requirement in configuration guides to prevent user errors.

---

## 14. [[BugFix] Assign page_size_padded when unifying kv cache spec.](https://github.com/vllm-project/vllm/pull/32283)


### Base Information

- **PR Number:** #32283
- **Author:** [Lumosis](https://github.com/Lumosis)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-14 12:10:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32283/files) (2):**
  - `tests/v1/core/test_kv_cache_utils.py`
  - `vllm/v1/core/kv_cache_utils.py`

### Summary

**What changed and why**  
The PR fixes a bug where the `page_size_padded` attribute was not being preserved when converting `SlidingWindowSpec` or `ChunkedLocalAttentionSpec` to `FullAttentionSpec` in the `unify_hybrid_kv_cache_specs` function. This attribute is newly added and must be retained during unification to ensure consistent KV cache behavior.

**Technical impact**  
The changes ensure that `page_size_padded` is correctly propagated when unifying hybrid KV cache specifications, maintaining alignment across different attention mechanisms. This prevents potential mismatches in memory layout or performance characteristics when layers use varied attention specs.

**Potential risks**  
If `page_size_padded` is not set (defaults to `None`), the unification may still proceed, but downstream logic might need to handle `None` values appropriately. Additionally, the test suite now includes comprehensive scenarios, but edge cases with mixed or invalid `page_size_padded` values across layers could arise.

**Key insights**  
Developers should verify that `page_size_padded` is consistently defined when using hybrid attention specs. The fix is minimal and focused, but ensure any future modifications to KV cache specs include similar attribute propagation. Review related unification logic for other newly added attributes to avoid similar oversights.

---

## 15. [[Bugfix][ROCm][performance] Resolve the performance regression issue of the Qwen3-Next-80B-A3B-Thinking under rocm_atten](https://github.com/vllm-project/vllm/pull/32336)


### Base Information

- **PR Number:** #32336
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-14 11:32:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32336/files) (1):**
  - `vllm/v1/attention/backends/rocm_attn.py`

### Summary

**What changed and why**  
The change modifies the `get_supported_kernel_block_sizes()` method in the ROCm attention backend to include block size 544 in the list of supported sizes. This restores the optimized performance path for models like Qwen3-Next-80B-A3B-Thinking, which were inadvertently throttled after a previous restriction limited support to only block sizes 16 and 32.

**Technical impact**  
This update allows the ROCm backend to correctly route models with non-standard block sizes (e.g., 544) through the Triton kernel path, which was previously optimized for such configurations. Standard models with block sizes 16 or 32 continue to use the native C++ kernel, ensuring no performance regression for typical use cases.

**Potential risks**  
Adding a hardcoded block size (544) may create maintenance overhead if other non-standard sizes emerge, requiring further updates. There is also a risk that future changes to the Triton kernel could inadvertently affect the performance or correctness of models using this specialized block size.

**Key insights**  
The fix is narrowly scoped and effectively restores performance for a specific model architecture without impacting standard models. Developers should consider generalizing the approach to handle arbitrary block sizes dynamically, or at least document the rationale for hardcoded exceptions to aid future maintenance.

---

## 16. [[MODEL] Fix handling of multiple channels for gpt-oss with speculative decoding](https://github.com/vllm-project/vllm/pull/26291)


### Base Information

- **PR Number:** #26291
- **Author:** [astralord](https://github.com/astralord)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-14 10:20:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/26291/files) (4):**
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/entrypoints/openai/test_serving_chat_stream_harmony.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/chat_completion/stream_harmony.py`

### Summary

**What changed and why**  
This PR fixes GPT-OSS Harmony streaming with speculative decoding by properly handling multiple channels (e.g., `<final>`, `<analysis>`, `None`) within a single decoding step. The issue previously caused generated messages to end abruptly because tokens from different channels weren't grouped correctly. The solution introduces a `TokenState` structure to track per-token channel/recipient information and refactors the delta extraction logic to merge consecutive tokens with the same state.

**Technical impact**  
The changes modify the streaming pipeline to accumulate token states per decoding step rather than processing a single aggregated delta. This ensures that content, reasoning, and tool calls are correctly grouped and emitted in the proper order, especially when speculative decoding produces multiple tokens across different channels in one batch. The architecture now supports complex interleaving of reasoning, tool calls, and final content within a single response chunk.

**Potential risks**  
If token grouping logic fails (e.g., due to unexpected channel transitions), it could lead to malformed tool calls or missing content. The increased complexity in delta extraction may introduce edge cases with multi-turn conversations where tool call indices are miscalculated. Additionally, the reliance on speculative decoding configurations could expose new failure modes if the draft model produces irregular channel sequences.

**Key insights**  
Developers should verify that the `TokenState` grouping correctly handles all channel transitions, especially when tool calls are interleaved with content. The updated logging now includes reasoning and tool call arguments, improving debuggability. Ensure tests cover speculative decoding scenarios thoroughly, as the fix is critical for maintaining streaming reliability with GPT-OSS models.

---

## 17. [[CI] Move rixl/ucx from Dockerfile.rocm_base to Dockerfile.rocm](https://github.com/vllm-project/vllm/pull/32295)


### Base Information

- **PR Number:** #32295
- **Author:** [qli88](https://github.com/qli88)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-14 08:53:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32295/files) (2):**
  - `docker/Dockerfile.rocm`
  - `docker/Dockerfile.rocm_base`

### Summary

**What changed and why**  
The RIXL/UCX build has been moved from Dockerfile.rocm_base to Dockerfile.rocm and placed in a new "test" stage. This change makes RIXL/UCX available without requiring a new base Docker image release, as it's currently only used in CI. The PR also switches to the official ROCm RIXL repository and removes the ETCD dependency.

**Technical impact**  
This change decouples the RIXL/UCX build from the base image, allowing faster iteration in CI testing without rebuilding the entire base image stack. The architecture now separates base dependencies from test-specific components, improving modularity. The removal of ETCD simplifies the build process since it's no longer required.

**Potential risks**  
Moving RIXL/UCX to the test stage means it won't be available in production images built from the base image, which could break if production code unexpectedly depends on it. The switch to a new RIXL repository and commit hash (f33a5599) introduces potential compatibility risks that should be validated in CI. The build now uses `uv pip` instead of plain `pip`, which may have different behavior.

**Key insights**  
This is a transitional change with a clear migration path - the team plans to create pre-compiled RIXL/UCX packages and remove this from Dockerfiles entirely. Developers should verify that all CI tests pass with the new RIXL version and ensure no production code has hidden dependencies on RIXL/UCX. The use of Docker multi-stage builds effectively isolates test dependencies from the base image.

---

## 18. [[1/N] Reorganize multimodal processing code](https://github.com/vllm-project/vllm/pull/32327)


### Base Information

- **PR Number:** #32327
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 07:25:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32327/files) (76):**
  - `docs/api/README.md`
  - `docs/contributing/model/multimodal.md`
  - `docs/design/mm_processing.md`
  - `tests/multimodal/test_processing.py`
  - `vllm/benchmarks/mm_processor.py`
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/bagel.py`
  - `vllm/model_executor/models/blip2.py`
  - `vllm/model_executor/models/chameleon.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/deepseek_vl2.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/fuyu.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/h2ovl.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/interns1.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/kanana_v.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/lightonocr.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/llava_next_video.py`
  - `vllm/model_executor/models/midashenglm.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/ovis.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen_vl.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/skyworkr1v.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/tarsier.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/multimodal.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_streaming.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/cache.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/processing/__init__.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/dummy_inputs.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This PR reorganizes multimodal processing code by moving `BaseDummyInputsBuilder` from `vllm.multimodal.profiling` to `vllm.multimodal.processing`. The primary goal is to resolve circular dependencies between `processor.py` and `dummy_inputs.py` by factoring out `context.py`. This restructuring consolidates related classes and improves module organization.

**Technical impact**  
The changes affect 76 files, primarily updating import statements to reference the new location of `BaseDummyInputsBuilder`. The `profiling` module is effectively deprecated, with its functionality merged into `processing`. This simplifies the dependency graph and enhances code maintainability. The new `context.py` centralizes context management and timing utilities, reducing duplication.

**Potential risks**  
While the changes are largely mechanical, there is a risk of missing import updates in less frequently used code paths. The removal of the `profiling` module could break external dependencies if they directly import from it. Additionally, the consolidation may introduce subtle behavioral differences if any class interactions were relying on the previous module separation.

**Key insights**  
Developers should update any custom multimodal processors to import `BaseDummyInputsBuilder` from `vllm.multimodal.processing`. The PR improves architectural clarity by grouping related multimodal processing logic together. Future work should ensure all documentation references are updated to reflect the new module structure.

---

## 19. [rename tokenize serving api request id prefix to tokenize](https://github.com/vllm-project/vllm/pull/32328)


### Base Information

- **PR Number:** #32328
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 06:52:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32328/files) (2):**
  - `vllm/entrypoints/serve/tokenize/protocol.py`
  - `vllm/entrypoints/serve/tokenize/serving.py`

### Summary

**What changed and why**  
The PR renames the request ID prefix for tokenize API endpoints from `"tokn-"` to `"tokenize-"`. This change aligns the prefix length with `"chatcmpl-"` used elsewhere and improves clarity by using the full word instead of an abbreviation.

**Technical impact**  
This is a cosmetic change that affects request ID generation for tokenize and detokenize API endpoints. It improves log readability and debugging by making request origins more immediately identifiable, but does not alter any functional behavior or API contracts.

**Potential risks**  
Low risk change since request IDs are typically used for logging and tracing rather than functional logic. However, any downstream systems parsing these IDs (monitoring, analytics) would need to be updated to recognize the new prefix pattern.

**Key insights**  
The change follows good naming conventions by using descriptive identifiers. Developers should ensure any automated log parsing or monitoring dashboards that rely on the `tokn-` prefix are updated. Consider adding a brief mention in release notes since this affects observable request identifiers.

---

## 20. [[Frontend] track responsesAPI server_load](https://github.com/vllm-project/vllm/pull/32323)


### Base Information

- **PR Number:** #32323
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 04:00:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32323/files) (2):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/responses/api_router.py`

### Summary

**What changed and why**  
This PR adds server load tracking to the responses API endpoints by applying the `@load_aware_call` decorator to three routes (`/v1/responses`, `/v1/responses/{response_id}`, and `/v1/responses/{response_id}/cancel`) and updates the documentation in `get_server_load_metrics` to include these endpoints. The change follows up on previous PRs (#13950 and #32256) that introduced load tracking for other OpenAI-compatible endpoints.

**Technical impact**  
The `@load_aware_call` decorator now monitors GPU utilization for responses API requests, aligning them with existing tracked routes like `/v1/chat/completions`. This ensures consistent server load metrics across all major endpoints, improving observability and potential load-balancing or autoscaling decisions.

**Potential risks**  
If the `load_aware_call` decorator has performance overhead, it could slightly increase latency for responses API calls. Additionally, any misconfiguration in the decorator might affect request handling or error reporting for these endpoints. The change assumes the decorator is thread-safe and compatible with the existing request flow.

**Key insights**  
This is a straightforward extension of existing monitoring infrastructure. Developers should verify that the decorator works correctly with streaming endpoints (e.g., `create_responses` with streaming enabled) and ensure no conflicts with other decorators like `@with_cancellation`. The update to the metrics endpoint documentation is essential for clarity.

---

## 21. [[Misc] Make mem utils can be reused by other platforms](https://github.com/vllm-project/vllm/pull/32322)


### Base Information

- **PR Number:** #32322
- **Author:** [shen-shanshan](https://github.com/shen-shanshan)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-14 03:46:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32322/files) (3):**
  - `vllm/platforms/interface.py`
  - `vllm/utils/mem_utils.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR refactors memory utility functions to be platform-agnostic by replacing direct `torch.cuda` calls with `current_platform` abstractions, enabling reuse across different hardware platforms. It also removes redundant memory clearing operations in the GPU worker that were already handled by the `memory_profiling()` function.

**Technical impact**  
The changes decouple memory profiling utilities from CUDA-specific implementations, making them reusable for other platforms like ROCm or custom accelerators. The architecture now relies on platform-specific implementations through the `current_platform` interface, improving code modularity and reducing duplication.

**Potential risks**  
The `__getattr__` modification in `interface.py` could mask platform compatibility issues by returning `None` instead of raising AttributeError, potentially leading to silent failures. There's also a risk if platform implementations don't properly handle edge cases like shared memory devices or if the memory profiling abstraction leaks platform-specific assumptions.

**Key insights**  
Developers should verify that all target platforms implement the required memory operations (`mem_get_info`, `memory_stats`, etc.) correctly. The string formatting change in `MemorySnapshot.__repr__` dynamically uses `device_name` but hardcodes "cuda_memory" in the variable name—consider renaming for consistency. Ensure thorough testing across all supported platforms to validate the abstraction works correctly.

---

## 22. [[Frontend] Standardize use of `create_error_response`](https://github.com/vllm-project/vllm/pull/32319)


### Base Information

- **PR Number:** #32319
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 03:22:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32319/files) (12):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/chat_completion/api_router.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/api_router.py`
  - `vllm/entrypoints/openai/serving_models.py`
  - `vllm/entrypoints/pooling/classify/api_router.py`
  - `vllm/entrypoints/pooling/embed/api_router.py`
  - `vllm/entrypoints/pooling/pooling/api_router.py`
  - `vllm/entrypoints/pooling/score/api_router.py`
  - `vllm/entrypoints/serve/disagg/api_router.py`
  - `vllm/entrypoints/serve/tokenize/api_router.py`
  - `vllm/entrypoints/utils.py`

### Summary

**What changed and why**  
This PR standardizes error handling across multiple API endpoints by replacing direct `HTTPException` raises with centralized `create_error_response` calls. The changes ensure consistent error message sanitization and proper error type classification (e.g., `OverflowError` and `NotImplementedError` are now mapped to appropriate HTTP status codes). This follows up on a prior PR (#31987) to sanitize error messages properly.

**Technical impact**  
Error handling is now unified through a single method (`create_error_response`), which applies sanitization to all error messages via `sanitize_message`. This improves maintainability and ensures consistent error responses across endpoints. The `utils.py` file also sees refactoring with conditional imports to avoid circular dependencies.

**Potential risks**  
If `sanitize_message` is overly aggressive, it might obscure debugging information. The removal of specific exception handlers (e.g., `OverflowError` in `api_server.py`) could lead to less granular error categorization if not all cases are covered in `create_error_response`. Changes to import structures in `utils.py` could introduce runtime issues if not all conditional imports are correctly handled.

**Key insights**  
Developers should verify that `sanitize_message` appropriately balances security and debuggability. The centralized error handler simplifies future updates but requires careful testing to ensure all exception types are correctly mapped. Reviewers should confirm no regressions in error responses for edge cases like template errors or validation failures.

---

## 23. [[Refactor] [9/N] to simplify the vLLM openai translations  serving ar chitecture](https://github.com/vllm-project/vllm/pull/32313)


### Base Information

- **PR Number:** #32313
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-14 02:20:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32313/files) (8):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/translations/__init__.py`
  - `vllm/entrypoints/openai/translations/api_router.py`
  - `vllm/entrypoints/openai/translations/protocol.py`
  - `vllm/entrypoints/openai/translations/serving.py`
  - `vllm/entrypoints/openai/translations/speech_to_text.py`

### Summary

**What changed and why**  
This PR refactors the OpenAI API server architecture by extracting transcription and translation functionality into a dedicated module. The main changes include moving protocol definitions and API routes from the core server file to a new `vllm/entrypoints/openai/translations/` directory, creating separate modules for protocol, API routing, and serving logic.

**Technical impact**  
The refactoring improves modularity by separating audio-related endpoints (transcription/translation) from the main API server. This reduces the complexity of `api_server.py` and establishes a clearer architecture where each feature area has its own protocol definitions and routing. The changes maintain backward compatibility as the same endpoints are exposed through a new router attached to the main app.

**Potential risks**  
There's a risk of breaking changes if dependencies between the moved modules aren't properly updated. The error handling in the new router uses `openai_serving_tokenization` for base server errors, which might not be the appropriate fallback. Additionally, any undiscovered coupling between the extracted code and the original server could cause runtime issues.

**Key insights**  
This is part 9 of a larger refactoring series aimed at simplifying the serving architecture. Developers should verify that all imports are correctly updated, especially in other files that might reference the old protocol locations. The new modular structure will make future maintenance easier but requires careful testing of audio endpoints to ensure no regression in functionality.

---

## 24. [[Refactor] Move top-level dummy data generation to registry](https://github.com/vllm-project/vllm/pull/32310)


### Base Information

- **PR Number:** #32310
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-14 02:17:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32310/files) (6):**
  - `tests/models/multimodal/processing/test_mllama4.py`
  - `tests/multimodal/test_processing.py`
  - `vllm/multimodal/processing.py`
  - `vllm/multimodal/profiling.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This refactor moves dummy data generation from the `BaseDummyInputsBuilder` class to the `MULTIMODAL_REGISTRY`. It eliminates circular dependencies between `MultiModalProcessor` and `DummyInputsBuilder` by removing the redundant `DummyEncoderData` and `DummyDecoderData` classes, replacing them with direct usage of `MultiModalInputs`.

**Technical impact**  
The architecture is simplified by centralizing dummy data creation in the registry. The `BaseDummyInputsBuilder` now only handles creating raw processor inputs (`get_dummy_processor_inputs`), while the registry orchestrates the full pipeline including processor application and token padding. This reduces coupling and makes the dummy data flow more linear.

**Potential risks**  
The removal of length validation in `get_dummy_mm_inputs` (previously in `get_decoder_dummy_data`) could allow undersized dummy sequences if the processor produces insufficient tokens. The `gpu_model_runner` change assumes the first item in `mm_kwargs` exists, which may fail if the modality list is empty.

**Key insights**  
This is a positive architectural cleanup that follows the single responsibility principle. Developers should ensure dummy data generation consistently meets sequence length requirements. The registry-based approach provides a cleaner API for profiling and reduces redundant code paths across the codebase.

---

