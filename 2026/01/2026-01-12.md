# vLLM Merged PR Report

**Report Date:** 2026-01-12 PST

**Total Merged PRs:** 48

---

## 1. [[ROCm][CI] Fix engine core client tests for ROCm spawn multiprocessing](https://github.com/vllm-project/vllm/pull/32061)


### Base Information

- **PR Number:** #32061
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 23:14:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32061/files) (1):**
  - `tests/v1/engine/test_engine_core_client.py`

### Summary

**What changed and why**  
The changes fix engine core client test failures on ROCm where `spawn` multiprocessing prevents child processes from inheriting monkey-patched methods. A `sitecustomize.py` mechanism injects test utility methods (`echo`, `echo_dc`, etc.) into spawned subprocesses by writing temporary patch files and setting `PYTHONPATH`. The dataclass `MyDataclass` was replaced with `TestMessage`, and its `__module__` is explicitly set to ensure proper serialization across processes.

**Technical impact**  
Tests now work consistently across both NVIDIA (`fork`) and ROCm (`spawn`) multiprocessing modes. The solution leverages Python’s startup hooks to apply patches before engine processes run, making the tests platform-agnostic. The refactored `echo_dc_nested` uses a cleaner mapping for nested structures, improving maintainability.

**Potential risks**  
If `PYTHONPATH` environment variable manipulation conflicts with other test fixtures or system configurations, it could cause unintended side effects. The reliance on `sitecustomize.py` assumes no other code in the test suite uses similar mechanisms, which could lead to interference. Serialization of custom dataclasses depends on correct `__module__` assignment; any mismatch may break cross-process type resolution.

**Key insights**  
This approach elegantly solves the spawn inheritance problem without modifying core engine code. Developers should ensure any new test utility methods added in the future also use the corresponding fixture pattern. The changes are backward-compatible with NVIDIA systems, but care must be taken to keep the `sitecustomize.py` content synchronized with the actual utility functions.

---

## 2. [[Doc] Update installation from source command](https://github.com/vllm-project/vllm/pull/32239)


### Base Information

- **PR Number:** #32239
- **Author:** [esmeetu](https://github.com/esmeetu)
- **Merged By:** [esmeetu](https://github.com/esmeetu)
- **Merged time:** 2026-01-12 23:10:28
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32239/files) (1):**
  - `docs/contributing/README.md`

### Summary

**What changed and why**  
The documentation update modifies the source installation instructions for developers working on vLLM's Python and CUDA/C++ code. It splits the installation into two steps: first installing PyTorch with CUDA support via a specific index URL, then installing vLLM with the `--no-build-isolation` flag.

**Technical impact**  
This change provides more explicit guidance for building from source, particularly for aarch64 systems. The `--no-build-isolation` flag ensures the build uses the previously installed PyTorch rather than creating an isolated environment, which is crucial for CUDA/C++ development. The separation of PyTorch installation gives developers control over the CUDA version.

**Potential risks**  
The specific PyTorch index URL (`cu129`) may not be appropriate for all CUDA versions or hardware configurations. Developers with different CUDA installations might need to adjust this URL. The instructions assume the user wants CUDA 12.9 specifically, which could cause confusion or build failures if their system differs.

**Key insights**  
Always verify your CUDA version matches the PyTorch index URL before running these commands. Consider making the CUDA version configurable or pointing to more general installation instructions. The `--no-build-isolation` flag is essential for C++/CUDA development but may cause dependency conflicts in some environments.

---

## 3. [[BugFix]Fix eagle draft_model_config and add tests](https://github.com/vllm-project/vllm/pull/31753)


### Base Information

- **PR Number:** #31753
- **Author:** [charlotte12l](https://github.com/charlotte12l)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-12 23:09:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31753/files) (4):**
  - `tests/config/draft_model_arch_groundtruth.json`
  - `tests/test_config.py`
  - `vllm/config/speculative.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`

### Summary

**What changed and why**  
This PR fixes an inconsistency where EAGLE draft models had mismatched architecture fields. The `draft_model_config`'s `model_info` and resolved `architecture` were not updated after `hf_config.architectures` was changed to `EagleLlamaForCausalLM`, causing internal fields like `draft_model_config.architecture` to remain as `LlamaForCausalLM`. The changes ensure all architecture-related fields are synchronized.

**Technical impact**  
The fix updates `draft_model_config`'s `hf_text_config`, `model_arch_config`, `_model_info`, and `_architecture` to reflect the EAGLE-specific architectures. This ensures consistent model type detection and proper handling of EAGLE draft models throughout the codebase, particularly in model loading and inference paths.

**Potential risks**  
If other parts of the system rely on the previous `text_model_type` values (e.g., "llama" or "deepseek_mtp"), the updated "eagle" values could cause unintended behavior. Additionally, the fix assumes `runner_type` and `convert_type` dependencies are irrelevant for draft models, which may need validation in edge cases.

**Key insights**  
Always verify that all derived configuration fields are updated when base fields change. The added test provides a clear validation pattern for EAGLE draft models. Developers should review any logic that depends on `text_model_type` to ensure compatibility with the new "eagle" value.

---

## 4. [[ROCm][CI] Fix HuggingFace flash_attention_2 accuracy issue in Isaac vision encoder](https://github.com/vllm-project/vllm/pull/32233)


### Base Information

- **PR Number:** #32233
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-12 22:33:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32233/files) (2):**
  - `tests/models/multimodal/conftest.py`
  - `tests/models/multimodal/generation/vlm_utils/model_utils.py`

### Summary

**What changed and why**  
Added a workaround to force HuggingFace vision encoder attention layers to use SDPA (Scaled Dot-Product Attention) instead of `flash_attention_2` when running on ROCm. This addresses a known accuracy issue in HuggingFace's `flash_attention_2` implementation on ROCm, which was causing vision embeddings to diverge significantly from CUDA results.

**Technical impact**  
The change ensures consistent cross-platform behavior by bypassing the buggy `flash_attention_2` path on ROCm. SDPA then uses the `math_sdp` backend (already configured in the test suite), producing identical outputs between ROCm and CUDA. This is a targeted patch applied only to vision encoder attention layers during model inference in tests.

**Potential risks**  
The workaround is platform-specific and may need to be removed or updated if HuggingFace fixes the underlying `flash_attention_2` issue. There is a risk of inadvertently affecting other attention mechanisms if the patching logic is too broad or if model structures change. Additionally, forcing SDPA could have performance implications on ROCm, though accuracy is prioritized here.

**Key insights**  
This is a temporary, surgical fix for a third‑party library bug. Developers should monitor the linked upstream issue (vllm‑project/vllm#30167) for resolutions. The patch is cleanly scoped to tests and ROCm only, but consider adding a version check or deprecation reminder to facilitate future removal. Ensure similar vision encoder models in the test suite also receive this patch if needed.

---

## 5. [[ROCm][Bugfix] Fix Mamba batched decode producing incorrect output](https://github.com/vllm-project/vllm/pull/32099)


### Base Information

- **PR Number:** #32099
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-12 21:46:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32099/files) (2):**
  - `vllm/model_executor/layers/mamba/mamba_mixer.py`
  - `vllm/model_executor/models/plamo2.py`

### Summary

**What changed and why**  
The fix addresses incorrect batched decode outputs on ROCm for Mamba-based models (Jamba, Mamba, FalconMamba, PLaMo2). The root cause is that `causal_conv1d_update` returns a non-contiguous tensor view, which leads to incorrect GEMM results in linear layers when batch size > 1 on ROCm. The solution enforces tensor contiguity before linear operations, guarded by ROCm-specific checks to avoid unnecessary copies on other platforms.

**Technical impact**  
These changes ensure that tensors passed to linear layers (`x_proj` in MambaMixer and `bcdt_proj` in PLaMo2) are contiguous on ROCm, correcting batched inference outputs. The modifications are minimal and localized, preserving existing behavior on CUDA and other platforms while fixing a critical platform-specific bug.

**Potential risks**  
The ROCM-guarded contiguity checks may introduce slight overhead on ROCm due to extra tensor copies, though this is necessary for correctness. If future platform-specific GEMM implementations have similar contiguity requirements, the fix may need to be extended. Additionally, the reliance on `current_platform.is_rocm()` assumes correct platform detection; misconfiguration could lead to undetected issues.

**Key insights**  
Always verify tensor contiguity when passing data between kernels, especially for platform-specific operations. Developers should consider adding contiguity assertions in cross-platform code to catch similar issues early. This fix highlights the importance of testing batched inference across different hardware platforms, as single-sequence tests may not reveal these bugs.

---

## 6. [[Perf] Optimize requests abort](https://github.com/vllm-project/vllm/pull/32211)


### Base Information

- **PR Number:** #32211
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-12 20:11:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32211/files) (1):**
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
The change modifies the `output_handler` in `async_llm.py` to conditionally call `engine_core.abort_requests_async` only when `processed_outputs.reqs_to_abort` is non-empty. Previously, this abort call was made unconditionally on each output chunk, even when no requests needed aborting. This optimization batches abort operations and reduces overhead by avoiding no-op calls.

**Technical impact**  
This reduces the frequency of abort invocations during chunked output processing, which can improve throughput and latency in high-concurrency scenarios. The core logic and interfaces remain unchanged, so the functional behavior is preserved while minimizing unnecessary async operations.

**Potential risks**  
If `reqs_to_abort` is incorrectly populated or cleared elsewhere, delayed abort handling could lead to requests lingering longer than intended. Additionally, moving the abort call outside the loop (if done in future changes) must ensure that aborts are still processed promptly to avoid resource leaks.

**Key insights**  
This is a low-risk performance optimization that eliminates redundant async calls. Developers should verify that abort logic elsewhere in the codebase correctly manages `reqs_to_abort` state. Consider similar patterns in other loops to batch operations where possible.

---

## 7. [Fix various typos found in `docs`](https://github.com/vllm-project/vllm/pull/32212)


### Base Information

- **PR Number:** #32212
- **Author:** [potatosalad](https://github.com/potatosalad)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 19:41:48
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32212/files) (21):**
  - `docs/contributing/deprecation_policy.md`
  - `docs/contributing/model/basic.md`
  - `docs/deployment/frameworks/cerebrium.md`
  - `docs/deployment/frameworks/hf_inference_endpoints.md`
  - `docs/deployment/integrations/production-stack.md`
  - `docs/design/custom_op.md`
  - `docs/design/fused_moe_modular_kernel.md`
  - `docs/design/logits_processors.md`
  - `docs/features/disagg_encoder.md`
  - `docs/features/disagg_prefill.md`
  - `docs/features/quantization/inc.md`
  - `docs/features/spec_decode.md`
  - `docs/features/structured_outputs.md`
  - `docs/getting_started/installation/cpu.arm.inc.md`
  - `docs/getting_started/quickstart.md`
  - `docs/governance/collaboration.md`
  - `docs/models/extensions/fastsafetensor.md`
  - `docs/models/generative_models.md`
  - `docs/serving/openai_compatible_server.md`
  - `docs/usage/troubleshooting.md`
  - `docs/usage/v1_guide.md`

### Summary

**What changed and why**  
This PR fixes various typographical errors, grammatical issues, and formatting inconsistencies across 21 documentation files. The changes include correcting misspelled words, fixing punctuation and spacing, repairing broken Markdown links, and standardizing terminology to improve clarity and professionalism in the documentation.

**Technical impact**  
These changes have no impact on the codebase, runtime behavior, or APIs. They are purely cosmetic and editorial, aimed at enhancing the readability and accuracy of the documentation for end users and contributors.

**Potential risks**  
The risk of introducing functional regressions is negligible since only documentation text is modified. However, there is a minor risk that some corrections could inadvertently alter the meaning of technical descriptions (e.g., "interleave sliding windows" → "interleaved sliding windows"), though all changes appear to be appropriate clarifications.

**Key insights**  
Maintaining high-quality documentation is critical for user adoption and contributor onboarding. While these fixes are minor individually, collectively they improve the project's polish and credibility. Developers should continue to prioritize documentation hygiene in code reviews to prevent such issues from accumulating.

---

## 8. [[Frontend] Add `reasoning_effort` to `OpenAIServing._preprocess_chat()`](https://github.com/vllm-project/vllm/pull/31956)


### Base Information

- **PR Number:** #31956
- **Author:** [sanghoon-yn](https://github.com/sanghoon-yn)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 19:21:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31956/files) (2):**
  - `vllm/entrypoints/openai/serving_chat.py`
  - `vllm/entrypoints/openai/serving_responses.py`

### Summary

**What changed and why**  
The changes add `reasoning_effort` to the `chat_template_kwargs` dictionary in two OpenAI serving endpoints (`serving_chat.py` and `serving_responses.py`). This ensures the `reasoning_effort` parameter is passed through to the chat template preprocessing, aligning with the PR's goal of making it usable in chat templates.

**Technical impact**  
These modifications extend the `chat_template_kwargs` to include `reasoning_effort`, which will be accessible during chat template rendering. This enables downstream components (like model-specific templates) to utilize this parameter, potentially influencing prompt formatting or model behavior for reasoning-intensive tasks.

**Potential risks**  
If `reasoning_effort` is not properly validated or handled by the chat template, it could lead to unexpected template rendering errors. Additionally, the conditional logic in `serving_responses.py` (setting `reasoning_effort` to `None` when `request.reasoning` is `None`) may introduce subtle bugs if the template expects a specific type or default value.

**Key insights**  
Developers should verify that all chat templates support the `reasoning_effort` key and handle `None` values appropriately. Consider adding validation or documentation for this new parameter to ensure consistent usage across different request types (chat completion vs. response streaming).

---

## 9. [[Misc] improve warning/assert messages](https://github.com/vllm-project/vllm/pull/32226)


### Base Information

- **PR Number:** #32226
- **Author:** [cjackal](https://github.com/cjackal)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-12 19:11:23
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32226/files) (6):**
  - `vllm/compilation/compiler_interface.py`
  - `vllm/config/compilation.py`
  - `vllm/config/model.py`
  - `vllm/config/vllm.py`
  - `vllm/lora/ops/triton_ops/utils.py`
  - `vllm/v1/attention/backends/fa_utils.py`

### Summary

**What changed and why**  
This PR fixes minor grammatical errors, typos, and missing whitespaces in warning and assertion messages across six files. The changes improve readability and professionalism of user-facing log output without altering any functional logic.

**Technical impact**  
The modifications are purely cosmetic and have no impact on system behavior, performance, or functionality. They only affect the textual content of log messages and assertions displayed to users or developers during operation.

**Potential risks**  
There is minimal risk since only string literals were modified. However, one should verify that no f-string placeholders or formatting operators were inadvertently broken, though the diff shows proper preservation of such elements.

**Key insights**  
Always maintain clear and grammatically correct user-facing messages. While these changes are low-risk, consider adding a linter or pre-commit hook to catch similar issues proactively. Ensure that any future modifications to log messages follow the same consistency in punctuation and spacing.

---

## 10. [[BugFix] Fix engine crash caused by chat tools + response_format](https://github.com/vllm-project/vllm/pull/32127)


### Base Information

- **PR Number:** #32127
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-12 18:33:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32127/files) (3):**
  - `tests/tool_use/test_chat_completions.py`
  - `vllm/tool_parsers/abstract_tool_parser.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The changes fix an engine crash when combining `response_format: json_object` with `tool_choice: required`. The issue occurred because both structured output constraints (JSON from the tool schema and `json_object` from `response_format`) were simultaneously set, causing validation conflicts. The fix clears `response_format` when tool-calling is enforced and ensures structured output parameters are properly validated after backend auto-selection.

**Technical impact**  
These modifications resolve a validation error by preventing conflicting structured output constraints. The `adjust_request` method now explicitly clears `response_format` when tools force calling, while `input_processor` runs `__post_init__()` to validate the structured output state and ensure serialization safety. This maintains compatibility with tool-calling workflows while avoiding crashes.

**Potential risks**  
Clearing `response_format` silently when tool-calling is enforced might cause unexpected behavior if callers rely on `response_format` being preserved. Additionally, the reliance on `__post_init__()` for validation could introduce subtle issues if the structured output parameters are modified later in the request lifecycle. Edge cases involving multiple tools or nested schemas may require further testing.

**Key insights**  
The fix addresses an immediate crash but highlights broader issues with input validation logic, as noted in the PR description. Developers should be aware that `response_format` is overridden when tool-calling is forced, and future refactoring of validation logic is warranted. The added regression test ensures the combination no longer crashes and completes with `finish_reason: tool_calls`.

---

## 11. [[Misc] Allow enabling NCCL for DP sync when async scheduling](https://github.com/vllm-project/vllm/pull/32197)


### Base Information

- **PR Number:** #32197
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-12 18:03:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32197/files) (4):**
  - `vllm/config/parallel.py`
  - `vllm/config/scheduler.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
The changes allow manual override of the NCCL backend for DP synchronization when async scheduling is enabled. Previously, `disable_nccl_for_dp_synchronization` was automatically set to `True` when async scheduling was active, with no option to override. Now, the field accepts `bool \| None`, with `None` triggering default logic (True if async scheduling is enabled, False otherwise), while explicit `True`/`False` values are respected.

**Technical impact**  
The defaulting logic has moved from `ParallelConfig` to `VllmConfig.__post_init__()`, centralizing control. The CLI (`EngineArgs`) now propagates `None` values, enabling users to explicitly choose between NCCL and Gloo backends regardless of scheduling mode. This maintains backward compatibility while adding flexibility.

**Potential risks**  
If users explicitly set `disable_nccl_for_dp_synchronization=False` with async scheduling enabled, it may lead to unexpected behavior or performance issues, as the original default was based on compatibility assumptions. The `None` passthrough validation could mask type errors if non-boolean values slip through.

**Key insights**  
Developers should document the trade-offs between NCCL and Gloo backends for async scheduling to guide user overrides. The centralized default logic in `VllmConfig` improves maintainability, but any future changes to async scheduling behavior must consider this override capability.

---

## 12. [[Model] Handle `trust_remote_code` for transformers backend](https://github.com/vllm-project/vllm/pull/32194)


### Base Information

- **PR Number:** #32194
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-12 17:30:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32194/files) (2):**
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/dynamic_module.py`

### Summary

**What changed and why**  
The changes add support for the `trust_remote_code` parameter when dynamically loading transformer modules. Specifically, the `try_get_class_from_dynamic_module` function now accepts this parameter and passes it to Hugging Face's `resolve_trust_remote_code` utility, while the model registry propagates the setting from `model_config` to the dynamic module resolution calls.

**Technical impact**  
This enables vLLM to safely load custom or remote transformer models that require code execution, aligning with Hugging Face's security model. The changes ensure that dynamic module loading respects the same `trust_remote_code` flag used elsewhere in the codebase, maintaining consistency in security handling across model initialization.

**Potential risks**  
If `trust_remote_code` is incorrectly set to `True` for untrusted sources, it could expose the system to arbitrary code execution. Additionally, the `has_local_code=False` and `has_remote_code=True` arguments are hardcoded in the call to `resolve_trust_remote_code`, which may not always reflect the actual state of local vs. remote code.

**Key insights**  
Developers should ensure `trust_remote_code` is only enabled for verified model repositories. Consider validating the hardcoded `has_local_code` and `has_remote_code` values against the actual model loading context to avoid potential misconfigurations. This change is essential for supporting custom architectures but requires careful security review.

---

## 13. [[responsesAPI] add unit test for optional function tool call id](https://github.com/vllm-project/vllm/pull/32036)


### Base Information

- **PR Number:** #32036
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [luccafong](https://github.com/luccafong)
- **Merged time:** 2026-01-12 16:14:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32036/files) (1):**
  - `tests/entrypoints/test_responses_utils.py`

### Summary

**What changed and why**  
This PR adds comprehensive unit tests for the `_maybe_combine_reasoning_and_tool_call` function, specifically covering edge cases related to optional function tool call IDs. The tests validate behavior when `id` is `None`, empty, non-MCP prefixed, or when messages lack required attributes, ensuring robust handling of MCP (Model Context Protocol) tool call integration.

**Technical impact**  
The new test suite strengthens test coverage for responses utilities, particularly around MCP tool call handling. It verifies that the function correctly combines assistant reasoning content with MCP function calls into `tool_calls` and returns `None` for invalid or unsupported inputs, maintaining backward compatibility with existing message construction logic.

**Potential risks**  
While the tests themselves are low-risk, they rely on correct imports (`MCP_PREFIX`, `ChatCompletionMessageParam`) and assume the underlying function’s behavior aligns with the tested edge cases. If the function’s implementation changes (e.g., handling of falsy IDs), these tests may need updates. Additionally, the test for non-`ResponseFunctionToolCall` items uses a dict, which could mask type-checking issues.

**Key insights**  
Developers should note the focus on MCP tool call integration and the importance of handling `None`/empty IDs to prevent `TypeError`. The tests provide a clear specification for expected behavior, which can guide future modifications. Ensure any changes to `_maybe_combine_reasoning_and_tool_call` maintain compatibility with these test cases to avoid regressions in MCP support.

---

## 14. [[ROCm][CI] Handle pytest status code 5 when a shard isn't allocated any tests](https://github.com/vllm-project/vllm/pull/32040)


### Base Information

- **PR Number:** #32040
- **Author:** [divakar-amd](https://github.com/divakar-amd)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-12 14:35:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32040/files) (2):**
  - `.buildkite/scripts/hardware_ci/run-amd-test.sh`
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
This PR updates AMD CI scripts to handle pytest exit code 5 ("no tests collected") gracefully in multi-GPU test sharding scenarios. It modifies the test runner to treat individual shard failures with status 5 as success while failing the build only if all shards report no tests. Additionally, it corrects the agent pool configuration to match the actual 8-way sharding used for language model tests.

**Technical impact**  
The changes improve CI robustness by preventing false failures when test distribution across GPU shards results in empty shards. The script now distinguishes between genuine test failures (non-zero exit codes) and empty shards (exit code 5), while maintaining proper validation to catch complete test collection failures. The agent pool correction ensures hardware resources align with the sharding strategy.

**Potential risks**  
The logic assumes pytest exit code 5 exclusively indicates "no tests collected" without other error conditions. Edge cases exist where mixed exit codes (some 5, some other non-zero) could mask real failures. The `at_least_one_shard_with_tests` tracking depends on exact exit code matching and could be affected by unexpected process terminations or signal-based exits.

**Key insights**  
This is a well-targeted fix for AMD CI's multi-GPU testing infrastructure. Developers should verify that pytest behavior aligns with documented exit codes and consider adding logging for other non-zero exit codes to aid debugging. The pattern of treating specific "benign" failures differently while maintaining overall validation is a useful approach for distributed test systems.

---

## 15. [[Kernel][MoE] fix computation order of MoE weight multiplication and improve flow](https://github.com/vllm-project/vllm/pull/31962)


### Base Information

- **PR Number:** #31962
- **Author:** [xuebwang-amd](https://github.com/xuebwang-amd)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-12 14:17:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31962/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
This PR fixes the computation order in the fused MoE kernel to ensure numerical correctness when quantization and bias are present. The new order is: 1) dequantize to float32, 2) add bias, 3) multiply router weights in float32, and 4) cast to the compute dtype. This resolves issues from previous PRs where the order was incorrect for quantized models or models with bias.

**Technical impact**  
The changes ensure that dequantization, bias addition, and router weight scaling all occur in float32 precision before the final cast, improving numerical stability—especially on ROCm platforms. This aligns the kernel's behavior across different quantization schemes (int8_w8a16, fp8_w8a8, int8_w8a8) and bias configurations, preventing accuracy degradation.

**Potential risks**  
The test results show some regressions (3/4 passed vs. 4/4 in certain configurations), indicating potential edge cases or platform-specific behavior that may need further validation. Additionally, the changes assume bias is not quantized; if future models use quantized biases, the logic may need adjustment.

**Key insights**  
Developers should verify that all quantization and bias combinations are covered in tests, particularly for ROCm. The cleanup (using `+=` and improved comments) enhances readability but does not affect functionality. Ensure any future modifications maintain the strict float32 ordering for dequantization, bias, and router multiplication to preserve numerical correctness.

---

## 16. [[Model Runner V2] Add support for M-RoPE](https://github.com/vllm-project/vllm/pull/32143)


### Base Information

- **PR Number:** #32143
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-12 13:37:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32143/files) (5):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/mm/__init__.py`
  - `vllm/v1/worker/gpu/mm/mrope_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
This PR adds end-to-end support for M-RoPE (Multimodal Rotary Position Embedding) to the v1 GPU model runner. It introduces a new `MRopeState` class to manage 3D position IDs, extends input buffers to store these positions, integrates M-RoPE logic into the model runner's batching and execution flow, and updates CUDA graph handling to accommodate the new position format. The changes enable models with M-RoPE support to process multimodal inputs correctly.

**Technical impact**  
The modifications extend the existing position ID handling from 1D to 3D tensors, requiring updates across input preparation, kernel execution, and CUDA graph capture. The `InputBuffers` now include a `mrope_positions` tensor with a dummy element to maintain torch.compile compatibility, and the `GPUModelRunner` conditionally uses 3D positions when M-RoPE is enabled. This adds a new code path for multimodal models while maintaining backward compatibility for text-only models.

**Potential risks**  
The addition of a large UVA-backed tensor for prefill positions could lead to high CPU memory usage, as noted in the code comments. The conditional logic for position handling (e.g., `if not self.uses_mrope`) increases complexity and may introduce subtle bugs if not consistently applied. The Triton kernel and staged write mechanisms add new failure points, especially during edge cases like partial prefill or mixed modality requests.

**Key insights**  
Developers should ensure that the `uses_mrope` flag is correctly propagated from model configurations. The dummy element in `mrope_positions` is critical for torch.compile; altering its structure may break compilation. Testing should cover both M-RoPE-enabled and disabled paths, as well as multimodal input scenarios. The memory footprint of `prefill_mrope_positions` should be monitored in production deployments.

---

## 17. [[Model Runner V2] Minor refactor for logit_bias](https://github.com/vllm-project/vllm/pull/32209)


### Base Information

- **PR Number:** #32209
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-12 13:08:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32209/files) (1):**
  - `vllm/v1/worker/gpu/sample/logit_bias.py`

### Summary

**What changed and why**  
The code extracts the kernel launch logic from `LogitBiasState.apply_logit_bias` into a standalone `apply_logit_bias` helper function. This separates kernel configuration (like computing `BLOCK_SIZE` from tensor shapes) from state management, improving testability and code reuse.

**Technical impact**  
The refactor centralizes kernel invocation logic, making it easier to test independently of the state object. It also replaces hard-coded constants (`MAX_NUM_ALLOWED_TOKEN_IDS`, etc.) with dynamic shape-based `BLOCK_SIZE` calculation, which may improve flexibility for varying input sizes.

**Potential risks**  
The dynamic `BLOCK_SIZE` calculation now relies on tensor shapes, which could lead to different kernel configurations if tensor dimensions change unexpectedly. There’s also a risk of incorrect strides being passed if the helper is called with tensors that don’t match the original state’s GPU tensor layout.

**Key insights**  
This change enhances modularity and testability, but developers should verify that the dynamic `BLOCK_SIZE` calculation aligns with performance expectations. Ensure that any future use of the helper function passes tensors with consistent strides and shapes to avoid kernel launch errors.

---

## 18. [[BUGFIX] Add missed remaping of the names of fp8 kv-scale](https://github.com/vllm-project/vllm/pull/32199)


### Base Information

- **PR Number:** #32199
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [pavanimajety](https://github.com/pavanimajety)
- **Merged time:** 2026-01-12 12:42:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32199/files) (1):**
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
The fix adds a call to `maybe_remap_kv_scale_name` in the `Qwen3NextModel.load_weights` method to handle FP8 KV-scale tensor names from checkpoints. This resolves warnings about missing parameters (e.g., `k_scale` or `v_scale`) by remapping them to the correct model parameter names during weight loading.

**Technical impact**  
This change ensures FP8 KV-scale tensors from specific checkpoints (like Qwen3-Next-NVFP4) are properly mapped and loaded, eliminating skip warnings and preventing potential model initialization issues. It integrates with the existing weight loading logic without altering other model components.

**Potential risks**  
If `maybe_remap_kv_scale_name` incorrectly maps or returns `None` for valid non-FP8 scale parameters, those tensors could be erroneously skipped. Additionally, the check `if name.endswith("scale")` might not cover all edge cases (e.g., suffixes or naming variations in future checkpoints).

**Key insights**  
Always validate remapping logic for FP8-related tensors to avoid silent skips. Consider extending the condition to handle potential naming patterns beyond simple suffix matching. This fix is critical for FP8 checkpoint compatibility but should be monitored for broader scale-tensor handling.

---

## 19. [[NIXL][Bugfix] Failure logging overhaul + early metadata free on failure](https://github.com/vllm-project/vllm/pull/32031)


### Base Information

- **PR Number:** #32031
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-12 12:38:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32031/files) (2):**
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR introduces a centralized `_log_failure` utility to replace scattered error logging in the NIXL connector with structured, context-rich logs. It also fixes a bug where metadata for failed transfers was prematurely released, ensuring cleanup occurs only in `get_finished`.

**Technical impact**  
The changes standardize failure logging across handshake, transfer setup, notification, and transfer state checks, providing comprehensive debugging context (e.g., request IDs, engine info, block counts). The metadata cleanup fix prevents double-free issues and ensures consistent handling of multi-transfer scenarios.

**Potential risks**  
If `_log_failure` encounters missing metadata or malformed context, it may log incomplete information. The reliance on `get_finished` for cleanup could delay resource release in edge cases. The updated logic for multi-transfer handling must be validated under high concurrency.

**Key insights**  
Developers should adopt the structured logging pattern for future error handling to improve observability. Ensure all failure paths call `_log_failure` with appropriate context. The metadata cleanup fix is critical for correctness in multi-TP configurations—verify it handles all failure modes.

---

## 20. [[Model Runner V2] Support logit_bias, allowed_token_ids, min_tokens](https://github.com/vllm-project/vllm/pull/32163)


### Base Information

- **PR Number:** #32163
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-12 11:31:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32163/files) (2):**
  - `vllm/v1/worker/gpu/buffer_utils.py`
  - `vllm/v1/worker/gpu/sample/logit_bias.py`

### Summary

**What changed and why**  
This PR adds GPU-side sampling constraints (logit_bias, allowed_token_ids, min_tokens) to the model runner. It introduces a new `LogitBiasState` class to manage per-request constraints using `UvaBackedTensor` and `StagedWriteTensor`, and a Triton kernel (`_bias_kernel`) to apply these constraints directly on GPU logits during batched decoding.

**Technical impact**  
The changes enable efficient GPU‑side manipulation of logits for advanced sampling controls, reducing CPU‑GPU transfer overhead. The architecture now supports staged writes for float data (extending `StagedWriteTensor`) and integrates constraint application into the decoding loop via `apply_logit_bias`. This maintains batch efficiency while adding flexible per‑request sampling rules.

**Potential risks**  
The kernel uses hard‑coded size limits (`MAX_NUM_*` constants) which may reject valid requests if constraints exceed these limits. Concurrent writes to staged tensors could cause data races if not properly synchronized. The kernel’s masking logic for `allowed_token_ids` rewrites the entire logit tensor to `-inf`—this may have performance implications for large vocabularies.

**Key insights**  
Developers should validate that constraint sizes (e.g., `allowed_token_ids`) stay within the defined maxima to avoid runtime errors. The `StagedWriteTensor` now supports `float32`, enabling broader use cases. Ensure `apply_staged_writes` is called before `apply_logit_bias` to guarantee data consistency. Consider profiling the Triton kernel’s impact on latency, especially with many constraints or large vocabularies.

---

## 21. [[Misc][BE] Type coverage for vllm/compilation [3/3]](https://github.com/vllm-project/vllm/pull/31748)


### Base Information

- **PR Number:** #31748
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-12 11:24:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31748/files) (11):**
  - `vllm/compilation/activation_quant_fusion.py`
  - `vllm/compilation/collective_fusion.py`
  - `vllm/compilation/fusion.py`
  - `vllm/compilation/fusion_attn.py`
  - `vllm/compilation/inductor_pass.py`
  - `vllm/compilation/matcher_utils.py`
  - `vllm/compilation/qk_norm_rope_fusion.py`
  - `vllm/compilation/rocm_aiter_fusion.py`
  - `vllm/compilation/sequence_parallelism.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/model_executor/layers/rotary_embedding/__init__.py`

### Summary

**What changed and why**  
This PR improves type hint coverage across the `vllm/compilation` module to enhance maintainability, readability, and error detection. Changes include adding explicit return and parameter types, introducing `ParamSpec` for decorators, tightening function signatures (e.g., `__init__`, `register`, `__call__`, `uuid`), and refining helper methods like `get_inputs()` and `empty_*` functions. The updates also address minor fixes in distributed and rotary embedding modules.

**Technical impact**  
The changes strengthen static type checking (mypy now passes on 28 files) and clarify APIs, reducing the risk of runtime type mismatches. By standardizing signatures and adding `ParamSpec`, the codebase becomes more consistent and easier to refactor. The introduction of typed `get_inputs()` methods centralizes pattern input generation, improving modularity. Additionally, safer handling of optional values (e.g., device capability checks) prevents potential `None`-related errors.

**Potential risks**  
- Overly strict type hints could introduce rigidity if future changes require more flexible signatures.  
- The use of `ParamSpec` and precise tuple returns may complicate extensions to existing decorators or pattern functions.  
- Conditional type ignores (e.g., `# type: ignore[arg-type]`) might mask underlying issues if not carefully reviewed.  
- Changes to distributed utilities (`parallel_state.py`) could affect downstream dependencies relying on implicit return types.

**Key insights**  
- The PR successfully enhances type safety without altering core logic, as evidenced by passing mypy checks.  
- Developers should leverage the new `get_inputs()` helpers for consistency when defining patterns.  
- Pay attention to `ParamSpec` usage in decorators to ensure forward compatibility with varied function signatures.  
- Review type ignore comments periodically to determine if they can be resolved as the codebase evolves.

---

## 22. [[Misc] Change log level for batch queue log](https://github.com/vllm-project/vllm/pull/32192)


### Base Information

- **PR Number:** #32192
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-12 10:59:32
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32192/files) (1):**
  - `vllm/v1/engine/core.py`

### Summary

**What changed and why**  
The change converts a log statement from `logger.info` to `logger.debug` for batch queue initialization in `vllm/v1/engine/core.py`. This reduces log verbosity by moving a non-critical informational message to debug level, aligning with feedback to decrease noise in standard logs.

**Technical impact**  
This modification affects log output visibility—developers will no longer see the batch queue size message in default `INFO` level logs. The change is minimal and localized, with no impact on system functionality or performance, only on logging behavior.

**Potential risks**  
If developers rely on this log message for monitoring batch queue activation in production environments without enabling debug logging, they may lose visibility. There is also a slight risk that other similar informational logs might need similar adjustments for consistency.

**Key insights**  
Consider reviewing other `logger.info` statements in the codebase to ensure consistent log level usage. Ensure documentation or monitoring tools are updated if this log was used for operational awareness. The change is safe but highlights the importance of log level strategy for production systems.

---

## 23. [[BugFix] scheduler: Fix ordering preserving of skipped requests](https://github.com/vllm-project/vllm/pull/32173)


### Base Information

- **PR Number:** #32173
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-12 10:39:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32173/files) (2):**
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/request_queue.py`

### Summary

**What changed and why**  
The PR fixes a bug where the scheduler incorrectly reversed the order of skipped requests when they were re-queued. Specifically, `FCFSRequestQueue.prepend_requests` was changed from `extendleft(reversed(requests))` to `extendleft(requests)` to maintain the original waiting order. Unused `__reversed__` method definitions were also removed from the request queue classes.

**Technical impact**  
This ensures that when requests are skipped (e.g., due to waiting for remote key-value stores) and later prepended back to the waiting queue, their original order is preserved. This maintains the intended first-come-first-served behavior and prevents starvation or unfair reordering of requests.

**Potential risks**  
If other parts of the codebase rely on the previous reverse-order behavior of `prepend_requests`, they may break. Additionally, the removal of `__reversed__` could affect any external code that iterates over request queues in reverse, though it was marked as unused.

**Key insights**  
Developers should verify that no other components depend on the reversed order when prepending requests. The added unit test effectively validates the fix, but consider extending test coverage to edge cases like empty queues or mixed priority queues. Ensure the change aligns with the overall scheduling policy guarantees across the system.

---

## 24. [[Misc] Set default torch num threads for input processing](https://github.com/vllm-project/vllm/pull/31879)


### Base Information

- **PR Number:** #31879
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-12 10:28:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31879/files) (2):**
  - `vllm/model_executor/models/internvl.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The PR moves thread management for CPU-bound preprocessing from the InternVL model's image transform function to the central input processor. This addresses CPU contention issues when multiple vLLM instances run on the same machine without `OMP_NUM_THREADS` set, defaulting to single-threaded preprocessing.

**Technical impact**  
Centralizing thread control in `input_processor.py` ensures consistent behavior across all multimodal models, not just InternVL. The change simplifies model-specific code and applies thread limits globally during input preprocessing, potentially reducing CPU contention in multi-instance deployments.

**Potential risks**  
If `OMP_NUM_THREADS` is set to a high value, preprocessing may become slower due to reduced parallelism. The change affects all multimodal models uniformly, which could inadvertently impact performance for models that benefit from multi-threaded preprocessing. The debug log only triggers when `OMP_NUM_THREADS` is unset, which may hide configuration issues.

**Key insights**  
This is a strategic fix that promotes consistency and addresses a systemic issue. Developers should verify that the default single-thread behavior doesn't degrade performance for latency-sensitive workloads. Consider exposing this configuration as a tunable parameter rather than solely relying on environment variables for finer control.

---

## 25. [[Refactor] EPLB rebalance algo to NumPy](https://github.com/vllm-project/vllm/pull/30697)


### Base Information

- **PR Number:** #30697
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-12 10:13:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30697/files) (3):**
  - `tests/distributed/test_eplb_algo.py`
  - `tests/distributed/test_eplb_execute.py`
  - `vllm/distributed/eplb/policy/default.py`

### Summary

**What changed and why**  
This PR refactors the EPLB (Expert Parallel Load Balancing) rebalancing algorithms from PyTorch to NumPy implementations. The primary goal is to unify the logic into CPU-only NumPy code, which serves as a prerequisite for moving rebalancing to an async EPLB thread. This will eliminate CPU waiting for GPU results in the main thread, improving overall performance.

**Technical impact**  
The core algorithms (`balanced_packing`, `replicate_experts`, `rebalance_experts_hierarchical`) now operate on NumPy arrays, while the public interfaces remain unchanged (still accept/return PyTorch tensors). This shift reduces GPU-CPU synchronization overhead and provides a 40-70% performance improvement in the rebalancing steps. The change also introduces an optional intra-GPU slot preservation feature to minimize expert rearrangement.

**Potential risks**  
The conversion between PyTorch and NumPy tensors adds minor overhead, though it's offset by the algorithmic speedups. There is a risk of subtle numerical differences due to floating-point handling between frameworks, but the validation shows matching results. The async migration (future PRs) must ensure thread safety and proper synchronization.

**Key insights**  
This refactoring is a strategic step toward fully asynchronous rebalancing. Developers should verify that all downstream code correctly handles the preserved PyTorch interfaces. The performance gains are significant, but thorough testing across diverse MoE configurations is recommended to ensure robustness, especially for edge cases with skewed expert distributions.

---

## 26. [[BugFix] fix FusedMoE.make_expert_params_mapping in EXAONE-MoE](https://github.com/vllm-project/vllm/pull/32196)


### Base Information

- **PR Number:** #32196
- **Author:** [lkm2835](https://github.com/lkm2835)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-12 10:00:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32196/files) (1):**
  - `vllm/model_executor/models/exaone_moe.py`

### Summary

**What changed and why**  
The change adds `self` as the first argument to `FusedMoE.make_expert_params_mapping` in the `get_expert_mapping` method. This fixes a bug where the expert parameter mapping for EXAONE-MoE models was incomplete, ensuring correct loading of expert weights—including redundant experts—by providing the necessary model context.

**Technical impact**  
This update ensures that the parameter mapping logic can access model-specific attributes (like the number of experts or expert configuration) from `self`, which is required for `FusedMoE.make_expert_params_mapping` to generate accurate mappings. Without this, weight loading for MoE layers might fail or incorrectly map experts, potentially causing model performance issues or initialization errors.

**Potential risks**  
If `FusedMoE.make_expert_params_mapping` expects additional arguments beyond `self` that aren't provided, it could lead to runtime errors. There's also a risk that the fix assumes a specific interface for `make_expert_params_mapping` that may not be backward-compatible with other model variants or future changes.

**Key insights**  
Always verify that helper functions like `make_expert_params_mapping` receive all required arguments, especially the model instance (`self`) when they depend on instance attributes. Review the function signature and usage across similar models to ensure consistency and prevent similar omissions.

---

## 27. [[3/N][Attention] Move AttentionMetadata-related code from utils.py to backend.py](https://github.com/vllm-project/vllm/pull/32054)


### Base Information

- **PR Number:** #32054
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-12 09:13:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32054/files) (37):**
  - `docs/design/cuda_graphs.md`
  - `tests/v1/attention/test_attention_backends.py`
  - `tests/v1/attention/test_mla_backends.py`
  - `tests/v1/attention/utils.py`
  - `tests/v1/e2e/test_async_spec_decode.py`
  - `tests/v1/spec_decode/test_tree_attention.py`
  - `vllm/model_executor/layers/attention/chunked_local_attention.py`
  - `vllm/model_executor/layers/attention/cross_attention.py`
  - `vllm/model_executor/layers/attention/encoder_only_attention.py`
  - `vllm/model_executor/layers/attention/static_sink_attention.py`
  - `vllm/model_executor/models/whisper_utils.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/cpu_attn.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/flashinfer.py`
  - `vllm/v1/attention/backends/flex_attention.py`
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/linear_attn.py`
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/attention/backends/mamba_attn.py`
  - `vllm/v1/attention/backends/mla/common.py`
  - `vllm/v1/attention/backends/mla/cutlass_mla.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/attention/backends/mla/flashinfer_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/indexer.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/attention/backends/rocm_attn.py`
  - `vllm/v1/attention/backends/tree_attn.py`
  - `vllm/v1/attention/backends/triton_attn.py`
  - `vllm/v1/attention/backends/utils.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR moves three attention metadata-related classes (`CommonAttentionMetadata`, `AttentionMetadataBuilder`, and `AttentionCGSupport`) from `vllm/v1/attention/backends/utils.py` to `vllm/v1/attention/backend.py`. The purpose is to consolidate these core types into a central location for clearer ownership and reuse, as part of a larger refactoring effort (step 3 of #31919). No functional changes were made to the class definitions themselves.

**Technical impact**  
The changes centralize fundamental attention metadata abstractions into the main attention backend module, making them directly accessible from the primary interface. This improves architectural clarity by establishing `backend.py` as the definitive source for these shared types. All dependent modules (attention backends, model layers, worker code, spec-decode, and tests) have been updated to import from the new location, reducing coupling to utility modules.

**Potential risks**  
While the PR states no functional changes, there's a risk of import-related issues if any modules were missed during the update (37 files changed suggests good coverage). The deprecated fields in `CommonAttentionMetadata` (marked for removal in v0.15.0) remain unchanged, maintaining backward compatibility but continuing technical debt. The refactoring could introduce subtle bugs if any code relied on implementation details of the utility module beyond the moved classes.

**Key insights**  
This is a well-executed code consolidation that improves maintainability by establishing clear ownership of core attention types. Developers should now import these classes exclusively from `vllm.v1.attention.backend` rather than the utils module. The extensive test coverage (CI runs all tests) provides confidence in the changes. Future work should address the deprecated fields marked for removal in v0.15.0 to complete the cleanup.

---

## 28. [[Benchmark] Share data between SLA runs](https://github.com/vllm-project/vllm/pull/32184)


### Base Information

- **PR Number:** #32184
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 09:12:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32184/files) (2):**
  - `tests/benchmarks/sweep/test_serve_sla.py`
  - `vllm/benchmarks/sweep/serve_sla.py`

### Summary

**What changed and why**  
The changes enable SLA benchmarking to reuse past run data when searching for SLA targets with the same serve/bench combination. This optimization avoids redundant benchmarking when sweeping multiple SLA targets (e.g., 200 ms, 500 ms, 1000 ms) by loading existing `summary.json` files from previous runs.

**Technical impact**  
The `solve_sla` function now pre-populates `SLAHistory` with results from prior runs via `_iter_sla_val_paths`, which discovers saved summaries under `"<sla_variable>=<value>"` directories. The extracted `_compute_margin` function standardizes margin calculation for both loaded and new results, while the search logic is refined to skip already-tested values and handle boundary cases efficiently.

**Potential risks**  
If existing `summary.json` files are corrupted or malformed, loading them could cause runtime errors. The reuse mechanism assumes that the serve/bench combination remains unchanged; changes in configuration could lead to invalid reuse. The modified initial value selection logic (`max(history, default=sla_min_value) < sla_max_value`) may behave unexpectedly if history contains values outside the current search range.

**Key insights**  
Developers should ensure that `summary.json` files are persisted correctly and remain valid across runs. The reuse feature significantly reduces benchmarking time for SLA sweeping but requires careful handling of file I/O and data consistency. Testing with `test_solve_reuse_history` validates cross-run reuse, and all existing tests now pass `base_path` to support the new functionality.

---

## 29. [[Misc][PD] Fix `get_attn_backend` usage in transfer connectors](https://github.com/vllm-project/vllm/pull/31988)


### Base Information

- **PR Number:** #31988
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-12 09:10:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31988/files) (4):**
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR fixes incorrect attention backend selection in KV transfer connectors (NixlConnector and MooncakeConnector) by replacing direct calls to `get_attn_backend` with a new `get_current_attn_backend` utility. The issue occurred because `get_attn_backend` was being called with partial configuration data, leading to mismatched backend selection (e.g., incorrectly choosing Flash Attention over Triton for Gemma-3). The new approach retrieves the backend directly from the instantiated model layers, ensuring alignment with the actual runtime configuration.

**Technical impact**  
The changes centralize backend detection logic into `get_current_attn_backend`, which extracts the backend from the first `AttentionLayerBase` instance in the model. This ensures connectors use the same backend as the model execution layer, improving consistency. The fallback to `get_attn_backend` is retained for test environments where layers may not be initialized. This modification affects initialization logic in both connectors but does not alter their runtime behavior or data transfer protocols.

**Potential risks**  
The assumption that all layers use the same attention backend may break for models with heterogeneous backends, as noted in the code comments. Additionally, reliance on the first layer’s backend could be problematic if layer initialization order changes. The fallback path in `get_current_attn_backend` depends on `get_layers_from_vllm_config`, which may have edge cases in distributed or multi-GPU setups. Test mocks were updated, but any untested usage of `get_attn_backend` elsewhere in the codebase remains a risk.

**Key insights**  
Developers should verify that `get_current_attn_backend` is used consistently across all KV connector implementations to prevent similar issues. The fallback mechanism is suitable for tests but should be monitored in production to ensure layers are always available. Future work should address models with multiple attention backends, possibly by extending the utility to validate backend uniformity across layers. Review other calls to `get_attn_backend` in the codebase to ensure they are not susceptible to similar partial-configuration problems.

---

## 30. [[Bugfix] Fix stale SSM state for new Mamba requests scheduled as decode](https://github.com/vllm-project/vllm/pull/32118)


### Base Information

- **PR Number:** #32118
- **Author:** [Josephasafg](https://github.com/Josephasafg)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-12 09:02:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32118/files) (2):**
  - `tests/v1/attention/test_batch_reordering.py`
  - `vllm/v1/attention/backends/utils.py`

### Summary

**What changed and why**  
The fix addresses SSM state corruption in Mamba models when new requests with only 1 scheduled token are incorrectly classified as decode instead of prefill. This occurs when token budgets are nearly exhausted, causing new prompts to read stale cache state. The change ensures `is_prefill` is determined solely by `num_computed_tokens == 0`, while `is_decode` and `is_extend` are computed only for non-prefill requests.

**Technical impact**  
This modifies the request classification logic in `reorder_batch_to_split_decodes_and_prefills` to guarantee new requests always initialize SSM state from zeros during prefill, regardless of scheduled token count. The decode→extend→prefill ordering is preserved, maintaining performance while fixing state initialization for recurrent models like Mamba.

**Potential risks**  
If `decode_threshold` logic or `num_computed_tokens` tracking has edge cases (e.g., negative values or overflow), misclassification could still occur. The change assumes `num_computed_tokens == 0` exclusively identifies new requests; any stateful models relying on different classification criteria might be affected.

**Key insights**  
Always treat zero-computed-token requests as prefill to ensure proper state initialization for recurrent architectures. The added unit tests validate single-token new requests, but consider extending tests to cover mixed scenarios with varying `decode_threshold` values. Review any other classification logic in the codebase that might depend on the old behavior.

---

## 31. [[Model] Standardize pooling heads](https://github.com/vllm-project/vllm/pull/32148)


### Base Information

- **PR Number:** #32148
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-12 09:01:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32148/files) (9):**
  - `vllm/model_executor/layers/pooler/common.py`
  - `vllm/model_executor/layers/pooler/seqwise/heads.py`
  - `vllm/model_executor/layers/pooler/seqwise/poolers.py`
  - `vllm/model_executor/layers/pooler/tokwise/heads.py`
  - `vllm/model_executor/layers/pooler/tokwise/poolers.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/bert_with_rope.py`
  - `vllm/model_executor/models/gritlm.py`
  - `vllm/model_executor/models/modernbert.py`

### Summary

**What changed and why**  
This PR standardizes pooling head construction across sequence and token tasks by refactoring `EmbeddingPoolerHead` and `ClassifierPoolerHead` (and their token variants) to accept explicit projector, activation, and dtype parameters. It centralizes head instantiation in `seqwise/poolers.py` and `tokwise/poolers.py`, ensuring that `PoolingParams` (like `dimensions`, `normalize`, `use_activation`) are consistently applied for custom models (BERT, BERT+RoPE, ModernBERT, GritLM).

**Technical impact**  
The changes decouple pooling head configuration from global state, making heads more modular and testable. By moving configuration logic to factory functions (`pooler_for_embed`, `pooler_for_classify`), the system now uniformly applies per-request settings (e.g., Matryoshka `dimensions` slicing, conditional normalization/activation) and dtype casting. This improves consistency across models and reduces duplicated inline head logic.

**Potential risks**  
If `head_dtype` is incorrectly set to `None`, dtype casting may be skipped unintentionally, potentially causing precision mismatches. The reliance on lambda functions for projectors/activations in model-specific poolers (e.g., `BertPooler`) could obscure parameter registration paths, complicating weight loading or serialization. Edge cases where pooling parameters vary within a batch (mixed `normalize` flags) are handled but add per-element processing overhead.

**Key insights**  
Developers should verify that `head_dtype` is properly configured in model configs to avoid silent dtype issues. The centralized pooling factories simplify adding new models but require careful alignment of `PoolingParams` with task types. The removal of `SupportsPP` from MTP classes indicates a cleanup of unused interfaces—ensure no downstream code depends on them.

---

## 32. [[FIX] Add NO_MUL activation support for modular kernel path](https://github.com/vllm-project/vllm/pull/31528)


### Base Information

- **PR Number:** #31528
- **Author:** [danielafrimi](https://github.com/danielafrimi)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-12 08:55:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31528/files) (17):**
  - `tests/kernels/moe/test_triton_moe_no_act_mul.py`
  - `vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/fallback.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/utils.py`

### Summary

**What changed and why**  
This PR adds support for `*_no_mul` activations (e.g., `relu2_no_mul`) in the modular kernel MoE path. Previously, the code assumed all activations used gate/up multiplication, where the activation output dimension is `N//2`. For `*_no_mul` activations, which apply the activation directly without gating, the output dimension should equal the input size `N`. The changes update workspace sizing, buffer allocation, and activation logic across all MoE backends to handle both gated and non-gated activation types correctly.

**Technical impact**  
The changes introduce a new `adjust_N_for_activation` method that centralizes activation-aware dimension calculation, replacing hardcoded `N//2` values. The `workspace_shapes` API now accepts an `activation` parameter, which is plumbed through all `FusedMoEPermuteExpertsUnpermute` implementations. Activation logic is unified into `utils.apply_moe_activation`, eliminating scattered local implementations. This ensures consistent buffer sizing and correct execution for both gated and non-gated activations across Triton, CUTLASS, DeepGemm, and other backends.

**Potential risks**  
- The widespread changes across multiple backends increase the risk of missing an edge case in buffer sizing or activation handling.  
- The new `activation` parameter must be correctly passed through all call paths; any omission could lead to incorrect workspace allocation.  
- The `adjust_N_for_activation` logic assumes activation names ending with `_no_mul` are non-gated; this convention must be strictly adhered to in future extensions.

**Key insights**  
- Centralizing activation dimension calculation and logic reduces duplication and improves maintainability.  
- Developers must ensure the `activation` parameter is propagated correctly in all new or modified MoE implementations.  
- The added comprehensive tests validate shape calculations and execution, providing a safety net for regression.

---

## 33. [[MODEL] New model support for kakaocorp/kanana-1.5-v-3b-instruct](https://github.com/vllm-project/vllm/pull/29384)


### Base Information

- **PR Number:** #29384
- **Author:** [kakao-steve-ai](https://github.com/kakao-steve-ai)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 08:39:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29384/files) (5):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/kanana_v.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds native support for the `kakaocorp/kanana-1.5-v-3b-instruct` multimodal model. The implementation includes a new model class `KananaVForConditionalGeneration` that wraps a Qwen2‑VL vision encoder with a custom `DynamicCAbstractor` projector, a multimodal processor for image token expansion and prompt replacement, and integration with the language model. The changes enable text‑and‑image generation using this model within the vLLM framework.

**Technical impact**  
The addition extends vLLM's multimodal model registry and provides a new vision‑language architecture. The model reuses existing Qwen2‑VL vision transformer components but introduces a custom projector and processing logic. This increases the diversity of supported models without modifying core inference pipelines, as the implementation follows vLLM's established interfaces (`SupportsMultiModal`, `MultiModalEmbeddings`).

**Potential risks**  
The model relies on `trust_remote_code=True`, which may introduce security or compatibility concerns. The custom `DynamicCAbstractor` projector includes legacy checkpoint handling that assumes specific tensor shapes, which could fail if future model weights change. Additionally, the implementation currently assumes single‑image input (T=1) and does not support video, which may limit use cases.

**Key insights**  
Developers should note that the model requires `trust_remote_code` and has been validated with specific benchmark scores (e.g., MMMU: 39.78). The integration follows vLLM's multimodal patterns, ensuring consistency with existing models. Ensure that any future updates to the Qwen2‑VL components or checkpoint formats are tested for compatibility with the custom projector logic.

---

## 34. [Add K-EXAONE-236B-A23B](https://github.com/vllm-project/vllm/pull/31621)


### Base Information

- **PR Number:** #31621
- **Author:** [lkm2835](https://github.com/lkm2835)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 08:30:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31621/files) (7):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/config/speculative.py`
  - `vllm/model_executor/models/exaone4.py`
  - `vllm/model_executor/models/exaone_moe.py`
  - `vllm/model_executor/models/exaone_moe_mtp.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds native support for the K-EXAONE-236B-A23B MoE model, including its speculative decoding path via Multi-Token Prediction (MTP). It introduces a new model class `ExaoneMoeForCausalLM` with a MoE stack using `FusedMoE`, shared experts, expert-parallel load balancing, and expert-aware weight loading. Additionally, it implements `ExaoneMoeMTP` and `ExaoneMoeMultiTokenPredictor` as drafters for speculative decoding, wiring them into the speculative config and model registry.

**Technical impact**  
The changes extend the model registry to support a new MoE architecture, enabling efficient inference for large-scale mixture-of-experts models. The integration with speculative decoding via MTP can improve inference throughput. The modifications to `Exaone4GatedMLP` (adding `reduce_results`) and updates to `SpeculativeConfig` ensure compatibility with shared expert reduction and proper model type mapping for speculative workflows.

**Potential risks**  
The MoE implementation relies on expert-parallel load balancing and shared experts, which may introduce complexity in distributed settings. The speculative decoding path assumes specific model configurations (e.g., `num_nextn_predict_layers`), and incorrect mappings could lead to runtime errors. Additionally, the PR lacks a documented test plan and results, raising concerns about validation coverage.

**Key insights**  
Developers should verify that the expert-parallel and tensor-parallel configurations align with the target hardware. The speculative decoding integration requires careful testing to ensure correct behavior with the new `exaone_moe_mtp` model type. Updating documentation (e.g., `supported_models.md`) is essential, but a comprehensive test suite is needed to validate performance and correctness.

---

## 35. [doc: Update model references in supported_models.md](https://github.com/vllm-project/vllm/pull/32188)


### Base Information

- **PR Number:** #32188
- **Author:** [andyzhangx](https://github.com/andyzhangx)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 08:15:29
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32188/files) (1):**
  - `docs/models/supported_models.md`

### Summary

**What changed and why**  
Updated two model references in `supported_models.md` to include full Hugging Face organization namespaces. Specifically, `ShieldLM-6B-chatglm3` was prefixed with `thu-coai/`, and `gpt2`/`gpt2-xl` were updated to `openai-community/gpt2` and `openai-community/gpt2-xl`. These changes ensure consistency for automated parsing tools that rely on fully qualified model identifiers.

**Technical impact**  
The modifications improve machine readability of the documentation, enabling reliable automated extraction of model names. This supports tooling that may generate model lists, validate configurations, or populate downstream systems. No functional changes to the codebase are introduced—only documentation clarity is enhanced.

**Potential risks**  
If any downstream scripts or documentation generators assume the previous unqualified names, they may break or produce incorrect results. Additionally, future model entries added without org prefixes could reintroduce parsing inconsistencies. There is no risk to runtime behavior, as this is purely a documentation update.

**Key insights**  
Adopting fully qualified model names (org/model) in documentation establishes a consistent pattern that aids automation. Developers should ensure all new model entries follow this convention and consider updating any existing tooling that parses this file to handle both qualified and unqualified formats during transition.

---

## 36. [[ROCm] [Bugfix] Fix order of mori build in Dockerfile.rocm_base](https://github.com/vllm-project/vllm/pull/32179)


### Base Information

- **PR Number:** #32179
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-12 07:33:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32179/files) (1):**
  - `docker/Dockerfile.rocm_base`

### Summary

**What changed and why**  
The Dockerfile was restructured to fix a Docker BuildKit stage dependency error. The `build_mori` stage was moved to appear after the `build_pytorch` stage, ensuring the `--mount=from=build_pytorch` mount works correctly. Additionally, `MORI_BRANCH` and `MORI_REPO` arguments were added to the final image stage and recorded in `/app/versions.txt` for better build metadata tracking.

**Technical impact**  
This change resolves the Docker build failure by correcting the stage order, allowing the MORI build to properly access wheels from the PyTorch build stage. The addition of MORI metadata to `versions.txt` improves traceability and debugging by documenting the exact source and branch used during the build.

**Potential risks**  
If other stages depend on `build_mori` outputs, their order may also need adjustment. The change assumes no hidden dependencies on the original stage sequence, and any downstream scripts expecting the previous layout should be verified.

**Key insights**  
Always define Docker stages before they are referenced in `--mount` directives to satisfy BuildKit's forward-reference constraints. Maintaining comprehensive build metadata in `versions.txt` is a best practice for reproducible builds and should be extended to all build arguments.

---

## 37. [doc: Update model name for Qwen3-Coder in documentation](https://github.com/vllm-project/vllm/pull/32185)


### Base Information

- **PR Number:** #32185
- **Author:** [andyzhangx](https://github.com/andyzhangx)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-12 07:10:50
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32185/files) (1):**
  - `docs/features/tool_calling.md`

### Summary

**What changed and why**  
Updated the model name from `Qwen/Qwen3-480B-A35B-Instruct` to `Qwen/Qwen3-Coder-480B-A35B-Instruct` in the tool_calling.md documentation. This correction aligns the documented model name with the actual model name listed on Hugging Face.

**Technical impact**  
This change ensures documentation accuracy for users configuring tool calling with the Qwen3-Coder model. It prevents potential user errors when referencing the model name in configuration flags or API calls.

**Potential risks**  
Minimal risk as this is a documentation-only change. However, if any automated scripts or tests rely on the exact model name string in the documentation, they may need updates to reflect the corrected name.

**Key insights**  
Always verify model names against official sources (like Hugging Face) before documenting. Consider adding a process to periodically audit documentation against upstream model repositories to maintain accuracy.

---

## 38. [OffloadingConnector: Add cpu_bytes_to_use configuration](https://github.com/vllm-project/vllm/pull/24498)


### Base Information

- **PR Number:** #24498
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-12 07:00:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/24498/files) (9):**
  - `docs/features/disagg_prefill.md`
  - `tests/v1/kv_connector/unit/test_config.py`
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `tests/v1/kv_offload/test_cpu_offloading.py`
  - `vllm/config/vllm.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/v1/kv_offload/cpu.py`
  - `vllm/v1/kv_offload/factory.py`
  - `vllm/v1/kv_offload/spec.py`

### Summary

**What changed and why**  
This PR replaces the `num_cpu_blocks` configuration with `cpu_bytes_to_use` for the `OffloadingConnector`. The change allows users to specify CPU memory allocation in bytes per vLLM instance rather than per-worker blocks, providing a more intuitive and instance-wide configuration for KV cache offloading.

**Technical impact**  
The `CPUOffloadingSpec` now dynamically calculates `num_blocks` based on `cpu_bytes_to_use`, `KVCacheConfig` (page size, tensors, world size), and `block_size`. This requires passing `kv_cache_config` to `OffloadingSpecFactory.create_spec()` and `OffloadingConnector`. The configuration logic in `VllmConfig` now sets `cpu_bytes_to_use` directly without per-rank splitting for native offloading.

**Potential risks**  
The calculation of `kv_bytes_per_offloaded_block` assumes uniform page sizes across all KV cache groups; an assertion failure will occur if multiple page sizes exist. There's also a risk of integer division truncation when computing `num_blocks`, potentially wasting allocated memory if bytes don't align perfectly with block size.

**Key insights**  
Developers must update their configuration from `num_cpu_blocks` to `cpu_bytes_to_use` in bytes. All tests and documentation have been updated accordingly. The change centralizes memory allocation configuration but adds dependency on `KVCacheConfig` during offloading spec initialization.

---

## 39. [[Feature] Support recording expert indices for rollout router replay](https://github.com/vllm-project/vllm/pull/28284)


### Base Information

- **PR Number:** #28284
- **Author:** [xhx1022](https://github.com/xhx1022)
- **Merged By:** [22quinn](https://github.com/22quinn)
- **Merged time:** 2026-01-12 06:23:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28284/files) (11):**
  - `vllm/config/model.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/routed_experts_capturer.py`
  - `vllm/outputs.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/engine/output_processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR adds optional support for recording MoE expert routing decisions during inference to enable Rollout Router Replay (R3). It introduces a new configuration flag `enable_return_routed_experts` that, when enabled, captures per-token, per-layer expert indices (`topk_ids`) and returns them to clients via the output pipeline. This is designed for reinforcement learning alignment research with MoE-based LLMs.

**Technical impact**  
The changes add a new end-to-end data flow: routing decisions are captured in `fused_moe/layer.py`, stored in shared memory buffers via `RoutedExpertsCapturer`, reconstructed by the scheduler using KV cache slot mappings, and attached to `CompletionOutput`. The implementation supports Tensor Parallel and CUDA Graph execution but disables async scheduling and prohibits context parallelism (DCP/PCP) when enabled.

**Potential risks**  
Shared memory management introduces cross-process coordination complexity and potential resource leaks if cleanup fails. The slot mapping reconstruction depends on KV cache block allocation details, which could break if the cache management changes. Disabling async scheduling and banning context parallelism may affect performance and compatibility for some deployment scenarios.

**Key insights**  
Developers should treat this as a specialized, opt-in feature for research use cases. The shared memory approach is suitable for single-node multi-process setups but may not scale to distributed environments. Future changes to KV cache or scheduling logic must maintain compatibility with the slot mapping reconstruction to avoid silent data corruption.

---

## 40. [[P/D] Refactor mooncake connector sender thread using async coroutines](https://github.com/vllm-project/vllm/pull/31573)


### Base Information

- **PR Number:** #31573
- **Author:** [dtcccc](https://github.com/dtcccc)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-12 04:35:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31573/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`

### Summary

**What changed and why**  
The PR refactors the Mooncake connector's sender thread from a synchronous threading model to an asynchronous coroutine-based approach. This consolidates all sender-related data and logic within a single thread, eliminating the need for multiple locks and simplifying maintenance.

**Technical impact**  
The changes replace threaded worker pools with asyncio tasks, using an `asyncio.Queue` to buffer requests. This reduces lock contention and thread synchronization overhead, potentially improving throughput in I/O-bound scenarios. The architecture now relies on a dedicated event loop for sender operations, aligning with the existing receiver implementation.

**Potential risks**  
The refactor introduces a dependency on asyncio correctness—ensuring proper task cancellation and queue management is critical. The increased number of sender tasks (`num_sender_tasks = num_sender_workers * 2`) could lead to higher memory usage if not tuned appropriately. Any unhandled exceptions in coroutines might silently fail without proper logging.

**Key insights**  
Developers should verify that the async event loops (`sender_loop`, `receiver_loop`) are properly started and stopped during shutdown. The removal of locks simplifies code but requires careful attention to thread-safety for shared structures like `reqs_need_send`. Consider adding metrics to monitor queue depth and task latency for performance tuning.

---

## 41. [[Bugfix] Fix missing scale passing for encoder Triton Attention implementation](https://github.com/vllm-project/vllm/pull/32149)


### Base Information

- **PR Number:** #32149
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 03:13:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32149/files) (4):**
  - `examples/offline_inference/basic/embed.py`
  - `examples/offline_inference/basic/score.py`
  - `vllm/v1/attention/backends/triton_attn.py`
  - `vllm/v1/attention/ops/triton_prefill_attention.py`

### Summary

**What changed and why**  
This PR fixes a bug where the configured softmax scale (`self.scale`) was not being passed to the Triton attention kernel in the encoder path. The change adds the missing `softmax_scale=self.scale` parameter in `triton_attn.py` and updates the `context_attention_fwd` function signature in `triton_prefill_attention.py` to accept an optional scale parameter, defaulting to `1/sqrt(d)` if not provided. Additionally, ROCm-specific attention backend overrides were removed from two example scripts, simplifying them.

**Technical impact**  
The fix ensures that encoder-side Triton attention correctly applies the intended softmax scaling, which is critical for attention score stability and model accuracy. The updated `context_attention_fwd` function now explicitly supports a configurable scale, improving flexibility and aligning with the decoder path behavior. The removal of ROCm-specific overrides in examples standardizes configuration across platforms.

**Potential risks**  
If the `softmax_scale` parameter is incorrectly set (e.g., to zero or extremely large values), it could lead to numerical instability or degraded model performance. The default fallback to `1/sqrt(d)` maintains backward compatibility, but any existing code relying on implicit scaling in the encoder path may see behavioral changes. The simplification of example scripts assumes that the default attention backend works correctly on ROCm, which should be validated.

**Key insights**  
Developers should verify that the `self.scale` value is appropriately initialized and tested for encoder models. The change highlights the importance of consistent parameter passing between decoder and encoder attention implementations. Consider adding unit tests to confirm that the scale parameter is correctly applied in both causal and non-causal (encoder) attention contexts.

---

## 42. [[Doc] Add documentation for offline API docs feature](https://github.com/vllm-project/vllm/pull/32134)


### Base Information

- **PR Number:** #32134
- **Author:** [ricky-chaoju](https://github.com/ricky-chaoju)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 02:33:48
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32134/files) (1):**
  - `docs/serving/openai_compatible_server.md`

### Summary

**What changed and why**  
Added documentation for the `--enable-offline-docs` flag in the OpenAI-compatible server guide. This explains that FastAPI's `/docs` endpoint normally requires internet access and provides instructions for enabling offline documentation in air-gapped environments.

**Technical impact**  
This change enhances documentation clarity without altering code functionality. It informs users about a server configuration option that improves usability in restricted network environments, aligning with the feature introduced in PR #30184.

**Potential risks**  
No technical risks, as this is purely documentation. However, users might misinterpret the flag's scope—it only affects the API documentation endpoint, not the core serving functionality or model loading.

**Key insights**  
Documentation updates like this improve user experience in enterprise or secure deployments. Developers should ensure the `--enable-offline-docs` flag is consistently documented across all relevant server guides and CLI help texts.

---

## 43. [[Doc] Improve LoRA docs](https://github.com/vllm-project/vllm/pull/32159)


### Base Information

- **PR Number:** #32159
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 02:19:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32159/files) (1):**
  - `docs/features/lora.md`

### Summary

**What changed and why**  
The documentation updates replace outdated LoRA examples with currently supported models. Specifically, `Llama-2-7b-hf` and `yard1/llama-2-7b-sql-lora-test` are replaced with `meta-llama/Llama-3.2-3B-Instruct` and `jeeejeee/llama32-3b-text2sql-spider`. This reflects vLLM's removal of the extended vocabulary feature for LoRA and ensures examples use functional, modern adapters.

**Technical impact**  
These changes have no functional impact on the codebase but improve documentation accuracy and user experience. The updates clarify that LoRA adapters must be compatible with the current vLLM version, and they demonstrate the simplified `--lora-modules` syntax, which now supports direct Hugging Face repository paths instead of local snapshot paths.

**Potential risks**  
The primary risk is that users might still attempt to use older, unsupported LoRA adapters that rely on extended vocabulary, leading to runtime errors. Additionally, the new examples assume the specified models remain publicly available and compatible; if they are deprecated or modified, the documentation could become outdated again.

**Key insights**  
Developers should note that vLLM no longer supports LoRA adapters with extended vocabulary, so they must verify adapter compatibility. The updated `--lora-modules` syntax simplifies deployment by allowing repository IDs directly, reducing manual path handling. Always test LoRA integrations with the documented examples to ensure they work as expected in your environment.

---

## 44. [[doc] fix broken links](https://github.com/vllm-project/vllm/pull/32158)


### Base Information

- **PR Number:** #32158
- **Author:** [minimAluminiumalism](https://github.com/minimAluminiumalism)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 02:18:38
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32158/files) (1):**
  - `docs/design/paged_attention.md`

### Summary

**What changed and why**  
The PR converts HTML `<img>` tags with relative paths to Markdown image syntax (`![alt](path)`) in `docs/design/paged_attention.md`. This fixes broken image links because MkDocs does not properly resolve relative paths within HTML tags but does process them correctly in Markdown images.

**Technical impact**  
This change ensures that images render correctly in the generated documentation. It is a purely cosmetic update that affects only documentation presentation—no functional code, build processes, or content logic is altered.

**Potential risks**  
Minimal risk exists as the change is limited to documentation formatting. However, if the Markdown images use different styling (e.g., centering, width/height attributes) than the original HTML, visual presentation may differ. The removal of `<p align="center">` wrappers could affect image alignment.

**Key insights**  
Always use Markdown image syntax in MkDocs projects to ensure proper path resolution. For future documentation updates, verify that images are tested in the built site. Consider adding a documentation linter or pre-commit hook to enforce Markdown image syntax and prevent similar issues.

---

## 45. [[Frontend] Fix Flaky MCP Streaming Test](https://github.com/vllm-project/vllm/pull/32153)


### Base Information

- **PR Number:** #32153
- **Author:** [daniel-salib](https://github.com/daniel-salib)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-12 02:03:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32153/files) (1):**
  - `tests/entrypoints/openai/responses/test_harmony.py`

### Summary

**What changed and why**  
The test `test_mcp_code_interpreter_streaming` was updated by changing the math problem from "Calculate 15 * 32 using python." to "Calculate 123 * 456 using python." This modification addresses flaky test failures caused by the original problem being too simple, which inconsistently triggered the MCP tool call. The new, more complex calculation ensures consistent tool call activation.

**Technical impact**  
This change improves test reliability by guaranteeing the MCP tool call is invoked, thereby stabilizing the streaming behavior under test. It does not affect production code or system functionality, only enhancing the deterministic nature of the unit test.

**Potential risks**  
While the test now passes consistently, there is a minor risk that the new problem could become trivial for future model improvements, potentially reintroducing flakiness. Additionally, the test's focus on a specific multiplication may not fully represent edge cases in tool-call triggering logic.

**Key insights**  
Developers should ensure test inputs are sufficiently complex to reliably exercise the intended code paths. Consider parameterizing test inputs or using a range of problems to guard against model evolution. This fix exemplifies the importance of designing robust, deterministic tests for AI-driven features.

---

## 46. [[cpu][bench] Add Fused MoE Micro Benchmark for CPU Backend](https://github.com/vllm-project/vllm/pull/32092)


### Base Information

- **PR Number:** #32092
- **Author:** [andikarachman](https://github.com/andikarachman)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-12 02:03:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32092/files) (1):**
  - `benchmarks/kernels/cpu/benchmark_cpu_fused_moe.py`

### Summary

**What changed and why**  
A new micro-benchmark script `benchmark_cpu_fused_moe.py` was added to measure the performance of the CPU fused MoE kernel. This addresses issue #31721 by providing a tool to profile the `cpu_fused_moe` operation with prepacked weights, supporting configurable shapes, ISA selection, activation functions, and bias usage.

**Technical impact**  
The script integrates with the existing CPU MoE infrastructure, using `cpu_prepack_moe_weight` and `cpu_fused_moe` from `vllm._custom_ops`. It adds a benchmarking utility that reports latency statistics and TFLOP/s, enabling performance analysis and optimization of the CPU MoE path. The script follows patterns from `test_cpu_fused_moe.py` for ISA detection and tensor initialization.

**Potential risks**  
The benchmark assumes the CPU extensions are available and will exit with an error if not, which is appropriate but may confuse users on unsupported platforms. The FLOPs calculation (`2 * batch * topk * (hidden * up_dim + intermediate * hidden)`) is a simplified estimate and may not account for all operations (e.g., activation functions, bias addition). Additionally, the default `topk_num` logic (`expert_num // 2`) could produce unexpected routing if `expert_num` is 1.

**Key insights**  
Developers should use this benchmark to evaluate CPU MoE kernel performance across different configurations and ISAs. Ensure the environment has the required CPU extensions (AVX2/AVX512 or AMX) before running. The script’s warmup phase and statistical reporting (min, max, mean, std) provide reliable measurements, but the TFLOP/s metric should be interpreted as an approximation for comparative analysis.

---

## 47. [[Misc] Disable default `--ready-check-timeout-sec` extra call in vllm bench](https://github.com/vllm-project/vllm/pull/30975)


### Base Information

- **PR Number:** #30975
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-12 01:58:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30975/files) (1):**
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
The PR changes the default value of `--ready-check-timeout-sec` from 600 seconds to 0 in the vLLM benchmark serve command. This disables the automatic health-check probe that previously sent an extra request to verify server readiness, as the current implementation relies on a non-uniform backend `/health` API. The change aims to eliminate misleading warm-up behavior and allow users to manually implement backend-specific readiness checks when needed.

**Technical impact**  
This modification removes the default overhead of at least one extra request during benchmark initialization, which could skew performance measurements by inadvertently warming up the server. Users must now explicitly set a timeout if they want automatic readiness checking, shifting responsibility for server health verification to the user or wrapper scripts.

**Potential risks**  
If users rely on the previous default behavior to ensure server readiness before benchmarking, they may encounter errors if the server isn't fully initialized when benchmarks start. Automated workflows that don't implement alternative health checks could fail or produce inconsistent results. There's also a risk of confusion for users unaware of the change who might expect automatic readiness verification.

**Key insights**  
Developers should update any automation scripts to include explicit health checks (e.g., using `curl` or backend-specific APIs) before running benchmarks. Consider documenting recommended patterns for server readiness verification in automated environments. This change aligns with giving users more control but requires awareness of the new default behavior to avoid runtime issues.

---

## 48. [[Model] Remove incorrect `SupportsPP` from MTP models](https://github.com/vllm-project/vllm/pull/32150)


### Base Information

- **PR Number:** #32150
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-12 01:19:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32150/files) (6):**
  - `vllm/model_executor/models/deepseek_mtp.py`
  - `vllm/model_executor/models/ernie_mtp.py`
  - `vllm/model_executor/models/glm4_moe_mtp.py`
  - `vllm/model_executor/models/longcat_flash_mtp.py`
  - `vllm/model_executor/models/openpangu_mtp.py`
  - `vllm/model_executor/models/qwen3_next_mtp.py`

### Summary

**What changed and why**  
The PR removes the `SupportsPP` interface inheritance from six MTP (Multi-Tensor Parallel) model implementations because these models don't actually support pipeline parallelism. This corrects inaccurate interface declarations that could mislead developers about the models' capabilities.

**Technical impact**  
These changes ensure the model interfaces accurately reflect their supported parallelism strategies. Removing `SupportsPP` prevents potential runtime errors or incorrect assumptions when attempting to use pipeline parallelism with these models. The codebase now maintains proper separation between models that support different parallelism techniques.

**Potential risks**  
If any downstream code was relying on the `SupportsPP` interface for type checking or feature detection, it may now fail or behave unexpectedly. The removal of `make_empty_intermediate_tensors` from `Qwen3NextMTP` could break functionality if this method was being called directly, though it appears to be delegated to the parent class.

**Key insights**  
Always verify that model interfaces accurately reflect their actual capabilities. Developers should check if any pipeline parallelism tests were incorrectly passing due to the interface mismatch. Consider adding runtime validation to prevent similar interface mismatches in the future.

---

