# vLLM Merged PR Report

**Report Date:** 2026-01-08 PST

**Total Merged PRs:** 44

---

## 1. [fix lora moe sharding when rank < max_lora_rank](https://github.com/vllm-project/vllm/pull/31994)


### Base Information

- **PR Number:** #31994
- **Author:** [gnovack](https://github.com/gnovack)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-08 22:43:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31994/files) (2):**
  - `tests/lora/test_gptoss_tp.py`
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
The PR fixes a bug in LoRA sharding for MoE layers under tensor parallelism. Previously, when a LoRA adapter's rank was less than `max_lora_rank`, the weight tensors (`w13_lora_a` and `w2_lora_b`) were sliced based on the current adapter's dimensions, leading to interspersed zeros after all-gather. Now, slicing uses the pre-allocated buffer sizes (`w13_lora_a_stacked[0].shape[2]` and `w2_lora_b_stacked[0].shape[2]`), ensuring zeros are right-padded and consistent across shards.

**Technical impact**  
This change ensures correct tensor alignment during all-gather operations in tensor-parallel settings, preventing corrupted outputs when using LoRA adapters with varying ranks. It maintains compatibility with S-LoRA's sharding strategy while fixing the interspersed-zero issue, which could otherwise lead to incorrect model predictions.

**Potential risks**  
If the pre-allocated buffer sizes are incorrectly initialized or mismatched with the tensor parallelism degree, sharding may still produce misaligned tensors. Additionally, the fix assumes that `w13_lora_a_stacked` and `w2_lora_b_stacked` are always properly initialized before slicing—any uninitialized or empty states could cause runtime errors.

**Key insights**  
Always use consistent shard sizes derived from pre-allocated buffers rather than dynamic adapter dimensions to ensure tensor parallelism works correctly. Developers should verify that buffer initialization occurs before any slicing logic and consider adding validation for buffer shapes during setup. The updated test (removing explicit `max_lora_rank`) effectively validates the fix under default conditions.

---

## 2. [[Bugfix] Fix FusedMoE LoRA w2_output_size](https://github.com/vllm-project/vllm/pull/31949)


### Base Information

- **PR Number:** #31949
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-08 21:54:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31949/files) (1):**
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
The PR fixes an incorrect calculation of `w2_output_size` in the FusedMoE LoRA implementation. Previously, it was incorrectly using the rank dimension from the LoRA adapter (`w2_lora_a_stacked[0].shape[-2]`), but now correctly returns the base layer's hidden size.

**Technical impact**  
This correction ensures that the output size for the LoRA W2 path matches the expected hidden dimension of the base model layer, maintaining consistency in tensor shapes during the fused MoE computation and preventing potential shape mismatch errors in downstream operations.

**Potential risks**  
If any downstream code was implicitly relying on the previously incorrect rank value, it could now break. Additionally, this fix assumes `base_layer.hidden_size` is always the correct output dimension, which should be validated across different model architectures.

**Key insights**  
Always verify that size calculations reference the intended model dimensions rather than adapter parameters. This fix highlights the importance of distinguishing between adapter-specific attributes (like rank) and core model dimensions in modular components like LoRA layers.

---

## 3. [[Cleanup] Remove obsolete spec decoding compatibility logic](https://github.com/vllm-project/vllm/pull/32003)


### Base Information

- **PR Number:** #32003
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-08 21:44:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32003/files) (8):**
  - `benchmarks/benchmark_ngram_proposer.py`
  - `tests/v1/sample/test_logprobs.py`
  - `tests/v1/spec_decode/test_ngram.py`
  - `vllm/v1/spec_decode/ngram_proposer.py`
  - `vllm/v1/spec_decode/suffix_decoding.py`
  - `vllm/v1/spec_decode/utils.py`
  - `vllm/v1/worker/gpu_input_batch.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR removes obsolete compatibility logic for speculative decoding that was originally included with the V1 ngram-only implementation. The logic filtered out requests with unsupported sampling parameters (like logprobs, penalties) from speculative decoding. These checks are now redundant because recent changes have added proper support for logprobs and penalty parameters with spec decoding, and unsupported parameters (min_p, min_tokens, logit_bias) now cause requests to fail explicitly when spec decoding is enabled.

**Technical impact**  
The removal simplifies the speculative decoding codebase by eliminating redundant filtering logic across multiple components (ngram_proposer, suffix_decoding, GPU input batch). The API for proposer methods is now cleaner with fewer parameters. Tests have been updated to reflect that ngram speculative decoding now supports logprobs, and the test coverage has been extended to validate both eagle and ngram methods with logprobs.

**Potential risks**  
Requests with still-unsupported parameters (min_p, min_tokens, logit_bias) will now fail at request validation rather than being silently filtered out, which changes error handling behavior. There's a risk that some edge cases of parameter combinations might not be properly validated. The benchmark changes remove the unsupported_reqs field initialization, which could affect benchmarking accuracy if those scenarios were being measured.

**Key insights**  
This cleanup represents architectural maturation - the system now properly validates unsupported parameters upfront rather than using runtime filtering. Developers should be aware that spec decoding now explicitly fails for unsupported parameters instead of silently skipping them. The extended test coverage for ngram+logprobs is crucial for ensuring the removed compatibility logic doesn't break existing functionality.

---

## 4. [[CI] [ROCm] Fix `tests/entrypoints/test_grpc_server.py` on ROCm](https://github.com/vllm-project/vllm/pull/31970)


### Base Information

- **PR Number:** #31970
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 20:54:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31970/files) (5):**
  - `requirements/common.txt`
  - `requirements/rocm-test.txt`
  - `requirements/rocm.txt`
  - `setup.py`
  - `tests/entrypoints/test_grpc_server.py`

### Summary

**What changed and why**  
This PR fixes a gRPC import error in ROCm CI tests by ensuring gRPC protobuf stubs are generated during both installation (`setup.py`) and development mode (`develop`). It adds `grpcio-tools` to ROCm requirements, confirms `grpcio-reflection` is in common dependencies, and increases the gRPC server startup timeout from 30 to 60 seconds to accommodate slower ROCm initialization.

**Technical impact**  
The changes ensure gRPC stubs are consistently generated across all installation methods (regular install and `develop` mode), preventing `ImportError` for `vllm_engine_pb2`. The timeout increase accounts for ROCm's slower server startup, reducing flaky test failures. This aligns ROCm's dependency setup with CUDA environments.

**Potential risks**  
The increased timeout may mask underlying performance regressions or resource contention issues. If `grpcio-tools` version mismatches occur between `common.txt` and `rocm.txt`, it could lead to inconsistent stub generation. The `develop` command override might interfere with custom development workflows.

**Key insights**  
Always generate gRPC stubs during both build and development phases to avoid import issues. Consider monitoring server startup times to determine if the timeout increase is a temporary workaround or a necessary adjustment. Verify that `grpcio-tools` and `grpcio-reflection` versions remain synchronized across all requirement files.

---

## 5. [[ROCm][CI] Fix test_token_classification.py::test_bert_models](https://github.com/vllm-project/vllm/pull/31993)


### Base Information

- **PR Number:** #31993
- **Author:** [divakar-amd](https://github.com/divakar-amd)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 20:04:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31993/files) (1):**
  - `tests/models/language/pooling/test_token_classification.py`

### Summary

**What changed and why**  
This PR fixes ROCm-specific accuracy issues in token classification tests by forcing Hugging Face models to use eager attention implementation on ROCm platforms, avoiding flash attention accuracy problems. It also improves the logits comparison by switching from `torch.allclose` to `torch.testing.assert_close` with explicit tolerances and refactors platform detection to use a top-level import.

**Technical impact**  
The changes ensure test correctness on ROCm platforms by circumventing known flash attention accuracy issues in the Hugging Face implementation. The updated assertion provides more precise control over numerical tolerance, making the test more robust while maintaining compatibility with existing test infrastructure.

**Potential risks**  
The increased absolute tolerance (1.2e-2 vs 1e-2) could potentially mask real regressions in model outputs. Platform-specific branching adds maintenance complexity, and the fix doesn't address the root cause of flash attention accuracy issues on ROCm, only works around them.

**Key insights**  
Developers should be aware that ROCm platforms require special handling for attention implementations in Hugging Face models. The assertion change provides better debugging information when tests fail. Consider whether similar fixes are needed for other model tests that might use flash attention on ROCm.

---

## 6. [[Bugfix] missing tokens occur in harmony streaming](https://github.com/vllm-project/vllm/pull/30437)


### Base Information

- **PR Number:** #30437
- **Author:** [Ri0S](https://github.com/Ri0S)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-08 19:59:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30437/files) (2):**
  - `vllm/entrypoints/context.py`
  - `vllm/entrypoints/openai/serving_responses.py`

### Summary

**What changed and why**  
Fixed a bug in Harmony streaming mode where only the last token was used when the engine yielded multiple tokens per step. The solution adds a `last_content_delta` field to `StreamingHarmonyContext` to accumulate text across all `token_ids` in each step, ensuring all tokens are captured and emitted.

**Technical impact**  
The changes ensure that all tokens generated per step are properly aggregated and emitted in streaming responses. This affects multiple delta emitters (final, analysis, MCP/code-interpreter, MCP prefix, and function-call arguments), which now use the accumulated `last_content_delta` instead of the parser's per-token delta.

**Potential risks**  
If `last_content_delta` is not properly reset at the start of each message, it could carry over text from previous messages. Additionally, the guard condition in `_emit_content_delta_events` now checks `ctx.last_content_delta`, which may introduce subtle behavior changes if the accumulation logic fails for edge cases (e.g., empty tokens).

**Key insights**  
Developers should verify that `last_content_delta` is correctly reset in `append_output` and that the accumulation logic handles all token scenarios. The fix is critical for streaming correctness, so thorough testing with varied token outputs per step is recommended to ensure no regressions.

---

## 7. [[Bugfix] Fix typo in FusedMoE LoRA reshape comment](https://github.com/vllm-project/vllm/pull/31992)


### Base Information

- **PR Number:** #31992
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 18:46:06
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31992/files) (1):**
  - `vllm/lora/model_manager.py`

### Summary

**What changed and why**  
This PR fixes a typo in a comment for the FusedMoE LoRA reshape operation. The comment incorrectly stated the tensor shape as `(output_size,num_experts,rank)` but has been corrected to `(output_size,rank,num_experts)` to accurately reflect the actual reshape logic.

**Technical impact**  
The change is purely cosmetic—it only updates a comment to match the code behavior. This does not affect functionality, performance, or system behavior, but improves code clarity and maintainability for developers working with LoRA and MoE integrations.

**Potential risks**  
There are no functional risks since only a comment was modified. However, if the comment had been relied upon for documentation or downstream tooling, the previous inaccuracy could have caused minor confusion. The fix mitigates this by ensuring alignment between comments and actual tensor shapes.

**Key insights**  
Always ensure comments accurately reflect code logic, especially in complex operations like tensor reshaping for fused MoE and LoRA. While this fix is minor, it highlights the importance of maintaining precise documentation to aid developer understanding and reduce potential misinterpretations.

---

## 8. [[Async][Feat] support apply penalty or bad_words for async + spec](https://github.com/vllm-project/vllm/pull/30495)


### Base Information

- **PR Number:** #30495
- **Author:** [izhuhaoran](https://github.com/izhuhaoran)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-08 18:31:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30495/files) (4):**
  - `tests/v1/e2e/test_async_scheduling.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/worker/gpu_input_batch.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR enables sampling penalties (frequency, presence, repetition) and `bad_words` support for async scheduling combined with speculative decoding. Previously, these features were incompatible due to missing draft token ID synchronization between GPU and CPU. The changes implement asynchronous copying of draft token IDs and update the sampling metadata with these tokens before penalty/bad_words calculations.

**Technical impact**  
The modifications allow penalty-based sampling and bad word filtering to work correctly in async speculative decoding scenarios. Key changes include: adding draft token ID copying mechanisms in `GPUModelRunner`, updating `InputBatch` to populate `spec_token_ids` with real draft tokens, and removing validation restrictions that previously blocked these features. This expands the supported parameter combinations without altering core sampling algorithms.

**Potential risks**  
Edge cases in token replacement logic could cause mismatches between placeholder counts and actual sampled tokens, particularly after KV-load failures. The conditional copying of draft tokens (`_copy_draft_token_ids_to_cpu`) might skip necessary copies if metadata flags aren't set correctly. Asynchronous operations introduce timing dependencies that could surface in high-load scenarios.

**Key insights**  
Developers should verify that `sampling_metadata.output_token_ids` is properly set when penalties or bad_words are used, as this triggers draft token copying. The token replacement logic now handles variable numbers of placeholders, which is critical for robustness. All existing tests pass, but new edge-case tests for KV-load failures with penalties would strengthen coverage.

---

## 9. [[Frontend] Add MCP tool streaming support to Responses API](https://github.com/vllm-project/vllm/pull/31761)


### Base Information

- **PR Number:** #31761
- **Author:** [daniel-salib](https://github.com/daniel-salib)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-08 17:19:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31761/files) (3):**
  - `tests/entrypoints/openai/test_response_api_mcp_tools.py`
  - `tests/entrypoints/openai/test_response_api_with_harmony.py`
  - `vllm/entrypoints/openai/serving_responses.py`

### Summary

**What changed and why**  
This PR adds streaming support for MCP (Model Context Protocol) tools in the Responses API when using GPT OSS. It extends harmony utilities and response-serving infrastructure to incrementally stream tool calls and their results to clients instead of returning them as a single batch. The changes include new event types for MCP tool streaming, updated streaming logic, and comprehensive test coverage.

**Technical impact**  
The implementation introduces a new `HarmonyStreamingState` class to manage streaming state and adds MCP-specific event types (`ResponseMcpCallArgumentsDeltaEvent`, `ResponseMcpCallCompletedEvent`, etc.). It modifies the streaming generator to differentiate between function calls (prefixed with `functions.`) and MCP tools, ensuring correct event emission. The changes also refactor test suites to validate MCP tool streaming behavior and ensure no interference with existing function-call streaming.

**Potential risks**  
The complexity of streaming state management increases, which could lead to edge cases in event sequencing or state corruption. The mapping of tool names to MCP server labels (`_TOOL_NAME_TO_MCP_SERVER_LABEL`) is hardcoded and may not cover all possible tools. There is a risk of mixing event types if the logic for distinguishing between function calls and MCP tools fails (e.g., due to unexpected recipient formats).

**Key insights**  
Developers should ensure that MCP tool streaming is only enabled when the corresponding environment flags are set. The hardcoded tool-to-server mapping should be reviewed and potentially made configurable. The test suite now includes robust validation for both MCP and function-call streaming, which should be maintained as the feature evolves. Pay close attention to the `_is_mcp_tool_by_namespace` method, as it is critical for correct event routing.

---

## 10. [[Bugfix] Fix Typo from NVFP4 Refactor](https://github.com/vllm-project/vllm/pull/31977)


### Base Information

- **PR Number:** #31977
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-08 16:18:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31977/files) (6):**
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutedsl-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutedsl-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt`
  - `tests/evals/gsm8k/configs/moe-refactor/config-test.txt`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
This PR fixes a typo in the `NvFp4MoeBackend` enum list where `FLASHINFER_TRTLLM` was duplicated instead of including `FLASHINFER_CUTEDSL`. It also adds new test configurations for the Qwen3-30B-A3B-NVFP4 model with the CUTEDSL backend and updates the test configuration list to include these new tests.

**Technical impact**  
The typo fix ensures the CUTEDSL backend is properly recognized and available for use with NVFP4 MoE layers. The new test configurations expand test coverage for different backend combinations (CUTEDSL with DeepEP low-latency, CUTLASS) and model variants (CT and ModelOpt), improving validation of the NVFP4 refactor changes.

**Potential risks**  
The changes are minimal and focused on configuration and a simple enum correction, so the risk is low. However, the new test configurations rely on specific environment variables (`VLLM_USE_FLASHINFER_MOE_FP4`, `VLLM_FLASHINFER_MOE_BACKEND`) which must be correctly set in the CI environment. There is also a risk that the test list update (`config-b200.txt`) could inadvertently exclude other important configurations if not carefully managed.

**Key insights**  
The typo was a simple but critical bug that would have prevented the CUTEDSL backend from being used. The addition of targeted test configurations demonstrates good practice for validating specific backend integrations. Developers should ensure all new test configurations are properly integrated into CI pipelines and that environment variables are consistently applied across test runs.

---

## 11. [[Feature] Add iteration level logging and enhance nvtx marker](https://github.com/vllm-project/vllm/pull/31193)


### Base Information

- **PR Number:** #31193
- **Author:** [maxyanghu](https://github.com/maxyanghu)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-08 16:13:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31193/files) (6):**
  - `vllm/config/observability.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/v1/core/sched/output.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/utils.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR adds detailed iteration-level logging to track scheduling performance metrics. It introduces a new configuration flag `--enable-logging-iteration-details` that logs per-iteration counts of context/generation requests and tokens, along with iteration elapsed time. Additionally, it enhances NVTX profiling markers to include these metrics for better performance analysis.

**Technical impact**  
The changes add lightweight monitoring capabilities to the scheduling loop without affecting core execution paths when disabled. A new `IterationDetails` dataclass and `compute_iteration_details()` function centralize metric calculation, while `log_iteration_details()` context manager handles logging. The iteration index is maintained per-EngineCore instance, making it data-parallel aware.

**Potential risks**  
The logging overhead could impact performance when enabled, especially in high-throughput scenarios. The `_req_id_to_num_output_tokens` cached property assumes `CachedRequestData` instances are immutable during iteration—any mutation could cause incorrect classification. There's also a risk of log spam if enabled by default in production.

**Key insights**  
The implementation cleanly separates concerns: metric computation is reusable across logging and profiling. Developers should ensure the feature remains opt-in for production deployments. The cached property optimization is appropriate but requires careful review if `CachedRequestData` usage changes. Consider adding unit tests for `compute_iteration_details()` to validate edge cases like empty schedules or mixed request states.

---

## 12. [[BugFix] Add spec-decode-incompatible request param validation](https://github.com/vllm-project/vllm/pull/31982)


### Base Information

- **PR Number:** #31982
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-08 16:08:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31982/files) (2):**
  - `tests/v1/distributed/test_eagle_dp.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The changes add request-scope validation to fail when unsupported sampling parameters (`min_tokens`, `min_p`, `logit_bias`) are used with speculative decoding, rather than silently ignoring them. A test was updated to remove `min_tokens` from a sampling configuration to align with this new validation.

**Technical impact**  
This improves system reliability by explicitly rejecting incompatible parameter combinations at runtime, ensuring consistent behavior. The validation is integrated into the existing `_validate_supported_sampling_params` method, maintaining the existing error handling pattern for unsupported features.

**Potential risks**  
If speculative decoding is enabled, existing requests using `min_tokens > 1`, `min_p`, or `logit_bias` will now fail with a `ValueError`, which could break client applications. The test change indicates one such case was already present and needed adjustment.

**Key insights**  
Developers must audit their sampling parameter usage when speculative decoding is active and remove or adjust unsupported parameters. The validation provides clear error messages, aiding in debugging. Consider documenting these limitations in user-facing APIs or configuration guides.

---

## 13. [[Quantization] Deprecate Long Tail of Schemes](https://github.com/vllm-project/vllm/pull/31688)


### Base Information

- **PR Number:** #31688
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-08 16:07:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31688/files) (8):**
  - `tests/compile/fullgraph/test_full_graph.py`
  - `tests/models/quantization/test_gptq_marlin_24.py`
  - `tests/quantization/test_auto_round.py`
  - `tests/quantization/test_experts_int8.py`
  - `tests/quantization/test_rtn.py`
  - `vllm/config/model.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/layers/quantization/__init__.py`

### Summary

**What changed and why**  
This PR initiates deprecation of multiple quantization schemes by adding a deprecation mechanism. It introduces an `allow_deprecated_quantization` flag that enables temporary usage of deprecated quantization methods with warnings, while preventing their use by default. The changes update configuration handling, CLI arguments, and test files to accommodate this deprecation process.

**Technical impact**  
The codebase now includes a centralized list of deprecated quantization methods (`DEPRECATED_QUANTIZATION_METHODS`) that triggers warnings when used with the allow flag, and raises errors when used without it. This affects model initialization, configuration validation, and test execution, requiring explicit opt-in for continued use of deprecated quantization schemes during the transition period.

**Potential risks**  
Tests may fail if they rely on deprecated quantization methods without the new flag. Users with existing configurations using deprecated schemes will encounter errors unless they explicitly enable the allow flag. There's also risk of confusion during the transition period if users are unaware of the deprecation or how to enable the flag.

**Key insights**  
Developers should update any tests or configurations using the listed quantization methods to include `allow_deprecated_quantization=True`. The deprecation follows a two-phase approach: warning with flag in current release, followed by complete removal. Consider documenting this change in release notes and migration guides to help users transition smoothly.

---

## 14. [[MoE Refactoring][Bugfix]Wrap WNA16 Triton kernel into mk and change compressed tensor kernel selection](https://github.com/vllm-project/vllm/pull/31752)


### Base Information

- **PR Number:** #31752
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-08 16:01:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31752/files) (3):**
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`

### Summary

**What changed and why**  
This PR refactors the WNA16 Triton kernel into a modular kernel format by creating a new `TritonWNA16Experts` class that inherits from `TritonExperts`. The change fixes a kernel selection issue where LoRA modules couldn't correctly select the WNA16 kernel after it was factored out from `TritonExperts`. The fix ensures the compressed tensor quantization path uses the new specialized class.

**Technical impact**  
The changes introduce a new expert class specifically for WNA16 quantization while maintaining the existing `TritonExperts` interface. This creates a cleaner separation between different quantization schemes and ensures proper kernel selection. The `invoke_fused_moe_wna16_triton_kernel` function now properly validates `block_shape` parameters, and the compressed tensors module correctly imports and uses the new class.

**Potential risks**  
The assertion change for `block_shape` (from `block_shape is None or block_shape[0] == 0` to `block_shape is not None and block_shape[0] == 0`) could break existing code that passes `None`. The new class inherits all parent functionality, so any bugs in `TritonExperts` would also affect `TritonWNA16Experts`. The specialized class adds complexity to the kernel selection logic.

**Key insights**  
The refactoring properly separates concerns between different quantization implementations. Developers should ensure `block_shape` is never `None` when using WNA16 kernels. The change demonstrates a pattern for extending modular kernels while maintaining backward compatibility. Future quantization implementations should follow this pattern of creating specialized expert classes.

---

## 15. [[Misc] Fix `Current vLLM config is not set.` warnings, assert to avoid issues in the future](https://github.com/vllm-project/vllm/pull/31747)


### Base Information

- **PR Number:** #31747
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-08 15:20:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31747/files) (48):**
  - `tests/compile/distributed/test_async_tp.py`
  - `tests/compile/test_config.py`
  - `tests/conftest.py`
  - `tests/kernels/attention/test_flashinfer_trtllm_attention.py`
  - `tests/kernels/attention/test_mha_attn.py`
  - `tests/kernels/core/test_activation.py`
  - `tests/kernels/core/test_fused_qk_norm_rope.py`
  - `tests/kernels/core/test_fused_quant_layernorm.py`
  - `tests/kernels/core/test_layernorm.py`
  - `tests/kernels/core/test_mrope.py`
  - `tests/kernels/core/test_pos_encoding.py`
  - `tests/kernels/core/test_rotary_embedding.py`
  - `tests/kernels/moe/test_cpu_fused_moe.py`
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/quantization/test_fp8_quant_group.py`
  - `tests/kernels/quantization/test_int8_kernel.py`
  - `tests/kernels/quantization/test_silu_mul_nvfp4_quant.py`
  - `tests/kernels/test_fused_quant_activation.py`
  - `tests/lora/conftest.py`
  - `tests/lora/test_layers.py`
  - `tests/lora/test_lora_manager.py`
  - `tests/model_executor/test_eagle_quantization.py`
  - `tests/models/multimodal/pooling/test_intern_vit.py`
  - `tests/models/multimodal/pooling/test_radio.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/plugins_tests/test_platform_plugins.py`
  - `tests/quantization/test_fp8.py`
  - `tests/utils.py`
  - `tests/v1/attention/test_attention_backends.py`
  - `tests/v1/attention/test_attention_backends_selection.py`
  - `tests/v1/attention/test_mla_backends.py`
  - `tests/v1/attention/test_sparse_mla_backends.py`
  - `tests/v1/determinism/test_rms_norm_batch_invariant.py`
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `tests/v1/kv_offload/test_cpu_gpu.py`
  - `tests/v1/worker/test_gpu_model_runner.py`
  - `tests/v1/worker/test_utils.py`
  - `vllm/attention/utils/fa_utils.py`
  - `vllm/config/__init__.py`
  - `vllm/config/vllm.py`
  - `vllm/distributed/device_communicators/base_device_communicator.py`
  - `vllm/distributed/device_communicators/quick_all_reduce.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/siglip2navit.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR addresses spurious warnings about "Current vLLM config is not set" by fixing improper accesses to `get_current_vllm_config()` outside of a `set_current_vllm_config()` context. The changes replace fallback behavior with an assertion to prevent silent use of default configurations, which previously caused config propagation bugs in custom ops. A new `get_current_vllm_config_or_none()` function is introduced for safe optional access, and a `default_vllm_config` pytest fixture is added to ensure tests run with a valid config.

**Technical impact**  
The core change converts `get_current_vllm_config()` from a warning+fallback to an assertion, enforcing explicit config management. This improves correctness by ensuring custom ops and distributed initialization always use the intended configuration. The addition of `get_current_vllm_config_or_none()` allows conditional access where a config might not be set, preserving backward compatibility in non-critical paths. Test fixtures and context managers are updated to wrap code that requires a config, preventing assertion failures in unit tests.

**Potential risks**  
- The assertion may break existing code that implicitly relied on the default config fallback, especially in custom ops instantiated at module import time (e.g., `ApplyRotaryEmb`).  
- The lazy initialization pattern for activation functions in `cpu_fused_moe.py` adds complexity and could obscure errors if the activation key is missing.  
- Changes to distributed initialization (`init_test_distributed_environment`) assume the config may already be set, which could lead to subtle ordering issues in multi-process tests.

**Key insights**  
- Developers must ensure `get_current_vllm_config()` is only called within a `set_current_vllm_config()` context; use `get_current_vllm_config_or_none()` for optional checks.  
- Custom ops that instantiate at import time (like `ApplyRotaryEmb`) should be moved to lazy initialization or explicitly created within a config context.  
- All new tests involving custom ops or distributed components should use the `default_vllm_config` fixture to avoid assertion failures.

---

## 16. [[Compressed-Tensors] Simplify NVFP4 Conditions, enable marlin support for NVFP4A16 MoEs](https://github.com/vllm-project/vllm/pull/30881)


### Base Information

- **PR Number:** #30881
- **Author:** [dsikka](https://github.com/dsikka)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-08 14:45:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30881/files) (2):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`

### Summary

**What changed and why**  
This PR simplifies the NVFP4 quantization scheme detection logic by consolidating two separate condition-checking methods (`_is_fp4a4_nvfp4` and `_is_fp4a16_nvfp4`) into a single, more generic `_is_nvfp4_format` method. It also enables NVFP4A16 (weight-only quantization) MoE (Mixture of Experts) layers to utilize the Marlin kernel pathway, expanding support for these models.

**Technical impact**  
The refactoring reduces code duplication and centralizes NVFP4 format validation. For MoE layers, the changes introduce a `use_marlin` flag that allows NVFP4A16 configurations (where input quantization is `None`) to leverage the Marlin kernel when supported, while NVFP4A4 configurations continue using the existing NVFP4 backend. This improves the flexibility and performance portability of NVFP4-quantized MoE models across different hardware platforms.

**Potential risks**  
The consolidated `_is_nvfp4_format` method now validates a single `QuantizationArgs` object, which could be called incorrectly if the caller passes the wrong argument (weight vs. input). The new validation in `get_moe_method` raises a `ValueError` for invalid activation quantization with NVFP4 weights, which is stricter and may break existing configurations that previously passed silently. There is also a risk if `is_fp4_marlin_supported()` has platform-specific bugs or inconsistencies.

**Key insights**  
The simplification makes the NVFP4 condition checks more maintainable. Developers should note that NVFP4A16 MoE now requires explicit Marlin kernel support checks. When modifying quantization schemes, ensure both weight and input quantization arguments are validated appropriately using the new single-parameter method. The changes are well-tested across SM90 and SM100 architectures, as shown by the coherent model outputs.

---

## 17. [[Misc][Refactor] Add FusedMoERouter object](https://github.com/vllm-project/vllm/pull/30519)


### Base Information

- **PR Number:** #30519
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-08 12:52:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30519/files) (20):**
  - `tests/test_routing_simulator.py`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_method_base.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_router.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/bitsandbytes.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/experts_int8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gguf.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/ipex_quant.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/moe_wna16.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`
  - `vllm/model_executor/layers/quantization/rtn.py`

### Summary

**What changed and why**  
This PR introduces an abstract `FusedMoERouter` class to encapsulate expert selection logic, replacing direct calls to `FusedMoE._select_experts`. A concrete implementation `FusedMoERouterImpl` wraps the existing selection method. This refactor is a preparatory step to decouple routing logic from the main `FusedMoE.apply` method, addressing architectural separation concerns.

**Technical impact**  
The changes create a clear abstraction layer for routing operations. All quantization method implementations now receive a `router` parameter and call `router.select_experts()` instead of directly accessing the layer's method. This improves modularity and prepares the codebase for future routing strategy variations without modifying quantization logic.

**Potential risks**  
The refactor touches numerous quantization files (20 modified), increasing the risk of missing updates in edge cases or less common code paths. The type annotation change for `routing_method_type` from `int \| None` to `RoutingMethodType \| None` could cause issues if callers pass raw integer values. Test coverage appears limited to existing CI.

**Key insights**  
This is a well-structured abstraction that follows good software design principles. Developers should verify all routing-related functionality works correctly across different quantization backends. Future implementations should extend `FusedMoERouter` rather than modifying the core `FusedMoE` class. The pattern established here will facilitate testing and development of alternative routing algorithms.

---

## 18. [[Frontend] Improve error message](https://github.com/vllm-project/vllm/pull/31987)


### Base Information

- **PR Number:** #31987
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [russellb](https://github.com/russellb)
- **Merged time:** 2026-01-08 12:07:04
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31987/files) (3):**
  - `tests/entrypoints/test_utils.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/utils.py`

### Summary

**What changed and why**  
The changes introduce a `sanitize_message` utility function that removes memory address information (hexadecimal values after "at 0x") from error message strings. This function is applied to error messages returned by the HTTP exception handler and validation exception handler in the OpenAI API server to prevent leaking internal memory addresses to clients.

**Technical impact**  
This improves security and privacy by ensuring that Python object representations containing memory addresses (like `<_io.BytesIO object at 0x7a95e299e750>`) are sanitized before being sent to API clients. The sanitization occurs at the presentation layer, leaving the original error details intact for server-side logging while providing cleaner, safer messages to end users.

**Potential risks**  
The regex pattern `r" at 0x[0-9a-f]+>"` may not catch all variations of object representations, potentially missing some memory address leaks. Additionally, if error messages contain legitimate content matching this pattern (unlikely but possible), they could be incorrectly modified. The sanitization only applies to two specific exception handlers, leaving other error paths unaffected.

**Key insights**  
The implementation correctly separates sanitization logic into a reusable utility function with accompanying tests. Developers should ensure this sanitization is applied consistently across all error-returning endpoints. Consider expanding test coverage to include edge cases and verifying that the sanitization doesn't interfere with legitimate error messaging in production scenarios.

---

## 19. [[Documentation][torch.compile] Add documentation for torch.compile + multimodal encoders](https://github.com/vllm-project/vllm/pull/31627)


### Base Information

- **PR Number:** #31627
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-08 11:33:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31627/files) (1):**
  - `docs/design/torch_compile_multimodal.md`

### Summary

**What changed and why**  
This PR adds comprehensive documentation (`torch_compile_multimodal.md`) explaining how to apply `torch.compile` to multimodal encoders (e.g., vision-language models) in vLLM. The documentation covers enablement steps, configuration, common pitfalls, and troubleshooting, aiming to guide developers in optimizing performance for encoder-based architectures.

**Technical impact**  
The documentation formalizes the process for extending `torch.compile` support beyond text backbones to multimodal components. It introduces a new configuration flag (`compile_mm_encoder`) and details scaffolding requirements (e.g., `set_model_tag` with `is_encoder=True`), which ensures proper caching and dynamic shape handling for encoder modules.

**Potential risks**  
- The default compilation range for encoders (`(1, MAX_INT)`) may lead to suboptimal performance or increased memory usage if not tightened later.  
- CUDAGraphs are unsupported for multimodal encoders, which could limit performance gains in GPU-intensive scenarios.  
- Dynamic image sizes and untraceable operations (e.g., `to_list`) may cause graph breaks, requiring careful debugging.

**Key insights**  
- Developers should apply `@supports_torch_compile` incrementally and use tools like `tlparse` to identify recompiles.  
- Enable `TORCH_LOGS="+dynamo"` for debugging graph breaks in vision encoders.  
- Always verify model functionality without compilation first (`VLLM_TORCH_COMPILE_LEVEL=0`) before enabling multimodal encoder compilation.

---

## 20. [Revert "feat(moe): Add is_act_and_mul=False support for Triton MoE kernels"](https://github.com/vllm-project/vllm/pull/31978)


### Base Information

- **PR Number:** #31978
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 11:31:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31978/files) (7):**
  - `tests/kernels/moe/test_triton_moe_no_act_mul.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`

### Summary

**What changed and why**  
This PR reverts support for `is_act_and_mul=False` in Triton MoE kernels, removing the feature that enabled non-fused activations (e.g., `relu2_no_mul`, `silu_no_mul`) used by models like Nemotron-H. The revert is due to concerns raised in the linked GitHub comment, likely related to implementation correctness, performance, or maintenance burden.

**Technical impact**  
The codebase now exclusively supports SwiGLU-style fused activations (`silu`, `gelu`, `swigluoai`). All logic for handling non-fused activations—including configuration flags, workspace size calculations, and activation kernels—has been removed. This simplifies the MoE implementation but eliminates support for models requiring non-fused activation paths.

**Potential risks**  
Models that depend on non-fused activations will no longer function correctly, potentially breaking compatibility with specific architectures. The removal of ROCm-specific handling for `is_act_and_mul=False` may also affect ROCm users who relied on this path. Additionally, the revert could introduce regressions if any dependent code or tests outside the changed files assumed the feature existed.

**Key insights**  
Developers should ensure no models in use require non-fused activations. Future work to re-add this feature must address the original concerns, likely requiring a more robust design. The removal of the dedicated test file (`test_triton_moe_no_act_mul.py`) means any future implementation will need comprehensive validation. Consider documenting this change in release notes to alert users of the reduced model support.

---

## 21. [[Model Runner V2] Simplify BlockTables with UVA](https://github.com/vllm-project/vllm/pull/31965)


### Base Information

- **PR Number:** #31965
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-08 10:24:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31965/files) (2):**
  - `vllm/v1/worker/gpu/block_table.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The PR simplifies block table management by replacing GPU-based diff updates with Unified Virtual Addressing (UVA) buffers. Previously, block tables were stored on GPU, requiring complex packing and async copies of diffs each step. Now, `UvaBuffer` stores block tables in CPU-pinned memory with GPU views, enabling direct CPU updates that are visible to GPU via UVA, eliminating the need for manual diff transfers.

**Technical impact**  
This change reduces code complexity by removing custom GPU kernel logic (`_append_block_ids_kernel`) and associated buffers (`CpuGpuBuffer`, `req_indices`, `cu_num_new_blocks`). The `append_block_ids` method now updates block tables via NumPy array operations on CPU, which are automatically visible to GPU through UVA. However, it introduces a CPU-to-GPU data transfer every step, potentially impacting performance for high-frequency updates.

**Potential risks**  
UVA availability becomes a hard dependency; the system will fail if UVA is unsupported. The CPU-based updates in `append_block_ids` may become a bottleneck due to repeated NumPy operations and Python-level loops. There is also a risk of increased latency from continuous CPU-GPU memory transfers, especially with large block tables or high request volumes.

**Key insights**  
The simplification significantly improves maintainability by removing intricate GPU copy logic. Developers should monitor performance to ensure CPU updates and UVA transfers do not degrade throughput. Consider optimizing the NumPy operations (noted as "Too many Numpy invocations" in the code) and evaluate UVA support across deployment environments. This change aligns with trends toward simpler memory management but trades off potential GPU efficiency gains.

---

## 22. [[CI][ROCm] Fix NIXL tests on ROCm](https://github.com/vllm-project/vllm/pull/31728)


### Base Information

- **PR Number:** #31728
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 09:34:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31728/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/distributed.yaml`

### Summary

**What changed and why**  
This PR updates CI configuration files to fix NIXL tests on ROCm by correcting test script names and adjusting timeouts. The changes address a previous fix (#31491) for AMD mirror pipelines and update timeouts based on latest nightly run results.

**Technical impact**  
The changes ensure ROCm-specific tests use the correct `config_sweep_accuracy_test.sh` script instead of the non-existent `tp_config_sweep_accuracy_test.sh`, enabling proper test execution. Timeout adjustments (30→40 minutes for distributed tests, 30→15 minutes for DP EP tests) reflect actual runtime requirements and improve CI efficiency.

**Potential risks**  
If the timeout reductions are too aggressive, tests may fail due to timeout rather than actual issues. The script name changes could mask underlying problems if the wrong script was being called intentionally for specific test scenarios. There's also a risk of inconsistency between ROCm and non-ROCm test configurations.

**Key insights**  
Always verify script names exist before merging changes. Timeout adjustments should be based on empirical data from multiple runs. Consider maintaining consistency between ROCm and non-ROCm test configurations where possible. The DP EP test timeout reduction from 30 to 15 minutes suggests significant performance improvements or test optimization.

---

## 23. [Fix ijson build for Power.](https://github.com/vllm-project/vllm/pull/31702)


### Base Information

- **PR Number:** #31702
- **Author:** [npanpaliya](https://github.com/npanpaliya)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-08 09:12:33
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31702/files) (1):**
  - `docker/Dockerfile.ppc64le`

### Summary

**What changed and why**  
Updated package versions in the PowerPC (ppc64le) Dockerfile from `9.0-24.el9` to `9.0-26.el9` for CentOS repository packages, and added `yajl-devel` as a new dependency. These changes ensure the Docker build uses updated repository metadata and includes necessary libraries for ijson compatibility on Power architecture.

**Technical impact**  
The changes maintain compatibility with the latest CentOS Stream 9 repositories and add a required JSON parsing library (`yajl-devel`) for ijson, which is likely needed for Power-specific builds. This ensures the Docker image can be built successfully on ppc64le systems without missing dependencies.

**Potential risks**  
If the updated package versions (`9.0-26.el9`) are not available in all mirrors or regions, the build could fail due to missing RPMs. Additionally, the removal step still references the old version (`centos-gpg-keys-9.0-24.el9.noarch`), which may cause a mismatch if the package name changed.

**Key insights**  
Always verify that RPM package versions are consistent across install and remove commands. Consider using version-agnostic package names or wildcards for removal to avoid conflicts. Ensure the added `yajl-devel` dependency is documented as a Power-specific requirement.

---

## 24. [[Misc] Tidy up some spec decode logic in GPUModelRunner](https://github.com/vllm-project/vllm/pull/31591)


### Base Information

- **PR Number:** #31591
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-08 09:10:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31591/files) (3):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The changes simplify speculative decoding logic in `GPUModelRunner.sample_tokens` by precomputing `effective_drafter_max_model_len` during initialization and only executing spec-decoding-specific logic when speculative decoding is actually enabled. A new `update_max_model_len` method was added to both model runner classes to properly propagate max model length updates.

**Technical impact**  
This refactoring reduces runtime overhead by moving the computation of `effective_drafter_max_model_len` from the hot path (executed every sampling step) to initialization. The logic flow is now cleaner with better separation between EAGLE and other speculative decoding methods, and the code avoids unnecessary condition checks when speculative decoding is disabled.

**Potential risks**  
The initialization-time computation of `effective_drafter_max_model_len` assumes that `max_model_len` won't change after initialization, though the new `update_max_model_len` method handles updates. There's a risk that the `propose_drafts_after_bookkeeping` flag logic could be misinterpreted since it's only set for non-EAGLE speculative decoding methods when `input_fits_in_drafter` is true.

**Key insights**  
The refactoring improves performance by reducing per-step computations and clarifies the speculative decoding flow. Developers should ensure that any future changes to max model length handling properly call the new `update_max_model_len` method. The separation of EAGLE-specific logic from other speculative decoding methods makes the code more maintainable.

---

## 25. [[Doc] Improve MM models LoRA notes](https://github.com/vllm-project/vllm/pull/31979)


### Base Information

- **PR Number:** #31979
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 08:55:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31979/files) (1):**
  - `docs/models/supported_models.md`

### Summary

**What changed and why**  
The documentation for multimodal models' LoRA support was simplified by removing detailed merging instructions and replacing them with a concise note. The note now directs users to the LoRA feature page for current information on supported modules.

**Technical impact**  
This change reduces documentation maintenance burden by centralizing LoRA support details in a dedicated feature page. It ensures users get up-to-date information without needing to update multiple documentation sections when LoRA capabilities evolve.

**Potential risks**  
Users accustomed to the previous detailed merging instructions might initially miss the procedural guidance, though the link provides comprehensive information. The term "experimentally supports" for tower/connector modules should be clearly documented on the linked page to set proper expectations.

**Key insights**  
Documentation simplification improves maintainability and consistency. Developers should ensure the linked LoRA feature page remains current with accurate support matrices and clear experimental status indicators for newly supported modules.

---

## 26. [[Bugfix] Fix vllm serve failure with Nemotron Nano V3 FP8](https://github.com/vllm-project/vllm/pull/31960)


### Base Information

- **PR Number:** #31960
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-08 08:08:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31960/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`

### Summary

**What changed and why**  
The fix addresses a runtime error in FP8 MoE serving with Nemotron Nano V3 by correcting how weight scales are handled for the `w1` case in tensor-wise quantization. Previously, `weight_scales.max()` returned a scalar, causing a dimension mismatch in downstream FlashInfer operations. The updated code now returns `max_scales` (shape `(num_experts,)`) to preserve per-expert scaling.

**Technical impact**  
This change ensures that FP8 MoE weights maintain the correct tensor dimensionality when using the FlashInfer CUTLASS backend. It aligns the scale tensor shape with the expected 1D structure, preventing the `fc1_dequant must be a 1D tensor` error and enabling successful model serving with FP8 kv-cache and tensor parallelism.

**Potential risks**  
If the assertion `max_scales.shape == (num_experts,)` fails due to unexpected weight scale dimensions, it could introduce new runtime errors. Additionally, the note about Triton backend failures (`VLLM_USE_FLASHINFER_MOE_FP8=0`) indicates unresolved issues in alternative backends that may affect users not using FlashInfer.

**Key insights**  
Always validate tensor shapes when collapsing or aggregating dimensions in quantization logic. Developers should ensure that scale tensors retain the expected batch/expert dimensions to avoid downstream integration errors. The separate Triton backend issue should be prioritized for a follow-up fix to maintain full FP8 MoE support across backends.

---

## 27. [[Bugfix]: Fix Step3ReasoningParser missing is_reasoning_end_streaming](https://github.com/vllm-project/vllm/pull/31969)


### Base Information

- **PR Number:** #31969
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 07:28:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31969/files) (1):**
  - `vllm/reasoning/step3_reasoning_parser.py`

### Summary

**What changed and why**  
Added the `is_reasoning_end_streaming` method to the `Step3ReasoningParser` class. This method checks if the reasoning end token appears in newly generated token IDs (`delta_ids`), which is necessary for proper streaming detection during incremental token generation.

**Technical impact**  
This change enables the parser to correctly identify when reasoning has completed during streaming generation. Without this method, the system would rely on the non-streaming `is_reasoning_end` method that checks the entire input sequence, potentially missing real-time completion detection in streaming scenarios.

**Potential risks**  
The implementation assumes `delta_ids` contains only newly generated tokens, which could fail if `delta_ids` includes previously processed tokens. There's also no handling for cases where the end token might be split across multiple delta chunks, though this is unlikely with standard tokenization.

**Key insights**  
This is a critical fix for streaming functionality with reasoning models. Developers should ensure all reasoning parsers implement both streaming and non-streaming end detection methods consistently. Consider adding unit tests specifically for streaming scenarios to validate the delta-based detection works correctly across different token chunking scenarios.

---

## 28. [[Model] Support IQuestCoder model](https://github.com/vllm-project/vllm/pull/31575)


### Base Information

- **PR Number:** #31575
- **Author:** [yxing-bj](https://github.com/yxing-bj)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 06:42:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31575/files) (4):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/iquest_loopcoder.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds support for IQuestCoder models to vLLM, including two new model families: IQuestCoder and IQuestLoopCoder. The changes introduce a new model implementation file (`iquest_loopcoder.py`) for the loop variant, update the model registry and documentation, and add test entries. The purpose is to enable users to serve these code-generation models via vLLM with features like tensor parallelism and reasoning support.

**Technical impact**  
The addition integrates a new architecture (IQuestLoopCoder) with a custom attention mechanism that supports multiple "loops" (global and local attention) and a gating mechanism. This impacts the model loading pipeline and extends vLLM's supported model families. The standard IQuestCoder reuses the existing Llama implementation, minimizing new code. The changes are backward-compatible and follow vLLM's existing patterns for model integration.

**Potential risks**  
The custom loop attention logic introduces complexity, especially in handling multiple cache configurations and the gating projection. There is a risk of incorrect tensor shapes or indexing in the looped attention layers. The model relies on `trust_remote_code` due to custom Hugging Face implementations, which may pose security or compatibility risks. The test coverage appears limited to basic serving; edge cases like long sequences or mixed loop indices need validation.

**Key insights**  
Developers should note that IQuestLoopCoder requires careful handling of the loop index and gate projection during inference. The model uses a sliding-window cache for loop indices >0, which may affect memory usage. Ensure the Hugging Face config includes `loop_num` and `loop_window_size` attributes. For production use, thorough testing of the reasoning parser integration and tensor parallelism is recommended.

---

## 29. [[Docs]: update claude code url](https://github.com/vllm-project/vllm/pull/31971)


### Base Information

- **PR Number:** #31971
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 06:04:56
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31971/files) (1):**
  - `docs/serving/integrations/claude_code.md`

### Summary

**What changed and why**  
Updated the Claude Code documentation URL from `https://docs.anthropic.com/en/docs/claude-code/overview` to `https://code.claude.com/docs/en/quickstart`. This change follows up on PR #31188 to ensure the documentation points to the correct, current resource.

**Technical impact**  
This is a documentation-only change with no impact on code functionality, system behavior, or architecture. It ensures users are directed to the proper Claude Code quickstart guide.

**Potential risks**  
Minimal risk—the main concern is if the new URL becomes outdated or inaccessible in the future. The change should be validated to confirm the new link is stable and officially maintained.

**Key insights**  
Always verify documentation links point to authoritative, up-to-date sources. Consider adding periodic link validation in CI/CD to catch broken or redirected URLs automatically.

---

## 30. [[CI] [Bugfix] Fix unbounded variable in `run-multi-node-test.sh`](https://github.com/vllm-project/vllm/pull/31967)


### Base Information

- **PR Number:** #31967
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 05:42:01
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31967/files) (2):**
  - `.buildkite/scripts/run-multi-node-test.sh`
  - `.buildkite/test-pipeline.yaml`

### Summary

**What changed and why**  
The PR fixes an "unbound variable" error in `run-multi-node-test.sh` by changing `$ROCM_HOME` to `${ROCM_HOME:-}` in a conditional check, ensuring safe evaluation when the variable is undefined. It also adds the script to `test-pipeline.yml` to trigger it in CI, addressing a regression from a previous PR.

**Technical impact**  
This change prevents the script from failing due to undefined environment variables in CI environments where ROCM may not be present. Adding the script to the pipeline ensures multi-node tests are executed as part of the CI workflow, improving test coverage for distributed scenarios.

**Potential risks**  
If `ROCM_HOME` is intentionally unset in some environments, the fix handles it gracefully, but there is a low risk that other unbound variables in the script could cause similar issues. The CI addition may slightly increase pipeline runtime, but this is justified for test coverage.

**Key insights**  
Always use parameter expansion like `${VAR:-}` when checking variables that may be undefined in bash scripts. This is a best practice for robustness. Ensure CI pipeline changes align with the scripts they trigger to avoid regressions in test execution.

---

## 31. [[OpenAI] Fix tool_choice=required streaming when output has trailing extra data](https://github.com/vllm-project/vllm/pull/31610)


### Base Information

- **PR Number:** #31610
- **Author:** [maylikenoother](https://github.com/maylikenoother)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-08 05:01:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31610/files) (2):**
  - `tests/tool_use/test_tool_choice_required.py`
  - `vllm/entrypoints/openai/serving_chat.py`

### Summary

**What changed and why**  
The fix addresses a JSON parsing issue in OpenAI streaming when `tool_choice="required"`. Previously, the parser would fail if the model output contained valid JSON followed by trailing extra data (e.g., `"\nDONE"`). The solution uses `partial_json_loads(..., Allow.ALL)` to parse only the first JSON value while ignoring trailing content, maintaining correct behavior for incomplete JSON.

**Technical impact**  
This change modifies the JSON parsing strategy in `extract_tool_call_required_streaming` to be more tolerant of trailing data after a valid JSON array. It ensures streaming responses with tool calls are correctly processed even when the model emits additional non-JSON tokens, improving robustness without affecting existing incomplete JSON handling.

**Potential risks**  
Using `Allow.ALL` might inadvertently accept malformed JSON if the trailing data is incorrectly structured, though the try-catch block mitigates this. The added test covers the specific trailing data case but may not cover all possible edge cases of mixed JSON and non-JSON content in streaming outputs.

**Key insights**  
Developers should note that the parser now prioritizes extracting the first valid JSON object, which aligns with the expected tool-call array. Ensure that any future changes to streaming output formatting consider this tolerant parsing approach. The regression test provides a good baseline for validating similar scenarios.

---

## 32. [[Model] Enable LoRA support for Pixtral](https://github.com/vllm-project/vllm/pull/31724)


### Base Information

- **PR Number:** #31724
- **Author:** [A1c0r-Z](https://github.com/A1c0r-Z)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 05:00:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31724/files) (2):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/pixtral.py`

### Summary

**What changed and why**  
This PR enables LoRA (Low-Rank Adaptation) support for Pixtral multimodal models by implementing the required LoRA interface methods. The changes allow LoRA adapters to be applied to the vision tower and connector components, addressing feature request #31479. The implementation specifically handles Pixtral's patch merging logic where image patches are spatially merged before processing.

**Technical impact**  
The Pixtral model now inherits from `SupportsLoRA` and implements three key methods: `get_mm_mapping()` defines the component mapping for LoRA injection, while `get_num_mm_encoder_tokens()` and `get_num_mm_connector_tokens()` handle token count scaling to account for the vision encoder's patch merging (typically 2×2). This enables fine-tuning of vision-language components via LoRA adapters while maintaining compatibility with the existing multimodal architecture.

**Potential risks**  
The patch merging scaling logic assumes integer division when scaling down connector tokens, which could cause issues if `num_vision_tokens` isn't perfectly divisible by `merge_size²`. There's also a risk of incorrect token budget allocation if the `patch_merger` attribute exists but `vision_args.spatial_merge_size` is inconsistent. The changes add minimal complexity but introduce new dependencies on the `MultiModelKeys` class.

**Key insights**  
Developers should verify that `num_vision_tokens` is always divisible by `merge_size²` when patch merging is active. The implementation correctly separates concerns between raw vision tokens and connector-processed tokens. The documentation update accurately reflects the new LoRA support status, making Pixtral consistent with other multimodal models in the codebase.

---

## 33. [[Model] Add LFM2-VL model support](https://github.com/vllm-project/vllm/pull/31758)


### Base Information

- **PR Number:** #31758
- **Author:** [tianshu-Michael-yu](https://github.com/tianshu-Michael-yu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 05:00:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31758/files) (6):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/siglip2.py`

### Summary

**What changed and why**  
This PR adds support for the LFM2-VL (Liquid Foundation Model 2 Vision-Language) model family from LiquidAI. The changes include a new model implementation (`Lfm2VLForConditionalGeneration`), a SigLIP2 vision encoder, registration in the multimodal model registry, and fixes for type hints and race conditions in GPU model runner.

**Technical impact**  
The integration expands vLLM's multimodal capabilities with a hybrid Mamba+Attention architecture, dynamic image processing, and support for pipeline parallelism. The model leverages existing multimodal infrastructure (registry, processing pipeline) and introduces a new vision encoder (`Siglip2Model`) that uses tensor-parallelism-aware attention layers.

**Potential risks**  
The dynamic image splitting logic and smart resize functions may have edge cases with unusual aspect ratios or extreme resolutions. The double-buffering fix for `is_mm_embed` could introduce subtle synchronization issues if not thoroughly tested across different hardware configurations. The model's reliance on `trust-remote-code` and specific compilation flags may affect deployment flexibility.

**Key insights**  
Developers should verify that the image tiling logic aligns with the original model's behavior, especially for non-standard resolutions. The SigLIP2 encoder implementation should be reviewed for tensor parallelism compatibility in mixed TP/DP scenarios. Ensure the race condition fix for async copies is validated under high-concurrency workloads.

---

## 34. [[Model] Add Grok-2](https://github.com/vllm-project/vllm/pull/31847)


### Base Information

- **PR Number:** #31847
- **Author:** [dangoldbj](https://github.com/dangoldbj)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 04:59:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31847/files) (8):**
  - `docs/models/supported_models.md`
  - `tests/models/language/generation/test_grok.py`
  - `tests/models/registry.py`
  - `tests/tokenizers_/test_basic.py`
  - `vllm/model_executor/models/grok1.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/tokenizers/grok2.py`
  - `vllm/tokenizers/registry.py`

### Summary

**What changed and why**  
This PR adds Grok-2 model support to vLLM by implementing the tokenizer, model architecture, and integration components. It introduces a new `Grok2Tokenizer` that reads `.tok.json` files via tiktoken, extends the existing Grok1 model implementation to handle Grok-2's architectural differences (like residual MoE and config variations), and updates documentation and tests accordingly.

**Technical impact**  
The changes extend the model registry to recognize Grok-2 configurations, modify the Grok1 model file to conditionally support Grok-2 features (e.g., residual MoE, different intermediate sizes, and config-based routing), and add a new tokenizer backend. This maintains backward compatibility with Grok-1 while enabling Grok-2 inference with proper tokenization and model behavior.

**Potential risks**  
The tokenizer requires `tiktoken` as an optional dependency, which could cause import failures if not installed. The model logic now includes version detection heuristics and conditional branches, which may introduce subtle bugs if config attributes are missing or misinterpreted. The residual MoE path adds a parallel MLP block, increasing memory usage and computational overhead for Grok-2.

**Key insights**  
Developers should ensure `tiktoken` is installed when using Grok-2. The model automatically detects Grok-2 via config attributes like `residual_moe` and `moe_intermediate_size`. Note the new `moe_router_renormalize` override option for MoE routing behavior. The tokenizer uses a custom chat template; verify it aligns with expected formatting for Grok-2 chat applications.

---

## 35. [[Voxtral] Fix speech transcription api](https://github.com/vllm-project/vllm/pull/31388)


### Base Information

- **PR Number:** #31388
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 02:34:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31388/files) (5):**
  - `vllm/config/speech_to_text.py`
  - `vllm/entrypoints/openai/speech_to_text.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_streaming.py`
  - `vllm/model_executor/models/whisper.py`

### Summary

**What changed and why**  
The changes enable speech-to-text transcription API functionality for Voxtral models by fixing audio processing logic and configuration handling. Key modifications include making `max_audio_clip_s` optional (allowing unlimited audio duration), adjusting audio chunking behavior, and ensuring proper handling of streaming vs. non-streaming transcription modes.

**Technical impact**  
These updates allow standard vLLM serving with speech transcription by refining audio segmentation, embedding generation, and prompt construction. The changes introduce conditional logic for audio padding based on transcription format, improve truncation for mel-spectrogram features, and ensure causal Whisper encoder compatibility. This enhances flexibility for both chunked and continuous audio inputs.

**Potential risks**  
Setting `max_audio_clip_s` to `None` could lead to memory exhaustion with very long audio files if not managed. The assertion in `_split_audio` assumes chunking is always required when `max_audio_clip_s` is not `None`, which may cause runtime errors if audio splitting is invoked incorrectly. Additionally, the left-truncation logic in `voxtral_streaming.py` might discard audio data if sequences are misaligned.

**Key insights**  
Developers should monitor memory usage with unbounded audio inputs and ensure audio chunking configurations align with use cases. The fixes for causal Whisper encoders and streaming transcription are critical for model compatibility. Reviewers should verify that all audio preprocessing paths handle edge cases, especially when `max_audio_clip_s` is `None`.

---

## 36. [[MM Encoder]: Make MMEncoderAttention's `scale` takes effect properly](https://github.com/vllm-project/vllm/pull/31950)


### Base Information

- **PR Number:** #31950
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 02:33:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31950/files) (13):**
  - `vllm/attention/layers/mm_encoder_attention.py`
  - `vllm/attention/ops/vit_attn_wrappers.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/moonvit.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/siglip2navit.py`

### Summary

**What changed and why**  
This PR fixes a bug where the `scale` parameter in `MMEncoderAttention` wasn't being passed to underlying attention operations (FlashAttention and SDPA). The changes propagate the `scale` parameter through wrapper functions and update model implementations to explicitly pass the scaling factor, even when it matches the default `head_dim**-0.5`.

**Technical impact**  
The fix ensures consistent attention scaling behavior across different multimodal encoder models. By passing the `scale` parameter through the attention wrapper functions (`vit_flash_attn_wrapper`, `torch_sdpa_wrapper`, etc.), the attention calculations now properly apply the specified scaling factor instead of potentially using incorrect defaults.

**Potential risks**  
The changes modify core attention wrapper functions that could affect other models using these components. There's a risk of regression if any model implementations were relying on the previous behavior where `scale` was ignored. The parameter reordering in `flash_attn_maxseqlen_wrapper_fake` and `torch_sdpa_wrapper_fake` could cause issues if these functions are called directly elsewhere.

**Key insights**  
All affected multimodal models now explicitly pass the scaling factor, ensuring consistent behavior. Developers should verify that the `scale` parameter is correctly calculated for each model architecture. The changes highlight the importance of properly propagating configuration parameters through wrapper layers to avoid silent bugs in attention mechanisms.

---

## 37. [[Model] Standardize common vision encoders](https://github.com/vllm-project/vllm/pull/31947)


### Base Information

- **PR Number:** #31947
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 02:33:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31947/files) (19):**
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/deepencoder.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/lightonocr.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/llava_next.py`
  - `vllm/model_executor/models/llava_next_video.py`
  - `vllm/model_executor/models/llava_onevision.py`
  - `vllm/model_executor/models/minimax_vl_01.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/siglip2navit.py`
  - `vllm/model_executor/models/tarsier.py`

### Summary

**What changed and why**  
This PR standardizes common vision encoders (CLIP, SigLIP, PixtralHF) used by multimodal models like LLaVA. The main changes include: adding a `multimodal_config` parameter to vision encoder constructors, forwarding it to `MMEncoderAttention` to enable `mm_encoder_tp_mode` handling, updating `SiglipVisionEmbeddings` to match the latest Transformers implementation, merging `Phi3ImageEmbeddingBase` into its sole subclass, and fixing missing `scale` parameter passes in attention layers.

**Technical impact**  
The changes enable consistent tensor parallelism configuration across vision encoders via `mm_encoder_tp_mode`, allowing data-parallel attention computation when needed. This affects initialization and forward passes of attention and MLP layers, with modifications to linear layers (`disable_tp` flag) and attention head partitioning. The refactoring reduces code duplication (e.g., removing `Phi3ImageEmbeddingBase`) and aligns embeddings with upstream implementations.

**Potential risks**  
- The `SiglipVisionEmbeddings` update changes position embedding interpolation logic, which could affect model outputs for non-standard image resolutions.
- Conditional logic for `MMEncoderAttention` initialization (checking `attn_cls`) may introduce subtle bugs if other attention classes are used.
- Changes to tensor parallelism behavior (`disable_tp`) could impact performance or memory usage if not configured correctly across all models.

**Key insights**  
- Developers must ensure `multimodal_config` is properly passed through all vision encoder initialization chains, especially in custom or new multimodal models.
- The `scale` parameter fix in `MMEncoderAttention` calls is critical for attention correctness.
- Reviewers should verify that the updated `SiglipVisionEmbeddings` interpolation matches expected behavior for all supported image sizes.

---

## 38. [[Chore] Further cleanup pooler](https://github.com/vllm-project/vllm/pull/31951)


### Base Information

- **PR Number:** #31951
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-08 02:16:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31951/files) (7):**
  - `tests/model_executor/test_model_load_with_params.py`
  - `tests/test_config.py`
  - `vllm/config/pooler.py`
  - `vllm/model_executor/layers/pooler.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/v1/outputs.py`

### Summary

**What changed and why**  
This PR refactors the pooling layer implementation to improve clarity and remove redundancy. Key changes include renaming `Tokens*` types to `Tokenwise*` for better semantic meaning, replacing the `PoolingType` enum with string literals to simplify the codebase, and optimizing `AllPool.forward` to avoid redundant operations.

**Technical impact**  
The removal of the `PoolingType` enum simplifies type handling by using string literals directly, reducing abstraction overhead. The `AllPool.forward` optimization improves performance by eliminating unnecessary list operations. Type renaming enhances code readability by more accurately describing the token-wise pooling behavior.

**Potential risks**  
Direct string comparisons for pooling types could introduce runtime errors if invalid strings are passed, whereas enums provided compile-time safety. The changes to `AllPool.forward` logic require careful validation to ensure chunked prefill functionality remains correct, especially with the modified indexing and state management.

**Key insights**  
Developers should verify that all pooling type strings are validated appropriately, as the switch from enums to strings removes built-in validation. The performance improvements in `AllPool.forward` are beneficial but should be tested with edge cases in chunked prefill scenarios. The type renaming improves maintainability but requires updates to any downstream code referencing the old type names.

---

## 39. [RayLLM Bugfix - Preserve obj store URL for multi engine_config creation](https://github.com/vllm-project/vllm/pull/30803)


### Base Information

- **PR Number:** #30803
- **Author:** [omer-dayan](https://github.com/omer-dayan)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-08 02:00:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30803/files) (4):**
  - `vllm/config/model.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/model_loader/runai_streamer_loader.py`
  - `vllm/model_executor/model_loader/sharded_state_loader.py`

### Summary

**What changed and why**  
A new `model_weights` field was added to `ModelConfig` and `EngineArgs` to preserve the original object store URI (e.g., from RunAI) when models are cloned to a local directory. This fixes a bug where repeated calls to `create_engine_config` would cause `model` and `model_weights` to diverge, breaking weight loading in RayLLM and vLLM serve.

**Technical impact**  
The changes ensure consistency between `model` (local directory) and `model_weights` (original URI) across multiple engine config creations. Weight loaders now prioritize `model_weights` if set, maintaining correct object store references. The `maybe_pull_model_tokenizer_for_runai` method skips re-pulling if `model_weights` is already populated.

**Potential risks**  
If `model_weights` is incorrectly set or persists stale values, it could lead to loading weights from unintended paths. The conditional check in `maybe_pull_model_tokenizer_for_runai` assumes `model_weights` being non-empty indicates a completed pull, which may not hold if the field is set elsewhere.

**Key insights**  
This fix elegantly separates local model paths from original URIs, crucial for distributed object storage scenarios. Developers should ensure `model_weights` is only set during the initial pull and remains immutable. Consider adding validation to prevent mismatches between `model` and `model_weights` in future updates.

---

## 40. [[Misc] Support qwen3-next lora](https://github.com/vllm-project/vllm/pull/31719)


### Base Information

- **PR Number:** #31719
- **Author:** [BJWang-ant](https://github.com/BJWang-ant)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-08 01:27:51
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31719/files) (1):**
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
The change replaces `torch.nn.Linear` with `ReplicatedLinear` for the `shared_expert_gate` layer in the Qwen3-Next model. This modification ensures that LoRA (Low-Rank Adaptation) can correctly identify and adapt the shared expert gate parameters during fine-tuning.

**Technical impact**  
This update aligns the `shared_expert_gate` with the existing pattern used for other linear layers in the model, such as the `gate` layer. It enables parameter replication and proper prefix naming, which is essential for distributed training and LoRA integration, maintaining consistency in the model's architecture.

**Potential risks**  
If `ReplicatedLinear` is not properly implemented or lacks full compatibility with standard `torch.nn.Linear` operations, it could introduce subtle behavioral differences or performance overhead. Additionally, any misconfiguration in the `quant_config` or `prefix` parameters might affect model loading or training stability.

**Key insights**  
Developers should verify that `ReplicatedLinear` supports all expected linear layer functionalities and that the prefix naming aligns with the model's parameter hierarchy. This change is a targeted fix for LoRA compatibility, but it's crucial to ensure it doesn't break existing fine-tuning or inference workflows.

---

## 41. [fix(compile): apply partition wrapper when loading AOT cached functions](https://github.com/vllm-project/vllm/pull/31536)


### Base Information

- **PR Number:** #31536
- **Author:** [devbyteai](https://github.com/devbyteai)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-08 01:27:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31536/files) (2):**
  - `tests/compile/test_aot_compile.py`
  - `vllm/compilation/decorators.py`

### Summary

**What changed and why**  
The fix ensures the `maybe_use_cudagraph_partition_wrapper` context manager is applied when loading AOT-compiled functions from cache. Previously, two code paths (subsequent calls with cached `aot_compiled_fn` and first call after restart) bypassed this wrapper, causing a 2x latency regression when using inductor graph partitioning with PIECEWISE CUDA graphs.

**Technical impact**  
This change ensures that `torch._inductor.utils.set_customized_partition_wrappers()` is invoked at runtime, enabling proper CUDA graph capture with inductor partitioning. Performance for AOT-compiled functions with partitioning now matches non-AOT performance (~2s vs. ~5s), while other configurations remain unaffected.

**Potential risks**  
The wrapper context is now applied unconditionally when `aot_compiled_fn` exists, which could introduce minor overhead if partitioning is disabled. However, the context manager is lightweight and only activates when `use_inductor_graph_partition=True`. Edge cases may arise if the wrapper’s state management interacts unexpectedly with other compilation features.

**Key insights**  
Always verify that runtime contexts required for correct behavior are applied in all code paths, especially caching and lazy-loading scenarios. The added test effectively validates both the initial load and subsequent calls, but developers should ensure similar patterns are checked in other decorators or compilation utilities.

---

## 42. [[CI/Build] Enable test_kv_cache_events_dp for AMD](https://github.com/vllm-project/vllm/pull/31834)


### Base Information

- **PR Number:** #31834
- **Author:** [rjrock](https://github.com/rjrock)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-08 01:00:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31834/files) (1):**
  - `tests/v1/engine/test_engine_core_client.py`

### Summary

**What changed and why**  
The change replaces a platform check from `is_cuda()` to `is_cuda_alike()` in a test file's module-level skip condition. This enables the specific test `test_kv_cache_events_dp` to run on AMD platforms (like ROCm) by recognizing them as valid "CUDA-alike" targets, not just NVIDIA CUDA.

**Technical impact**  
This modification broadens the test suite's compatibility to include AMD's ROCm platform within the V1 engine's supported test matrix. It aligns the test's platform guard with a more inclusive abstraction (`is_cuda_alike`) that likely encapsulates both NVIDIA and AMD GPU backends, allowing the CI pipeline for AMD to execute this previously skipped test.

**Potential risks**  
If the `is_cuda_alike()` function's implementation is incorrect or overly permissive, it could inadvertently allow the test to run on unsupported platforms, leading to runtime failures or false test passes. There is also a risk that the test itself may have platform-specific dependencies not fully covered by the `is_cuda_alike` check, potentially causing flaky results on AMD.

**Key insights**  
The change is a targeted fix to expand CI coverage. Developers should verify that the `is_cuda_alike()` function's logic correctly identifies all intended platforms (e.g., CUDA and ROCm) and no others. It's also good practice to ensure the test's internal logic and any hardware-specific operations are compatible with both supported backends.

---

## 43. [Decouple page_size_bytes calculation in AttentionSpec for TPU/RPA Compatibility.](https://github.com/vllm-project/vllm/pull/31635)


### Base Information

- **PR Number:** #31635
- **Author:** [Lumosis](https://github.com/Lumosis)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-08 01:00:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31635/files) (6):**
  - `tests/v1/core/test_kv_sharing.py`
  - `tests/v1/core/test_prefix_caching.py`
  - `tests/v1/core/test_scheduler.py`
  - `tests/v1/core/utils.py`
  - `tests/v1/kv_connector/unit/utils.py`
  - `vllm/v1/kv_cache_interface.py`

### Summary

**What changed and why**  
The changes introduce a new `page_size_padded` field to `AttentionSpec` classes, allowing explicit control over page size bytes instead of relying on implicit calculation via `num_gpu_blocks_override`. This decouples page size calculation from GPU block overrides, enabling compatibility with backends like TPU that require memory alignment for Ragged Paged Attention (RPA) and fixing multi-host inference issues with the Ray executor.

**Technical impact**  
The `page_size_bytes` property now returns either the padded size (if set) or the real calculated size, while a new `real_page_size_bytes` property retains the original calculation logic. All `AttentionSpec` subclasses (`FullAttentionSpec`, `MLAAttentionSpec`, etc.) are updated to use keyword-only arguments and override `real_page_size_bytes` instead of `page_size_bytes`. This maintains backward compatibility while enabling explicit padding for alignment-sensitive hardware.

**Potential risks**  
If `page_size_padded` is set incorrectly (smaller than `real_page_size_bytes`), the assertion will fail, potentially causing runtime errors. The changes assume that all subclasses correctly override `real_page_size_bytes`; any missed overrides could lead to incorrect page size calculations. Additionally, the keyword-only dataclass change may break existing code that uses positional arguments to instantiate these specs.

**Key insights**  
Developers must now use keyword arguments when creating `AttentionSpec` instances due to the `kw_only=True` dataclass change. For TPU/RPA backends, set `page_size_padded` to an aligned size greater than or equal to the real page size. Ensure all custom `AttentionSpec` subclasses override `real_page_size_bytes` (not `page_size_bytes`) to maintain correct behavior.

---

## 44. [[Models] Allow converting Qwen3-VL into Reranker model](https://github.com/vllm-project/vllm/pull/31890)


### Base Information

- **PR Number:** #31890
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-08 00:10:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31890/files) (8):**
  - `docs/models/supported_models.md`
  - `examples/pooling/pooling/vision_language_pooling.py`
  - `examples/pooling/score/template/qwen3_vl_reranker.jinja`
  - `examples/pooling/score/vision_language_reranker.py`
  - `tests/models/registry.py`
  - `vllm/entrypoints/score_utils.py`
  - `vllm/model_executor/models/adapters.py`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
This PR adds reranker support for Qwen3-VL models by enabling conversion of Qwen3-VL into a sequence classification architecture for scoring tasks. The changes include new configuration handling, example scripts, and documentation updates to support multimodal (text+image+video) reranking.

**Technical impact**  
The implementation extends the existing sequence classification framework to handle Qwen3-VL's multimodal inputs. Key changes include: 1) Adding `Qwen3VLForSequenceClassification` configuration that inherits from `Qwen3ForSequenceClassificationConfig`, 2) Modifying weight loading logic to properly extract classification scores from token logits ("no"/"yes"), and 3) Supporting video inputs in the scoring pipeline through new type definitions.

**Potential risks**  
1. The `hf_overrides` configuration required for loading official models adds complexity and may confuse users if not documented properly. 2. The weight loading logic assumes specific model architecture attributes (e.g., `embed_tokens`), which could break with future model variations. 3. Video modality support is newly introduced but only minimally tested in the provided example.

**Key insights**  
1. The PR follows existing patterns for reranker conversion but requires special handling for Qwen3-VL's multimodal nature. 2. Developers should note the mandatory `hf_overrides` for official models, as highlighted in the documentation. 3. The video input support expands the system's capabilities but should be validated with more diverse test cases.

---

