# vLLM Merged PR Report

**Report Date:** 2026-01-05 PST

**Total Merged PRs:** 36

---

## 1. [[Bugfix][ROCm] Fix Unsupported attention metadata type for speculative decoding in `eagle.py`](https://github.com/vllm-project/vllm/pull/31714)


### Base Information

- **PR Number:** #31714
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-05 23:53:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31714/files) (1):**
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
The PR fixes an import issue for ROCm platforms in speculative decoding by replacing the unsupported `FlashAttentionMetadata` import with `RocmAttentionMetadata`. This resolves a runtime error when using the ROCm attention backend with EAGLE speculative decoding.

**Technical impact**  
This change ensures the ROCm platform correctly identifies supported attention metadata types during initialization. The `allowed_attn_types` list now includes the appropriate ROCm-specific metadata class, enabling compatibility with the ROCm attention backend for speculative decoding workflows.

**Potential risks**  
If other attention backends (e.g., ROCM_AITER_FA) are not properly configured or imported, similar issues could arise. Additionally, the conditional import approach might introduce subtle platform-specific bugs if the module structure changes.

**Key insights**  
Always verify platform-specific imports and dependencies, especially for conditional backends. Consider adding runtime validation to ensure all expected metadata types are available. This fix highlights the importance of testing cross-platform compatibility for attention backends in speculative decoding scenarios.

---

## 2. [[Doc] Show that `use_audio_in_video` is supported in docs](https://github.com/vllm-project/vllm/pull/30837)


### Base Information

- **PR Number:** #30837
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 23:27:19
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30837/files) (4):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/qwen2_5_omni/README.md`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
This PR removes documentation and code comments indicating that the `use_audio_in_video` feature is unsupported for Qwen2.5-Omni and Qwen3-Omni models. The changes follow the merge of PR #27721, which presumably added support for this functionality, addressing the issue referenced in the PR description.

**Technical impact**  
The updates signal that `use_audio_in_video` is now fully supported in the V1 engine for the specified models. This allows users to process interleaved audio and vision modalities from video files without encountering previous limitations, enhancing multimodal inference capabilities.

**Potential risks**  
If the underlying implementation in PR #27721 is incomplete or buggy, users may experience runtime errors or incorrect model behavior when enabling `use_audio_in_video`. Additionally, the removal of TODO comments could obscure any remaining technical debt or edge cases related to overlapping modality embeddings.

**Key insights**  
Developers should verify that the merged changes in #27721 comprehensively handle overlapping modality embeddings and interleaved inputs. It is also advisable to add integration tests for `use_audio_in_video` to ensure robustness, and to update any related API documentation or examples that may still reference the old limitations.

---

## 3. [[Bugfix][Hardware][AMD] Fix exception types in AITER MLA FP8 check](https://github.com/vllm-project/vllm/pull/31177)


### Base Information

- **PR Number:** #31177
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-05 22:11:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31177/files) (2):**
  - `tests/rocm/aiter/test_mla_fp8_support_check.py`
  - `vllm/_aiter_ops.py`

### Summary

**What changed and why**  
The PR replaces a generic `except Exception:` clause with specific exception types (`ImportError`, `ModuleNotFoundError`, `AttributeError`, `ValueError`, `TypeError`) in the `_check_aiter_mla_fp8_support()` function. This prevents masking unexpected errors (e.g., `SyntaxError`, `MemoryError`) while maintaining graceful fallback behavior when the AITER module is unavailable or incompatible.

**Technical impact**  
The change improves debuggability by allowing legitimate errors to propagate, while the fallback logic remains unchanged for expected failure scenarios. It also ensures the function’s caching mechanism (`_AITER_MLA_SUPPORTS_FP8`) works correctly across all covered exceptions, preserving performance.

**Potential risks**  
If an unexpected exception type occurs (e.g., `OSError` due to file system issues or `MemoryError`), it will now propagate and could crash the detection routine. This is intentional but requires callers to handle such failures appropriately. Additionally, the list of caught exceptions must stay updated if the underlying `inspect.signature` or import logic changes.

**Key insights**  
This is a best-practice update that enhances error visibility without altering core functionality. Developers should ensure similar broad exception handlers are reviewed elsewhere in the codebase. The comprehensive unit tests provide strong validation for each exception case, setting a good precedent for future error-handling changes.

---

## 4. [[CI] Fix CPU MM Processor Test](https://github.com/vllm-project/vllm/pull/31764)


### Base Information

- **PR Number:** #31764
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-05 20:22:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31764/files) (1):**
  - `requirements/test.in`

### Summary

**What changed and why**  
The change pins `timm` to version `1.0.17` instead of using `>=1.0.17`. This addresses an incompatibility issue in CPU CI where a newer version (`timm==1.0.23`) was being installed, which broke tests for models like InternVL and Gemma3n-MM.

**Technical impact**  
This ensures consistent dependency resolution in CI environments by preventing automatic upgrades to potentially incompatible newer versions of `timm`. It stabilizes test execution but reduces flexibility for future updates, requiring manual intervention to upgrade.

**Potential risks**  
Pinning to an exact version may cause conflicts with other dependencies that require a different `timm` version. It also introduces technical debt, as the version constraint will need to be updated later when compatibility with newer `timm` releases is verified.

**Key insights**  
Use version pinning cautiously; consider adding a comment linking to the incompatibility issue. For long-term maintenance, explore dependency isolation strategies (e.g., using virtual environments per test suite) rather than hard pinning in the main requirements file.

---

## 5. [[Bugfix] Add init_workspace_manager to moe kernel benchmarks](https://github.com/vllm-project/vllm/pull/31042)


### Base Information

- **PR Number:** #31042
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 19:14:33
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31042/files) (3):**
  - `benchmarks/kernels/benchmark_cutlass_moe_fp8.py`
  - `benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py`
  - `benchmarks/kernels/benchmark_grouped_gemm_cutlass.py`

### Summary

**What changed and why**  
Three benchmark scripts (`benchmark_cutlass_moe_fp8.py`, `benchmark_cutlass_moe_nvfp4.py`, and `benchmark_grouped_gemm_cutlass.py`) were updated to initialize the workspace manager by calling `init_workspace_manager()` at the start of their `main()` functions. This change resolves an `AssertionError` that occurred because CUTLASS MoE kernels require the workspace manager to be initialized before use.

**Technical impact**  
The changes ensure that the benchmark scripts can run successfully by meeting the prerequisite condition for CUTLASS MoE kernel operations. This does not affect production code or alter benchmarking logic; it only adds a necessary initialization step that is specific to the benchmarking environment.

**Potential risks**  
Hardcoding `device = torch.device("cuda:0")` assumes a single-GPU setup, which may cause issues in multi-GPU benchmarking scenarios. Additionally, if other scripts or modules rely on similar CUTLASS kernels but miss this initialization, they could encounter the same error.

**Key insights**  
Developers should verify that any script using CUTLASS MoE kernels includes `init_workspace_manager()` with the appropriate device context. For future robustness, consider making the device selection configurable (e.g., via command-line arguments) to support multi-GPU environments. This fix is minimal and focused, but it highlights a dependency that should be documented for kernel usage.

---

## 6. [[UX] Add `-ep` shorthand for `--enable-expert-parallel`](https://github.com/vllm-project/vllm/pull/30890)


### Base Information

- **PR Number:** #30890
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 19:13:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30890/files) (1):**
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
Added `-ep` as a shorthand command-line argument for `--enable-expert-parallel` to reduce verbosity when enabling expert parallel mode. This aligns with existing shorthand patterns (like `-tp`, `-dp`) but differs slightly since it's a boolean flag rather than requiring a numeric value.

**Technical impact**  
The change modifies the argument parser configuration in `arg_utils.py` to accept both the long form (`--enable-expert-parallel`) and the new short form (`-ep`). This maintains backward compatibility while improving user experience for frequent command-line usage.

**Potential risks**  
Minimal risk since the change only adds an alias without altering existing functionality. However, future expansion to support an optional integer (as hinted in the PR description) could introduce parsing conflicts if not carefully designed to maintain consistency with other parallel arguments.

**Key insights**  
The shorthand improves usability, but developers should note the inconsistency with other parallel flags (`-tp 2`, `-dp 2` vs. `-ep`). If extending to support integer values later, ensure the argument parser gracefully handles both boolean and integer modes to avoid breaking changes.

---

## 7. [[Cleanup] Remove redundant `decoder_layer_type` assignment in `Qwen2`](https://github.com/vllm-project/vllm/pull/31760)


### Base Information

- **PR Number:** #31760
- **Author:** [maang-h](https://github.com/maang-h)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 18:09:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31760/files) (1):**
  - `vllm/model_executor/models/qwen2.py`

### Summary

**What changed and why**  
Removed a redundant assignment and comment in the Qwen2 model's `__init__` method. The `decoder_layer_type` parameter already defaults to `Qwen2DecoderLayer` in the function signature, making the explicit `or` fallback unnecessary.

**Technical impact**  
This change eliminates dead code without affecting functionality. The `make_layers` function will still receive the correct `decoder_layer_type` value directly from the parameter, maintaining identical model construction behavior.

**Potential risks**  
Minimal risk since this is a pure cleanup. The only theoretical concern would be if `decoder_layer_type` could be explicitly passed as `None` (breaking the default), but the parameter's default value already prevents this scenario.

**Key insights**  
This cleanup improves code readability by removing redundant logic. Developers should ensure function parameter defaults are properly set in signatures rather than using internal fallbacks, which simplifies maintenance and reduces cognitive load.

---

## 8. [[Perf] Optimize additional `fill(0)` in cutlass moe, 2.9% E2E throughput improvement, 10.8% TTFT improvement](https://github.com/vllm-project/vllm/pull/31754)


### Base Information

- **PR Number:** #31754
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 18:01:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31754/files) (1):**
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`

### Summary

**What changed and why**  
The PR optimizes two Mixture of Experts (MoE) kernels (`run_cutlass_moe_fp8` and `run_cutlass_moe_w4a8_fp8`) by removing an explicit `fill_(0)` call on the `mm2_out` tensor when expert mapping is used. Instead, it leverages the `expert_first_token_offset` array (renamed from `expert_offsets`) passed to the `moe_unpermute` function, allowing the unpermute operation to handle zero-initialization implicitly.

**Technical impact**  
This change reduces unnecessary memory writes in the GPU kernel execution path, improving overall throughput. The performance gains are demonstrated by benchmark results showing a 2.9% end-to-end throughput improvement and a 10.8% reduction in time-to-first-token (TTFT). The modification maintains functional correctness, as confirmed by accuracy tests on the GSM8K benchmark.

**Potential risks**  
If the `expert_first_token_offset` array is incorrectly structured or contains invalid offsets, the unpermute operation may produce incorrect outputs or access out-of-bounds memory. Additionally, the change assumes that the unpermute kernel now handles zero-initialization internally; any regression in that kernel could introduce silent errors.

**Key insights**  
The optimization highlights the importance of minimizing redundant GPU operations, especially memory writes, in performance-critical paths. Developers should verify that downstream kernels (like `moe_unpermute`) correctly manage tensor initialization to avoid dependency on explicit zeroing. This pattern could be applied to other similar kernels where temporary buffers are reused.

---

## 9. [[Docs] Improve malformed exception caused by backslash line continuations](https://github.com/vllm-project/vllm/pull/31694)


### Base Information

- **PR Number:** #31694
- **Author:** [maang-h](https://github.com/maang-h)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 17:51:54
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31694/files) (7):**
  - `vllm/config/compilation.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/model_loader/weight_utils.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/multimodal/registry.py`
  - `vllm/utils/argparse_utils.py`

### Summary

**What changed and why**  
This PR replaces backslash line continuations within multi-line string literals with implicit string concatenation. The changes fix malformed error messages where backslashes and trailing spaces were being included in the displayed text, improving readability and correctness.

**Technical impact**  
The modifications affect exception messages and assertions across configuration, model loading, and multimodal components. This enhances debugging by ensuring error messages are clean and properly formatted without unintended whitespace or continuation characters appearing in output.

**Potential risks**  
While low risk, there is a slight chance of introducing syntax errors if quotes are mismatched during conversion. Additionally, any automated tools parsing these error messages should be verified to handle the new formatting correctly, though the change is purely cosmetic.

**Key insights**  
Always use implicit string concatenation for multi-line error messages to avoid formatting issues. Reviewers should check for similar backslash continuations elsewhere in the codebase. This change demonstrates the importance of clean error messaging for developer experience.

---

## 10. [Revert "[CI Failure] Disable B200 tests while runner is broken"](https://github.com/vllm-project/vllm/pull/31750)


### Base Information

- **PR Number:** #31750
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 17:26:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31750/files) (1):**
  - `.buildkite/test-pipeline.yaml`

### Summary

**What changed and why**  
This PR reverts a previous change that made B200 GPU tests optional in the CI pipeline. The reversion removes the `optional: true` flag and associated TODO comments from four test steps, effectively re-enabling these tests as mandatory requirements for CI success.

**Technical impact**  
The changes restore B200 GPU tests to their original mandatory status in the CI pipeline. This means CI builds will now fail if any of these tests fail, ensuring B200-specific functionality is properly validated before code merges. The pipeline will now provide stricter quality control for Blackwell architecture support.

**Potential risks**  
If the underlying B200 runner issues mentioned in the original PR aren't resolved, this change could cause frequent CI failures and block legitimate merges. There's also risk of increased CI runtime and resource consumption if B200 tests are flaky or slow, potentially impacting developer productivity.

**Key insights**  
This reversion indicates confidence that B200 runner issues have been resolved. Developers should monitor CI stability closely after this change and be prepared to investigate any new failures in attention, quantization, or MoE tests on Blackwell architecture. Consider implementing better test isolation or retry mechanisms if flakiness emerges.

---

## 11. [[Bugfix] vLLM produces invalid UTF-8 tokens and “�”](https://github.com/vllm-project/vllm/pull/28874)


### Base Information

- **PR Number:** #28874
- **Author:** [johncalesp](https://github.com/johncalesp)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-01-05 16:23:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28874/files) (3):**
  - `tests/v1/engine/test_output_processor.py`
  - `tests/v1/sample/test_logprobs.py`
  - `vllm/v1/engine/logprobs.py`

### Summary

**What changed and why**  
The changes fix a bug where vLLM's V1 engine produces invalid UTF-8 tokens (displayed as "�") when decoding token-by-token with logprobs. This occurs with byte-fallback tokenization where multi-byte UTF-8 characters (like curly quotes) are split across tokens. The fix introduces UTF-8 correction logic in `LogprobsProcessor` to merge incomplete byte sequences into valid characters.

**Technical impact**  
The `LogprobsProcessor` now includes a `_correct_decoded_token` method that attempts to decode token combinations (current token with previous token or previous logprob token) to form valid UTF-8 sequences. This ensures decoded tokens in logprobs output are either valid UTF-8 strings or empty strings, eliminating replacement characters. The correction is applied both during sampling and prompt logprobs processing.

**Potential risks**  
- The correction logic may incorrectly merge tokens in edge cases, potentially altering the intended output.  
- Empty strings returned as fallbacks could affect downstream processing that expects non-empty tokens.  
- Performance may slightly degrade due to additional decode calls and token lookups, especially for long sequences.  
- The fix assumes "�" only appears at the end of tokens due to byte-fallback issues, which may not cover all invalid UTF-8 scenarios.

**Key insights**  
- The correction prioritizes combining tokens within the current list before using previous logprob tokens, mimicking incremental decoding.  
- Developers should verify that empty string tokens in logprobs output are handled correctly in their applications.  
- Consider adding metrics to monitor correction frequency and ensure it doesn’t impact latency in production.  
- The failing spec decode tests in the PR are noted as pre-existing issues and should be addressed separately.

---

## 12. [[CI/Build] Allow user to configure NVSHMEM version via ENV or command line](https://github.com/vllm-project/vllm/pull/30732)


### Base Information

- **PR Number:** #30732
- **Author:** [eicherseiji](https://github.com/eicherseiji)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 15:56:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30732/files) (2):**
  - `docker/Dockerfile`
  - `tools/ep_kernels/install_python_libraries.sh`

### Summary

**What changed and why**  
The changes add configuration support for the NVSHMEM version used when building EP kernels. Users can now specify the NVSHMEM version via a new `--nvshmem-ver` CLI flag, an environment variable (`NVSHMEM_VER`), or a Docker build argument. This follows the established pattern for `PPLX_COMMIT_HASH` and `DEEPEP_COMMIT_HASH`, enabling flexibility for testing, debugging, and custom container builds.

**Technical impact**  
The modifications extend the build system's configurability without altering default behavior (NVSHMEM 3.3.24 remains the default). The Dockerfile now passes the `NVSHMEM_VER` argument to the installation script, and the script includes validation to prevent path traversal attacks by restricting version strings to alphanumeric characters, dots, and hyphens.

**Potential risks**  
If users specify an unsupported or incompatible NVSHMEM version, it could lead to build failures or runtime issues. The validation regex, while preventing slashes, may still allow unconventional version strings that could cause unexpected behavior in downstream processes. Additionally, the changes assume the underlying build process handles the version variable correctly.

**Key insights**  
This enhancement aligns with existing configuration patterns, promoting consistency. Developers should ensure any new NVSHMEM versions are compatible with the CUDA version and other dependencies. Consider documenting supported versions and adding integration tests to verify builds with different NVSHMEM versions work as intended.

---

## 13. [[Bugfix] Properly apply v_scale for mimo_v2_flash](https://github.com/vllm-project/vllm/pull/31175)


### Base Information

- **PR Number:** #31175
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-05 15:20:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31175/files) (1):**
  - `vllm/model_executor/models/mimo_v2_flash.py`

### Summary

**What changed and why**  
The fix addresses a bug where the `v_scale` (attention value scale) parameter was incorrectly applied during weight loading instead of during the attention computation. The scale was being multiplied with weight tensors, but should be applied to the value projections before attention. This aligns with the Transformers implementation and improves model accuracy.

**Technical impact**  
The change moves `v_scale` application from the weight loading phase to the forward pass of the attention layer. This ensures the scaling is correctly applied to the value projections (`v`) before attention computation, matching the intended architectural behavior. The model's attention mechanism now properly scales value vectors, which improves accuracy as shown in the GSM8K evaluation.

**Potential risks**  
If `v_scale` is applied multiple times (e.g., in both weight loading and forward pass in some edge cases), it could cause incorrect scaling. The removal of scaling during weight loading might affect models that previously relied on that behavior, though this appears to be the correct fix. There's a slight performance impact from applying the scaling in the forward pass rather than pre-scaling weights.

**Key insights**  
Always apply scaling factors during computation rather than modifying stored weights when possible, as this maintains weight integrity and matches reference implementations. The fix demonstrates the importance of comparing with reference implementations (Transformers) to catch subtle behavioral differences. Developers should verify that architectural parameters like scaling factors are applied at the correct layer in the computation graph.

---

## 14. [[Bugfix] Fix Broken ModelOpt NVFP4 MoE](https://github.com/vllm-project/vllm/pull/31742)


### Base Information

- **PR Number:** #31742
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-05 15:18:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31742/files) (4):**
  - `vllm/model_executor/layers/fused_moe/all2all_utils.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`

### Summary

**What changed and why**  
This PR fixes a regression where NVFP4 ModelOpt MoE models broke after a previous change (#31533). The issue was that the `build_flashinfer_fp8_cutlass_moe_prepare_finalize` function was incorrectly removed from the general `all2all_utils.py` path, but not properly re-integrated into the quantization-specific backends (FP8 and ModelOpt). The fix restores this functionality by adding the prepare/finalize builder calls to the appropriate quantization modules.

**Technical impact**  
The changes ensure that NVFP4 ModelOpt MoE models can correctly initialize their FlashInfer-Cutlass prepare/finalize kernels. This affects the model loading and execution path for quantized MoE models using FlashInfer backends, particularly in distributed (non-TP=1) scenarios. The architecture now correctly delegates kernel construction to the quantization-specific modules rather than a shared utility.

**Potential risks**  
The conditional logic around `moe.dp_size == 1` in ModelOpt could introduce subtle bugs if TP/DP configurations change. There's also a risk of inconsistency since the fix adds similar but not identical logic to both `fp8.py` and `modelopt.py`. The added `use_nvfp4=True` parameter in `flashinfer_cutlass_moe.py` might need validation for other quantization types.

**Key insights**  
Developers should note that FlashInfer-Cutlass MoE kernel initialization is now quantization-backend specific. The `all2all_utils.py` file should no longer handle these kernels directly. When modifying MoE or quantization code, ensure both FP8 and ModelOpt backends are updated consistently. The TODO comment indicates this area is pending a broader MoE refactor.

---

## 15. [[MoE Refactor] Aiter Experts for BF16 MoE](https://github.com/vllm-project/vllm/pull/31542)


### Base Information

- **PR Number:** #31542
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 14:52:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31542/files) (1):**
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`

### Summary

**What changed and why**  
This PR refactors the ROCm Aiter fused MoE implementation to integrate it into the modular kernel framework. Previously, the `rocm_aiter_fused_experts` function was called directly in the forward pass when enabled. Now, it's wrapped into an `AiterExperts` kernel module and used through the unified `FusedMoEModularKernel` interface, aligning with the refactor started in #31504.

**Technical impact**  
The changes consolidate the ROCm Aiter MoE path into the modular kernel architecture, removing conditional branching in the forward method. Weight shuffling is now performed during weight processing when ROCm Aiter is enabled, and the kernel is initialized upfront. This improves code structure and maintains performance, as shown by slightly improved benchmark results.

**Potential risks**  
There's a risk if the `AiterExperts` kernel doesn't fully replicate the behavior of the original `rocm_aiter_fused_experts` function, especially regarding tensor shapes or memory layouts. The weight shuffling is now done earlier (in `process_weights_after_loading`), which could affect other kernel paths if not properly isolated. Additionally, the `use_inplace` flag is set to `True` for ROCm Aiter, which may have implications for memory usage or compatibility.

**Key insights**  
The refactor successfully integrates ROCm Aiter MoE into the modular kernel system, simplifying the forward pass. Developers should verify that the `AiterExperts` kernel handles all edge cases (e.g., varying expert counts, activation functions) correctly. The benchmark shows a slight performance improvement, but thorough testing across different models and configurations is recommended to ensure no regressions.

---

## 16. [[Bug] Revert torch warning fix](https://github.com/vllm-project/vllm/pull/31585)


### Base Information

- **PR Number:** #31585
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-05 14:31:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31585/files) (3):**
  - `tests/v1/e2e/test_async_scheduling.py`
  - `vllm/envs.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR reverts a previous workaround for a PyTorch warning by changing the environment variable `VLLM_FLOAT32_MATMUL_PRECISION` from custom values ("ieee"/"tf32") to the standard PyTorch values ("highest"/"high"/"medium"). The change updates the default from "ieee" to "highest" and switches from directly setting `torch.backends.cuda.matmul.fp32_precision` to using the official `torch.set_float32_matmul_precision()` API.

**Technical impact**  
The code now aligns with PyTorch's official API for controlling float32 matrix multiplication precision. This improves maintainability and ensures compatibility with PyTorch versions where the old warning has been removed (specifically PyTorch 2.9.1+). The behavior change from "ieee" to "highest" should be functionally equivalent as both represent the highest precision setting.

**Potential risks**  
There's a risk if users have come to rely on the specific "ieee" or "tf32" string values in their configurations or scripts. The mapping between old and new values isn't documented in the changes, though "ieee" → "highest" and "tf32" → likely "medium" or "high" would be reasonable assumptions. The test change suggests "highest" is the new default, but the exact precision equivalence should be verified.

**Key insights**  
This is a positive cleanup that removes technical debt by adopting PyTorch's official API. Developers should update any configuration files or scripts using the old "ieee"/"tf32" values to use "highest"/"high"/"medium". The PR demonstrates good practice in tracking upstream dependency changes and removing temporary fixes once they're no longer needed.

---

## 17. [[CI][DeepSeek] Add nightly DeepSeek R1 `lm_eval` tests on H200](https://github.com/vllm-project/vllm/pull/30356)


### Base Information

- **PR Number:** #30356
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-05 14:17:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30356/files) (5):**
  - `.buildkite/test-pipeline.yaml`
  - `tests/evals/gsm8k/configs/DeepSeek-R1-DP.yaml`
  - `tests/evals/gsm8k/configs/DeepSeek-R1-TP.yaml`
  - `tests/evals/gsm8k/configs/models-h200.txt`
  - `tests/evals/gsm8k/test_gsm8k_correctness.py`

### Summary

**What changed and why**  
This PR adds nightly `lm_eval` tests for DeepSeek R1 on 8xH200 GPUs to detect regressions earlier. It includes both TP8-EP and DP8-EP configurations, as DP8-EP is particularly sensitive to vLLM's memory footprint and requires non‑default launch parameters.

**Technical impact**  
The changes introduce two new configuration files for DeepSeek R1 (TP and DP variants) and a dedicated H200 model list. The test pipeline now runs a nightly evaluation job on H200 hardware, and the test harness has been updated to respect a configurable startup timeout (defaulting to 600 seconds if not specified).

**Potential risks**  
The DP8-EP configuration relies on manual memory tuning (`--gpu-memory-utilization=0.85` and reduced `--max-model-len`), which may become invalid if the model or vLLM memory management changes. The nightly schedule could miss breakages introduced between runs, and the 20‑minute startup timeout may be insufficient if model loading becomes slower.

**Key insights**  
These tests are crucial for catching DeepSeek R1 regressions, but the DP configuration’s fragility warrants monitoring. Consider adding a periodic (e.g., every few hours) run if resources allow, and document the memory constraints in the configuration files to aid future debugging.

---

## 18. [[Cleanup] Remove deprecated fields from CachedRequestData class](https://github.com/vllm-project/vllm/pull/31734)


### Base Information

- **PR Number:** #31734
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-05 13:07:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31734/files) (2):**
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/output.py`

### Summary

**What changed and why**  
Removed deprecated fields `resumed_from_preemption` and `resumed_req_token_ids` from the `CachedRequestData` class, as they were scheduled for removal in the next release. Updated corresponding test assertions to use the replacement fields `resumed_req_ids` and `all_token_ids`.

**Technical impact**  
This cleanup simplifies the `CachedRequestData` interface by eliminating deprecated computed properties, reducing maintenance overhead. Tests now directly validate the underlying data structures (`resumed_req_ids` and `all_token_ids`) instead of relying on deprecated derived properties, improving clarity and aligning with the updated API.

**Potential risks**  
If any external code still references the removed deprecated fields, it will break at runtime. The changes assume all callers have migrated to the new fields, which may not be fully validated beyond the updated tests. Edge cases around `None` handling in `resumed_req_token_ids` are no longer relevant, but the new structure requires explicit membership checks.

**Key insights**  
This is a straightforward cleanup that follows deprecation policies. Developers should ensure no internal or external dependencies remain on the removed fields. The test updates demonstrate correct usage of the new fields, which map request IDs to token lists via dictionaries rather than parallel lists.

---

## 19. [[Model] Nemotron Parse 1.1 Support](https://github.com/vllm-project/vllm/pull/30864)


### Base Information

- **PR Number:** #30864
- **Author:** [amitz-nv](https://github.com/amitz-nv)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 13:00:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30864/files) (13):**
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/conftest.py`
  - `tests/models/multimodal/generation/test_nemotron_parse.py`
  - `tests/models/multimodal/pooling/test_radio.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/registry.py`
  - `vllm/assets/image.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/radio.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/configs/radio.py`

### Summary

**What changed and why**  
This PR adds support for NVIDIA Nemotron Parse 1.1, a multimodal model for visual parsing tasks. The implementation adapts an existing external implementation and integrates it into vLLM's model registry. Additionally, the RADIO vision model is updated to align with its Hugging Face counterpart, including changes to its forward method return signature and configuration parameter names.

**Technical impact**  
The changes introduce a new encoder-decoder model architecture (`NemotronParseForConditionalGeneration`) that leverages BART-based decoder layers and integrates with vLLM's multimodal processing pipeline. The RADIO model updates ensure compatibility with Nemotron Parse's expected input format, returning both summary and feature tensors. This also affects the NanoNemotronVL model, which uses RADIO as its vision backbone.

**Potential risks**  
The RADIO model's forward method signature change from returning a single tensor to a tuple could break existing code that directly calls the model. The test suite shows a workaround for a Hugging Face compatibility issue (`use_cache=False`), which may indicate instability in the reference implementation. There is also a risk of configuration mismatches if the updated `RadioConfig` parameters are not correctly propagated in all usage contexts.

**Key insights**  
Developers should note that the RADIO model now returns `(summary, features)` instead of just `features`, requiring updates in any custom code that interacts with it. The Nemotron Parse integration follows vLLM's established patterns for multimodal models, but thorough testing is advised due to the model's complexity. Ensure all configuration parameters (like `cpe_max_size` and `register_multiple`) are correctly set when using RADIO in new contexts.

---

## 20. [[docker] install cuda13 version of lmcache and nixl](https://github.com/vllm-project/vllm/pull/30913)


### Base Information

- **PR Number:** #30913
- **Author:** [soodoshll](https://github.com/soodoshll)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 12:50:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30913/files) (1):**
  - `docker/Dockerfile`

### Summary

**What changed and why**  
The Dockerfile was modified to conditionally install the CUDA13-compatible version of `nixl` (`nixl-cu13`) when building with CUDA version 13 or higher. Additionally, it adds logic to install necessary CUDA development packages (`libcusparse-dev`, `libcublas-dev`, `libcusolver-dev`) to support source compilation of `lmcache` on arm64 architectures, which is triggered when the standard installation fails.

**Technical impact**  
This change ensures the Docker image correctly selects the appropriate CUDA version for the `nixl` key-value connector dependency. The installation process now has a fallback path that installs build dependencies if the initial `uv pip install` fails, improving robustness for architectures requiring source compilation. The `CUDA_VERSION` build argument is now utilized to drive conditional logic.

**Potential risks**  
The conditional logic assumes CUDA major version 13 or higher uses `nixl-cu13`, which may not hold for future CUDA versions. The fallback installation path runs `apt-get update` and installs packages inside the conditional block, which could increase layer size if build packages are not properly purged. There's also a risk of leaving residual build dependencies if the `apt-get purge` command fails.

**Key insights**  
The implementation correctly handles architecture-specific compilation needs for `lmcache`. Developers should ensure the `CUDA_VERSION` argument is consistently passed during builds. Consider extracting the CUDA development package installation to a separate layer or script for better maintainability. Future CUDA version support should be reviewed to update the version check logic.

---

## 21. [pin lora_b moe weights on cpu](https://github.com/vllm-project/vllm/pull/31317)


### Base Information

- **PR Number:** #31317
- **Author:** [gnovack](https://github.com/gnovack)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 12:15:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31317/files) (2):**
  - `vllm/lora/layers/fused_moe.py`
  - `vllm/lora/model_manager.py`

### Summary

**What changed and why**  
The PR moves LoRA B weight permutation logic for 3D MoE models from `FusedMoE3DWithLoRA.set_lora()` to `model_manager._stack_moe_lora_weights()`. This ensures weight permutation occurs only once when the adapter is loaded from disk, not during each CPU-to-GPU transfer. The permuted weights are made contiguous and pinned in memory, improving transfer efficiency.

**Technical impact**  
This change reduces CPU-to-GPU data transfer latency by ~40% for LoRA swapping scenarios, as evidenced by the benchmark showing median TTFT dropping from 222.25 ms to 134.17 ms. The architecture now separates weight preparation (one-time) from runtime swapping, optimizing memory transfer patterns.

**Potential risks**  
The refactoring assumes the permutation logic is idempotent and safe to execute earlier in the loading pipeline. If the weight preparation depends on runtime state (e.g., device placement), it could introduce subtle bugs. The fallback path for non-3D MoE models retains complex chunking logic that may be error-prone.

**Key insights**  
Pinning and ensuring memory contiguity for frequently transferred weights is a high-impact optimization. Developers should apply similar patterns elsewhere in the codebase where weights move between CPU/GPU repeatedly. The change also simplifies `FusedMoE3DWithLoRA.set_lora()` by removing weight reshaping logic.

---

## 22. [[BugFix] Fix architecture flags to prevent issues on SM103](https://github.com/vllm-project/vllm/pull/31150)


### Base Information

- **PR Number:** #31150
- **Author:** [LopezCastroRoberto](https://github.com/LopezCastroRoberto)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-05 12:11:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31150/files) (2):**
  - `cmake/external_projects/qutlass.cmake`
  - `tools/flashinfer-build.sh`

### Summary

**What changed and why**  
This PR addresses GPU architecture flag compatibility for SM103 (Compute Capability 10.3) by replacing the deprecated `10.0a` flag with `10.0f` when using CUDA ≥ 13.0. For earlier CUDA versions (12.8-12.9), it adds `10.3a` to the architecture list to maintain SM103 support, ensuring consistent targeting across different CUDA toolkits.

**Technical impact**  
The changes modify build configuration logic in both CMake and shell scripts to correctly map architecture flags to their corresponding compute capabilities. This ensures that the QuTLASS library and FlashInfer kernels are compiled with appropriate GPU architecture targets, preventing potential compilation failures or runtime issues on SM103 hardware when using newer CUDA versions.

**Potential risks**  
The conditional logic based on CUDA version ranges could become complex to maintain as new CUDA versions are released. There's a risk that the `10.3a` flag might not be universally supported across all CUDA 12.x versions, potentially causing issues on some systems. The regex pattern `10\.(0a\|3a\|0f)` might need updating if additional architecture variants are introduced.

**Key insights**  
Developers should verify that their CUDA toolkit version aligns with the expected architecture flag mappings, particularly when building on systems with CUDA 12.8-12.9. The changes highlight the importance of keeping architecture flags synchronized between different build systems (CMake and shell scripts) to ensure consistent compilation behavior across the project.

---

## 23. [[Misc][Model][Refactor] Pass the prefix into Linear layers](https://github.com/vllm-project/vllm/pull/31669)


### Base Information

- **PR Number:** #31669
- **Author:** [kunpengW-code](https://github.com/kunpengW-code)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-05 12:03:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31669/files) (17):**
  - `vllm/model_executor/layers/mamba/mamba_mixer.py`
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/gpt_neox.py`
  - `vllm/model_executor/models/hunyuan_v1.py`
  - `vllm/model_executor/models/jamba.py`
  - `vllm/model_executor/models/jina_vl.py`
  - `vllm/model_executor/models/minicpm.py`
  - `vllm/model_executor/models/minicpm_eagle.py`
  - `vllm/model_executor/models/mistral_large_3_eagle.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/olmoe.py`
  - `vllm/model_executor/models/phimoe.py`
  - `vllm/model_executor/models/qwen.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/models/qwen_vl.py`
  - `vllm/model_executor/models/zamba2.py`

### Summary

**What changed and why**  
This PR refactors multiple model implementations to consistently pass the `prefix` parameter into Linear layer constructors (ColumnParallelLinear, RowParallelLinear, MergedColumnParallelLinear, ReplicatedLinear). The changes ensure proper parameter naming and weight loading by propagating hierarchical module names through the model architecture.

**Technical impact**  
The modifications standardize how linear layers are initialized across 17 different model architectures, improving consistency in parameter naming for tensor parallelism and weight loading. This affects weight mapping during model loading and potentially improves debugging by providing clearer parameter hierarchies. No functional changes to forward passes or model behavior are introduced.

**Potential risks**  
The main risk involves backward compatibility if any existing weight loading logic depends on specific parameter naming patterns. There's also a risk of missing some linear layers in less frequently used model components, though the changes appear comprehensive. The addition of default empty string prefixes could cause issues if callers don't properly propagate prefixes through nested structures.

**Key insights**  
This is a systematic refactoring that improves code consistency across the model zoo. Developers should ensure all new model implementations follow this pattern. The changes are mechanical but important for maintaining proper weight loading behavior in distributed settings. Reviewers should verify that prefix propagation is complete in each modified model, particularly checking nested structures and conditional branches.

---

## 24. [Fix GLM-4.6v flash tool calling in transformers 5.x](https://github.com/vllm-project/vllm/pull/31622)


### Base Information

- **PR Number:** #31622
- **Author:** [baonudesifeizhai](https://github.com/baonudesifeizhai)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 11:32:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31622/files) (2):**
  - `examples/tool_chat_template_glm4.jinja`
  - `vllm/tool_parsers/glm4_moe_tool_parser.py`

### Summary

**What changed and why**  
Added a new Jinja template (`examples/tool_chat_template_glm4.jinja`) and modified the GLM-4 MoE tool parser to fix tool-calling behavior for the GLM-4.6V-Flash model in transformers 5.x. The changes ensure that tool-call tokens (`<tool_call>`, `</tool_call>`) are properly preserved during decoding by setting `skip_special_tokens=False` when tools are present.

**Technical impact**  
The Jinja template provides a structured prompt format that explicitly defines tool-calling syntax for GLM-4 models, while the parser adjustment prevents the tokenizer from stripping tool-call tokens. This ensures that the model's output can be correctly parsed into structured tool calls, enabling functional tool usage in the updated transformers library.

**Potential risks**  
Setting `skip_special_tokens=False` may cause unintended tokens (like special control tokens) to appear in the output if not properly managed. The template assumes a specific tool-call format; deviations in model output could lead to parsing failures. Additionally, the fix is tightly coupled to transformers 5.x behavior and may require updates if the library's decoding logic changes again.

**Key insights**  
Developers should verify that the new template aligns with the GLM-4.6V-Flash model's expected input format. The adjustment to `skip_special_tokens` is critical for compatibility but should be monitored for side effects in other tokenization scenarios. Consider adding validation to ensure tool-call tokens are always correctly identified and parsed.

---

## 25. [[Misc] Enable Paligemma's PrefixLM attention mask computation](https://github.com/vllm-project/vllm/pull/31725)


### Base Information

- **PR Number:** #31725
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-05 11:31:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31725/files) (2):**
  - `tests/models/multimodal/generation/test_common.py`
  - `vllm/config/model.py`

### Summary

**What changed and why**  
The PR enables Paligemma's PrefixLM attention mask computation, which was previously disabled because FlexAttention didn't support soft cap attention. With the recent Triton Attention backend update (#30386) adding soft cap support, Paligemma can now use bidirectional attention for multimodal positions.

**Technical impact**  
This change allows Paligemma to correctly process multimodal inputs with PrefixLM attention, improving model accuracy for image-text tasks. The update leverages the new Triton Attention backend capability, shifting from a workaround to proper support.

**Potential risks**  
If the Triton Attention backend has undiscovered bugs with soft cap handling, Paligemma's attention computations could produce incorrect results. The change also assumes all Paligemma variants use the same attention mechanism, which may not hold for future model versions.

**Key insights**  
Developers should verify that all Paligemma inference tests pass consistently and monitor for any performance regressions. This fix demonstrates how backend improvements can unlock previously disabled model features, highlighting the importance of keeping model configurations aligned with backend capabilities.

---

## 26. [Triton Attention: Support cross-layers blocks](https://github.com/vllm-project/vllm/pull/30687)


### Base Information

- **PR Number:** #30687
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-05 11:29:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30687/files) (2):**
  - `tests/v1/kv_offload/test_cpu_offloading.py`
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
This PR enables cross-layer blocks support for the Triton attention backend by adding a `get_kv_cache_stride_order` method. It also updates the CPU KV offloading test to include "TRITON_ATTN" in the attention backends list for testing.

**Technical impact**  
The new method defines the memory layout permutation for KV caches when cross-layer blocks are used, allowing Triton attention to correctly handle multi-layer KV cache structures. The test change ensures Triton attention is validated alongside other backends in CPU offloading scenarios.

**Potential risks**  
The stride order logic assumes a specific tensor dimension ordering that must remain consistent with the KV cache implementation. If the underlying cache shape changes, this permutation could become incorrect. The test modification might increase test execution time by adding another backend to the parameterization.

**Key insights**  
The implementation correctly separates cross-layer and single-layer stride logic. Developers should verify that the dimension indices in the returned tuples align with `get_kv_cache_shape` outputs. The test update demonstrates backward compatibility, but performance comparisons between backends should be monitored.

---

## 27. [[Bugfix] Add missing extra_tensors arg to DeviceCommunicatorBase.disp…](https://github.com/vllm-project/vllm/pull/31644)


### Base Information

- **PR Number:** #31644
- **Author:** [kzwrime](https://github.com/kzwrime)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-05 09:26:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31644/files) (2):**
  - `vllm/distributed/device_communicators/base_device_communicator.py`
  - `vllm/distributed/device_communicators/xpu_communicator.py`

### Summary

**What changed and why**  
The PR fixes a signature mismatch in the `DeviceCommunicatorBase.dispatch()` method by adding the missing `extra_tensors` parameter to the base class and the XPU communicator implementation. This resolves a TypeError when running MoE models with data parallelism on backends (like CPU) that rely on the base class implementation.

**Technical impact**  
This change ensures API consistency across all device communicator implementations, allowing the fused MoE layer to work correctly with data parallelism on any backend. The XPU communicator now properly forwards the `extra_tensors` argument to its underlying all2all manager.

**Potential risks**  
The fix adds a new parameter with a default value (`None`), maintaining backward compatibility. However, other backend-specific communicator implementations (if any exist beyond XPU) must also be updated to accept this parameter to avoid similar signature mismatches.

**Key insights**  
Always keep base class signatures synchronized with all concrete implementations and their callers. The use of `# type: ignore` comments in the XPU changes suggests potential type annotation issues in the underlying all2all manager that should be addressed separately for cleaner type safety.

---

## 28. [[Bugfix][CPU] Fix RotaryEmbedding fallback causing gibberish with --enforce-eager](https://github.com/vllm-project/vllm/pull/31643)


### Base Information

- **PR Number:** #31643
- **Author:** [rickychen-infinirc](https://github.com/rickychen-infinirc)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-05 09:25:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31643/files) (2):**
  - `vllm/model_executor/custom_op.py`
  - `vllm/model_executor/layers/rotary_embedding/base.py`

### Summary

**What changed and why**  
The fix addresses gibberish output on CPU when using `--enforce-eager` by correcting the fallback behavior for custom operations. The root cause was that several `CustomOp` subclasses (including `RotaryEmbedding`) lacked explicit `forward_cpu()` methods, causing them to incorrectly delegate to `forward_cuda()` and use C++ CPU kernels instead of the correct PyTorch native implementations.

**Technical impact**  
This change modifies the base `CustomOp.forward_cpu()` default to delegate to `forward_native()` instead of `forward_cuda()`, ensuring consistent use of PyTorch-native implementations on CPU. For `RotaryEmbedding`, an explicit `forward_cpu()` method is added that calls the optimized `ops.rotary_embedding` kernel, maintaining performance while fixing correctness.

**Potential risks**  
The change assumes PyTorch-native implementations are always correct for CPU, which may not hold if there are device-specific optimizations or numerical differences. The `RotaryEmbedding.forward_cpu()` relies on an in-place operation (`ops.rotary_embedding`), which could cause side effects if the input tensors are used elsewhere. Other affected classes (`RMSNorm`, `GemmaRMSNorm`, `RMSNormGated`) still need their own `forward_cpu()` implementations.

**Key insights**  
Developers should add explicit `forward_cpu()` methods to all `CustomOp` subclasses used on CPU to avoid incorrect fallbacks. The fix prioritizes correctness over performance by ensuring native PyTorch execution paths are used. This highlights the importance of testing custom ops across all supported backends (CPU, CUDA, XPU) when `enforce_eager` or custom dispatch is enabled.

---

## 29. [[CI Failure] Disable B200 tests while runner is broken](https://github.com/vllm-project/vllm/pull/31732)


### Base Information

- **PR Number:** #31732
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-05 08:50:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31732/files) (1):**
  - `.buildkite/test-pipeline.yaml`

### Summary

**What changed and why**  
The PR disables four B200 GPU test steps in the CI pipeline by marking them as optional, due to ongoing connectivity issues with the Blackwell (B200) runners on DGX Cloud. This is a temporary workaround to prevent CI failures while the infrastructure problem is being resolved.

**Technical impact**  
These changes make the failing test steps non-blocking for the CI pipeline, allowing other tests to proceed normally. The pipeline will continue to execute but will not fail if these specific B200 tests cannot run, maintaining overall CI stability during the outage.

**Potential risks**  
If the infrastructure issue persists and these tests remain disabled for too long, regressions specific to B200 hardware or the tested features (V1 attention, quantization, and MoE) could go undetected. There's also a risk of forgetting to re-enable the tests once the runners are fixed, despite the TODO comments.

**Key insights**  
This is a pragmatic, temporary fix to unblock CI. The TODO comments with attribution are good practice for tracking. The team should monitor the infrastructure status and remove the `optional: true` flags promptly when the B200 runners are restored to ensure comprehensive testing coverage.

---

## 30. [[Frontend] [Doc] Exclude log deltas feature](https://github.com/vllm-project/vllm/pull/30322)


### Base Information

- **PR Number:** #30322
- **Author:** [Catacomba](https://github.com/Catacomba)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-05 08:34:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30322/files) (3):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/entrypoints/openai/serving_chat.py`

### Summary

**What changed and why**  
Added a `--exclude-log-deltas` flag to suppress logging of intermediate streaming deltas, logging only the final output. This reduces log verbosity during streaming while preserving request/response logging capabilities.

**Technical impact**  
The flag introduces a conditional check in the streaming generator to skip delta logging when enabled. It maintains backward compatibility by requiring `--enable-log-outputs` and `--enable-log-requests` as prerequisites, ensuring existing logging workflows remain unaffected.

**Potential risks**  
If the flag is used without `--enable-log-outputs`, validation will raise an error, but developers might overlook this dependency. Additionally, debugging streaming issues becomes harder without delta logs, though this is an intentional trade-off for reduced log volume.

**Key insights**  
This change is a clean, optional optimization for high-volume streaming environments. Developers should ensure both prerequisite flags are enabled when using `--exclude-log-deltas`. Consider documenting this flag in user-facing logs or help text to clarify its behavior and dependencies.

---

## 31. [[v1] Add encoder-only/cross attention support to Triton Attention backend](https://github.com/vllm-project/vllm/pull/31406)


### Base Information

- **PR Number:** #31406
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-05 08:00:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31406/files) (6):**
  - `tests/kernels/attention/test_triton_prefill_attention.py`
  - `tests/models/multimodal/generation/test_whisper.py`
  - `tests/v1/attention/test_attention_backends.py`
  - `vllm/attention/ops/triton_prefill_attention.py`
  - `vllm/platforms/rocm.py`
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
This PR adds encoder-only and cross-attention support to the Triton Attention backend by implementing a new Triton kernel (`context_attention_fwd`) adapted from SGLang. The changes enable Whisper model support on Turing/Volta GPUs (which lack FA support) and provide a balanced solution for incompatible head sizes after xformers deprecation. The kernel includes bidirectional/causal sliding window attention capabilities.

**Technical impact**  
The Triton Attention backend now supports all attention types (DECODER, ENCODER, ENCODER_ONLY, ENCODER_DECODER) through a unified implementation. Encoder attention bypasses KV caching entirely, using direct tensor computation with the new kernel. The architecture maintains backward compatibility while extending functionality to handle encoder models like Whisper and embedding models with sliding window attention.

**Potential risks**  
The sliding window implementation may have edge cases where entire rows are masked (resulting in -inf max values), though the kernel includes safeguards. FP8 quantization is explicitly unsupported for encoder attention paths. The removal of encoder-only fallback to FlexAttention in ROCm platforms could affect performance on AMD hardware for certain models.

**Key insights**  
Developers should note that encoder attention now uses a completely different code path without KV caching. The sliding window parameters are transformed differently for encoder vs decoder attention types. Testing coverage is comprehensive but should be expanded to verify performance parity across different hardware configurations, especially for the new encoder pathways.

---

## 32. [[Model] Let more models to support the score template.](https://github.com/vllm-project/vllm/pull/31335)


### Base Information

- **PR Number:** #31335
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-05 03:54:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31335/files) (23):**
  - `docs/models/supported_models.md`
  - `examples/pooling/score/convert_model_to_seq_cls.py`
  - `examples/pooling/score/offline_reranker.py`
  - `examples/pooling/score/offline_using_template.py`
  - `examples/pooling/score/online_using_template.py`
  - `examples/pooling/score/qwen3_reranker_offline.py`
  - `examples/pooling/score/qwen3_reranker_online.py`
  - `examples/pooling/score/template/bge-reranker-v2-gemma.jinja`
  - `examples/pooling/score/template/mxbai_rerank_v2.jinja`
  - `examples/pooling/score/template/qwen3_reranker.jinja`
  - `examples/pooling/score/using_template_offline.py`
  - `examples/pooling/score/using_template_online.py`
  - `tests/conftest.py`
  - `tests/models/language/pooling_mteb_test/mteb_score_utils.py`
  - `tests/models/language/pooling_mteb_test/test_baai.py`
  - `tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py`
  - `tests/models/language/pooling_mteb_test/test_cross_encoder.py`
  - `tests/models/language/pooling_mteb_test/test_gte.py`
  - `tests/models/language/pooling_mteb_test/test_jina.py`
  - `tests/models/language/pooling_mteb_test/test_mxbai_rerank.py`
  - `tests/models/language/pooling_mteb_test/test_nemotron.py`
  - `tests/models/language/pooling_mteb_test/test_qwen3_reranker.py`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
This PR extends support for score templates to additional reranker models (bge-reranker-v2-gemma, mxbai-rerank, qwen3-reranker, nemotron-rerank). It updates documentation, adds new example scripts, refactors the model conversion utility, and updates test suites to use standardized template handling. The changes ensure these models can be properly formatted for the `LLM.score()` API.

**Technical impact**  
The modifications introduce a unified approach for handling reranker models with specific prompt templates. Key changes include: adding a "Score template" column to the supported models table, creating reusable example scripts for offline/online scoring, and refactoring test infrastructure to use a common `HFMtebCrossEncoder` base class. This reduces code duplication and improves maintainability.

**Potential risks**  
- The refactored test helpers assume all models use the same `chat_template` interface, which may not hold for future models.  
- Removing old example scripts (`offline_reranker.py`, `offline_using_template.py`, etc.) could break external references.  
- The updated `convert_model_to_seq_cls.py` script now includes more complex logic; incorrect token mappings could lead to misconfigured models.

**Key insights**  
- Developers should use the new `using_template_offline.py` and `using_template_online.py` as reference implementations for integrating new reranker models.  
- When adding support for additional models, ensure the template mapping in `get_chat_template()` and `get_hf_overrides()` is updated.  
- The test refactoring simplifies adding new models but requires adherence to the `MtebCrossEncoderMixin` interface for consistency.

---

## 33. [[platform] Support additional forward context for OOT](https://github.com/vllm-project/vllm/pull/31674)


### Base Information

- **PR Number:** #31674
- **Author:** [zzzzwwjj](https://github.com/zzzzwwjj)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-05 02:25:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31674/files) (2):**
  - `vllm/forward_context.py`
  - `vllm/platforms/interface.py`

### Summary

**What changed and why**  
Added `additional_kwargs` field to `ForwardContext` and a `set_additional_forward_context` method in the platform interface. This allows OOT (out-of-tree) plugins to inject custom parameters into the forward context that are only needed by specific platforms or extensions.

**Technical impact**  
The changes extend the forward context mechanism to be more extensible without modifying core logic. OOT plugins can now override `set_additional_forward_context` to pass platform-specific data through the forward pipeline, maintaining separation of concerns while enabling customization.

**Potential risks**  
If OOT plugins return large or complex objects in `additional_kwargs`, it could increase memory usage or serialization overhead. There's also a risk of key collisions if multiple plugins write to the same dictionary without coordination. The default empty dict ensures backward compatibility.

**Key insights**  
This is a clean extension point that follows the open-closed principle. Developers implementing platform-specific features should ensure their additional context is lightweight and use unique keys to avoid conflicts. Consider adding validation or namespacing for keys in future iterations.

---

## 34. [[KVconnector][LMCache] remove the import of legacy LMCache code](https://github.com/vllm-project/vllm/pull/31704)


### Base Information

- **PR Number:** #31704
- **Author:** [ApostaC](https://github.com/ApostaC)
- **Merged By:** [KuntaiDu](https://github.com/KuntaiDu)
- **Merged time:** 2026-01-05 02:11:01
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31704/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils.py`

### Summary

**What changed and why**  
This change removes the import and usage of the legacy LMCache configuration class (`Config`) from before version 0.3.11. The code now exclusively uses the newer `V1Config` class, eliminating the deprecated experimental flag logic that allowed switching between old and new configurations.

**Technical impact**  
The codebase now has a simpler, single-path configuration system for LMCache integration. The removal of conditional logic based on the `LMCACHE_USE_EXPERIMENTAL` environment variable means all deployments will use the V1 configuration format, reducing code complexity and maintenance overhead.

**Potential risks**  
If any existing deployments still rely on the legacy configuration format (by setting `LMCACHE_USE_EXPERIMENTAL=False`), they will break after this change. The warning message about deprecation has been removed, so users won't receive advance notice of the breaking change.

**Key insights**  
This is a breaking change that completes the migration from legacy LMCache to the V1 configuration system. Developers should ensure all deployments are updated to use V1 configuration files before deploying this change. Consider adding a version check or migration utility if backward compatibility is needed during transition periods.

---

## 35. [[LoRA] LoRA PDL improvement](https://github.com/vllm-project/vllm/pull/31660)


### Base Information

- **PR Number:** #31660
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-05 00:28:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31660/files) (1):**
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`

### Summary

**What changed and why**  
The PR optimizes LoRA (Low-Rank Adaptation) performance by adjusting the placement of a GDC (GPU Device Control) wait operation within a Triton kernel. Specifically, the `tl.extra.cuda.gdc_wait()` call is moved from a fixed location to inside the main computation loop, right after prefetching the LoRA weight `b`. This change aims to reduce synchronization overhead and improve pipeline efficiency during fused MoE (Mixture of Experts) LoRA operations.

**Technical impact**  
By repositioning the GDC wait to occur per iteration after loading the LoRA weight, the kernel can better overlap computation with memory operations, reducing idle time. This is reflected in the benchmark results showing slight improvements in output token throughput (2112.20 vs. 2090.29 tokens/s) and reduced mean time per output token (41.19 ms vs. 41.70 ms). The change maintains correctness by preserving synchronization but making it more fine-grained.

**Potential risks**  
If the GDC wait is not correctly aligned with dependencies, it could introduce race conditions or subtle synchronization errors, especially in multi-GPU or concurrent execution scenarios. The change assumes that waiting after loading `b` is sufficient; if other hidden dependencies exist earlier in the loop, this could lead to inconsistent results. Additionally, the impact on edge cases (e.g., varying `K` sizes or sparse token masks) should be validated.

**Key insights**  
This optimization demonstrates how careful placement of synchronization barriers can enhance kernel performance without altering logic. Developers should verify that the moved wait does not break any implicit ordering constraints, particularly for non-primary program paths. Consider adding comments to clarify why this specific placement is safe, and ensure thorough testing across different hardware configurations and input sizes to confirm robustness.

---

## 36. [[Model] Enable LoRA support for BLIP2](https://github.com/vllm-project/vllm/pull/31620)


### Base Information

- **PR Number:** #31620
- **Author:** [ppppqp](https://github.com/ppppqp)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-05 00:02:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31620/files) (2):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/blip2.py`

### Summary

**What changed and why**  
This PR adds LoRA support to the BLIP-2 model by implementing required methods from the `SupportsLoRA` interface. The changes include calculating vision tokens per image, adding LoRA mapping, and implementing token conversion methods between vision encoder and connector outputs.

**Technical impact**  
The BLIP-2 model now implements the `SupportsLoRA` interface, enabling Low-Rank Adaptation fine-tuning. The implementation correctly handles BLIP-2's architecture where each image produces `num_query_tokens` output tokens from the QFormer connector, and the vision encoder requires `_vision_tokens_per_image` tokens per image (patches + class token).

**Potential risks**  
The division operations in `get_num_mm_encoder_tokens` and `get_num_mm_connector_tokens` assume integer division but use `/` operator which returns float in Python. This could cause type issues since token counts should be integers. The assertions check divisibility but don't ensure integer results.

**Key insights**  
The implementation correctly understands BLIP-2's token flow: vision encoder processes patches + class token per image, QFormer outputs `num_query_tokens` per image. Use `//` instead of `/` for integer division in token calculation methods to ensure proper integer return types. Consider adding type hints or explicit `int()` conversion for clarity.

---

