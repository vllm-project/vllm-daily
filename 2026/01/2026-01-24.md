# vLLM Merged PR Report

**Report Date:** 2026-01-24 PST

**Total Merged PRs:** 19

---

## 1. [[Bugfix] fix encoder cache hang in Qwen3VL](https://github.com/vllm-project/vllm/pull/32684)


### Base Information

- **PR Number:** #32684
- **Author:** [JJJYmmm](https://github.com/JJJYmmm)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-24 21:17:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32684/files) (3):**
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`

### Summary

**What changed and why**  
This PR fixes an encoder cache hang issue in Qwen3VL by addressing video processing inconsistencies. The changes remove hardcoded video dimension overrides in tests, improve frame padding logic using `round_up`, and separate image/video size calculations to prevent incorrect dummy data generation that could cause caching problems.

**Technical impact**  
The modifications ensure proper alignment between dummy data generation and actual model processing by correctly calculating video dimensions based on processor constraints. Video processing now uses appropriate temporal patch size considerations, and test overrides are removed to prevent mismatches between test configurations and model capabilities.

**Potential risks**  
Removing test overrides could expose underlying issues with video resizing that were previously masked. The hardcoded `target_num_frames = 2` in dummy data generation might not adequately test edge cases with longer videos. Changes to minimum dimension calculations (from 1 to 2) could affect edge case handling for very small inputs.

**Key insights**  
The core issue was inconsistent video dimension calculations between dummy data generation and actual processing. Developers should ensure video processor constraints (temporal patch size, max pixels) are properly considered when calculating optimal dimensions. The `round_up` utility provides cleaner frame padding logic than manual modulo operations.

---

## 2. [[Docs] Fix Apple silicon include path in CPU installation docs](https://github.com/vllm-project/vllm/pull/32977)


### Base Information

- **PR Number:** #32977
- **Author:** [sjhddh](https://github.com/sjhddh)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-24 17:51:50
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32977/files) (1):**
  - `docs/getting_started/installation/cpu.md`

### Summary

**What changed and why**  
The PR fixes a documentation error where the Apple silicon installation guide incorrectly referenced `cpu.arm.inc.md` instead of `cpu.apple.inc.md`. This ensures the correct include file is used when building documentation for Apple silicon platforms.

**Technical impact**  
This change only affects documentation rendering—specifically the "Build image from source" section for Apple silicon. It corrects a broken include path, ensuring the documentation builds without errors and displays the proper content for Apple silicon users.

**Potential risks**  
The risk is minimal as this is a documentation-only change with no impact on runtime code. The only potential issue is if the referenced file (`cpu.apple.inc.md`) does not exist or contains incorrect content, but that would be a separate issue.

**Key insights**  
Always verify include paths in documentation to avoid broken builds. For similar documentation updates, ensure cross-references are validated against actual file names. Consider adding a pre-commit check or CI step to validate include paths in Markdown files.

---

## 3. [[Perf][Kernel] Optimize FP4 quantization kernels (SM100F)](https://github.com/vllm-project/vllm/pull/32520)


### Base Information

- **PR Number:** #32520
- **Author:** [LopezCastroRoberto](https://github.com/LopezCastroRoberto)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-24 17:45:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32520/files) (18):**
  - `benchmarks/kernels/bench_nvfp4_quant.py`
  - `csrc/ops.h`
  - `csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu`
  - `csrc/quantization/fp4/nvfp4_experts_quant.cu`
  - `csrc/quantization/fp4/nvfp4_quant_entry.cu`
  - `csrc/quantization/fp4/nvfp4_quant_kernels.cu`
  - `csrc/quantization/fp4/nvfp4_utils.cuh`
  - `csrc/torch_bindings.cpp`
  - `tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py`
  - `tests/kernels/quantization/test_nvfp4_quant.py`
  - `vllm/_custom_ops.py`
  - `vllm/compilation/activation_quant_fusion.py`
  - `vllm/compilation/collective_fusion.py`
  - `vllm/compilation/fusion_attn.py`
  - `vllm/model_executor/layers/fused_moe/utils.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
This PR introduces hardware-gated optimizations for FP4 quantization kernels targeting SM100F GPUs with CUDA ≥ 12.9. The key changes include using 256-bit global loads (via PTX 8.8) to reduce memory instructions, an improved partitioning scheme for better occupancy, and support for both swizzled and non-swizzled scale-factor layouts. These optimizations aim to significantly boost kernel performance while maintaining numerical correctness.

**Technical impact**  
The optimizations substantially improve memory bandwidth utilization and reduce global memory instruction counts, leading to up to 65% faster quantization compared to previous vLLM implementations. The changes are conditionally enabled via compile-time macros (`NVFP4_ENABLE_ELTS16`) and runtime checks, ensuring compatibility only on supported hardware (SM100F, CUDA ≥ 12.9). The kernel now supports two scale-factor layouts (swizzled for Tensor Core compatibility and row-major for flexibility), with grid partitioning adjusted to enhance occupancy, especially for large K dimensions.

**Potential risks**  
The optimizations are tightly coupled to specific hardware and toolchain versions (SM100F, CUDA ≥ 12.9), which may limit portability and complicate testing on older GPUs. The use of inline PTX assembly and conditional compilation could increase maintenance complexity. Additionally, the introduction of new layout options (`is_sf_swizzled_layout`) requires careful integration across call sites to ensure consistent behavior.

**Key insights**  
Developers should verify that their deployment environment meets the strict hardware/toolchain requirements before enabling these optimizations. The performance gains are substantial, but the changes are invasive; thorough testing on target hardware is essential. Future work should consider fallback paths for unsupported GPUs and ensure the optimizations do not inadvertently affect other quantization kernels or model architectures.

---

## 4. [[DOC] [ROCm] Update doc for v0.14.1](https://github.com/vllm-project/vllm/pull/32998)


### Base Information

- **PR Number:** #32998
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-24 17:13:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32998/files) (1):**
  - `docs/getting_started/installation/gpu.rocm.inc.md`

### Summary

**What changed and why**  
Updated documentation references from vLLM version 0.14.0 to 0.14.1 for ROCm GPU installation instructions. This change aligns the documentation with the upcoming v0.14.1 release, ensuring users receive accurate installation commands.

**Technical impact**  
The changes are purely documentation updates that affect installation URLs and version specifications. No functional code or build processes are modified, so there is no impact on system behavior or architecture.

**Potential risks**  
If the v0.14.1 ROCm wheels are not yet available or properly staged at the specified URLs, users may encounter installation failures. Additionally, the PR must be merged only after the release to avoid broken links.

**Key insights**  
This is a routine documentation update for a version bump. Developers should verify that the new wheels are published before merging and ensure all related documentation (e.g., other GPU backends) is consistently updated if needed.

---

## 5. [[Feature] add session based streaming input support to v1](https://github.com/vllm-project/vllm/pull/28973)


### Base Information

- **PR Number:** #28973
- **Author:** [joshuadeng](https://github.com/joshuadeng)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-24 12:06:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28973/files) (16):**
  - `tests/v1/core/test_scheduler.py`
  - `tests/v1/e2e/test_streaming_input.py`
  - `tests/v1/streaming_input/__init__.py`
  - `tests/v1/streaming_input/test_async_llm_streaming.py`
  - `tests/v1/streaming_input/test_gpu_model_runner_streaming.py`
  - `tests/v1/streaming_input/test_scheduler_streaming.py`
  - `tests/v1/test_request.py`
  - `vllm/outputs.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/output_processor.py`
  - `vllm/v1/engine/utils.py`
  - `vllm/v1/request.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces session-based streaming input support to vLLM's v1 engine. It allows `AsyncLLM.generate` to accept an async generator of `StreamingInput` objects, enabling incremental input updates for low-latency interactive workloads (e.g., ASR). The design reuses vLLM's request-centric architecture and KV cache blocks, minimizing scheduler changes and avoiding re-prefill.

**Technical impact**  
The changes extend the core scheduler, engine interfaces, and GPU model runner to handle streaming sessions. Key additions include a new `RequestStatus.WAITING_FOR_STREAMING_REQ`, streaming queues in `Request` objects, and logic to merge incremental prompts while preserving KV and multimodal alignment. The scheduler now manages streaming updates by appending new prompt tokens and discarding the final unscheduled output token from previous chunks, ensuring correct token counting and cache reuse.

**Potential risks**  
- The current implementation discards the last sampled output token from each chunk, which may not suit all use cases (the PR notes this will be parameterized later).  
- Limited support for certain input types: pooling, `prompt_embeds`, `n > 1`, `output_kind == FINAL_ONLY`, and stop strings are not yet supported.  
- Complex state management increases the risk of bugs in edge cases like cancellation, garbage collection, or multimodal feature merging.  
- The scheduler's handling of duplicate request IDs was adjusted, which required skipping a priority scheduling test for further investigation.

**Key insights**  
- The architecture cleverly reuses existing KV cache blocks to avoid re-prefill overhead, making it efficient for streaming.  
- Developers should note the intentional token-discarding behavior and plan for the upcoming parameterization.  
- Extensive unit tests have been added, but integration testing under high load and varied streaming patterns is recommended to ensure robustness.  
- The changes are backward-compatible for non-streaming requests, maintaining existing API behavior.

---

## 6. [Using max_loras + 1 to construct grid in fused_moe_lora](https://github.com/vllm-project/vllm/pull/32277)


### Base Information

- **PR Number:** #32277
- **Author:** [yugong333](https://github.com/yugong333)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-24 09:39:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32277/files) (1):**
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`

### Summary

**What changed and why**  
The PR fixes an indexing issue in the fused MoE LoRA kernel where the grid dimension for LoRA adapters was incorrectly sized. The change increases the grid size from `max_loras` to `max_loras + 1` to accommodate the "no-LoRA" case (represented by `lora_id == -1`), ensuring correct bounds checking and memory access.

**Technical impact**  
This adjustment ensures the kernel correctly handles mixed batches containing both base model tokens and tokens with active LoRA adapters. The grid dimension now aligns with the actual number of possible states (including the no-LoRA case), preventing out-of-bounds errors and enabling proper parallelization across all adapter scenarios.

**Potential risks**  
If `max_loras` is zero or negative, the calculation `max_loras = tl.num_programs(axis=2) - 1` could underflow. Additionally, any downstream logic relying on the original grid size assumption may need validation. The test warnings about leaked resources suggest potential cleanup issues in test execution, though they appear unrelated to the core change.

**Key insights**  
Always account for sentinel values (like `-1`) when designing kernel grids. Developers should verify that similar patterns exist elsewhere in the codebase to ensure consistency. Consider adding an assertion to guard against invalid `max_loras` values and investigate the resource leaks in tests to maintain system stability.

---

## 7. [[CPU] Improve CPU Docker build](https://github.com/vllm-project/vllm/pull/30953)


### Base Information

- **PR Number:** #30953
- **Author:** [maryamtahhan](https://github.com/maryamtahhan)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-24 09:08:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30953/files) (3):**
  - `cmake/cpu_extension.cmake`
  - `docker/Dockerfile.cpu`
  - `docs/getting_started/installation/cpu.x86.inc.md`

### Summary

**What changed and why**  
This PR enhances the CPU Docker build process by replacing bind mounts with COPY operations for requirements files to ensure Podman compatibility, adds OCI labels to track build configurations (like CPU instruction sets), and introduces cross-compilation support via environment variables. These changes resolve build failures on macOS with Podman and enable building images for different CPU architectures regardless of the host's capabilities.

**Technical impact**  
The modifications improve container build portability across Docker and Podman, enable explicit control over CPU instruction sets for cross-compilation scenarios, and embed metadata directly into images via OCI labels. This allows users to verify build configurations post-build and supports building ARM images on x86 hosts (and vice versa) without requiring matching host CPU features.

**Potential risks**  
Using COPY instead of bind mounts may increase image layer sizes slightly if requirements files change frequently. The reliance on environment variables for cross-compilation could lead to misconfigured builds if users incorrectly set flags (e.g., enabling AVX512 on a non-AVX512 target). Additionally, the added complexity in build arguments may confuse users who are not familiar with CPU instruction sets.

**Key insights**  
Developers should update build scripts to leverage the new cross-compilation arguments when targeting different architectures. The OCI labels provide valuable debugging information; consider integrating label checks into CI/CD pipelines. Ensure documentation clearly distinguishes between auto-detection (default) and manual flag overrides to prevent misuse.

---

## 8. [[CPU Backend][BugFix] Fix failing Darwin pipelines](https://github.com/vllm-project/vllm/pull/33002)


### Base Information

- **PR Number:** #33002
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-24 09:02:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33002/files) (1):**
  - `.github/workflows/macos-smoke-test.yml`

### Summary

**What changed and why**  
The PR fixes failing Darwin (macOS) CI pipelines by adding `--no-build-isolation` to the `uv pip install -e .` command and installing `requirements/cpu-build.txt` first. This resolves a version mismatch where the build used torch 2.9.1 (from `pyproject.toml`) but runtime required torch 2.10.0 (from `cpu.txt`).

**Technical impact**  
This change ensures the CPU backend builds against the exact torch version specified in the runtime requirements, eliminating version incompatibility errors. It aligns the CI pipeline with the documented vLLM CPU installation instructions that recommend using `--no-build-isolation`.

**Potential risks**  
Using `--no-build-isolation` could potentially cause conflicts if other build dependencies have incompatible versions between the build and runtime environments. The change also adds a dependency on the `cpu-build.txt` file, which must be kept in sync with other requirement files.

**Key insights**  
Always ensure build and runtime dependency versions match, especially when using isolated build environments. The solution follows existing documentation patterns, but developers should verify that `cpu-build.txt` contains appropriate build-time dependencies for all target platforms. Consider whether this fix should be applied to other CI pipelines using similar configurations.

---

## 9. [[Tests] Replace flaky sleep with polling in test_background_cancel](https://github.com/vllm-project/vllm/pull/32986)


### Base Information

- **PR Number:** #32986
- **Author:** [sjhddh](https://github.com/sjhddh)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-24 08:39:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32986/files) (1):**
  - `tests/v1/entrypoints/openai/serving_responses/test_stateful.py`

### Summary

**What changed and why**  
The test replaced a fixed 0.5-second sleep with a polling mechanism that waits for the response status to change from "queued" before attempting cancellation. This addresses flakiness caused by variable server response times, using adaptive polling (0.1s intervals, up to 5s) instead of a hardcoded delay.

**Technical impact**  
The change improves test reliability by synchronizing test execution with actual server state transitions. It eliminates timing dependencies that could cause failures on slower CI machines while avoiding unnecessary waits on faster systems. The post-cancel verification now uses a loop with multiple retries to ensure status persistence.

**Potential risks**  
If the server takes longer than 5 seconds to transition from "queued," the test may still fail or incorrectly proceed to cancellation. The polling interval (0.1s) could add overhead in high-latency environments, though this is minimal. The retry loop for verifying cancellation assumes status stability, which might not hold if the server has eventual consistency.

**Key insights**  
Polling for state changes is preferred over fixed sleeps in integration tests to reduce flakiness. Consider extracting the polling logic into a reusable helper for similar tests. Ensure the timeout (5s) aligns with expected service-level agreements, and monitor for any new sporadic failures that could indicate deeper issues.

---

## 10. [[MLA] Fuse cat and qaunt for fp8 kv-cache](https://github.com/vllm-project/vllm/pull/32950)


### Base Information

- **PR Number:** #32950
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-24 08:03:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32950/files) (1):**
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
This PR introduces a fused operation for concatenating and quantizing FP8 key-value cache tensors in MLA attention. The change replaces manual tensor concatenation and quantization with a custom `_DecodeConcatQuantFP8` operation that performs these steps together, aiming to reduce overhead and improve performance through potential kernel fusion.

**Technical impact**  
The modification centralizes the concatenation and FP8 quantization logic into a reusable custom operation registered as `mla_decode_concat_quant_fp8`. This replaces the previous pattern of creating an empty tensor, copying slices, and then quantizing. The operation is designed to be compiled via `torch.compile` when `compile_native=True`, which could fuse the cat/reshape/quant/view operations into a single optimized kernel.

**Potential risks**  
The custom operation assumes specific tensor shapes and dimensions (concatenating along the last dimension). If `decode_ql_nope` and `decode_q_pe` have mismatched dimensions beyond the validated first two, it could lead to runtime errors. The static quantization with `GroupShape.PER_TENSOR` may not be optimal for all model configurations or hardware. There's also a risk if the custom op's compilation fails silently and falls back to a slower path.

**Key insights**  
This is a performance optimization that reduces Python-level overhead and enables potential kernel fusion. Developers should verify that the custom operation's shape assumptions hold for all model variants using MLA attention. The use of `torch.compile` suggests this is targeting inference performance, and the impact should be validated with benchmarks. Consider making the `compile_native` parameter configurable for debugging.

---

## 11. [Update CPU doc according to feedback](https://github.com/vllm-project/vllm/pull/32963)


### Base Information

- **PR Number:** #32963
- **Author:** [louie-tsai](https://github.com/louie-tsai)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-24 08:02:44
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32963/files) (3):**
  - `docs/benchmarking/dashboard.md`
  - `docs/models/hardware_supported_models/cpu.md`
  - `docs/models/hardware_supported_models/xpu.md`

### Summary

**What changed and why**  
This PR updates documentation to change "supported model" to "recommended model" for CPU and XPU hardware pages, aligning terminology with the TPU documentation and clarifying that while vLLM may functionally support many official models, only the listed models are tested for function and performance. It also updates a commit hash and corrects an environment variable in a benchmarking example.

**Technical impact**  
The changes are purely documentation and terminology updates with no functional impact on the codebase. They improve clarity for users by setting accurate expectations about model support versus recommendation, and fix a minor error in a Docker command example.

**Potential risks**  
There is minimal risk as these are non-code changes. However, the updated commit hash in the benchmarking example must correspond to a valid, existing Docker image tag; if incorrect, users may encounter errors when pulling the image. The environment variable change from `ON_ARM64_CPU` to `ON_CPU` should be verified for consistency across the codebase.

**Key insights**  
Documentation accuracy is crucial for user experience. Ensure that all referenced commit hashes and environment variables are validated. Consider adding a note explaining the distinction between "supported" and "recommended" models to prevent confusion. Future updates should maintain consistency across all hardware documentation pages.

---

## 12. [[Bugfix]: resolve torch.compile cache conflict between mm_encoder_tp_modes](https://github.com/vllm-project/vllm/pull/32842)


### Base Information

- **PR Number:** #32842
- **Author:** [HirokenOvo](https://github.com/HirokenOvo)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-24 06:45:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32842/files) (2):**
  - `vllm/config/multimodal.py`
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
This PR fixes a torch.compile cache conflict when switching between `mm_encoder_tp_mode` values ("weights" vs "data") for the vision encoder in Qwen2.5-VL. The issue occurred because `mm_encoder_tp_mode` was not included in the configuration hash, causing incorrect cached kernels to be reused across different tensor-parallel strategies, leading to shape assertion errors.

**Technical impact**  
The changes ensure that `mm_encoder_tp_mode` is factored into the hash calculation in `MultiModalConfig.compute_hash()`. Additionally, `VllmConfig.compute_hash()` now conditionally includes the multimodal config hash only when `compile_mm_encoder` is enabled, preventing unnecessary hash variations for non-compiled runs while isolating caches correctly for compiled ones.

**Potential risks**  
If `mm_encoder_tp_mode` is modified in future multimodal configurations without updating the hash logic, similar cache conflicts could reoccur. The conditional inclusion of the multimodal hash relies on the `compile_mm_encoder` flag; any misalignment between this flag and actual compilation states might lead to missed cache isolations or redundant hashing.

**Key insights**  
Always include all configuration parameters that affect kernel generation in hash computations to avoid torch.compile cache collisions. The conditional hash inclusion pattern is a good practice to minimize unnecessary cache fragmentation, but ensure it aligns precisely with the compilation behavior. Reviewers should verify that no other multimodal parameters are missing from the hash factors.

---

## 13. [[EncoderCacheManager] Remove unnecessary copy](https://github.com/vllm-project/vllm/pull/32800)


### Base Information

- **PR Number:** #32800
- **Author:** [lgeiger](https://github.com/lgeiger)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-24 06:28:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32800/files) (1):**
  - `vllm/v1/core/encoder_cache_manager.py`

### Summary

**What changed and why**  
Removed an unnecessary `.copy()` call when retrieving cached input IDs in the `free` method. The `get_cached_input_ids` method now always returns a new set, making the copy redundant since the original implementation was introduced in PR #13173.

**Technical impact**  
This change eliminates a shallow copy operation on a set of input IDs, slightly improving memory efficiency and performance when freeing encoder cache entries. The behavior remains unchanged because `get_cached_input_ids` already provides a fresh set.

**Potential risks**  
Low risk, assuming `get_cached_input_ids` consistently returns a new set as documented. If future modifications cause it to return a mutable shared reference, removing the copy could lead to unintended side effects during iteration.

**Key insights**  
Always verify that method contracts (like returning new collections) are upheld before removing defensive copies. This cleanup is safe and aligns with the current implementation, but developers should ensure any changes to `get_cached_input_ids` preserve its immutability guarantee.

---

## 14. [feat: Complete LoRA support for MiniMaxM2 Fixes #32736](https://github.com/vllm-project/vllm/pull/32763)


### Base Information

- **PR Number:** #32763
- **Author:** [Chenhao-Guan](https://github.com/Chenhao-Guan)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-24 04:48:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32763/files) (2):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/minimax_m2.py`

### Summary

**What changed and why**  
This PR completes LoRA support for MiniMaxM2ForCausalLM by adding the SupportsLoRA interface and embedding_modules configuration. The changes enable efficient fine-tuning with LoRA adapters without full model retraining, addressing issue #32736.

**Technical impact**  
The model now inherits from SupportsLoRA, allowing it to leverage existing LoRA infrastructure for adapter-based fine-tuning. The packed_modules_mapping (previously added) and embedding_modules enable proper parameter targeting for LoRA injections, ensuring compatibility with vLLM's LoRA implementation.

**Potential risks**  
If the embedding_modules configuration is incomplete or incorrectly specified, LoRA adapters may not apply to all intended layers (embed_tokens and lm_head). There's also a risk of breaking existing functionality if the SupportsLoRA interface introduces unexpected side effects in model inference or weight loading.

**Key insights**  
Developers should verify that all target modules for LoRA are correctly listed in embedding_modules. The change is minimal and builds on prior work (#31703), but thorough testing of LoRA fine-tuning workflows with MiniMaxM2 is recommended to ensure full compatibility.

---

## 15. [[Models]: Make Multimodal config implicit in ViT implementation](https://github.com/vllm-project/vllm/pull/31972)


### Base Information

- **PR Number:** #31972
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-24 04:34:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31972/files) (38):**
  - `vllm/model_executor/layers/attention/mm_encoder_attention.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/deepencoder.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/eagle2_5_vl.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2_siglip2.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/lightonocr.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/llava_next.py`
  - `vllm/model_executor/models/llava_next_video.py`
  - `vllm/model_executor/models/llava_onevision.py`
  - `vllm/model_executor/models/minimax_vl_01.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/moonvit.py`
  - `vllm/model_executor/models/opencua.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/siglip2navit.py`
  - `vllm/model_executor/models/tarsier.py`
  - `vllm/model_executor/models/vision.py`

### Summary

**What changed and why**  
This PR removes explicit passing of `MultiModalConfig` through ViT (Vision Transformer) model constructors. Instead, configuration parameters like attention backend overrides and tensor-parallel mode are accessed implicitly via `get_current_vllm_config()` and helper functions (`get_vit_attn_backend`, `is_vit_use_data_parallel`). The goal is to simplify the API and reduce boilerplate by centralizing config access.

**Technical impact**  
The changes significantly reduce parameter propagation across 38 files, removing ~470 lines of code. ViT-related modules now rely on global config state rather than explicit dependency injection. This centralizes config handling but introduces implicit dependencies on the global context, which may affect testability and modularity. The attention backend and TP mode decisions are now made inside helper functions that read from the current `VllmConfig`.

**Potential risks**  
- Implicit global state may lead to configuration errors if `get_current_vllm_config()` is not properly set (e.g., during unit tests or standalone model initialization).
- The change assumes that all ViT instances operate under the same multimodal config, which could be problematic if multiple configs are needed in a single process.
- Removing explicit parameters reduces clarity in function signatures, making it harder to understand dependencies at a glance.

**Key insights**  
- Developers should ensure `get_current_vllm_config()` is correctly initialized before any ViT model usage, especially in tests or non-standard workflows.
- The refactor improves code maintainability by eliminating repetitive config passing, but care must be taken to avoid hidden side effects.
- Future changes to multimodal config should be validated across all ViT models, as they now share a single access pattern.

---

## 16. [[Bugfix] Fix E2E latency calculation and add warmup support in mm_processor benchmark](https://github.com/vllm-project/vllm/pull/32646)


### Base Information

- **PR Number:** #32646
- **Author:** [HirokenOvo](https://github.com/HirokenOvo)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-24 02:31:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32646/files) (1):**
  - `vllm/benchmarks/mm_processor.py`

### Summary

**What changed and why**  
This PR fixes a bug in E2E latency calculation by replacing invalid metric field accesses (`finished_time`, `last_token_time`) with valid `RequestStateStats` fields (`first_token_latency`, `last_token_ts`, `first_token_ts`). It also adds a warm-up mechanism (`--num-warmups`) to stabilize benchmark results by excluding initial requests from statistics.

**Technical impact**  
The correction ensures accurate latency distribution metrics (P99/P50) instead of falling back to a simple average. The warm-up feature reduces variance in timing measurements by allowing system caches and internal states to stabilize before benchmarking, leading to more representative performance data.

**Potential risks**  
If `first_token_ts` or `last_token_ts` are not properly aligned (e.g., from different clock sources), latency calculations could be inaccurate. The warm-up implementation modifies `args.seed` for warm-up requests, which may affect reproducibility if the seed increment interacts with other random components. Edge cases where `decode_time` could be negative are handled with `max(0.0, ...)`.

**Key insights**  
Always validate metric field availability against the actual data structures (here, `RequestStateStats`). Warm-up periods are essential for stable benchmarks, but ensure they don’t inadvertently affect test conditions (e.g., seed changes). The fallback warning now clearly indicates when detailed metrics are unavailable, improving debuggability.

---

## 17. [[Perf] Cache exc.errors() result in validation exception handler](https://github.com/vllm-project/vllm/pull/32984)


### Base Information

- **PR Number:** #32984
- **Author:** [sjhddh](https://github.com/sjhddh)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-24 02:01:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32984/files) (1):**
  - `vllm/entrypoints/openai/api_server.py`

### Summary

**What changed and why**  
The change caches the result of `exc.errors()` in a local variable `errors` to avoid calling the method three separate times in the validation exception handler. This eliminates redundant method calls and slightly improves code readability.

**Technical impact**  
This is a minor performance optimization within a single error-handling function. It reduces the number of method invocations from three to one, which could provide a negligible performance benefit during request validation failures. The logical behavior and output of the handler remain completely unchanged.

**Potential risks**  
The risk is extremely low. The primary concern would be if `exc.errors()` returned a mutable object that was later modified, but caching it here is safe as the handler's execution is synchronous and contained. The change does not alter any external API or error response format.

**Key insights**  
This is a clean, safe refactor that follows good practice by avoiding repeated identical method calls. Developers should apply similar caching patterns for repeated expensive or identical calls in hot paths, but the impact here is minimal. Ensure the cached variable is used consistently, as done in this change.

---

## 18. [[UX] Deduplicate sampling parameter startup logs](https://github.com/vllm-project/vllm/pull/32953)


### Base Information

- **PR Number:** #32953
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-24 01:37:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32953/files) (4):**
  - `vllm/config/model.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`

### Summary

**What changed and why**  
The PR moves logging of default sampling parameters from multiple serving classes into the centralized `get_diff_sampling_param()` method. This eliminates duplicate log messages that appeared when multiple `OpenAIServing*` classes were instantiated during server startup, reducing log spam and improving UX.

**Technical impact**  
The logging logic is now consolidated in the model configuration layer, ensuring the warning about overridden sampling parameters is emitted only once per model configuration instance. This reduces redundant log output while maintaining the same informational content about parameter sources.

**Potential risks**  
The `logger.warning_once` with `scope="local"` may still produce multiple warnings if different model configurations are loaded. There's a risk that developers relying on the removed info logs for debugging might lose visibility into which serving class is using which parameters, though the warning provides equivalent information.

**Key insights**  
Centralizing logging at the configuration source is a clean architectural improvement. Developers should note that sampling parameter information is now only logged as a warning when parameters differ from vLLM defaults, not as info logs for every serving class. The warning message has been improved to show the exact source and parameter values.

---

## 19. [feat(benchmark): add encoder forward pass benchmarking to mm-processor](https://github.com/vllm-project/vllm/pull/31655)


### Base Information

- **PR Number:** #31655
- **Author:** [reaganjlee](https://github.com/reaganjlee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-24 00:24:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31655/files) (5):**
  - `vllm/benchmarks/mm_processor.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR adds encoder forward-pass latency observability to the multimodal processor benchmark and extends CLI support for HuggingFace datasets. It instruments the `embed_multimodal` call in `gpu_model_runner` to collect per-request timing (`encoder_forward_time`, `num_encoder_calls`), merges these stats with existing preprocessing metrics, and updates the benchmark to report the new measurements. Additionally, it introduces a `--dataset-name hf` option with required `--dataset-path` and validation.

**Technical impact**  
The changes enhance observability by splitting `total_time` into `preprocessor_total_time` and `encoder_forward_time`, providing finer-grained latency breakdowns. The encoder timing is collected per request via a thread-safe registry and aggregated across workers using max values. The benchmark output now includes encoder-specific metrics and an encoder call summary, while the CLI supports real HuggingFace datasets for more realistic benchmarking.

**Potential risks**  
The thread-safe registry in `gpu_model_runner` adds minimal overhead but could become a bottleneck under high concurrency. The request ID matching logic in `get_timing_stats_from_engine_client` assumes a specific suffix pattern for V1 engine request IDs, which may break if the ID format changes. The new CLI validation for HF datasets could reject valid custom datasets not in the predefined supported list.

**Key insights**  
Developers should note that `total_time` is replaced by `preprocessor_total_time` in all stats outputs, requiring updates to any downstream monitoring. The encoder timing is aggregated per request across workers using max values, which may overestimate latency in distributed setups. The new HF dataset support enables benchmarking with real-world data, but users must ensure their dataset paths match the supported list or extend validation accordingly.

---

