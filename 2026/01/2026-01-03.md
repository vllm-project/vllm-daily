# vLLM Merged PR Report

**Report Date:** 2026-01-03 PST

**Total Merged PRs:** 3

---

## 1. [fix no think of GLM-4.5 / GLM-4.7](https://github.com/vllm-project/vllm/pull/31449)


### Base Information

- **PR Number:** #31449
- **Author:** [zRzRzRzRzRzRzR](https://github.com/zRzRzRzRzRzRzR)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-03 19:43:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31449/files) (1):**
  - `vllm/reasoning/glm4_moe_reasoning_parser.py`

### Summary

**What changed and why**  
The GLM-4.5/GLM-4.7 reasoning parser has been completely replaced by inheriting from `DeepSeekR1ReasoningParser`. The previous custom implementation that extracted content between `<think>` and `</think>` tokens was removed, and the class now delegates all reasoning extraction logic to the DeepSeek-R1 parser.

**Technical impact**  
This change significantly simplifies the codebase by eliminating 163 lines of custom parsing logic and reusing an existing, tested parser. The GLM-4.5/GLM-4.7 models will now follow the same reasoning extraction approach as DeepSeek-R1 and Qwen3, which reportedly handles cases where reasoning tokens may not explicitly appear in the output.

**Potential risks**  
If the DeepSeek-R1 parser's extraction logic differs substantially from GLM's expected token behavior (e.g., different special tokens or formatting), it could lead to incorrect reasoning/content separation. Additionally, any bugs or limitations in the reused parser will now affect both model families.

**Key insights**  
This refactor promotes code reuse and reduces maintenance overhead, but developers should verify that the DeepSeek-R1 parser's behavior aligns with GLM-4.5/GLM-4.7's actual output patterns. Monitoring for regressions in reasoning extraction is advised, especially for edge cases where `<think>` tokens are absent or malformed.

---

## 2. [[Docs] Fix argparse include path for mm-processor benchmark](https://github.com/vllm-project/vllm/pull/31654)


### Base Information

- **PR Number:** #31654
- **Author:** [reaganjlee](https://github.com/reaganjlee)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-03 19:31:29
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31654/files) (1):**
  - `docs/cli/bench/mm_processor.md`

### Summary

**What changed and why**  
The change fixes a documentation include path for the mm-processor benchmark page. It updates the reference from `docs/argparse/bench_mm_processor.inc.md` to `docs/generated/argparse/bench_mm_processor.inc.md` to ensure command-line arguments are displayed correctly, aligning with other benchmark documentation pages.

**Technical impact**  
This is a documentation-only change that corrects a broken include path, restoring proper rendering of argparse-generated help text on the mm-processor benchmark documentation page. It ensures consistency with similar pages like the throughput benchmark.

**Potential risks**  
The risk is minimal as it only affects documentation rendering. However, if the target file `docs/generated/argparse/bench_mm_processor.inc.md` is missing or incorrectly generated, the page may still not display arguments properly.

**Key insights**  
Always verify include paths in documentation match the actual generated file locations. This fix highlights the importance of cross-referencing similar pages (e.g., throughput benchmark) to maintain consistency across the documentation suite.

---

## 3. [[MoE Refactor][13/N] Convert FI to Use PFNoEP](https://github.com/vllm-project/vllm/pull/31533)


### Base Information

- **PR Number:** #31533
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-03 12:26:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31533/files) (7):**
  - `vllm/model_executor/layers/fused_moe/all2all_utils.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`

### Summary

**What changed and why**  
This PR refactors FlashInfer FP8 MoE to use the standard `MoEPrepareAndFinalizeNoEP` class instead of custom TP implementations. It removes the shared-expert stream overlap logic from `ModularKernelMethod` since TP cases no longer use that class, simplifying the code and improving performance by enabling cleaner overlap with the router. The changes also fix a correctness issue, boosting GSM8K accuracy on Llama4 from 75% to over 90%.

**Technical impact**  
The refactor unifies the FP8 MoE code path by leveraging `MoEPrepareAndFinalizeNoEP` for non-data-parallel cases, eliminating the need for custom FlashInfer prepare/finalize logic in TP scenarios. This reduces code complexity and allows the removal of shared-expert stream handling from the modular kernel, shifting overlap management to the router. The changes affect multiple quantization backends (FlashInfer CUTLASS, AITER, Marlin, Triton/DeepGEMM) by standardizing their kernel initialization.

**Potential risks**  
Removing shared-expert stream overlap from `ModularKernelMethod` could impact performance for models using shared experts, though the PR claims improved overlap elsewhere. The conditional logic in `flashinfer_cutlass_prepare_finalize.py` now returns different types (`MoEPrepareAndFinalizeNoEP` vs. `FlashInferAllGatherMoEPrepareAndFinalize`), which may require careful handling in callers. There is a TODO note to migrate non-DP cases fully, indicating the refactor is incomplete.

**Key insights**  
Developers should note that the shared-expert stream is now managed outside the modular kernel, potentially altering performance characteristics for shared-expert models. The PR simplifies the codebase but introduces a hybrid return type in the factory function, which should be monitored for type consistency. Follow-up work is needed to apply similar changes to NVFP4 and remove non-DP support from FlashInfer All Gather/Reduce Scatter classes.

---

