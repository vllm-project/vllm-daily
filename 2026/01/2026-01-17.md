# vLLM Merged PR Report

**Report Date:** 2026-01-17 PST

**Total Merged PRs:** 9

---

## 1. [[Model Runner V2] Minor optimization for eagle input processing](https://github.com/vllm-project/vllm/pull/32535)


### Base Information

- **PR Number:** #32535
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-17 21:55:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32535/files) (2):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`

### Summary

**What changed and why**  
The changes optimize Eagle speculative decoding input processing by removing redundant tensor indexing operations. Instead of pre-indexing `last_sampled_tokens` and `next_prefill_tokens` using `input_batch.idx_mapping` before passing them to the speculator, the raw tensors are now passed directly, and the kernel handles the indexing internally.

**Technical impact**  
This reduces memory operations and simplifies the data flow by moving the indexing logic into the Triton kernel. The kernel now loads `req_state_idx` from `idx_mapping_ptr` and uses it to access the correct elements in `last_sampled` and `next_prefill_tokens` tensors, which have been documented as `[max_num_reqs]` sized tensors.

**Potential risks**  
The kernel now assumes `last_sampled` and `next_prefill_tokens` are sized `[max_num_reqs]` rather than `[num_reqs]`. If callers pass incorrectly sized tensors, it could cause out-of-bounds memory access. The documentation updates help, but runtime validation might be limited.

**Key insights**  
This is a sensible micro-optimization that reduces overhead in the Python layer. Developers should ensure all callers of the `propose` method provide tensors with the `[max_num_reqs]` dimension as documented. The change centralizes indexing logic, improving consistency.

---

## 2. [[Performance] Improve Triton prefill attention kernel's performance](https://github.com/vllm-project/vllm/pull/32403)


### Base Information

- **PR Number:** #32403
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-17 20:19:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32403/files) (3):**
  - `tests/models/language/pooling/test_token_classification.py`
  - `vllm/utils/math_utils.py`
  - `vllm/v1/attention/ops/triton_prefill_attention.py`

### Summary

**What changed and why**  
This PR optimizes the Triton prefill attention kernel by implementing two key changes: replacing `float("-inf")` mask values with `-1.0e8` to avoid separate invalid value masking for sliding window attention (SWA), and pre-scaling `sm_scale` by `1/ln(2)` to enable fusion of `triton.math.exp` into the faster `triton.math.exp2` operation.

**Technical impact**  
The kernel now uses `exp2` instead of `exp` for softmax computation, which is typically faster on GPU hardware. The mask value change simplifies the sliding window attention logic by using a large negative constant that reliably overflows in FP32 when exponentiated. The implementation also streamlines the online softmax algorithm by removing redundant masking checks and consolidating accumulation updates.

**Potential risks**  
Using `-1.0e8` instead of `-inf` could theoretically affect numerical stability in extreme cases, though the value is chosen to overflow in FP32. The increased tolerance in test assertions (from `1.2e-2` to `3.2e-2`) suggests minor numerical differences that require validation. The `exp2` optimization assumes consistent hardware behavior across platforms.

**Key insights**  
The benchmark shows consistent performance improvements across batch sizes (1-3% throughput gains). Developers should verify the numerical stability of the `-1.0e8` mask value across different hardware and precision settings. The test tolerance adjustments indicate acceptable numerical variance, but rigorous testing with edge cases (very long sequences, extreme sliding window configurations) is recommended before deployment.

---

## 3. [[MoE Refactor] Move Test Impl into Test Dirs](https://github.com/vllm-project/vllm/pull/32129)


### Base Information

- **PR Number:** #32129
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-17 20:16:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32129/files) (3):**
  - `docs/design/moe_kernel_features.md`
  - `tests/kernels/moe/test_moe.py`
  - `vllm/model_executor/layers/fused_moe/moe_torch_iterative.py`

### Summary

**What changed and why**  
The PR moves the iterative MoE implementation from production code into the test directory. Specifically, it removes `moe_torch_iterative.py` from the fused MoE layer module and inlines its `fused_moe` function directly into `test_moe.py` as `iterative_moe`. The documentation is updated to remove the "iterative" kernel from the features table.

**Technical impact**  
This change eliminates a production code dependency on a baseline/test implementation, simplifying the codebase structure. The iterative MoE function is now only available as a reference implementation within the test suite, which clarifies its role as a testing baseline rather than a production-ready kernel.

**Potential risks**  
If any other parts of the codebase were importing `iterative_moe` from the original module, they will now break. The PR description indicates the test plan is "ci," so existing CI tests should validate no regressions. However, external users or custom integrations that relied on this import path would be affected.

**Key insights**  
This is a clean-up refactor that improves separation of concerns by confining test utilities to test directories. Developers should ensure all references to the removed module are updated. The change also signals that the iterative MoE is not a supported production kernel, which is consistent with its removal from the documentation table.

---

## 4. [[Model Runner V2] Move mrope_positions buffer to MRopeState](https://github.com/vllm-project/vllm/pull/32532)


### Base Information

- **PR Number:** #32532
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-17 20:09:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32532/files) (4):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/mm/mrope_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The changes move the `mrope_positions` buffer from `InputBuffers` to `MRopeState` class. This refactoring centralizes M-RoPE related state management within the dedicated `MRopeState` class, improving code organization and separation of concerns. The modifications update all dependent functions to receive `mrope_positions` as a parameter or retrieve it from `MRopeState` when needed.

**Technical impact**  
This change improves architectural clarity by grouping M-RoPE specific functionality together. The `InputBuffers` class becomes simpler by removing M-RoPE specific buffers, while `MRopeState` now fully encapsulates all M-RoPE related state including positions, prefill positions, and delta values. Functions like `capture_graph`, `prepare_mrope_positions`, and model execution paths now consistently handle M-RoPE positions through the state object.

**Potential risks**  
The main risk involves ensuring all code paths properly handle the `mrope_positions` parameter which can now be `None`. The `assert mrope_positions is not None` check in `capture_graph` when `uses_mrope` is true could fail if the parameter isn't properly passed. Additionally, the `prepare_mrope_positions` function now operates on `self.mrope_positions` instead of receiving it as a parameter, which changes its interface and could affect callers.

**Key insights**  
This refactoring demonstrates good separation of concerns by moving modality-specific position handling to a dedicated class. Developers should ensure that `MRopeState` is properly initialized with `max_num_tokens` parameter (now added to constructor). When working with M-RoPE functionality, always check `self.uses_mrope` flag before accessing `mrope_positions` to avoid null pointer issues. The changes maintain backward compatibility for non-M-RoPE use cases.

---

## 5. [[Feature] Add FIPS 140-3 compliant hash algorithm option for multimodal hashing](https://github.com/vllm-project/vllm/pull/32386)


### Base Information

- **PR Number:** #32386
- **Author:** [karanb192](https://github.com/karanb192)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-17 19:02:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32386/files) (2):**
  - `vllm/envs.py`
  - `vllm/multimodal/hasher.py`

### Summary

**What changed and why**  
This PR adds support for configurable hash algorithms in multimodal content hashing, primarily to enable FIPS 140-3 compliance. The default algorithm remains `blake3`, but users can now set `VLLM_MM_HASHER_ALGORITHM` to `sha256` or `sha512` via an environment variable, allowing the system to meet stricter security requirements in government and enterprise deployments.

**Technical impact**  
The changes introduce a factory function `_get_hasher_factory` that dynamically selects the hash algorithm based on the environment variable, with the selection cached via `lru_cache`. This maintains backward compatibility while decoupling the hashing logic from a specific algorithm, making the system more flexible and compliant with security standards. The core hashing method `hash_kwargs` now uses this factory to instantiate the appropriate hasher.

**Potential risks**  
If the environment variable is set to an unsupported value (though validated), the factory could raise a `ValueError`. There is a minor performance difference between algorithms (`blake3` is fastest, `sha512` may be faster on 64-bit systems), which could affect throughput in high-volume hashing scenarios. Additionally, any future changes to the hashing interface must ensure all supported algorithms remain compatible.

**Key insights**  
Developers should note that `blake3` is not FIPS-compliant; for regulated environments, `sha256` or `sha512` must be explicitly configured. The factory pattern with caching is efficient and scalable. Ensure any new hash algorithms added in the future are validated in `envs.py` and integrated into the factory function to maintain consistency and security.

---

## 6. [[build] fix cu130 related release pipeline steps and publish as nightly image](https://github.com/vllm-project/vllm/pull/32522)


### Base Information

- **PR Number:** #32522
- **Author:** [Harry-Chen](https://github.com/Harry-Chen)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-01-17 18:36:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32522/files) (3):**
  - `.buildkite/release-pipeline.yaml`
  - `.buildkite/scripts/cleanup-nightly-builds.sh`
  - `.buildkite/scripts/push-nightly-builds.sh`

### Summary

**What changed and why**  
This PR fixes CUDA 13.0-related release pipeline steps and introduces a nightly image publishing workflow. The changes include updating CUDA version from 13.0.2 to 13.0.1, adding compute capability 12.1 for Blackwell GPUs, extracting a shared script (`push-nightly-builds.sh`) to avoid command duplication, and adding a separate nightly publishing step for CUDA 13.0 images.

**Technical impact**  
The pipeline now correctly builds and publishes CUDA 13.0 nightly images alongside the default CUDA images. The refactoring into reusable scripts reduces duplication and improves maintainability. The updated `torch_cuda_arch_list` ensures compatibility with newer GPU architectures like RTX 50 series and DGX Spark.

**Potential risks**  
Changing the CUDA version from 13.0.2 to 13.0.1 could introduce subtle compatibility differences with dependencies. The new tagging scheme (`cu130-nightly`) must be consistently applied across all scripts to avoid cleanup conflicts. The cleanup script modifications assume proper prefix handling, which could fail if tags are incorrectly formatted.

**Key insights**  
Always validate CUDA version changes with NVIDIA to ensure alignment with their recommendations. The script extraction is a good practice for reducing duplication in CI/CD pipelines. Ensure the cleanup script’s prefix logic is thoroughly tested to prevent accidental deletion of production tags.

---

## 7. [[CI/Build][Docker] Add centralized version manifest for Docker builds](https://github.com/vllm-project/vllm/pull/31492)


### Base Information

- **PR Number:** #31492
- **Author:** [mritunjaysharma394](https://github.com/mritunjaysharma394)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-17 05:45:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31492/files) (4):**
  - `.pre-commit-config.yaml`
  - `docker/Dockerfile`
  - `docker/versions.json`
  - `tools/generate_versions_json.py`

### Summary

**What changed and why**  
This PR introduces a centralized version manifest (`docker/versions.json`) to consolidate all pinned dependency versions previously scattered across the Dockerfile and install scripts. The changes establish the Dockerfile as the single source of truth for version defaults, with an auto-generated JSON file that integrates natively with Docker Buildx Bake for improved CI and tooling support.

**Technical impact**  
The Dockerfile ARG defaults now serve as the authoritative version source, enabling programmatic access via `versions.json`. This enhances build reproducibility and simplifies version overrides in Bake-based workflows. The auto-generation script ensures consistency between the Dockerfile and the manifest, while pre-commit hooks validate synchronization.

**Potential risks**  
If the generation script fails to parse complex ARG expressions or variable interpolations correctly, version mismatches could occur. Manual edits to `versions.json` would be overwritten and could cause confusion. Additionally, the script’s dependency on `dockerfile-parse` introduces a new external requirement that must be maintained.

**Key insights**  
Developers should update versions exclusively in the Dockerfile and regenerate the JSON file via the provided script. The `--check` flag is valuable for CI to prevent drift. This approach significantly improves maintainability and aligns with modern Docker Bake practices, but teams must ensure the generation script is robust against future Dockerfile changes.

---

## 8. [[Misc] Fix typo: seperator -> separator in flashmla_sparse.py](https://github.com/vllm-project/vllm/pull/32411)


### Base Information

- **PR Number:** #32411
- **Author:** [T1mn](https://github.com/T1mn)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-17 04:18:31
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32411/files) (1):**
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`

### Summary

**What changed and why**  
This PR fixes a typo in `flashmla_sparse.py` by renaming `FP8SeperatePrefillDecode` to `FP8SeparatePrefillDecode` (correcting "seperator" to "separator"). The change is purely cosmetic, aimed at improving code readability and maintaining consistent spelling.

**Technical impact**  
The change updates the class name and all references to it across the file, ensuring the codebase uses the correct spelling. Since this is a rename-only change with no functional modifications, it does not affect system behavior, performance, or architecture.

**Potential risks**  
The main risk is if any external code or dependencies reference the old class name (e.g., via imports or dynamic lookups), which could cause runtime errors. However, given the scope is limited to one file and the PR description indicates no functional changes, such external dependencies are unlikely.

**Key insights**  
Always verify that renaming changes are confined to the intended scope and do not break external interfaces. For future similar fixes, consider using IDE refactoring tools to ensure all references are updated consistently. Additionally, adding a unit test that imports the renamed class could help catch any missed references.

---

## 9. [[Model] Molmo2: Enable quantized weight mapping for vision backbone](https://github.com/vllm-project/vllm/pull/32385)


### Base Information

- **PR Number:** #32385
- **Author:** [George-Polya](https://github.com/George-Polya)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-17 01:33:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32385/files) (1):**
  - `vllm/model_executor/models/molmo2.py`

### Summary

**What changed and why**  
This PR enables FP8 quantized weight loading for the Molmo2 vision backbone by adding `prefix` parameters to vision layer components. The changes fix an import path and extend weight mapping patterns to support quantized weight names, which previously lacked proper prefix propagation for correct weight mapping during model loading.

**Technical impact**  
The modifications ensure that quantized linear layers (e.g., `ColumnParallelLinear`, `RowParallelLinear`) within the vision backbone receive the correct `prefix` argument, enabling the weight mapper to properly identify and load FP8 quantized weights. This maintains architectural consistency with other model components and allows the system to serve FP8 quantized Molmo2 models.

**Potential risks**  
Removing the trailing dot from weight mapping patterns could cause conflicts if other weight names contain these substrings as prefixes. The changes assume uniform prefix handling across all vision layers, which may not hold if future modifications introduce nested or differently structured components. There is also a risk of regression for non-quantized model loading if the prefix logic interferes with standard weight mapping.

**Key insights**  
Developers should verify that the prefix parameter propagation is consistent across all multimodal model architectures. The weight mapping pattern changes should be reviewed to ensure they don't inadvertently match unintended weight names. Consider adding tests for both quantized and non-quantized model loading to prevent regressions.

---

