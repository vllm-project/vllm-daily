# vLLM Merged PR Report

**Report Date:** 2026-01-09 PST

**Total Merged PRs:** 50

---

## 1. [[Bugfix] fix offline chat output prompt](https://github.com/vllm-project/vllm/pull/32076)


### Base Information

- **PR Number:** #32076
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 23:50:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32076/files) (1):**
  - `examples/offline_inference/basic/chat.py`

### Summary

**What changed and why**  
The PR fixes an issue where offline chat inference wasn't displaying prompt text in output. Since offline LLM chat tokenizes prompts to token IDs, the `output.prompt` field becomes `None`. The solution modifies the `print_outputs` function to accept separate prompt inputs alongside outputs.

**Technical impact**  
The `print_outputs` function signature now requires both outputs and prompts parameters, changing how it's called throughout the script. This maintains backward compatibility for display functionality while addressing the underlying issue that prompt text isn't available in `RequestOutput` objects during offline inference.

**Potential risks**  
The assertion `assert len(outputs) == len(prompts)` could fail if mismatched data is passed, potentially crashing the script. There's also a risk that developers might forget to update calls to `print_outputs` when reusing this pattern elsewhere, leading to runtime errors.

**Key insights**  
This fix properly separates concerns between model output data and input prompt data. Developers should note that offline inference requires explicit prompt passing since tokenization occurs before generation. Consider adding type hints for the `prompts` parameter to improve code clarity and maintainability.

---

## 2. [[Benchmark][1/2] Generalize SLA criterion validation from binary flags to margins](https://github.com/vllm-project/vllm/pull/32075)


### Base Information

- **PR Number:** #32075
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 23:11:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32075/files) (6):**
  - `tests/benchmarks/sweep/__init__.py`
  - `tests/benchmarks/sweep/test_param_sweep.py`
  - `tests/benchmarks/sweep/test_serve_sla.py`
  - `vllm/benchmarks/sweep/param_sweep.py`
  - `vllm/benchmarks/sweep/serve_sla.py`
  - `vllm/benchmarks/sweep/sla_sweep.py`

### Summary

**What changed and why**  
This PR transitions SLA validation from binary pass/fail checks to margin-based evaluation. It replaces the `validate` method with `compute_margin` across all SLA criterion classes (`<`, `<=`, `>`, `>=`) and updates the search algorithms in `serve_sla.py` to track margin histories. This refactoring prepares the codebase for a future PR that will implement spline interpolation for more efficient SLA boundary detection.

**Technical impact**  
The changes introduce a continuous margin metric (negative/zero for passing, positive for failing) instead of discrete boolean results. This enables more granular analysis and paves the way for advanced search techniques. The search functions now return history dictionaries mapping SLA variable values to margins, improving debuggability and providing richer data for interpolation algorithms.

**Potential risks**  
The `SLA_EPS` constant introduces a small epsilon offset for strict inequalities (`<` and `>`), which could cause subtle numerical issues if not carefully calibrated. The history tracking adds memory overhead for large search ranges, though this is minimal. Edge cases like out-of-bound searches are now explicitly tested, but the transition from boolean to margin logic requires thorough validation to ensure all SLA criteria behave identically to the previous implementation.

**Key insights**  
Developers should note that all SLA criteria now use a unified margin-based interface, making it easier to extend or modify validation logic. The history data returned by search functions provides valuable insights for debugging and future algorithmic improvements. Ensure that any downstream code consuming SLA validation results is updated to handle margins rather than booleans.

---

## 3. [[Bugfix] fix encoder cache leak of waiting requests in scheduler to solve stuck in CPU scheduling](https://github.com/vllm-project/vllm/pull/31857)


### Base Information

- **PR Number:** #31857
- **Author:** [frelam](https://github.com/frelam)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-09 22:27:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31857/files) (1):**
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
Added a call to free encoder cache for waiting requests that cannot be scheduled. This prevents waiting requests from holding encoder cache slots indefinitely, which was causing a deadlock in multimodal inference scenarios where the encoder cache became fully occupied by unschedulable requests.

**Technical impact**  
The change ensures that encoder cache slots are released when a request cannot be scheduled, allowing other requests to progress. This breaks the deadlock condition in the scheduler and improves resource utilization, particularly for encoder-decoder or multimodal models where encoder cache is a limited resource.

**Potential risks**  
If the encoder cache is freed prematurely (e.g., before the request is actually finished), it could lead to recomputation of encoder outputs when the request is later scheduled, increasing latency. Additionally, there may be edge cases where requests frequently oscillate between schedulable and unschedulable states, causing repeated cache allocation/free cycles.

**Key insights**  
This is a workaround that addresses the symptom (cache leak) but not the root cause. The underlying issue likely involves improper lifecycle management of encoder cache for waiting requests. A more robust solution would involve ensuring encoder cache is only held by actively running requests, not waiting ones. Consider adding metrics to monitor cache churn and investigating why waiting requests retain cache in the first place.

---

## 4. [[Misc] Delay deprecation of CommonAttentionMetadata properties](https://github.com/vllm-project/vllm/pull/32074)


### Base Information

- **PR Number:** #32074
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-09 21:06:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32074/files) (1):**
  - `vllm/v1/attention/backends/utils.py`

### Summary

**What changed and why**  
The PR delays the deprecation timeline for CPU-based fields (`_seq_lens_cpu`, `_num_computed_tokens_cpu`) and their accessors (`seq_lens_cpu`, `num_computed_tokens_cpu`) in `CommonAttentionMetadata`. The removal target is shifted from v0.14.0 to v0.15.0 due to dependent PRs (#31852, #32073) being more complex and not on track for v0.14.

**Technical impact**  
This change extends the availability of deprecated CPU-based attention metadata properties, allowing dependent code more time to migrate to device-based alternatives. It maintains backward compatibility for one additional release cycle, preventing breaking changes in v0.14.

**Potential risks**  
If the dependent PRs are not completed by v0.15.0, further deprecation delays may be needed, potentially prolonging technical debt. Additionally, users might continue relying on deprecated CPU-based fields, missing performance optimizations from device-based alternatives.

**Key insights**  
Developers should prioritize migrating from deprecated CPU fields to device-based `seq_lens` and derived CPU copies to avoid future breakage. Monitor progress on PRs #31852 and #32073 to ensure the v0.15.0 removal target is feasible.

---

## 5. [[ROCm][CI] Fix flaky `test_function_calling_with_stream` and reduce schema test examples](https://github.com/vllm-project/vllm/pull/32063)


### Base Information

- **PR Number:** #32063
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 21:02:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32063/files) (2):**
  - `tests/entrypoints/openai/test_openai_schema.py`
  - `tests/entrypoints/openai/test_response_api_with_harmony.py`

### Summary

**What changed and why**  
This PR addresses two distinct test failures. First, it fixes an `UnboundLocalError` in `test_function_calling_with_stream` by ensuring variables are initialized before use, as LLM responses are non-deterministic and may not produce the expected function call. Second, it reduces the `max_examples` in `test_openapi_stateless` to mitigate timeout failures caused by schemathesis generating malformed, slow-to-process requests.

**Technical impact**  
The changes improve test robustness by handling the non-deterministic nature of LLM outputs and reducing exposure to pathological test cases. The schema test adjustment is a pragmatic workaround that decreases test coverage slightly but ensures CI reliability by avoiding timeouts from problematic request generation.

**Potential risks**  
Reducing `max_examples` may decrease the fuzzing effectiveness of the schema test, potentially allowing edge-case bugs to slip through. The function-calling test fix assumes at least one tool call is present; if the model returns no tool calls at all, the assertion will fail, though this is now explicit. The timeout issue indicates the server may have inefficient validation for malformed Unicode payloads.

**Key insights**  
Always initialize variables before conditional assignment, especially when dealing with non-deterministic systems like LLMs. When fuzz testing with tools like schemathesis, consider balancing test thoroughness with CI stability. The underlying server-side validation inefficiency for garbage Unicode data should be investigated separately for a more robust fix.

---

## 6. [Update modelopt KV cache quantization resolution to new scheme](https://github.com/vllm-project/vllm/pull/31895)


### Base Information

- **PR Number:** #31895
- **Author:** [roikoren755](https://github.com/roikoren755)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-09 20:54:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31895/files) (1):**
  - `vllm/utils/torch_utils.py`

### Summary

**What changed and why**  
The PR updates vLLM's KV cache quantization resolution logic to support modelopt's new configuration scheme. Previously, modelopt stored quantization settings under `kv_cache_quant_algo`, but newer versions use `kv_cache_scheme`, which can be either a string or a dictionary. The changes ensure vLLM correctly interprets these new formats to avoid dtype mismatches during model execution.

**Technical impact**  
This modification extends the `get_kv_cache_quant_algo_string` function to first check for `kv_cache_scheme` before falling back to the legacy `kv_cache_quant_algo`. When the scheme is a dictionary, it maps specific configurations (e.g., `{"dynamic": false, "num_bits": 8, "type": "float"}`) to known quantization algorithms like "fp8". This maintains compatibility with both old and new modelopt versions and prevents incorrect dtype inference.

**Potential risks**  
If modelopt introduces new dictionary configurations not covered by the current mapping, they will trigger a warning and fall back to "auto", which may lead to suboptimal performance or dtype mismatches. Additionally, the warning message references `kv_cache_quant_algo` instead of `kv_cache_scheme`, which could cause confusion during debugging.

**Key insights**  
Developers should ensure that any future modelopt quantization schemes are added to the mapping logic to avoid silent fallbacks. The warning message should be updated to accurately reflect the field name (`kv_cache_scheme`). Testing with models using both old and new modelopt configurations is recommended to validate compatibility.

---

## 7. [[Refactor] Separate sequence and token pooling types](https://github.com/vllm-project/vllm/pull/32026)


### Base Information

- **PR Number:** #32026
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 20:53:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32026/files) (42):**
  - `tests/model_executor/test_model_load_with_params.py`
  - `tests/models/language/pooling/test_embedding.py`
  - `tests/models/language/pooling/test_mm_classifier_conversion.py`
  - `tests/models/language/pooling_mteb_test/mteb_embed_utils.py`
  - `tests/models/language/pooling_mteb_test/mteb_score_utils.py`
  - `tests/models/language/pooling_mteb_test/test_baai.py`
  - `tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py`
  - `tests/models/language/pooling_mteb_test/test_cross_encoder.py`
  - `tests/models/language/pooling_mteb_test/test_gte.py`
  - `tests/models/language/pooling_mteb_test/test_intfloat.py`
  - `tests/models/language/pooling_mteb_test/test_jina.py`
  - `tests/models/language/pooling_mteb_test/test_mxbai_rerank.py`
  - `tests/models/language/pooling_mteb_test/test_nemotron.py`
  - `tests/models/language/pooling_mteb_test/test_nomic.py`
  - `tests/models/language/pooling_mteb_test/test_qwen3_reranker.py`
  - `tests/models/language/pooling_mteb_test/test_snowflake_arctic_embed.py`
  - `tests/models/language/pooling_mteb_test/test_st_projector.py`
  - `tests/models/utils.py`
  - `tests/test_config.py`
  - `tests/test_pooling_params.py`
  - `vllm/config/model.py`
  - `vllm/config/pooler.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/model_executor/layers/pooler/seqwise/methods.py`
  - `vllm/model_executor/layers/pooler/seqwise/poolers.py`
  - `vllm/model_executor/layers/pooler/tokwise/methods.py`
  - `vllm/model_executor/layers/pooler/tokwise/poolers.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/bert_with_rope.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/gritlm.py`
  - `vllm/model_executor/models/interfaces_base.py`
  - `vllm/model_executor/models/internlm2.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/model_executor/models/qwen2_rm.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/roberta.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/pooling_params.py`
  - `vllm/tasks.py`
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
This refactor separates pooling configuration into distinct sequence and token pooling types (`seq_pooling_type` and `tok_pooling_type`), replacing the previous single `pooling_type` field. The change enables simultaneous support for both sequence-level (CLS/LAST/MEAN) and token-level (ALL/STEP) pooling methods, which is necessary for models that require different pooling strategies for different tasks.

**Technical impact**  
The changes affect the entire pooling architecture: `PoolerConfig` now has separate fields with backward compatibility via deprecation mapping, model interfaces define separate default pooling types, and pooling implementations use the appropriate getter methods. Key system properties like `attn_type`, `is_prefix_caching_supported`, and `is_chunked_prefill_supported` now derive from the new fields, altering caching and prefill behavior for certain pooling combinations.

**Potential risks**  
Backward compatibility relies on deprecated `pooling_type` mapping, which may cause confusion if both old and new fields are set simultaneously. The changes to caching/prefill logic could unintentionally disable optimizations for models using MEAN/CLS/STEP pooling. There is also a risk of inconsistent state if internal code continues to reference the old `pooling_type` field instead of the new getters.

**Key insights**  
Developers must update any code using `pooling_type` to use `seq_pooling_type` or `tok_pooling_type` explicitly. The refactor clarifies pooling semantics but requires careful validation of model behavior, especially for edge cases like reward models with STEP pooling. All new pooling-related features should be implemented using the separated types to maintain consistency.

---

## 8. [[Misc] Refactor ColumnParallelLinear: remove unused parameter and optimize forward](https://github.com/vllm-project/vllm/pull/31939)


### Base Information

- **PR Number:** #31939
- **Author:** [maang-h](https://github.com/maang-h)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-09 20:19:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31939/files) (1):**
  - `vllm/model_executor/layers/linear.py`

### Summary

**What changed and why**  
The changes remove an unused `output_sizes` parameter from `ColumnParallelLinear.__init__()` and optimize bias computation in the forward methods of three linear layer classes. The parameter removal is possible because the subclass `MergedColumnParallelLinear` sets `self.output_sizes` before calling the parent constructor. The bias computation is moved inside the conditional branch where it's actually needed, avoiding unnecessary operations.

**Technical impact**  
These are non-functional refactoring changes that improve code clarity and provide minor performance optimization. The removal of dead code and unused parameters simplifies the API and reduces maintenance overhead. The conditional bias calculation eliminates tensor operations in the common case where `return_bias=False`, which could provide measurable performance benefits in inference scenarios.

**Potential risks**  
The main risk involves backward compatibility for any code that might be passing the `output_sizes` parameter to `ColumnParallelLinear`. However, since the parameter was effectively unused (the subclass overrides it), this risk is minimal. There's also a theoretical risk if other subclasses depend on the removed parameter initialization logic, but the PR description indicates this was dead code.

**Key insights**  
This is a clean refactoring that follows good software engineering practices by removing unused code and optimizing hot paths. Developers should verify that no external code was passing `output_sizes` to `ColumnParallelLinear`. The performance optimization is particularly valuable for inference workloads where bias return is often unnecessary. Future similar optimizations could examine other conditional computations in forward methods.

---

## 9. [[Bugfix][Hardware][AMD] Use dynamic WARP_SIZE in sampler vectorized_process](https://github.com/vllm-project/vllm/pull/31295)


### Base Information

- **PR Number:** #31295
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-09 19:57:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31295/files) (1):**
  - `csrc/sampler.cu`

### Summary

**What changed and why**  
The change replaces a hardcoded `WARP_SIZE = 32` constant with a dynamic `kWarpSize` derived from the `WARP_SIZE` macro in `cuda_compat.h`. This ensures compatibility with AMD GPUs that use 64-wide wavefronts (e.g., MI300X) while maintaining support for 32-wide wavefronts on NVIDIA and AMD RDNA architectures.

**Technical impact**  
This aligns the sampler kernel with architecture-dependent warp sizes across the codebase, eliminating a maintenance hazard where inconsistent definitions could lead to latent bugs. The change is minimal and localized, affecting only the `vectorized_process` function’s static assertions and loop bounds, which now correctly scale with the actual hardware warp size.

**Potential risks**  
If `cuda_compat.h` defines `WARP_SIZE` incorrectly for a given architecture, kernel behavior may diverge. Additionally, any future warp-dependent logic added to this function must explicitly use `kWarpSize` to avoid reintroducing the shadowing issue. The static assertion (`kWarpSize >= items_per_scalar`) assumes `items_per_scalar` remains small; if this assumption changes, validation may fail on Wave32 hardware.

**Key insights**  
Always use the centralized `WARP_SIZE` macro for hardware portability. The `kWarpSize` naming convention (Google C++ style) effectively avoids macro shadowing. Developers should audit other kernels for similar hardcoded warp sizes and ensure `cuda_compat.h` is included where needed. This fix is part of a broader ROCm compatibility effort—consider cross-referencing related changes for consistency.

---

## 10. [Fuse RoPE and MLA KV-cache write](https://github.com/vllm-project/vllm/pull/25774)


### Base Information

- **PR Number:** #25774
- **Author:** [PatrykSaffer](https://github.com/PatrykSaffer)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-09 19:18:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/25774/files) (6):**
  - `CMakeLists.txt`
  - `csrc/cache.h`
  - `csrc/cache_kernels_fused.cu`
  - `csrc/torch_bindings.cpp`
  - `tests/kernels/core/test_rotary_embedding_mla_cache_fused.py`
  - `vllm/_custom_ops.py`

### Summary

**What changed and why**  
A new fused kernel `concat_and_cache_mla_rope_fused` was added to combine Rotary Position Embedding (RoPE) and MLA KV-cache writing into a single CUDA kernel. This reduces kernel launch overhead and memory traffic by performing RoPE on query/key tensors and writing the rotated key plus KV-cache components in one pass, rather than separate operations.

**Technical impact**  
The change introduces a new kernel without modifying existing execution paths, maintaining backward compatibility. It optimizes the inference pipeline for models using MLA (Multi-Layer Attention) KV-caches by fusing two computational steps, which can improve throughput and reduce latency, as evidenced by benchmark improvements in token throughput and TTFT.

**Potential risks**  
The kernel handles padded tokens (slot_idx < 0) but assumes correct input tensor dimensions and memory layouts. Edge cases include improper handling of different data types (e.g., FP8 quantization) and potential thread divergence due to conditional branches. The kernel’s correctness relies on precise tensor strides and alignment, which could lead to subtle bugs if inputs are malformed.

**Key insights**  
This optimization is a performance-focused addition that should be integrated into the model execution flow to realize benefits. Developers must ensure the kernel is correctly dispatched for supported data types and configurations. The extensive test suite provides good coverage, but integration testing is needed to validate end-to-end functionality with actual models.

---

## 11. [feature/issac 0.2](https://github.com/vllm-project/vllm/pull/31550)


### Base Information

- **PR Number:** #31550
- **Author:** [AkshatSh](https://github.com/AkshatSh)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-09 19:18:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31550/files) (4):**
  - `tests/models/multimodal/generation/test_common.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/transformers_utils/configs/isaac.py`

### Summary

**What changed and why**  
This PR updates the Isaac implementation to support Isaac-0.2, which introduces a `text_config` sub-configuration. The changes modify the `IsaacConfig` to properly handle `text_config` and update the model initialization to apply rope parameters correctly via `patch_rope_parameters`. Additionally, weight loading mappings are extended to accommodate the new model structure.

**Technical impact**  
The changes maintain backward compatibility with Isaac-0.1 while enabling support for Isaac-0.2's nested configuration. The model now correctly initializes rope parameters from `text_config` (or falls back to the root config) and adjusts weight loading prefixes to map `model.text_model.*` and `model.lm_head.*` to the appropriate internal modules.

**Potential risks**  
If `text_config` is provided as a dict but contains unexpected fields, initialization may fail. The fallback logic for rope parameters assumes compatibility between config versions, which could lead to subtle mismatches if the structure differs significantly. Additionally, the extended weight mappings might inadvertently affect other models if not carefully scoped.

**Key insights**  
Developers should verify that `text_config` and `rope_scaling` fields are correctly populated in Isaac-0.2 checkpoints. The use of `patch_rope_parameters` centralizes rope configuration handling, improving consistency. Ensure that any future config changes are reflected in both `IsaacConfig` and the model initialization logic to avoid regressions.

---

## 12. [[Misc][LLaMa4] Compile LLaMa Vision Encoder](https://github.com/vllm-project/vllm/pull/30709)


### Base Information

- **PR Number:** #30709
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-09 19:01:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30709/files) (7):**
  - `tests/compile/fullgraph/test_multimodal_compile.py`
  - `vllm/config/compilation.py`
  - `vllm/model_executor/layers/attention/mm_encoder_attention.py`
  - `vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/v1/attention/ops/vit_attn_wrappers.py`

### Summary

**What changed and why**  
This PR enables `torch.compile` for the LLaMa4 vision encoder (VisionEncoder + PixelShuffleMLP) to accelerate multimodal inference, similar to existing support for Qwen2.5-VL. Changes include gating compilation via `compile_mm_encoder`, fixing argument ordering in attention wrappers, optimizing rotary embedding cache handling, and adding dynamic-shape hints to reduce recompilations.

**Technical impact**  
The vision encoder path is now compiled when enabled, improving inference throughput and reducing latency for LLaMa4 multimodal models. The changes align compilation infrastructure across vision models, ensure proper argument plumbing in attention layers, and mitigate recompilation triggers through cache optimizations and shape hints.

**Potential risks**  
Compilation may introduce overhead during initial model loading or cause instability on untested hardware/platforms. The rotary embedding change avoids in-place updates but could affect memory usage if cache copies are frequent. The forked test is skipped in CI due to resource constraints, leaving some coverage gaps.

**Key insights**  
Developers should enable `compile_mm_encoder` in `CompilationConfig` for LLaMa4 to benefit from speedups. Ensure attention wrapper arguments match expected order when modifying related code. Monitor for recompilation events in production, as dynamic shapes may still trigger recompiles despite added hints.

---

## 13. [resolve pydantic error in startup benchmark](https://github.com/vllm-project/vllm/pull/31348)


### Base Information

- **PR Number:** #31348
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-09 18:41:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31348/files) (2):**
  - `tests/benchmarks/test_bench_startup.py`
  - `vllm/benchmarks/startup.py`

### Summary

**What changed and why**  
The PR fixes a Pydantic validation error that occurred when running `vllm bench startup`. The error arose because `CompilationConfig` fields were being incorrectly validated during subprocess spawning. The fix modifies `run_startup_in_subprocess` to accept an `EngineArgs` object directly instead of a dictionary, avoiding the problematic dict-to-model reconstruction that triggered the validation.

**Technical impact**  
This change simplifies the subprocess communication by removing the intermediate dictionary serialization step. The `EngineArgs` object is now passed directly, which eliminates the need to reconstruct the model in the subprocess and prevents Pydantic from validating fields that are not part of the `EngineArgs` schema. The addition of an end-to-end test ensures the benchmark command runs successfully.

**Potential risks**  
If the `EngineArgs` object contains non-pickleable attributes, multiprocessing could fail. Additionally, any future changes to `EngineArgs` that affect its serialization may break the subprocess communication. The test only checks for a successful exit code, which might not catch subtle performance regressions or timing inaccuracies in the benchmark.

**Key insights**  
Always validate that objects passed between processes are fully serializable. Consider adding more comprehensive benchmark validation, such as checking that timing metrics are within expected ranges. This fix highlights the importance of avoiding unnecessary model reconstruction when passing configuration objects across process boundaries.

---

## 14. [[Bugfix] Narrow broad exceptions in compilation backends](https://github.com/vllm-project/vllm/pull/31616)


### Base Information

- **PR Number:** #31616
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-09 18:39:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31616/files) (1):**
  - `vllm/compilation/backends.py`

### Summary

**What changed and why**  
The PR narrows a broad `except Exception:` handler to `except OSError:` when reading files during compilation cache key hashing. This change improves debuggability by allowing unexpected errors (like `MemoryError` or `KeyboardInterrupt`) to propagate rather than being silently caught and logged.

**Technical impact**  
The modification maintains the same functional behavior for expected file system errors (OSError covers IOError in Python 3) while reducing error masking. The cache initialization will still handle file access issues gracefully but will now properly surface unrelated system-level exceptions that should not be caught at this level.

**Potential risks**  
If the code encounters file-related exceptions not covered by OSError (though OSError is quite comprehensive in Python 3), they might now propagate unexpectedly. However, this is intentional and aligns with the PR's goal of better error visibility. The change assumes OSError adequately covers all expected file reading failures.

**Key insights**  
This change follows Python best practices for exception handling by catching specific exceptions rather than broad categories. Developers should ensure that any calling code is prepared to handle the narrower exception scope, though in this context, propagating unexpected errors is the desired behavior for improved system observability.

---

## 15. [[CI] Allow Deprecated Quantization For LM Eval Tests](https://github.com/vllm-project/vllm/pull/32065)


### Base Information

- **PR Number:** #32065
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-09 18:10:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32065/files) (1):**
  - `.buildkite/lm-eval-harness/test_lm_eval_correctness.py`

### Summary

**What changed and why**  
A single line was added to `test_lm_eval_correctness.py` to include `allow_deprecated_quantization=True` in the model arguments. This change aligns the LM Eval test suite with other updated tests, allowing deprecated quantization schemes (like FBGEMM) to run without errors during CI testing.

**Technical impact**  
The modification ensures that the LM Eval test harness can execute models using deprecated quantization methods, preventing CI failures. It maintains consistency with similar fixes in other test files, such as `test_full_graph.py`, and does not affect production code or runtime behavior outside of testing.

**Potential risks**  
There is a minor risk that enabling deprecated quantization in tests could mask issues related to the deprecation process itself. Additionally, if the deprecation timeline is accelerated, the test may need to be updated again to either remove this flag or adjust to new quantization requirements.

**Key insights**  
This change is a straightforward fix to keep CI green while deprecation efforts are ongoing. Developers should note that this flag is temporary and should be removed once deprecated quantization methods are fully phased out. Ensure other test files that use deprecated quantization are similarly updated to avoid inconsistent failures.

---

## 16. [[Perf] Optimize async scheduling placeholder using empty](https://github.com/vllm-project/vllm/pull/32056)


### Base Information

- **PR Number:** #32056
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-09 16:46:12
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32056/files) (1):**
  - `vllm/v1/engine/output_processor.py`

### Summary

**What changed and why**  
The change replaces a zero-length placeholder tensor created with `torch.randn(0, device="cpu")` with a pre-defined `torch.empty(0, device="cpu")` constant (`EMPTY_CPU_TENSOR`). This avoids unnecessary random number generation for a tensor that serves only as a non‑None placeholder in abort handling for pooling requests.

**Technical impact**  
This micro‑optimization reduces overhead in the abort path by eliminating a call to the random number generator. The shared constant `EMPTY_CPU_TENSOR` is allocated once and reused, which may slightly improve performance during frequent abort scenarios. The logic remains unchanged because both tensors are empty (size 0) and serve the same semantic purpose.

**Potential risks**  
The risk is minimal since the tensor is empty and only used as a sentinel value. However, if downstream code inadvertently inspects the tensor’s values (e.g., expecting random data), behavior could differ—though this is unlikely given the placeholder’s documented role. The constant’s reuse across requests is safe because the tensor is immutable in size and device.

**Key insights**  
This is a clean, low‑risk performance improvement that follows good practices: avoiding unnecessary computation and promoting reuse. Developers should note that `torch.empty` does not initialize memory, which is fine for placeholder usage. Similar patterns could be applied elsewhere in the codebase where zero‑size placeholders are needed.

---

## 17. [[Core] Use weights_only=True with torch.load](https://github.com/vllm-project/vllm/pull/32045)


### Base Information

- **PR Number:** #32045
- **Author:** [russellb](https://github.com/russellb)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-09 16:28:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32045/files) (1):**
  - `vllm/model_executor/model_loader/tensorizer.py`

### Summary

**What changed and why**  
The change adds `weights_only=True` parameter to `torch.load()` when loading `.bin` files in the LoRA adapter tensorization function. This ensures deserialization is restricted to loading only model weights, preventing arbitrary code execution via unsafe pickling. While PyTorch 2.6+ defaults to `weights_only=True`, this explicit setting enforces consistency with other usages and avoids false positives from security scanners.

**Technical impact**  
This modification hardens security for loading `.bin` format LoRA adapters by eliminating the risk of malicious payload execution during deserialization. The `.safetensors` loading path remains unchanged, as it inherently avoids pickle-based risks. The change aligns with broader codebase practices and future-proofs against potential default behavior changes in older PyTorch versions.

**Potential risks**  
If any existing `.bin` files contain non-weight data (e.g., custom objects), loading with `weights_only=True` may raise errors or fail silently, potentially disrupting LoRA adapter functionality. Additionally, reliance on PyTorch 2.6+ for the default behavior means older versions could still be vulnerable if this explicit parameter isn't universally applied elsewhere.

**Key insights**  
Explicitly setting `weights_only=True` is a security best practice that should be applied consistently across all `torch.load()` calls. Developers should audit other uses of `torch.load()` in the codebase to ensure similar hardening. Consider documenting or validating that `.bin` adapter files contain only serialized tensors to prevent runtime issues.

---

## 18. [[2/N][Attention] Fix pre-commit errors](https://github.com/vllm-project/vllm/pull/32052)


### Base Information

- **PR Number:** #32052
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-09 16:27:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32052/files) (3):**
  - `tools/pre_commit/mypy.py`
  - `vllm/v1/attention/backends/fa_utils.py`
  - `vllm/v1/attention/ops/paged_attn.py`

### Summary

**What changed and why**  
This PR fixes mypy pre-commit errors that emerged after moving attention-related files into regions with stricter type checking. The changes remove a file exclusion from the mypy configuration and refactor import patterns in two backend files to use direct imports instead of intermediate module aliases, ensuring proper type checking compliance.

**Technical impact**  
The removal of `fa_utils.py` from the mypy exclude list brings this file under active type checking, improving code quality and catching potential type errors early. The import refactoring simplifies the module structure by eliminating unnecessary intermediate `ops` variables, making the code more direct and easier to maintain.

**Potential risks**  
The targeted `type: ignore[no-redef]` comment in `paged_attn.py` could mask legitimate redefinition warnings. There's a minor risk that the import changes might affect other files that depend on the previous `ops` module structure, though this appears contained within the modified files.

**Key insights**  
These changes represent a positive step toward stricter type safety in the attention codebase. Developers should verify that no other files implicitly rely on the old `ops` variable pattern and consider whether the `type: ignore` comment is truly necessary or if the import could be restructured to avoid the warning entirely.

---

## 19. [[Misc][BE] Type coverage for vllm/compilation [2/3]](https://github.com/vllm-project/vllm/pull/31744)


### Base Information

- **PR Number:** #31744
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-09 15:30:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31744/files) (12):**
  - `vllm/compilation/backends.py`
  - `vllm/compilation/caching.py`
  - `vllm/compilation/cuda_graph.py`
  - `vllm/compilation/decorators.py`
  - `vllm/compilation/fix_functionalization.py`
  - `vllm/compilation/inductor_pass.py`
  - `vllm/compilation/noop_elimination.py`
  - `vllm/compilation/pass_manager.py`
  - `vllm/compilation/piecewise_backend.py`
  - `vllm/compilation/wrapper.py`
  - `vllm/config/compilation.py`
  - `vllm/distributed/device_communicators/pynccl_allocator.py`

### Summary

**What changed and why**  
This PR improves type hint coverage across the `vllm/compilation` module to enhance maintainability, readability, and error detection. Changes include adding explicit type annotations to function signatures, return types, and class attributes, replacing vague `Any`/`Callable` types with precise signatures, and ensuring consistent type safety across 12 files.

**Technical impact**  
The changes strengthen static type checking (via mypy) across the compilation stack, reducing runtime type-related errors. Key behavioral refinements include stricter validation in `piecewise_backend` (disallowing `"cudagraph_capture_sizes"` in `compile_sizes`), deterministic UUID generation in `PostGradPassManager`, and safer serialization in `caching.py`. No functional API changes are expected, but internal type safety is significantly improved.

**Potential risks**  
- New type strictness may reveal previously hidden type mismatches in downstream code.  
- The `# type: ignore[misc]` comments in several places (e.g., `PiecewiseCompileInterpreter`, `VllmSerializableFunction`) could mask unresolved mypy issues.  
- Changes to `piecewise_backend`'s `_find_range_for_shape` return type (`RangeEntry \| None` instead of `Range \| None`) may affect callers if not fully type-checked.

**Key insights**  
- Developers should run `mypy vllm/compilation` to verify no new type errors are introduced.  
- The added `__all__` exports (`CustomGraphPass`, `Range`) improve module clarity and should be used in imports.  
- Pay attention to the `compile_sizes` validation in `piecewise_backend` to avoid runtime `NotImplementedError` when using `"cudagraph_capture_sizes"`.

---

## 20. [[Misc] Enable async scheduling by default with spec decoding](https://github.com/vllm-project/vllm/pull/31998)


### Base Information

- **PR Number:** #31998
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-09 15:09:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31998/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The changes enable async scheduling by default when using compatible speculative decoding methods (EAGLE/MTP types), provided no incompatible configurations are present. This follows the resolution of previous gaps in async scheduling support with spec decoding. The logic now explicitly disables async scheduling for unsupported scenarios with clearer warnings.

**Technical impact**  
Async scheduling will now be automatically enabled for EAGLE/MTP speculative decoding, potentially improving throughput and resource utilization. The validation is tightened with scoped `warning_once` calls to reduce log spam, and error messages are more concise. The system will still disable async scheduling for pipeline parallelism, unsupported backends, or when `disable_padded_drafter_batch=True`.

**Potential risks**  
If the dependent PR (#30495) isn't merged first, enabling async scheduling by default could introduce instability. Edge cases may arise if users rely on the previous default behavior (disabled) or if there are hidden incompatibilities with specific executor backends not covered by the validation.

**Key insights**  
Developers should ensure PR #30495 is merged before this change. The updated warning system reduces noise but may obscure repeated issues in distributed logs. Verify that all EAGLE/MTP speculative decoding paths are thoroughly tested with async scheduling enabled to confirm performance gains and stability.

---

## 21. [[perf][async] support non cpu sync get logprob tensors for spec](https://github.com/vllm-project/vllm/pull/31336)


### Base Information

- **PR Number:** #31336
- **Author:** [izhuhaoran](https://github.com/izhuhaoran)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-09 13:24:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31336/files) (3):**
  - `vllm/v1/outputs.py`
  - `vllm/v1/sample/rejection_sampler.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR eliminates implicit CPU-GPU synchronization in speculative decoding's async scheduling path. Previously, `RejectionSampler` used dynamic boolean masking to identify accepted tokens, requiring CPU-side tensor size queries that stalled GPU execution. The solution computes logprobs for all draft tokens using static shapes and defers filtering of rejected tokens to the CPU-side `parse_output` stage.

**Technical impact**  
The changes decouple GPU computation from data-dependent CPU decisions, allowing GPU kernels to run without synchronization bubbles. This improves GPU utilization and throughput in async speculative decoding, as evidenced by benchmark results showing ~16% higher output token throughput and reduced latency metrics. The architecture now separates computation (GPU) from filtering logic (CPU), making the async path more efficient.

**Potential risks**  
Computing logprobs for rejected tokens increases temporary GPU memory usage for logprobs tensors, though this is offset by avoiding synchronization overhead. The `filter` operation on CPU could become a bottleneck if mask sizes are large, but this is mitigated by processing after GPU work completes. Edge cases around placeholder token handling (replaced with ID 0) require careful validation to avoid gather errors.

**Key insights**  
The PR demonstrates a effective pattern for async GPU workflows: move dynamic, data-dependent operations to CPU post-processing. Developers should apply similar strategies elsewhere in the codebase where dynamic tensor shapes cause synchronization. The benchmark improvements validate that reducing GPU stalls significantly boosts speculative decoding performance in high-concurrency scenarios.

---

## 22. [[NIXL] refine decoder side post process for heterogeneous BlockSize and kv_layout](https://github.com/vllm-project/vllm/pull/30275)


### Base Information

- **PR Number:** #30275
- **Author:** [xuechendi](https://github.com/xuechendi)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-09 13:22:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30275/files) (2):**
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR consolidates heterogeneous KV cache post-processing into a unified approach. It removes the separate `permute_device_kv` and `blocksize_post_process` methods and introduces a single `post_process_device_kv_on_receive` function that handles three scenarios: layout conversion, block-size conversion, or both. The logic is extracted into reusable utility functions (`kv_postprocess_*_on_receive`) to reduce duplication and improve maintainability.

**Technical impact**  
The changes streamline the decoder-side KV cache transformation pipeline by centralizing post-processing logic. This reduces code complexity and ensures consistent handling of heterogeneous block sizes and KV layouts (HND vs. NHD). The architecture now cleanly separates concerns, with utilities in `utils.py` and orchestration in `nixl_connector.py`, making the system more modular and easier to test.

**Potential risks**  
The refactoring assumes that `block_size_ratio >= 1` (local block size ≥ remote block size), which could break if remote blocks are larger. The consolidated logic depends on correct detection of `enable_permute_local_kv` and `block_size_ratio`; misconfiguration might lead to incorrect tensor layouts. Additionally, the removal of separate methods may affect debugging or monitoring of specific post-processing steps.

**Key insights**  
Developers should verify that all heterogeneous configurations (layout and block size) are covered by the updated test suite. The utility functions in `utils.py` are now critical shared components; any changes to KV cache formats must be reflected there. Ensure that `block_size_ratio` is always ≥1 in production scenarios to avoid unsupported cases.

---

## 23. [[1/N][Attention] Restructure attention: move files](https://github.com/vllm-project/vllm/pull/31916)


### Base Information

- **PR Number:** #31916
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-09 13:10:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31916/files) (195):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/kernels.yaml`
  - `.github/CODEOWNERS`
  - `.github/mergify.yml`
  - `benchmarks/kernels/benchmark_reshape_and_cache_flash.py`
  - `docs/contributing/model/basic.md`
  - `docs/design/custom_op.md`
  - `docs/design/plugin_system.md`
  - `examples/offline_inference/basic/embed.py`
  - `examples/offline_inference/basic/score.py`
  - `tests/compile/fullgraph/test_full_cudagraph.py`
  - `tests/compile/fullgraph/test_full_graph.py`
  - `tests/compile/test_fusion_attn.py`
  - `tests/compile/test_qk_norm_rope_fusion.py`
  - `tests/config/test_multimodal_config.py`
  - `tests/engine/test_arg_utils.py`
  - `tests/kernels/attention/test_aiter_flash_attn.py`
  - `tests/kernels/attention/test_attention.py`
  - `tests/kernels/attention/test_attention_selector.py`
  - `tests/kernels/attention/test_cache.py`
  - `tests/kernels/attention/test_flashmla.py`
  - `tests/kernels/attention/test_flashmla_sparse.py`
  - `tests/kernels/attention/test_merge_attn_states.py`
  - `tests/kernels/attention/test_mha_attn.py`
  - `tests/kernels/attention/test_pack_unpack_triton.py`
  - `tests/kernels/attention/test_prefix_prefill.py`
  - `tests/kernels/attention/test_rocm_attention_selector.py`
  - `tests/kernels/attention/test_triton_decode_attention.py`
  - `tests/kernels/attention/test_triton_prefill_attention.py`
  - `tests/kernels/attention/test_triton_unified_attention.py`
  - `tests/kernels/utils.py`
  - `tests/models/multimodal/generation/test_vit_backend_functionality.py`
  - `tests/models/quantization/test_fp8.py`
  - `tests/test_attention_backend_registry.py`
  - `tests/v1/attention/test_attention_backends.py`
  - `tests/v1/attention/test_mla_backends.py`
  - `tests/v1/attention/test_rocm_attention_backends_selection.py`
  - `tests/v1/attention/test_sparse_mla_backends.py`
  - `tests/v1/attention/utils.py`
  - `tests/v1/determinism/utils.py`
  - `tests/v1/kv_connector/unit/test_backwards_compatibility.py`
  - `tests/v1/spec_decode/test_eagle.py`
  - `tests/v1/spec_decode/test_mtp.py`
  - `tests/v1/spec_decode/test_tree_attention.py`
  - `tests/v1/worker/test_gpu_model_runner.py`
  - `tools/pre_commit/mypy.py`
  - `vllm/attention/layer.py`
  - `vllm/attention/ops/__init__.py`
  - `vllm/config/attention.py`
  - `vllm/config/model.py`
  - `vllm/config/multimodal.py`
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/base.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/envs.py`
  - `vllm/forward_context.py`
  - `vllm/model_executor/layers/attention/__init__.py`
  - `vllm/model_executor/layers/attention/chunked_local_attention.py`
  - `vllm/model_executor/layers/attention/cross_attention.py`
  - `vllm/model_executor/layers/attention/encoder_only_attention.py`
  - `vllm/model_executor/layers/attention/mm_encoder_attention.py`
  - `vllm/model_executor/layers/attention/static_sink_attention.py`
  - `vllm/model_executor/layers/attention_layer_base.py`
  - `vllm/model_executor/layers/batch_invariant.py`
  - `vllm/model_executor/layers/kda.py`
  - `vllm/model_executor/layers/mamba/abstract.py`
  - `vllm/model_executor/layers/mamba/linear_attn.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`
  - `vllm/model_executor/layers/mamba/short_conv.py`
  - `vllm/model_executor/models/afmoe.py`
  - `vllm/model_executor/models/aimv2.py`
  - `vllm/model_executor/models/apertus.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/bert_with_rope.py`
  - `vllm/model_executor/models/blip.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/deepencoder.py`
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/gemma3.py`
  - `vllm/model_executor/models/glm4.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/gpt_oss.py`
  - `vllm/model_executor/models/hunyuan_v1.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/idefics2_vision_model.py`
  - `vllm/model_executor/models/intern_vit.py`
  - `vllm/model_executor/models/interns1_vit.py`
  - `vllm/model_executor/models/iquest_loopcoder.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/llama4.py`
  - `vllm/model_executor/models/mimo_v2_flash.py`
  - `vllm/model_executor/models/minimax_text_01.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/moonvit.py`
  - `vllm/model_executor/models/nemotron_nas.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/openpangu.py`
  - `vllm/model_executor/models/ouro.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/qwen2.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/seed_oss.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/siglip2.py`
  - `vllm/model_executor/models/siglip2navit.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/transformers/base.py`
  - `vllm/model_executor/models/vision.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/model_executor/models/whisper_utils.py`
  - `vllm/platforms/cpu.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/interface.py`
  - `vllm/platforms/rocm.py`
  - `vllm/platforms/xpu.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/cpu_attn.py`
  - `vllm/v1/attention/backends/fa_utils.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/flash_attn_diffkv.py`
  - `vllm/v1/attention/backends/flashinfer.py`
  - `vllm/v1/attention/backends/flex_attention.py`
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/linear_attn.py`
  - `vllm/v1/attention/backends/mamba1_attn.py`
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/attention/backends/mla/common.py`
  - `vllm/v1/attention/backends/mla/cutlass_mla.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/attention/backends/mla/flashinfer_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/indexer.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/backends/mla/triton_mla.py`
  - `vllm/v1/attention/backends/registry.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/attention/backends/rocm_aiter_unified_attn.py`
  - `vllm/v1/attention/backends/rocm_attn.py`
  - `vllm/v1/attention/backends/short_conv_attn.py`
  - `vllm/v1/attention/backends/tree_attn.py`
  - `vllm/v1/attention/backends/triton_attn.py`
  - `vllm/v1/attention/backends/utils.py`
  - `vllm/v1/attention/ops/__init__.py`
  - `vllm/v1/attention/ops/chunked_prefill_paged_decode.py`
  - `vllm/v1/attention/ops/common.py`
  - `vllm/v1/attention/ops/flashmla.py`
  - `vllm/v1/attention/ops/merge_attn_states.py`
  - `vllm/v1/attention/ops/paged_attn.py`
  - `vllm/v1/attention/ops/pallas_kv_cache_update.py`
  - `vllm/v1/attention/ops/prefix_prefill.py`
  - `vllm/v1/attention/ops/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/ops/triton_decode_attention.py`
  - `vllm/v1/attention/ops/triton_merge_attn_states.py`
  - `vllm/v1/attention/ops/triton_prefill_attention.py`
  - `vllm/v1/attention/ops/triton_reshape_and_cache_flash.py`
  - `vllm/v1/attention/ops/triton_unified_attention.py`
  - `vllm/v1/attention/ops/vit_attn_wrappers.py`
  - `vllm/v1/attention/selector.py`
  - `vllm/v1/kv_offload/cpu.py`
  - `vllm/v1/kv_offload/spec.py`
  - `vllm/v1/kv_offload/worker/cpu_gpu.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/kv_connector_model_runner_mixin.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
This PR is the first step in restructuring the attention module as part of issue #31919. It involves moving attention-related files from `vllm/attention/` to new locations: core attention layers are moved to `vllm/model_executor/layers/attention/`, while backend, ops, and selector files are moved to the `vllm/v1/attention/` namespace. The changes are purely organizational—renaming files and updating import paths across the codebase—with no functional modifications intended.

**Technical impact**  
The restructuring centralizes attention layers within the model executor and establishes a clearer `v1` namespace for attention components, improving code organization and maintainability. This change affects nearly every model implementation, test, and configuration file that imports attention modules, requiring updates to import statements. The CI configuration, code ownership rules, and documentation references are also updated to reflect the new paths.

**Potential risks**  
While the PR aims for no functional changes, the large number of file moves (195 files) increases the risk of missed import updates or broken references, especially in less frequently tested code paths. The temporary exclusion of `fa_utils.py` and `ops` from mypy checks could hide type errors. Additionally, any downstream projects or plugins that directly import from the old paths will break until they update their imports.

**Key insights**  
Developers must update all import statements for attention modules to use the new `v1` paths. The restructuring is a foundational change that enables future attention system improvements, so careful review of import updates is critical. Ensure that all CI tests pass and that no hidden dependencies on the old paths remain, particularly in external integrations or custom backends.

---

## 24. [[responsesAPI] fix incomplete_messages for simple/parsable context](https://github.com/vllm-project/vllm/pull/31836)


### Base Information

- **PR Number:** #31836
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [yeqcharlotte](https://github.com/yeqcharlotte)
- **Merged time:** 2026-01-09 13:00:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31836/files) (4):**
  - `tests/entrypoints/openai/test_response_api_parsable_context.py`
  - `tests/entrypoints/openai/test_response_api_simple.py`
  - `vllm/entrypoints/openai/parser/responses_parser.py`
  - `vllm/entrypoints/openai/serving_responses.py`

### Summary

**What changed and why**  
The changes ensure the OpenAI Responses API correctly reports incomplete generations when output is truncated by `max_output_tokens` for both `SimpleContext` and `ParsableContext`. Previously, only GPTOSS models handled this case. Now, the parser stores the last `finish_reason`, and the server sets `status="incomplete"` when `finish_reason == "length"`.

**Technical impact**  
This extends the incomplete-message reporting feature to non-GPTOSS models, aligning behavior across different context types. The parser now retains `finish_reason` state, and the server uses it to determine response status, ensuring consistent API output when generation hits token limits.

**Potential risks**  
If `finish_reason` is not properly propagated or reset between requests, it could cause incorrect status reporting. The change assumes `"length"` is the only reason for incomplete status due to token limits; other finish reasons (e.g., `"stop"`) should not trigger this. There’s also a risk of missing edge cases where `finish_reason` is `None` or unexpected values.

**Key insights**  
Developers should verify that `finish_reason` is cleared per request to avoid cross-request contamination. The addition of tests for both completed and incomplete responses is commendable, ensuring robustness. Consider adding similar handling for other incomplete reasons (e.g., `"content_filter"`) if supported in the future.

---

## 25. [[Quant] Make static quant support all group shapes](https://github.com/vllm-project/vllm/pull/30833)


### Base Information

- **PR Number:** #30833
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-09 12:49:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30833/files) (7):**
  - `csrc/ops.h`
  - `csrc/quantization/w8a8/fp8/common.cu`
  - `csrc/torch_bindings.cpp`
  - `tests/kernels/quantization/test_fp8_quant.py`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`

### Summary

**What changed and why**  
This PR extends static FP8 quantization to support arbitrary 2D group shapes, enabling per-head quantization and other flexible quantization schemes. It introduces a new CUDA kernel with stride-based dispatch, vectorized paths, and group-aware indexing to handle per-tensor, per-channel, per-token, and arbitrary 2D group scaling configurations.

**Technical impact**  
The changes create a unified quantization framework where the `static_scaled_fp8_quant` function now accepts optional `group_shape` parameter and can process 0D, 1D, or 2D scale tensors. The kernel intelligently dispatches to optimized paths based on group dimensions and vectorization opportunities. This enables more granular quantization strategies while maintaining backward compatibility with existing per-tensor quantization.

**Potential risks**  
The 1D scale path requires explicit `group_shape` to disambiguate per-channel vs per-token quantization, which could cause confusion or errors if users forget to specify it. The scalar path for small column groups (group_n < VEC_SIZE) may have performance implications. Edge cases where tensor dimensions aren't divisible by group shapes could lead to runtime errors or skipped tests.

**Key insights**  
The implementation provides a foundation for per-head quantization (PR #30141) by enabling flexible group shapes. Developers should note that 1D scales now require explicit `group_shape` specification. The kernel's three execution paths (full-row vectorization, multi-group vectorization, scalar path) optimize for different scenarios, with the scalar path being less efficient for small groups.

---

## 26. [[fix] add cutedsl to global sf](https://github.com/vllm-project/vllm/pull/32001)


### Base Information

- **PR Number:** #32001
- **Author:** [jiahanc](https://github.com/jiahanc)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-09 12:03:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32001/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`

### Summary

**What changed and why**  
Added `FlashinferMoeBackend.CUTEDSL` to the list of backends that support global scale factors in the `is_flashinfer_supporting_global_sf` function. This change resolves issue #31918 by enabling the CUTEDSL backend to work correctly with global scale factor handling in FlashInfer's MoE quantization.

**Technical impact**  
The modification ensures that when the CUTEDSL backend is selected (via the `VLLM_FLASHINFER_MOE_BACKEND="masked_gemm"` environment variable, as indicated in the test plan), the system will correctly apply global scale factors. This aligns CUTEDSL's behavior with other supported backends like CUTLASS and TENSORRT_LLM, maintaining consistency in the quantization workflow.

**Potential risks**  
If the CUTEDSL backend does not fully implement or properly handle global scale factors in all scenarios, this change could introduce subtle numerical inaccuracies or runtime errors. Additionally, since the change is minimal and focused on a allowlist, any pre-existing issues with the CUTEDSL backend itself may now become more apparent when global scale factors are enabled.

**Key insights**  
This is a straightforward fix that extends existing functionality to a new backend, but it should be validated that CUTEDSL's implementation of global scale factors is robust and matches the behavior of other backends. Developers should ensure that the test plan's environment variables and configurations are documented or integrated into CI to prevent regressions.

---

## 27. [Add unpermute-aware fused MoE path and small-batch fallback](https://github.com/vllm-project/vllm/pull/29354)


### Base Information

- **PR Number:** #29354
- **Author:** [RunkaiTao](https://github.com/RunkaiTao)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-09 11:58:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29354/files) (2):**
  - `tests/kernels/moe/test_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
This PR adds an unpermute-aware execution path to the fused MoE kernel, introducing a `naive_block_assignment` mode that bypasses the expensive `moe_align_block_size` permutation step. It includes a heuristic to automatically select this path when the workload is sparse (few tokens activate many experts), improving performance for small-batch, many-expert scenarios.

**Technical impact**  
The changes modify the kernel to handle `sorted_token_ids = None`, adjust token offset calculations, and add a heuristic in `fused_experts_impl` to choose the naive path. This reduces data movement overhead for sparse MoE workloads, as shown by benchmark improvements (up to ~17% speed-up for small batch sizes). The existing permute path remains unchanged for dense workloads.

**Potential risks**  
The heuristic relies on a `SPARSITY_FACTOR` constant (currently 4) which may not generalize across all models or hardware. Edge cases include block-quantized models, which are explicitly excluded but may need future support. Incorrect heuristic decisions could degrade performance if the naive path is selected inappropriately for dense workloads.

**Key insights**  
The naive path is most beneficial for models with many experts and small batch sizes (e.g., ≤16 tokens). Developers should validate the heuristic’s threshold for their specific use cases. The PR includes comprehensive tests for large-expert configurations, ensuring correctness. Future work could extend support to block-quantized models and refine the heuristic based on real-world data.

---

## 28. [[Fix] Introduce audio channels spec](https://github.com/vllm-project/vllm/pull/31595)


### Base Information

- **PR Number:** #31595
- **Author:** [jeremyteboul](https://github.com/jeremyteboul)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 11:34:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31595/files) (9):**
  - `docs/features/multimodal_inputs.md`
  - `tests/models/multimodal/processing/test_qwen3_omni.py`
  - `tests/multimodal/test_audio.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/audio.py`
  - `vllm/multimodal/parse.py`

### Summary

**What changed and why**  
This PR introduces an extensible AudioSpec framework to handle audio channel normalization for multimodal models. It addresses the stereo-to-mono conversion issue where audio loaders (e.g., torchaudio) return `[channels, time]` format, but Whisper-based models expect mono audio as `[time]`. The framework includes a dataclass for audio format specification, channel reduction methods, and integration into the multimodal data parser.

**Technical impact**  
The changes create a generic audio normalization pipeline that automatically converts multi-channel audio to the target format (mono for Whisper-based models). This is integrated into the model processors via a new `get_target_channels()` method and the `MultiModalDataParser` now supports a `target_channels` parameter. The framework is designed to be extensible for future audio formats (e.g., stereo, surround sound).

**Potential risks**  
- The auto-detection logic for `(time, channels)` vs `(channels, time)` formats relies on a heuristic (`shape[0] > shape[1]`), which may fail for square matrices or unusual audio shapes.  
- Channel expansion (e.g., mono to stereo) is explicitly unsupported and will raise an error, limiting future use cases.  
- The changes affect multiple model families (Whisper, Qwen, Ultravox), so any bugs in the normalization logic could impact all supported audio models.

**Key insights**  
- The framework successfully decouples audio format handling from specific models, making it easier to add support for new audio requirements.  
- Developers should ensure audio inputs are in a supported shape (1D or 2D) and be aware of the format auto-detection behavior.  
- The comprehensive test suite added for `normalize_audio` and parser integration is critical for maintaining reliability across model families.

---

## 29. [[Perf] Optimize cutlass moe problem size calculation, 5.3% E2E Throughput improvement, 2.2% TTFT improvement](https://github.com/vllm-project/vllm/pull/31830)


### Base Information

- **PR Number:** #31830
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-09 11:13:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31830/files) (6):**
  - `csrc/ops.h`
  - `csrc/quantization/w8a8/cutlass/moe/moe_data.cu`
  - `csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu`
  - `csrc/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`

### Summary

**What changed and why**  
This PR introduces a new CUDA kernel (`compute_problem_sizes_from_expert_offsets`) to accelerate CUTLASS MoE problem-size calculations. Instead of counting tokens per expert from `topk_ids`, it directly computes problem sizes from pre‑computed `expert_first_token_offset` arrays, which are already available from the `moe_permute` kernel. This optimization reduces overhead and improves end‑to‑end throughput (5.3%) and time‑to‑first‑token (2.2%).

**Technical impact**  
The changes replace the existing `topk_ids`‑based problem‑size calculation with a more efficient offset‑based approach in both FP8 and W4A8 fused MoE paths. The new kernel is integrated through a dedicated C++/Python API (`get_cutlass_moe_mm_problem_sizes_from_expert_offsets`), and the problem‑size tensors are now allocated per local expert count (`local_E`) rather than global expert count, reducing memory usage. The `swap_ab` logic is also streamlined, using a simple heuristic (M ≤ 64) for FP8 and always enabled for RS GEMM in W4A8.

**Potential risks**  
The refactor assumes `expert_first_token_offset` is always available (even without expert parallelism), which could break if the offset tensor is incorrectly constructed. The `swap_ab` heuristic (M ≤ 64) is a performance tuning that may not generalize to all hardware or problem sizes. Additionally, the removal of `expert_map`‑based `topk_ids` translation could affect correctness in expert‑parallel scenarios if not properly validated.

**Key insights**  
This optimization successfully reduces computational overhead by leveraging existing expert‑offset data. Developers should ensure that `expert_first_token_offset` is correctly populated in all execution paths, including non‑expert‑parallel cases. The performance heuristic for `swap_ab` should be monitored across diverse workloads to avoid regressions. The changes also simplify the code by removing redundant `topk_ids` processing and unifying the offset handling in `moe_unpermute`.

---

## 30. [[Refactor] Remove numpy split in async scheduling](https://github.com/vllm-project/vllm/pull/32034)


### Base Information

- **PR Number:** #32034
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 11:09:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32034/files) (1):**
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
The code removes NumPy-based array splitting in the async output handler, replacing `np.array_split` with a simple Python `range` loop and slicing. This eliminates NumPy and `cdiv` imports, simplifying the dependency footprint and slightly improving performance in a hot path.

**Technical impact**  
The change maintains the same chunked processing behavior (controlled by `VLLM_V1_OUTPUT_PROC_CHUNK_SIZE`) and cooperative yielding via `await asyncio.sleep(0)`. It reduces external dependencies and replaces a heavyweight numerical library with lightweight Python iteration, which may lower memory overhead and improve startup time.

**Potential risks**  
If `outputs.outputs` is not a sliceable sequence (e.g., a custom iterator), the new indexing approach could fail. The loop assumes `num_outputs` accurately reflects the length of `outputs.outputs`; a mismatch could cause an `IndexError` or incomplete processing. Edge cases with very large `chunk_size` values or empty outputs should be validated.

**Key insights**  
This refactor is a positive simplification that aligns with minimizing dependencies in performance-critical paths. Developers should ensure `outputs.outputs` supports efficient slicing and that `num_outputs` is always in sync. Consider adding a safeguard for non-sequence outputs or using `itertools.islice` for broader compatibility.

---

## 31. [[Frontend][gpt-oss] Allow system message to overwrite model identity](https://github.com/vllm-project/vllm/pull/31737)


### Base Information

- **PR Number:** #31737
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-09 11:03:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31737/files) (3):**
  - `tests/entrypoints/openai/test_response_api_with_harmony.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`
  - `vllm/entrypoints/openai/serving_responses.py`

### Summary

**What changed and why**  
This change allows users to override the default system prompt by including a system message in their request, addressing use cases where a custom system prompt should replace rather than augment the default. The implementation modifies the Harmony integration to pass the user-provided system message as the `model_identity` parameter, while filtering out system messages from the regular message flow to prevent duplication.

**Technical impact**  
The system message from the request is now extracted and passed to `get_system_message()` as `model_identity`, overriding the default identity. System messages are excluded from the parsed message list in `_construct_input_messages_with_harmony`, ensuring they don't appear as developer messages. This maintains backward compatibility for non-system messages while enabling custom prompting.

**Potential risks**  
If the `model_identity` parameter in `get_system_message()` expects a specific format or validation, malformed system messages could cause errors. The filtering logic assumes only one system message exists per request; multiple system messages may lead to unexpected behavior. Edge cases where `request.input` is a string or contains nested structures might not be handled correctly.

**Key insights**  
Developers should ensure `get_system_message()` properly handles `None` and string values for `model_identity`. Consider adding validation or logging for system message extraction. The test suite comprehensively validates functionality but should be extended to cover edge cases like multiple system messages or empty content.

---

## 32. [[Feat][Core] Support multiple KV cache groups in Hybrid KV Coordinator](https://github.com/vllm-project/vllm/pull/31707)


### Base Information

- **PR Number:** #31707
- **Author:** [ivanium](https://github.com/ivanium)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-09 10:53:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31707/files) (2):**
  - `tests/v1/core/test_prefix_caching.py`
  - `vllm/v1/core/kv_cache_coordinator.py`

### Summary

**What changed and why**  
The PR refactors `HybridKVCacheCoordinator` to support an arbitrary number of KV cache groups (beyond the previous limit of two) and multiple attention types (full, sliding-window, Mamba). This enables emerging models that require mixed attention mechanisms with varying window sizes or configurations. The core change replaces the hardcoded two-group logic with a generic grouping mechanism and an iterative fixed-point algorithm for cache-hit detection.

**Technical impact**  
The coordinator now groups KV cache groups by identical `KVCacheSpec`, prioritizes `FullAttentionSpec` for efficient scanning, and computes the LCM of all block sizes for alignment. The `find_longest_cache_hit` algorithm iteratively constrains hit length across all attention types, reusing full-attention hits when possible. This removes architectural constraints and allows flexible hybrid model compositions without sacrificing correctness.

**Potential risks**  
The iterative fixed-point algorithm may introduce performance overhead with many groups, though prioritization of full-attention groups mitigates this. Edge cases where block sizes differ significantly could affect alignment and cache efficiency. The removal of interleaving restrictions may complicate debugging if group ordering assumptions exist elsewhere in the codebase.

**Key insights**  
Developers should verify that new hybrid model configurations adhere to block-size divisibility rules and LCM alignment. The updated test suite provides comprehensive coverage for 2–4 groups, interleaved specs, and sliding-window variants—use these as a reference for validation. Ensure any downstream code relying on the previous two-group assumption is updated to handle the generic tuple-based return format.

---

## 33. [[UX] Add vLLM model inspection view](https://github.com/vllm-project/vllm/pull/29450)


### Base Information

- **PR Number:** #29450
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-09 09:12:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29450/files) (6):**
  - `vllm/entrypoints/llm.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/rotary_embedding/common.py`
  - `vllm/model_executor/model_loader/base_loader.py`
  - `vllm/model_inspection.py`
  - `vllm/v1/worker/worker_base.py`

### Summary

**What changed and why**  
This PR adds a model inspection feature to vLLM that provides a hierarchical, transformers-style view of the loaded model structure. It enables users to see attention backends, quantization methods, and layer configurations via either setting `VLLM_LOG_MODEL_INSPECTION=1` or printing the `LLM` object. This addresses the initial request for a kernels view by offering visibility into model architecture details.

**Technical impact**  
The changes introduce a new `model_inspection.py` module with tree-formatting logic that groups identical layers and displays quantization schemes. The inspection is integrated into the model loading pipeline (via `base_loader.py`) and exposed through the `LLM` class’s `__repr__` method. A caching mechanism prevents repeated collective RPC calls in distributed settings, and environment variable control ensures optional logging.

**Potential risks**  
The layer-grouping logic assumes that modules with identical signatures are functionally equivalent, which may not hold if internal state differs. The caching in `LLM.__repr__` could become stale if the model changes after initialization (though this is unlikely). Additionally, the string-based signature comparison might be inefficient for very large models, though it only runs on demand.

**Key insights**  
Developers should note that the inspection output is primarily for debugging and visibility—it does not affect runtime performance. The feature elegantly handles complex models (e.g., MoE, VLMs) and distributed setups by returning consistent results across workers. Ensure that any future quantization methods or module types are compatible with the `_get_module_info` extraction logic.

---

## 34. [[Doc] Add developer guide for CustomOp](https://github.com/vllm-project/vllm/pull/30886)


### Base Information

- **PR Number:** #30886
- **Author:** [shen-shanshan](https://github.com/shen-shanshan)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-09 08:21:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30886/files) (24):**
  - `docs/design/custom_op.md`
  - `vllm/attention/layers/mm_encoder_attention.py`
  - `vllm/config/compilation.py`
  - `vllm/model_executor/custom_op.py`
  - `vllm/model_executor/layers/activation.py`
  - `vllm/model_executor/layers/conv.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/layernorm.py`
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/layers/logits_processor.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`
  - `vllm/model_executor/layers/mamba/short_conv.py`
  - `vllm/model_executor/layers/mla.py`
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`
  - `vllm/model_executor/layers/rotary_embedding/base.py`
  - `vllm/model_executor/layers/rotary_embedding/common.py`
  - `vllm/model_executor/layers/rotary_embedding/dual_chunk_rope.py`
  - `vllm/model_executor/layers/vocab_parallel_embedding.py`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/transformers/moe.py`

### Summary

**What changed and why**  
This PR adds comprehensive developer documentation (`docs/design/custom_op.md`) explaining the `CustomOp` system in vLLM, including its registration mechanism, platform-specific dispatch logic, and configuration controls. It also inserts snippet anchors (`--8<--`) into 23 existing `CustomOp` implementations across the codebase to enable documentation reuse, with minor clarifications to comments and docstrings.

**Technical impact**  
The changes are purely documentation and non-functional—no logic is altered. The new guide centralizes knowledge about `CustomOp` for both internal developers and OOT plugin authors, while the snippet anchors prepare the codebase for automated documentation generation. The clarified comments in `CompilationConfig` and `CustomOp.dispatch_forward` improve readability but do not affect runtime behavior.

**Potential risks**  
The snippet anchors could become stale if the referenced code blocks are moved or renamed, potentially breaking documentation generation. Additionally, the guide’s accuracy depends on ongoing maintenance as the `CustomOp` system evolves—especially noted temporary mechanisms like `enforce_enable` for multi-modal models.

**Key insights**  
This documentation fills a critical gap for developers extending vLLM with custom operations. Reviewers should verify that all anchored code snippets are correctly scoped and that the guide’s examples align with current `CustomOp` usage patterns. Future work should ensure the documentation stays updated, particularly when the multi-modal `compilation_config` separation is implemented.

---

## 35. [Rename --exclude-log-deltas to --enable-log-deltas](https://github.com/vllm-project/vllm/pull/32020)


### Base Information

- **PR Number:** #32020
- **Author:** [Catacomba](https://github.com/Catacomba)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 07:30:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32020/files) (3):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/entrypoints/openai/serving_chat.py`

### Summary

**What changed and why**  
The PR renames the `--exclude-log-deltas` flag to `--enable-log-deltas` (defaulting to `True`) to align with the standard `--enable`/`--no-enable` naming convention. It also removes the validation that required `--enable-log-outputs` when using `--exclude-log-deltas`, since the new flag is independent and only relevant when `--enable-log-outputs` is already set.

**Technical impact**  
This change standardizes flag naming across the codebase, improving consistency and reducing cognitive load for users. The default behavior remains unchanged (deltas are logged by default), but the flag now uses positive semantics. The validation logic is simplified, as `--enable-log-deltas` no longer directly depends on `--enable-log-outputs`, though it only takes effect when output logging is enabled.

**Potential risks**  
If users were relying on the old `--exclude-log-deltas` flag in scripts or automation, those invocations will break unless updated. The removal of validation could lead to confusion if `--no-enable-log-deltas` is used without `--enable-log-outputs`, as it would have no visible effect. Additionally, the default value change (from `False` to `True`) could subtly alter behavior for users who previously omitted the flag.

**Key insights**  
Developers should update any existing scripts using `--exclude-log-deltas` to use `--enable-log-deltas` or `--no-enable-log-deltas`. The flag’s dependency on `--enable-log-outputs` is now implicit rather than enforced, so documentation should clarify that delta logging only applies when output logging is enabled. This change improves API consistency but requires attention to backward compatibility.

---

## 36. [[Doc] Remove hardcoded Whisper in example openai translation client](https://github.com/vllm-project/vllm/pull/32027)


### Base Information

- **PR Number:** #32027
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 06:44:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32027/files) (1):**
  - `examples/online_serving/openai_translation_client.py`

### Summary

**What changed and why**  
The PR removes the hardcoded `openai/whisper-large-v3` model reference from the example OpenAI translation client. Both `sync_openai` and `stream_openai_response` functions now accept a `model` parameter, and the `main` function dynamically selects the first available model from the client's model list. This change makes the example more flexible for testing different translation models.

**Technical impact**  
The code changes introduce parameterization of the model selection, decoupling the example from a specific Whisper variant. The architecture now supports any model available through the OpenAI-compatible API endpoint, improving the example's utility for testing and demonstration purposes. The core functionality remains unchanged—only the model specification mechanism has been generalized.

**Potential risks**  
Selecting the first model via `client.models.list().data[0].id` may choose an unsuitable model for translation tasks if the list order changes or includes non-audio models. This could lead to runtime errors or incorrect behavior. Additionally, the change assumes the API endpoint supports all listed models for the translation endpoint, which may not hold true.

**Key insights**  
Developers should consider adding validation to ensure the selected model supports audio translation, potentially by checking model metadata or using a known model fallback. The example would benefit from a configurable model selection mechanism (e.g., via command-line argument or environment variable) to enhance usability and robustness in different testing scenarios.

---

## 37. [[Perf][Kernel] Fused SiLU+Mul+Quant kernel for NVFP4 cutlass_moe](https://github.com/vllm-project/vllm/pull/31832)


### Base Information

- **PR Number:** #31832
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-09 06:40:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31832/files) (8):**
  - `csrc/ops.h`
  - `csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu`
  - `csrc/quantization/fp4/nvfp4_experts_quant.cu`
  - `csrc/quantization/fp4/nvfp4_quant_entry.cu`
  - `csrc/quantization/fp4/nvfp4_utils.cuh`
  - `csrc/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`

### Summary

**What changed and why**  
This PR introduces a fused SiLU+Mul+NVFP4 quantization kernel for Mixture of Experts (MoE) models, generalizing existing dense NVFP4 fusion to expert quantization. The changes add a new kernel that combines the SiLU activation and element-wise multiplication operations directly within the quantization step, eliminating an intermediate tensor and kernel launch. This optimization improves latency by ~2% and throughput by ~4% as shown in benchmarks.

**Technical impact**  
The kernel fusion reduces memory bandwidth and kernel launch overhead by processing gate\|\|up layout inputs in a single pass. The implementation extends the existing `cvt_fp16_to_fp4` template with a `FUSE_SILU_MUL` boolean flag, reuses the `compute_silu_mul` utility (moved to a shared header), and adds a new Python binding. This maintains architectural consistency with the dense NVFP4 fusion while optimizing the MoE path.

**Potential risks**  
The fused kernel assumes input tensors have a specific gate\|\|up layout (width doubled), which could cause issues if incorrectly shaped. The removal of the intermediate `c2` workspace changes memory usage patterns, potentially affecting other operations that reuse buffers. Edge cases with very small expert counts or unusual tensor dimensions may need validation.

**Key insights**  
Developers should use the new `silu_and_mul_scaled_fp4_experts_quant` function for MoE layers to benefit from the fusion. Ensure input tensors follow the gate\|\|up layout requirement. The optimization demonstrates the value of kernel fusion for memory-bound operations, and similar patterns could be applied to other activation-quantization pairs in the codebase.

---

## 38. [[CPU] Add head sizes 80 and 112 with vec16 fallback](https://github.com/vllm-project/vllm/pull/31968)


### Base Information

- **PR Number:** #31968
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-09 06:14:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31968/files) (4):**
  - `csrc/cpu/cpu_attn.cpp`
  - `csrc/cpu/cpu_attn_amx.hpp`
  - `csrc/cpu/cpu_attn_neon.hpp`
  - `vllm/v1/attention/backends/cpu_attn.py`

### Summary

**What changed and why**  
The changes reintroduce support for head dimensions 80 and 112 in the CPU attention backend, which were previously removed. These head sizes are used by models like IBM Granite deployed on IBM Z architectures but are not optimal for Intel AMX instructions. The implementation now falls back to the vec16 ISA path for these head sizes.

**Technical impact**  
This expands model compatibility by adding two new head sizes to the dispatch table and supported-head-sizes list. The ISA selection logic now considers head size when choosing between AMX, NEON, VEC, and vec16 backends, ensuring non-32-multiple head sizes use vec16. Static assertions in AMX and NEON implementations are relaxed to avoid compilation failures for these cases.

**Potential risks**  
Performance may be suboptimal for head sizes 80 and 112 since they cannot leverage AMX or NEON optimizations designed for 32-byte multiples. The commented-out static asserts could hide future alignment issues if the code is reused for other head dimensions. There is also a risk of inconsistent behavior across architectures if the vec16 fallback is not uniformly efficient.

**Key insights**  
Developers should note that head sizes 80 and 112 will always use the vec16 path, which may affect performance benchmarks. The changes are backward compatible and safe for deployment, but further optimization for these head sizes on specific ISAs could be explored. Ensure any new head size additions follow the same pattern of ISA fallback logic.

---

## 39. [[Model] Remove redundant None check in DeepSeekOCR image input processing](https://github.com/vllm-project/vllm/pull/32016)


### Base Information

- **PR Number:** #32016
- **Author:** [maang-h](https://github.com/maang-h)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-09 06:12:44
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32016/files) (1):**
  - `vllm/model_executor/models/deepseek_ocr.py`

### Summary

**What changed and why**  
The change removes a redundant `if pixel_values is not None` check and an unreachable assertion in the `_parse_and_validate_image_input` method. Since lines 437-438 already handle the `None` or zeroed `pixel_values` case with an early return, the subsequent conditional and error path were unnecessary. This cleanup simplifies control flow and improves code readability.

**Technical impact**  
The logic remains functionally identical—the method still returns `None` for empty inputs or constructs a `DeepseekOCRImagePixelInputs` object otherwise. By eliminating dead code and redundant branching, the implementation becomes more straightforward and easier to maintain, with no effect on runtime behavior or external interfaces.

**Potential risks**  
The risk is minimal as the removed code was unreachable. However, developers should verify that the early return condition (`pixel_values is None or torch.sum(pixel_values).item() == 0`) comprehensively covers all invalid input cases, ensuring no edge cases inadvertently bypass validation.

**Key insights**  
This is a clean refactoring that enhances code clarity without altering functionality. It highlights the importance of reviewing control flow to eliminate redundant checks and dead code. Future modifications should maintain the early return pattern to keep the method simple and avoid reintroducing unnecessary complexity.

---

## 40. [Fix type error](https://github.com/vllm-project/vllm/pull/31999)


### Base Information

- **PR Number:** #31999
- **Author:** [Adolfo-Karim](https://github.com/Adolfo-Karim)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 06:03:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31999/files) (1):**
  - `vllm/entrypoints/responses_utils.py`

### Summary

**What changed and why**  
Added a null check for `item.id` before calling `startswith(MCP_PREFIX)` in `_maybe_combine_reasoning_and_tool_call`. This prevents `AttributeError` when `id` is `None`, which can occur during reasoning combined with MCP tool calls.

**Technical impact**  
The change ensures the function handles `ResponseFunctionToolCall` objects with `None` IDs gracefully, avoiding runtime exceptions. It maintains existing logic for valid IDs while adding a defensive guard, improving robustness without altering core behavior.

**Potential risks**  
If `item.id` is `None` due to an underlying data issue, the function will now skip processing silently. This could mask data integrity problems where IDs should never be `None`. Additionally, the check uses truthiness (`and item.id`), which would also skip empty strings—potentially unintended if empty IDs are valid.

**Key insights**  
Always validate object attributes before method calls, especially when dealing with external data. Consider logging or raising a more informative error for `None` IDs to aid debugging. Review whether `id` should be nullable in the data model to prevent similar issues elsewhere.

---

## 41. [[ROCm][PD] add moriio kv connector.](https://github.com/vllm-project/vllm/pull/29304)


### Base Information

- **PR Number:** #29304
- **Author:** [inkcherry](https://github.com/inkcherry)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-09 06:01:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29304/files) (10):**
  - `docker/Dockerfile.rocm_base`
  - `docs/getting_started/installation/gpu.rocm.inc.md`
  - `examples/online_serving/disaggregated_serving/moriio_toy_proxy_server.py`
  - `tests/v1/kv_connector/unit/test_moriio_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/factory.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/__init__.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_common.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_engine.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
This PR introduces the MoRIIO KV connector for AMD devices, enabling disaggregated KV cache transfer between prefill and decode stages using RDMA via the MORI library. It adds a new connector implementation with both PULL (serial) and PUSH (parallel, non-blocking) transfer modes, supporting TP/DP parallelism. The changes include connector logic, a proxy server for service discovery, Docker build updates, environment variables, and unit tests.

**Technical impact**  
The integration extends vLLM's KV transfer framework with a high-performance RDMA-based connector for ROCm platforms, improving throughput and reducing TTFT through overlapped computation and transfer. It maintains compatibility with existing scheduler interfaces and supports chunked prefill, decode graph mode, and various parallel strategies. The connector is isolated to a single module, minimizing impact on core vLLM components.

**Potential risks**  
- Dependency on the external MORI library, which may introduce build complexity and versioning issues.  
- The proxy server (`moriio_toy_proxy_server.py`) is labeled "toy" and may lack production-ready robustness (e.g., error handling, scaling).  
- Edge cases in heterogeneous topologies are not yet supported (noted in TODO).  
- Asynchronous transfer logic could lead to race conditions if synchronization (e.g., CUDA events) is mishandled.

**Key insights**  
- The connector leverages vLLM's well-abstracted KV transfer interface, allowing clean integration without modifying core files.  
- Performance tests show PUSH mode reduces TTFT compared to PULL mode (e.g., 2.614s vs 2.758s at 32k input length).  
- Developers should ensure MORI is correctly installed and configured, and consider the proxy server's limitations for production deployments.  
- The implementation includes comprehensive unit tests, but integration and stress testing under high load are recommended.

---

## 42. [[Misc] Hash keys only when value is `None` in kwargs](https://github.com/vllm-project/vllm/pull/32025)


### Base Information

- **PR Number:** #32025
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 05:20:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32025/files) (1):**
  - `vllm/multimodal/hasher.py`

### Summary

**What changed and why**  
The change modifies `MultiModalHasher.iter_item_to_bytes` to handle `None` values in multimodal keyword arguments. When a value is `None`, the method now yields only the key's bytes and returns early, preventing attempts to serialize `None` that would generate unhelpful serialization warnings.

**Technical impact**  
This adjustment ensures the hashing process gracefully skips `None` values, reducing log noise from serialization warnings. The hash computation will now exclude `None` values, which may slightly alter hash outputs for inputs containing `None`, but this is intentional to avoid warnings and maintain consistency for valid data.

**Potential risks**  
If downstream logic relies on `None` values being included in the hash (e.g., for caching or deduplication), this change could cause mismatches. Additionally, if `None` is used as a meaningful placeholder (distinct from missing values), its exclusion might lead to unintended collisions or behavior changes.

**Key insights**  
Developers should verify that excluding `None` values aligns with the intended hashing semantics, particularly for caching or identity purposes. Consider documenting this behavior if `None` values are expected in multimodal kwargs. The change is minimal and focused, but review any dependent systems that might be affected by hash variability.

---

## 43. [[ROCm][CI][V1] Fix `nixl_connector` test failure and achieve CUDA parity in `test_async_scheduling`](https://github.com/vllm-project/vllm/pull/32000)


### Base Information

- **PR Number:** #32000
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-09 04:48:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32000/files) (3):**
  - `tests/v1/e2e/test_async_scheduling.py`
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
This PR enables FlexAttention backend support for ROCm in the EAGLE speculative decoding proposer, removing platform-specific attention backend restrictions and unifying behavior with CUDA. It also fixes a test failure on ROCm by ensuring the fake package creation for `nixl_connector` tests includes both `nixl` and `rixl` packages, allowing Ray workers to import correctly on either platform.

**Technical impact**  
The changes eliminate ROCm-specific workarounds in attention backend selection and logprobs comparison tolerances, achieving parity between CUDA and ROCm in async scheduling tests. The test infrastructure now transparently supports both platforms via dual fake package creation, improving maintainability and reducing conditional logic.

**Potential risks**  
Removing ROCm-specific tolerances for logprobs comparison could introduce test flakiness if numerical variances between platforms persist. The dual fake package approach assumes `nixl` and `rixl` are functionally equivalent, which may mask platform-specific issues if the real implementations diverge.

**Key insights**  
Developers should verify that FlexAttention performs reliably on ROCm in production scenarios, not just in tests. The unified attention backend handling simplifies future maintenance, but any new platform-specific optimizations should be carefully evaluated to avoid regressions. Ensure all cross-platform tests validate both `nixl` and `rixl` import paths.

---

## 44. [[Feature][Benchmarks] Custom dataset: read output length from dataset](https://github.com/vllm-project/vllm/pull/31881)


### Base Information

- **PR Number:** #31881
- **Author:** [sducouedic](https://github.com/sducouedic)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-09 04:40:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31881/files) (1):**
  - `vllm/benchmarks/datasets.py`

### Summary

**What changed and why**  
This change enables custom datasets to specify per-request output lengths via an optional `"output_tokens"` field in JSONL files. The existing `--custom-output-len` CLI argument now acts as an override; dataset values are only used when the argument is `None` or `-1`.

**Technical impact**  
The `CustomDataset.sample()` method now reads and validates `output_tokens` from each dataset entry, allowing dynamic output lengths per request. This enhances flexibility for benchmarking scenarios where varying generation lengths are needed, while maintaining backward compatibility via the CLI override.

**Potential risks**  
If `--custom-output-len` is `None` or `-1` but a dataset entry lacks `output_tokens` or contains a non-integer value, a `ValueError` will be raised. This could break existing workflows that omit the field. Additionally, mixing dataset-provided lengths with the CLI override may cause confusion if the override semantics are misunderstood.

**Key insights**  
Developers should update JSONL files to include `output_tokens` when using per-request lengths, and ensure values are integers. The CLI help text clarifies the override behavior, but teams should document this change to avoid runtime errors. Consider adding a validation step during dataset loading to catch missing/invalid fields early.

---

## 45. [fix: remove duplicate engine_id check in nixl_connector](https://github.com/vllm-project/vllm/pull/31948)


### Base Information

- **PR Number:** #31948
- **Author:** [xbfs](https://github.com/xbfs)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-09 04:13:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31948/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
Removed a duplicate `engine_id` validation check in the `_nixl_handshake` function. The redundant check was identical to the one that follows it, so this change simplifies the handshake logic without altering functional behavior.

**Technical impact**  
This is a minor code cleanup that reduces unnecessary duplication. The handshake process remains functionally identical, as the same validation is still performed once after decoding `NixlAgentMetadata`. No architectural or performance impact is expected.

**Potential risks**  
Low risk since the removed code is an exact duplicate. However, ensure that the remaining check correctly handles all edge cases (e.g., null metadata, type mismatches) and that no other logic depended on the duplicate for side effects (unlikely here).

**Key insights**  
This change improves code maintainability by eliminating redundancy. Developers should verify that the single remaining validation is robust and consider adding a comment to prevent future duplication. Such cleanups are beneficial but should be paired with thorough testing to confirm no regression in handshake error handling.

---

## 46. [[Bugfix] Fix Triton FusedMoE LoRA](https://github.com/vllm-project/vllm/pull/30585)


### Base Information

- **PR Number:** #30585
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-09 03:46:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30585/files) (3):**
  - `pyproject.toml`
  - `tests/lora/test_gptoss_tp.py`
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`

### Summary

**What changed and why**  
This PR fixes a Triton fused MoE LoRA implementation bug where `intermediate_cache1` rows were misaligned during LoRA weight application. The core fix reorders rows using `gather_indx.dst_indx` to ensure LoRA weights are added to correct rows. Additionally, tests now parametrize `VLLM_MXFP4_USE_MARLIN` to validate both Marlin and Triton backends across single-GPU and tensor-parallel configurations.

**Technical impact**  
The fix corrects tensor indexing in the unfused MoE path, ensuring proper alignment between intermediate activations and LoRA weight matrices. This resolves accuracy issues in Triton-backend LoRA inference, particularly for GPT-OSS models. Enhanced test coverage now validates both quantization backends and tensor-parallel scenarios, improving regression detection.

**Potential risks**  
The indexing change assumes `gather_indx.dst_indx` and `gather_indx.src_indx` are correctly populated—any mismatch in these indices could propagate errors. The fix is specific to the unfused MoE path; other MoE implementations (e.g., fused variants) may require separate validation. Environment variable toggling in tests could introduce flakiness if not isolated properly.

**Key insights**  
Always validate tensor index alignment when reordering operations in MoE layers, as misalignment directly impacts model accuracy. The expanded test parametrization is crucial for catching backend-specific issues early. Developers should verify similar indexing logic in other MoE kernels to ensure consistency across fused/unfused implementations.

---

## 47. [[Bugfix][ROCm]Fix Qwen3-Next-80B-A3B-Thinking inference and optimize non-standard block size (544) support under rocm_atten](https://github.com/vllm-project/vllm/pull/31380)


### Base Information

- **PR Number:** #31380
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-09 03:28:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31380/files) (5):**
  - `tests/kernels/attention/test_prefix_prefill.py`
  - `vllm/attention/ops/chunked_prefill_paged_decode.py`
  - `vllm/attention/ops/prefix_prefill.py`
  - `vllm/attention/ops/triton_reshape_and_cache_flash.py`
  - `vllm/v1/attention/backends/rocm_attn.py`

### Summary

**What changed and why**  
This PR fixes ROCm attention backend support for models with non-power-of-2 KV block sizes (e.g., Qwen3-Next-80B with block size 544). It introduces a dynamic dispatching mechanism that uses optimized bitwise addressing for power-of-2 block sizes and falls back to generalized arithmetic addressing for non-standard sizes, ensuring Triton kernel compatibility.

**Technical impact**  
The changes modify multiple attention kernels (`chunked_prefill_paged_decode`, `prefix_prefill`, `triton_reshape_and_cache_flash`) to accept a `PHYSICAL_BLOCK_SIZE` parameter and compute offsets using division/modulo instead of bit-shifts. The ROCm backend now routes non-power-of-2 block sizes through Triton kernels while preserving existing HIP C++ paths for standard sizes, maintaining performance for common models.

**Potential risks**  
The dual-path logic increases code complexity and could introduce subtle bugs if the power-of-2 detection fails. Edge cases with very large block sizes or mixed block layouts may not be fully covered. The forced Triton path for non-power-of-2 sizes may have performance implications compared to native HIP kernels.

**Key insights**  
Developers should verify that `is_pow2` detection aligns with actual cache strides in all configurations. The addition of small epsilon in division (`1e-10`) improves numerical stability but may affect low-precision computations. Regression testing for power-of-2 block sizes is critical, as the changes touch core attention kernels used across many models.

---

## 48. [[Model] Reorganize pooling layers](https://github.com/vllm-project/vllm/pull/31973)


### Base Information

- **PR Number:** #31973
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-09 03:02:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31973/files) (34):**
  - `.github/CODEOWNERS`
  - `tests/model_executor/test_model_load_with_params.py`
  - `tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py`
  - `vllm/config/pooler.py`
  - `vllm/model_executor/layers/pooler.py`
  - `vllm/model_executor/layers/pooler/__init__.py`
  - `vllm/model_executor/layers/pooler/abstract.py`
  - `vllm/model_executor/layers/pooler/activations.py`
  - `vllm/model_executor/layers/pooler/common.py`
  - `vllm/model_executor/layers/pooler/seqwise/__init__.py`
  - `vllm/model_executor/layers/pooler/seqwise/heads.py`
  - `vllm/model_executor/layers/pooler/seqwise/methods.py`
  - `vllm/model_executor/layers/pooler/seqwise/poolers.py`
  - `vllm/model_executor/layers/pooler/special.py`
  - `vllm/model_executor/layers/pooler/tokwise/__init__.py`
  - `vllm/model_executor/layers/pooler/tokwise/heads.py`
  - `vllm/model_executor/layers/pooler/tokwise/methods.py`
  - `vllm/model_executor/layers/pooler/tokwise/poolers.py`
  - `vllm/model_executor/models/adapters.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/bert_with_rope.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/gpt2.py`
  - `vllm/model_executor/models/gritlm.py`
  - `vllm/model_executor/models/internlm2.py`
  - `vllm/model_executor/models/jamba.py`
  - `vllm/model_executor/models/jina_vl.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/model_executor/models/qwen2_rm.py`
  - `vllm/model_executor/models/roberta.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/pooling.py`
  - `vllm/v1/outputs.py`

### Summary

**What changed and why**  
The PR reorganizes the monolithic `pooler.py` file into a modular package structure to address code bloat and improve maintainability. Key changes include splitting functionality into specialized submodules (`activations`, `common`, `abstract`, `special`, `seqwise`, `tokwise`), introducing factory methods for `DispatchPooler`, and refactoring model-specific poolers to leverage new abstractions like `SequencePooler` and `TokenPooler`.

**Technical impact**  
This refactor enhances code organization by separating concerns—pooling methods, heads, and activations are now isolated into logical modules. The introduction of factory methods (`DispatchPooler.for_embedding`, `DispatchPooler.for_seq_cls`) simplifies pooler instantiation across models. The changes also unify type aliases (e.g., `PoolerOutput`) and update numerous models (BERT, GPT-2, CLIP, etc.) to use the new APIs, ensuring consistency and reducing duplication.

**Potential risks**  
The extensive refactoring touches many model files, increasing the risk of regression if any integration is misaligned. Edge cases like chunked prefill handling in `AllPool`/`StepPool` or custom pooling logic in models like `GritLM` require careful validation. Additionally, the removal of direct `Pooler` class usage in favor of factories may break downstream custom extensions that relied on the old API.

**Key insights**  
Developers should adopt the new factory methods (`DispatchPooler.for_embedding/for_seq_cls`) and helper functions (`pooler_for_token_embed/classify`) when adding or modifying poolers. The modular structure makes it easier to extend pooling methods or heads without modifying core logic. Ensure thorough testing of pooling tasks (embed, classify, token-wise operations) across supported models to verify behavior post-refactor.

---

## 49. [[Bugfix] Fix OpenAPI schema test failures](https://github.com/vllm-project/vllm/pull/31921)


### Base Information

- **PR Number:** #31921
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-09 02:56:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31921/files) (2):**
  - `tests/entrypoints/openai/test_openai_schema.py`
  - `vllm/entrypoints/openai/protocol.py`

### Summary

**What changed and why**  
This PR fixes three distinct issues causing OpenAPI schema test failures: 1) Added an upper bound (`le=_LONG_INFO.max`) to the `truncate_prompt_tokens` field in request protocols to prevent server crashes from extremely large integers, 2) Fixed a test filter that incorrectly assumed all content items were dictionaries by adding an `isinstance(item, dict)` check, and 3) Added a longer timeout for the `/v1/completions` endpoint in tests to prevent response timeouts.

**Technical impact**  
The changes improve the robustness of the API by ensuring proper input validation (returning 400 errors instead of 500 crashes) and test reliability. The validation fix aligns `truncate_prompt_tokens` with the `seed` field's constraints, while the test adjustments prevent false failures during schema testing, ensuring the OpenAPI specification is correctly exercised.

**Potential risks**  
The upper bound validation might be overly restrictive if `_LONG_INFO.max` is not the appropriate limit for `truncate_prompt_tokens` in all contexts. The test filter fix assumes string content items never have a "type" field, which is correct for the current spec but could mask future issues if the content schema evolves. The timeout increase may hide genuine performance regressions in the completions endpoint.

**Key insights**  
Always validate both lower and upper bounds for integer fields exposed via API to prevent server crashes. When writing tests that traverse nested, polymorphic data structures, use defensive type checks. Monitor endpoint performance; consider if the timeout increase for `/v1/completions` indicates a need for optimization rather than just a test adjustment.

---

## 50. [[Bugfix] Fix Var Length Batched Padding in Granite Speech](https://github.com/vllm-project/vllm/pull/31906)


### Base Information

- **PR Number:** #31906
- **Author:** [alex-jw-brooks](https://github.com/alex-jw-brooks)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-09 02:28:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31906/files) (1):**
  - `vllm/model_executor/models/granite_speech.py`

### Summary

**What changed and why**  
The fix addresses a runtime error when batching variable-length audio features in the Granite Speech model. When multimodal inputs are batched as a list of 2D tensors `[feat_len, 160]`, the padding function expects 3D tensors, causing a dimension mismatch. The solution unsqueezes 2D tensors to 3D before padding, ensuring consistent tensor dimensions.

**Technical impact**  
This change ensures that the padding logic correctly handles both single and batched audio inputs by standardizing tensor dimensions to `[bsz, longest_feature, 160]`. It maintains backward compatibility while fixing the batching pipeline, preventing crashes during inference with multiple audio samples.

**Potential risks**  
If any input features already have 3D shape, the conditional unsqueeze may incorrectly add an extra dimension. Additionally, the padding assumes zero-padding is acceptable for all audio features, which might not hold for all preprocessing requirements. Edge cases with empty or malformed feature lists are not explicitly handled.

**Key insights**  
Always validate tensor dimensions in batching logic to avoid silent errors. Consider adding explicit shape assertions or documentation for expected input formats. Testing with mixed-dimension inputs (2D and 3D) is recommended to ensure robustness. The fix is minimal and focused, but future refactors could centralize dimension validation.

---

