# vLLM Merged PR Report

**Report Date:** 2026-01-10 PST

**Total Merged PRs:** 17

---

## 1. [[Hardware][AMD][CI][Bugfix] Fix AMD Quantization test group](https://github.com/vllm-project/vllm/pull/31713)


### Base Information

- **PR Number:** #31713
- **Author:** [mawong-amd](https://github.com/mawong-amd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 23:19:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31713/files) (12):**
  - `.buildkite/test-amd.yaml`
  - `tests/quantization/test_compressed_tensors.py`
  - `tests/quantization/test_configs.py`
  - `tests/quantization/test_cpu_offload.py`
  - `tests/quantization/test_fp8.py`
  - `tests/quantization/test_gptq_dynamic.py`
  - `tests/quantization/test_ptpc_fp8.py`
  - `tests/quantization/utils.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py`
  - `vllm/model_executor/layers/quantization/ptpc_fp8.py`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
This PR fixes remaining failures in AMD quantization tests by making the test suite and runtime code platform-aware. Changes include updating CI configuration to run quantization tests on AMD production hardware, adding platform-specific test skips and parameters, fixing kernel implementations for ROCm compatibility, and ensuring proper handling of quantization methods across different hardware platforms.

**Technical impact**  
The changes improve cross-platform compatibility by introducing platform-aware logic throughout the quantization system. Tests now dynamically adjust expectations based on CUDA/ROCm platforms, kernel implementations handle scale conversions correctly for fused modules, and quantization registry automatically marks custom methods as supported. This creates a more robust foundation for AMD hardware support while maintaining CUDA compatibility.

**Potential risks**  
Platform-specific logic could introduce subtle bugs if not consistently applied across all quantization paths. The scale conversion in Triton kernels for fused modules adds complexity that may affect performance. Some changes rely on runtime checks rather than compile-time validation, which could lead to late failures in production scenarios.

**Key insights**  
Developers should use the `current_platform` utilities consistently when adding platform-dependent logic. The automatic registration of custom quantization methods simplifies extension but requires careful testing on all target platforms. Kernel-level fixes demonstrate the importance of handling edge cases like fused modules with per-tensor scales across different hardware architectures.

---

## 2. [[CPU][BugFix] Disable AOT Compile for CPU](https://github.com/vllm-project/vllm/pull/32037)


### Base Information

- **PR Number:** #32037
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 23:15:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32037/files) (1):**
  - `vllm/envs.py`

### Summary

**What changed and why**  
The PR modifies the `use_aot_compile()` function to disable ahead-of-time (AOT) compilation specifically for CPU platforms. This change adds a condition `not current_platform.is_cpu()` to the default enablement logic, ensuring AOT compilation remains active only for non-CPU platforms when Torch ≥ 2.10.0.dev and the compile cache is not disabled. The fix addresses issue #32033, which likely involved failures or performance regressions when AOT compilation was used on CPU.

**Technical impact**  
This change narrows the scope of AOT compilation to exclude CPU execution paths, which may affect performance or compatibility for CPU-only deployments. The system will now bypass AOT compilation entirely on CPU, potentially falling back to alternative compilation or execution strategies. This ensures consistency and stability for CPU workloads while preserving AOT benefits for GPU/accelerated platforms.

**Potential risks**  
If the underlying issue (#32033) is not fully resolved by this conditional disablement, CPU users might experience degraded performance due to missed optimization opportunities. Additionally, there is a risk of platform detection errors if `current_platform.is_cpu()` behaves unexpectedly in hybrid or edge-case environments (e.g., mixed CPU/GPU setups). The change assumes AOT compilation is inherently problematic for all CPU scenarios, which may not hold for future Torch versions or configurations.

**Key insights**  
Developers should verify that the fix adequately addresses the root cause in #32033 and consider whether a more granular approach (e.g., runtime checks or configurable flags) is needed for specific CPU use cases. It's important to monitor CPU performance post-change and update documentation if AOT compilation is now unsupported on CPU. Future work could explore re-enabling AOT for CPU once Torch or vLLM resolves the underlying compatibility issue.

---

## 3. [make assume_32_bit_indexing configurable](https://github.com/vllm-project/vllm/pull/32044)


### Base Information

- **PR Number:** #32044
- **Author:** [laithsakka](https://github.com/laithsakka)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 23:15:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32044/files) (2):**
  - `vllm/compilation/decorators.py`
  - `vllm/config/compilation.py`

### Summary

**What changed and why**  
This PR makes the Inductor compiler's indexing assumption configurable by adding an `assume_32_bit_indexing` field to `DynamicShapesConfig` (default `True`). The change allows models requiring 64-bit indexing for compilation to override the default. It also fixes a compilation config hashing issue by including `dynamic_shapes_config` in the hash computation.

**Technical impact**  
The modification wires the configuration through the compile path in `decorators.py`, setting `torch._inductor.config.assume_32bit_indexing` based on the user-provided value (for Torch ≥ 2.10.0.dev). This enables finer control over Inductor's indexing behavior, potentially affecting kernel generation and memory access patterns for large tensors. The hash fix ensures configuration changes properly invalidate cached compilation artifacts.

**Potential risks**  
If users incorrectly set `assume_32_bit_indexing=False` for models that do not require 64-bit indexing, it may introduce unnecessary overhead. The Torch version guard (`≥2.10.0.dev`) could cause silent misconfiguration if older versions are used. Edge cases around very large tensor indices (exceeding 32-bit range) might still arise if the setting is not aligned with actual model requirements.

**Key insights**  
Developers should verify that `assume_32_bit_indexing=False` is only set when targeting PyTorch ≥2.10.0.dev and when models genuinely require 64-bit indexing. The hash update correctly accounts for dynamic shape configurations, preventing stale cache issues. Consider adding runtime validation or warnings for version mismatches to improve robustness.

---

## 4. [[MTP][GLM][Bugfix] Fixed .weight_scale loading logic that dropped MTP prediction accuracy with fp8+mtp](https://github.com/vllm-project/vllm/pull/32101)


### Base Information

- **PR Number:** #32101
- **Author:** [andyl98](https://github.com/andyl98)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 23:14:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32101/files) (1):**
  - `vllm/model_executor/models/glm4_moe_mtp.py`

### Summary

**What changed and why**  
The fix relocates the `.weight_scale` tensor skipping logic from the early name processing stage to the final fallback loading path. This ensures the skip only occurs after all name remappings and stacked/expert parameter handling are complete, preventing valid weight scales from being incorrectly skipped when loading FP8 checkpoints for GLM MTP models.

**Technical impact**  
This change restores correct weight loading for FP8-quantized GLM-4.5/4.6/4.7 models with Multi-Token Prediction enabled. It resolves a `KeyError` during checkpoint loading and fixes a critical performance regression where MTP draft acceptance rates dropped from ~90% to ~1% due to missing weight scale parameters.

**Potential risks**  
The fix assumes `.weight_scale` tensors are only present for the LM head in FP8 checkpoints. If other model components introduce weight scale tensors with different naming patterns, they might be incorrectly skipped. The logic also depends on the `params_dict` being fully populated after all transformations, which could be fragile if the loading pipeline changes.

**Key insights**  
The key insight is that parameter filtering logic must be applied after all name transformations and routing logic completes. Developers should ensure weight loading skip conditions are evaluated in the correct context—after parameter mapping resolution but before actual tensor assignment—to avoid inadvertently dropping required parameters.

---

## 5. [[Model Runner V2] Support structured outputs + spec decoding](https://github.com/vllm-project/vllm/pull/32102)


### Base Information

- **PR Number:** #32102
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-10 22:58:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32102/files) (3):**
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/structured_outputs.py`

### Summary

**What changed and why**  
This PR enables structured outputs (grammar-constrained generation) to work with speculative decoding by introducing a dedicated `StructuredOutputsWorker` and updating batching metadata. The changes replace a previous helper function with a worker that uses a new Triton kernel to apply grammar bitmasks to logits via `logits_indices`, supporting multiple logits per request during speculative decoding.

**Technical impact**  
The architecture now cleanly separates structured output logic into a reusable worker, which is initialized with capacity for speculative steps. Batching metadata (`cu_num_logits_np`) is extended to track logit counts per request, enabling accurate mapping between grammar bitmasks and the corresponding logits (including draft tokens). This ensures grammar constraints are applied correctly across all speculative decoding paths.

**Potential risks**  
If `cu_num_logits_np` is not populated correctly in all code paths (e.g., edge cases in `prepare_inputs`), mapping could misalign, causing incorrect grammar application. The Triton kernel assumes `logits_indices` correctly maps each bitmask to its target logits; any mismatch may silently corrupt logits. Additionally, the worker’s `max_num_logits` calculation must account for the maximum speculative steps to avoid buffer overflows.

**Key insights**  
Developers should verify that `cu_num_logits_np` is consistently set in both dummy and real input preparation, especially when speculative decoding is active. The shift from a helper to a stateful worker improves modularity but requires careful initialization to match the model runner’s configuration. Testing should cover mixed batches with and without grammar constraints during speculative decoding to ensure robust mapping.

---

## 6. [[Benchmark][2/2] Use spline interpolation to tune SLA variables](https://github.com/vllm-project/vllm/pull/32095)


### Base Information

- **PR Number:** #32095
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 20:27:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32095/files) (3):**
  - `docs/benchmarking/sweeps.md`
  - `tests/benchmarks/sweep/test_serve_sla.py`
  - `vllm/benchmarks/sweep/serve_sla.py`

### Summary

**What changed and why**  
The PR replaces a two-stage SLA tuning algorithm (boundary scanning + binary search) with a single-stage spline interpolation approach using `PchipInterpolator`. This change simplifies the control flow and aims for faster convergence, especially for large QPS values, by allowing more intelligent candidate selection between known data points rather than always picking the midpoint.

**Technical impact**  
The new `solve_sla` function with `SLAHistory` class centralizes the interpolation logic, removing the separate `_estimate_sla_bounds` and `_find_sla_value` functions. This reduces code complexity and improves maintainability. The algorithm now iteratively refines estimates based on spline roots, falling back to binary search only when interpolation fails to find a root.

**Potential risks**  
The spline interpolation assumes a reasonably smooth relationship between QPS and SLA margin; noisy or highly discontinuous data could lead to poor convergence or incorrect root estimates. The fallback to binary search may not fully compensate for pathological cases. Additionally, the removal of explicit boundary scanning could miss edge cases where the initial min/max runs don't adequately bracket the solution.

**Key insights**  
Developers should verify that the interpolation behaves well across diverse SLA metric relationships (linear, convex, concave, etc.), as tested. The convergence condition (`max_passing + 1 < min_failing`) is robust but assumes integer QPS values; consider if fractional values are ever needed. Ensure `scipy` is available in all deployment environments, as it's now a required dependency.

---

## 7. [[BugFix] Wait for compute before offloading KV to CPU](https://github.com/vllm-project/vllm/pull/31341)


### Base Information

- **PR Number:** #31341
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-10 14:25:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31341/files) (3):**
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/v1/kv_offload/worker/cpu_gpu.py`

### Summary

**What changed and why**  
This PR fixes KV cache offloading synchronization by ensuring GPU→CPU transfers wait for compute completion before starting. It defers store job submission to the next engine step to avoid contention with sampling-related GPU→CPU copies, and removes ineffective stream priorities since DMA-based copies aren't affected by them.

**Technical impact**  
The changes introduce a two-phase offloading mechanism where store jobs are queued (`_unsubmitted_store_jobs`) and submitted at the beginning of the next step before loads. This staggers offloading to reduce interference with token sampling. Stream synchronization is now explicit via `wait_stream` on the compute stream, ensuring data consistency.

**Potential risks**  
If compute streams are not properly synchronized before offloading, race conditions could corrupt KV cache data. The deferred store submission adds complexity to job lifecycle management, and incorrect bookkeeping (e.g., `unsubmitted_stores_count`) may lead to missed offloads or resource leaks. Test updates must accurately reflect the new timing.

**Key insights**  
Developers should verify that all GPU→CPU offloads properly `wait_stream` on the relevant compute contexts. The new queuing mechanism requires careful tracking of pending vs. unsubmitted jobs across engine steps. Stream priority removal simplifies code but shifts synchronization responsibility to explicit waits, which must be consistently applied.

---

## 8. [[Bugfix] Fix Qwen3-VL-Reranker model loading for sequence classification](https://github.com/vllm-project/vllm/pull/32089)


### Base Information

- **PR Number:** #32089
- **Author:** [ricky-chaoju](https://github.com/ricky-chaoju)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 12:40:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32089/files) (2):**
  - `vllm/model_executor/models/adapters.py`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
The changes fix weight loading failures for Qwen3-VL-Reranker models when used in sequence classification mode. The issue occurred because vision-language models have a nested structure (`model.language_model.lm_head`) while the sequence classification adapter assumed a flat structure (`model.lm_head`). The solution detects VL models and accesses the correct parent module.

**Technical impact**  
These modifications enable proper weight extraction and mapping for VL reranker models by supporting nested language model architectures. The changes affect weight loading logic in adapters.py and ensure configuration properties are correctly propagated to the text config layer, maintaining compatibility with existing text-based models.

**Potential risks**  
The conditional logic for detecting language_model attributes could fail if future model architectures use different naming conventions. The weight mapping cleanup relies on proper hf_to_vllm_mapper implementation, which might not exist for all models. There's also a risk of memory leaks if the temporary lm_head isn't properly cleaned up in all code paths.

**Key insights**  
Developers should verify that similar nested structures in other VL models are handled correctly. The solution maintains backward compatibility by falling back to top-level model access when language_model isn't present. Testing should include both VL and non-VL models to ensure the conditional logic works correctly across different architectures.

---

## 9. [[MISC] Add strict contiguity check for FlashInfer attention tensors](https://github.com/vllm-project/vllm/pull/32008)


### Base Information

- **PR Number:** #32008
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 12:40:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32008/files) (2):**
  - `vllm/utils/torch_utils.py`
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
Added a stricter contiguity check utility (`is_strictly_contiguous`) and updated FlashInfer attention paths to use it instead of PyTorch's `is_contiguous()`. This prevents CUDA kernel memory access issues by detecting tensors with degenerate strides—where dimensions of size 1 have non-canonical strides—that PyTorch incorrectly reports as contiguous.

**Technical impact**  
The change strengthens memory layout validation for FlashInfer’s TRTLLM attention backend, ensuring tensors adhere to canonical contiguous stride patterns. This preemptively catches layout mismatches before launching CUDA kernels, reducing the risk of silent memory access errors or undefined behavior during prefill and decode operations.

**Potential risks**  
Overly strict validation could reject tensors that are functionally safe for some kernels, potentially causing unnecessary failures. If the utility is not adopted consistently across all attention backends, similar issues may persist elsewhere. Additionally, the performance overhead of stride verification per tensor should be monitored, though it is likely negligible.

**Key insights**  
Developers should use `is_strictly_contiguous` for any CUDA kernels that rely on canonical strides, especially when handling tensors with singleton dimensions. Consider extending this check to other attention backends if they share similar stride dependencies. Ensure thorough testing with edge-case tensor shapes to validate the utility’s correctness and avoid false positives.

---

## 10. [[Bugfix][Quantization] Ensure input contiguity in per_token_quant_int8](https://github.com/vllm-project/vllm/pull/31637)


### Base Information

- **PR Number:** #31637
- **Author:** [Flink-ddd](https://github.com/Flink-ddd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 12:40:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31637/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/int8_utils.py`

### Summary

**What changed and why**  
The function `per_token_quant_int8` was modified to handle non-contiguous and higher-dimensional input tensors. Specifically, the assertion `assert x.is_contiguous()` was replaced with `x = x.contiguous()`, and the input is now flattened to 2D `(M, N)` before processing, with outputs reshaped back to the original dimensions. This fixes crashes in AWQ-Marlin quantization for MLA-based models (e.g., DeepSeek) during long-context inference.

**Technical impact**  
The changes ensure compatibility with non-contiguous tensors generated by slicing or view operations in MLA architectures and vLLM's chunked prefill. The flattening and reshaping preserve the mathematical correctness of per-token quantization while maintaining the kernel’s expected 2D input shape. Performance impact is minimal, as `.contiguous()` is a no-op for already contiguous tensors.

**Potential risks**  
If the input tensor is extremely large, the explicit `contiguous()` call could temporarily increase memory usage. Additionally, the flattening assumes the last dimension is the feature dimension; any future changes to tensor layout expectations must ensure this assumption holds. Edge cases with unusual strides or memory layouts should be validated.

**Key insights**  
This fix follows PyTorch best practices by replacing assertions with defensive contiguous conversions. Developers should be aware that quantization functions now transparently handle non-contiguous inputs, improving robustness for complex model architectures. Ensure similar patterns are applied elsewhere in the codebase to prevent analogous issues.

---

## 11. [Revert "[Kernels][FI] Skip trtllm attention when num_kv_heads=1 (#308…](https://github.com/vllm-project/vllm/pull/31617)


### Base Information

- **PR Number:** #31617
- **Author:** [shyeh25](https://github.com/shyeh25)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 12:39:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31617/files) (2):**
  - `tests/kernels/attention/test_flashinfer_trtllm_attention.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR reverts a previous change that prevented TRTLLM attention from being used when `num_kv_heads=1` (MQA configurations). The revert was necessary because the restriction caused a `NotImplementedError` in GPT-OSS-120B TP8 when using FlashInfer with attention sinks, as FlashInfer does not support this configuration.

**Technical impact**  
The change removes the guard that rejected MQA (`num_kv_heads=1`) configurations from using TRTLLM attention. Now, TRTLLM attention can be used for any configuration where the platform supports it and `num_qo_heads % num_kv_heads == 0`. This may restore performance benefits for MQA models but reintroduces the risk of degenerate tensor strides that originally motivated the restriction.

**Potential risks**  
The primary risk is that degenerate KV cache strides (where `stride_heads == stride_batch`) could cause CUDA's `cuTensorMapEncodeTiled` to fail when building TMA descriptors, as noted in the original commit. This could lead to runtime errors or undefined behavior on certain hardware (e.g., Blackwell platforms). Additionally, the removal of the associated test reduces coverage for this edge case.

**Key insights**  
Developers should monitor for any CUDA errors or performance regressions in MQA configurations, especially on Blackwell GPUs. If issues arise, consider implementing a more robust workaround (e.g., stride padding) rather than outright rejection. Ensure comprehensive testing of attention sinks and speculative decoding with MQA models to validate stability.

---

## 12. [[BugFix] scheduler: Fix resuming of preempted requests after async load](https://github.com/vllm-project/vllm/pull/31583)


### Base Information

- **PR Number:** #31583
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 12:39:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31583/files) (3):**
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/request.py`

### Summary

**What changed and why**  
The fix addresses a bug where preempted requests undergoing async KV loading were incorrectly classified as new requests instead of resumed requests. When a request transitions from `WAITING_FOR_REMOTE_KVS` back to the scheduler, the status is now set to `PREEMPTED` if `num_preemptions > 0`, ensuring proper classification in `SchedulerOutput`.

**Technical impact**  
This change ensures the scheduler correctly distinguishes between resumed preempted requests and new requests after async KV transfers. It affects request state transitions and output categorization (`scheduled_cached_reqs` vs. `scheduled_new_reqs`), maintaining accurate tracking of preempted workloads.

**Potential risks**  
If `num_preemptions` tracking is inaccurate elsewhere, requests could be misclassified. The fix assumes async loading only occurs for preempted requests, which may not hold if other states use `WAITING_FOR_REMOTE_KVS`. Edge cases with concurrent preemptions and async loads need validation.

**Key insights**  
Developers should verify that `num_preemptions` is reliably incremented on preemption. The test updates parameterize async behavior, but ensure similar fixes apply to other async operations. Review any other states transitioning through `WAITING_FOR_REMOTE_KVS` to prevent regression.

---

## 13. [fused_moe_kernel - cast accumulator after applying router weights](https://github.com/vllm-project/vllm/pull/32002)


### Base Information

- **PR Number:** #32002
- **Author:** [gnovack](https://github.com/gnovack)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-10 12:36:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32002/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The change modifies the fused_moe_kernel to defer casting the accumulator to compute_type until after applying router weights (moe_weight). Previously, the accumulator was cast to compute_type before applying dequantization scales and router weights, which altered numerical precision and caused test failures and accuracy degradation in certain LoRA and MoE configurations.

**Technical impact**  
This restores the original computation order where router weights are applied in float32 precision before the final cast, improving numerical stability. The dequantization logic is simplified by removing early casting branches, and bias addition and router-weight multiplication now occur in the accumulator's original higher precision before the final downcast.

**Potential risks**  
The change assumes that bias and router weights are not quantized, which could be problematic if future modifications introduce quantization for these components. Additionally, the simplified dequantization branches might not handle all possible quantization schemes if new ones are added, requiring careful extension.

**Key insights**  
Maintaining higher precision during router-weight application is critical for model accuracy, as evidenced by the restored test passes and improved lm-eval scores. Developers should ensure that any future changes to quantization or kernel optimizations preserve this computation order to avoid similar numerical regressions.

---

## 14. [[LoRA][Perf] Improve FusedMoE LoRA performance for small rank](https://github.com/vllm-project/vllm/pull/32019)


### Base Information

- **PR Number:** #32019
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 11:04:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32019/files) (3):**
  - `vllm/lora/layers/fused_moe.py`
  - `vllm/lora/layers/utils.py`
  - `vllm/lora/ops/triton_ops/utils.py`

### Summary

**What changed and why**  
This PR optimizes FusedMoE LoRA performance for small ranks by adjusting kernel tiling parameters. The default block sizes (BLOCK_SIZE_N=64, BLOCK_SIZE_K=32) cause redundant computation when rank is smaller, as weights are loaded repeatedly. The changes dynamically set block sizes based on rank: for shrink operations, BLOCK_SIZE_N is capped at the next power of two of rank; for expand operations, BLOCK_SIZE_K is similarly adjusted with a minimum of 16.

**Technical impact**  
The modifications introduce a new helper function `try_get_optimal_moe_lora_config` that wraps the existing MOE config logic, overriding block sizes for LoRA-specific kernels. This ensures that kernel tiling aligns with the actual rank dimensions, reducing unnecessary memory loads and computations. The Triton kernel defaults in `get_lora_op_configs` are also updated to reflect these rank-aware block sizes, improving throughput for small-rank LoRA adapters.

**Potential risks**  
If the rank is not a power of two, `next_power_of_2` may over-allocate slightly, though this is minimal. The hardcoded minimum K=16 for expand operations must align with Triton’s requirements; any future changes to these constraints could affect correctness. Edge cases where rank exceeds default block sizes (e.g., rank > 64) are handled by existing min/max logic, but performance gains may diminish.

**Key insights**  
Developers should note that this optimization is most beneficial for small-rank LoRA adapters (e.g., rank=8), as shown by the 9% throughput improvement. The changes are backward-compatible and only affect performance, not functionality. When modifying kernel configurations in the future, ensure that rank-dependent adjustments respect hardware constraints and alignment requirements.

---

## 15. [[Kernel] Optimize Sliding Window Attention in 3D Triton Kernel](https://github.com/vllm-project/vllm/pull/31984)


### Base Information

- **PR Number:** #31984
- **Author:** [jvlunteren](https://github.com/jvlunteren)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-10 10:13:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31984/files) (1):**
  - `vllm/v1/attention/ops/triton_unified_attention.py`

### Summary

**What changed and why**  
The PR optimizes sliding window attention in the 3D Triton kernel by applying tile pruning, similar to the existing 2D kernel optimization. It calculates `tile_start` and `tile_end` based on the sliding window bounds and restricts tile iteration to only those within the allowed range, reducing unnecessary computation.

**Technical impact**  
This change improves performance for long sequences by limiting tile processing to the sliding window region, yielding a 3–4% speedup for outputs of 6,400–12,800 tokens. The optimization is disabled when `USE_MM_PREFIX` is active, preserving compatibility with prefix-based attention patterns.

**Potential risks**  
The optimization does not apply when `USE_MM_PREFIX` is set, which could lead to inconsistent behavior across configurations. Additionally, the TODO note about sliding window pruning with image bidirectional masks indicates an unresolved edge case that may affect specialized use cases.

**Key insights**  
Developers should verify that `USE_MM_PREFIX` scenarios are adequately tested, as they bypass the optimization. The performance gains are most significant for long sequences, making this particularly beneficial for large-context models. Ensure existing unit tests cover both sliding window and non-sliding window configurations to maintain correctness.

---

## 16. [[Quant] Support MXFP4 W4A16 for compressed-tensors dense models](https://github.com/vllm-project/vllm/pull/31926)


### Base Information

- **PR Number:** #31926
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-10 06:44:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31926/files) (3):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_mxfp4.py`

### Summary

**What changed and why**  
Added support for MXFP4 (4-bit float) weight-only quantization in compressed-tensors dense models. This enables loading and running models quantized with the MXFP4 format using the existing Marlin kernel, which is already well-tested. The changes include a new scheme class (`CompressedTensorsW4A16Mxfp4`) and logic to detect MXFP4 quantization parameters.

**Technical impact**  
The PR extends the compressed-tensors quantization backend to support a new 4-bit float format (E2M1 with per-group E8M0 scales, group_size=32). It reuses the Marlin FP4 kernel for inference, maintaining performance and compatibility. The architecture follows the existing pattern for weight-only schemes, adding minimal overhead.

**Potential risks**  
The implementation currently ignores activation quantization, which may limit performance for W4A4 models until future PRs add support. There is a risk of misdetection if quantization parameters overlap with other formats (e.g., NVFP4). The renaming of `weight_packed` to `weight` in `process_weights_after_loading` assumes no naming conflicts in the layer.

**Key insights**  
This is a foundational step for MXFP4 support; developers should note that activation quantization is not yet handled. The scheme leverages existing Marlin utilities, ensuring stability. Future work should integrate FlashInfer or CuTLASS kernels for W4A4 support and validate edge cases in parameter detection.

---

## 17. [[Bugfix] Fix integer overflow in Gemma3n audio processing](https://github.com/vllm-project/vllm/pull/31657)


### Base Information

- **PR Number:** #31657
- **Author:** [jeremyteboul](https://github.com/jeremyteboul)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-10 01:52:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31657/files) (3):**
  - `tests/models/multimodal/processing/test_gemma3.py`
  - `vllm/model_executor/models/gemma3n_audio_utils.py`
  - `vllm/model_executor/models/gemma3n_mm.py`

### Summary

**What changed and why**  
The fix addresses an integer overflow bug in Gemma3n audio processing that occurred when the audio encoder produced more tokens than the expected placeholder count (188). The root cause was that the original code always attempted to pad the audio features, which resulted in a negative padding size when the sequence was longer than expected, causing a tensor expansion overflow. The solution introduces a new utility function `adjust_audio_features_to_expected_length` that conditionally pads or truncates audio embeddings to match the expected token length.

**Technical impact**  
The changes normalize audio feature lengths to a fixed expected token count (188) by either padding with a predefined embedding or truncating excess tokens. This ensures consistency in tensor dimensions downstream, preventing runtime errors and maintaining compatibility with the model's placeholder token structure. The utility function is decoupled from CUDA dependencies, making it easily testable on CPU.

**Potential risks**  
Truncation may discard meaningful audio information if the encoder consistently produces longer sequences, potentially degrading model performance for longer audio inputs. The warning log for truncation might be noisy in production if overflows are frequent. Additionally, the fix assumes the last vocabulary token is an appropriate padding embedding, which may not hold if the vocabulary semantics change.

**Key insights**  
Developers should monitor truncation warnings to assess whether the expected token count aligns with real-world audio lengths. Consider validating the padding embedding choice and exploring adaptive token counts if truncation becomes common. The comprehensive test suite provides a solid foundation for regression testing, but integration tests with actual audio inputs are recommended to ensure end-to-end functionality.

---

