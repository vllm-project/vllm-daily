# vLLM Merged PR Report

**Report Date:** 2026-01-19 PST

**Total Merged PRs:** 26

---

## 1. [[Refactor] Make FP8 Linear Ops use kernel abstraction](https://github.com/vllm-project/vllm/pull/27814)


### Base Information

- **PR Number:** #27814
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-19 22:48:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/27814/files) (30):**
  - `.buildkite/lm-eval-harness/configs/models-small-rocm.txt`
  - `tests/compile/distributed/test_fusion_all_reduce.py`
  - `tests/compile/distributed/test_sequence_parallelism.py`
  - `tests/compile/test_functionalization.py`
  - `tests/compile/test_fusion.py`
  - `tests/compile/test_fusion_attn.py`
  - `tests/compile/test_silu_mul_quant_fusion.py`
  - `tests/kernels/quantization/test_scaled_mm_kernel_selection.py`
  - `tests/quantization/test_compressed_tensors.py`
  - `tests/utils.py`
  - `vllm/_aiter_ops.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py`
  - `vllm/model_executor/layers/quantization/fbgemm_fp8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/flashinfer.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/pytorch.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/rocm.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/ptpc_fp8.py`
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py`
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`
  - `vllm/model_executor/layers/quantization/utils/w8a8_utils.py`

### Summary

**What changed and why**  
This PR introduces a unified kernel abstraction (`ScaledMMLinearKernel`) for scaled matrix multiplication (GEMM) operations, replacing the previous `Fp8LinearOp` and similar implementations. The changes create a consistent interface for FP8 and INT8 linear kernels across different backends (CUTLASS, FlashInfer, ROCm, PyTorch, Triton, CPU), improving code clarity and maintainability. The refactor also updates all quantization schemes (compressed-tensors, FBGEMM, ModelOpt, PTPC, Quark) to use the new abstraction.

**Technical impact**  
The refactor centralizes kernel selection logic through `init_fp8_linear_kernel` and `init_int8_linear_kernel` helpers, enabling backend-agnostic execution. This simplifies the codebase by removing duplicate dispatch logic and legacy utilities (e.g., `maybe_create_device_identity`). The abstraction supports various quantization granularities (per-tensor, per-token, per-channel) and symmetries, ensuring consistent behavior across platforms. Test suites are updated to use new helper classes (`TestFP8Layer`, `TestBlockFP8Layer`), improving test coverage and parameterization.

**Potential risks**  
- The removal of `Fp8LinearOp` may break external integrations that directly depend on its interface.  
- Kernel selection logic now relies on environment variables (`VLLM_DISABLED_KERNELS`), which could lead to unexpected fallbacks if misconfigured.  
- The abstraction introduces additional layers of indirection, potentially complicating debugging. Edge cases in backend-specific implementations (e.g., ROCm skinny GEMM) require careful validation.

**Key insights**  
- Developers should use the new `init_fp8_linear_kernel` and `init_int8_linear_kernel` functions instead of directly instantiating kernel classes.  
- The `ScaledMMLinearKernel` base class enforces a consistent contract (`is_supported`, `can_implement`, `apply_weights`), making it easier to add new backends.  
- Follow-up tasks (e.g., unifying `TestFP8Layer` and `TestBlockFP8Layer`) are critical to fully deprecate legacy code paths. Ensure environment variables are properly set to disable unsupported kernels.

---

## 2. [[Model Runner V2] Skip kernel launch for penalties & logit_bias](https://github.com/vllm-project/vllm/pull/32634)


### Base Information

- **PR Number:** #32634
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-19 22:20:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32634/files) (3):**
  - `vllm/v1/worker/gpu/sample/logit_bias.py`
  - `vllm/v1/worker/gpu/sample/penalties.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`

### Summary

**What changed and why**  
This PR optimizes CPU overhead by skipping GPU kernel launches for logit bias and penalty operations when they are not needed. It introduces boolean tracking arrays (`use_logit_bias` and `use_penalty`) to detect when no requests require these modifications, allowing early exit before costly kernel launches.

**Technical impact**  
The changes reduce unnecessary GPU synchronization and kernel launch overhead when logit bias or penalty features are unused. This improves performance for common inference scenarios where these advanced sampling features are not applied, while maintaining full functionality when they are needed. The architecture now requires passing NumPy index arrays alongside PyTorch tensors to enable efficient CPU-side checks.

**Potential risks**  
The early-exit logic depends on accurate tracking of `use_logit_bias` flags, which now includes stop token IDs when `min_tokens > 0` - this subtle condition could cause incorrect behavior if misunderstood. Passing both `idx_mapping` and `idx_mapping_np` creates data consistency risks if these arrays become desynchronized. The NumPy dependency in GPU sampling code introduces mixed backend complexity.

**Key insights**  
The optimization effectively targets a common performance bottleneck by avoiding unnecessary GPU operations. Developers should ensure the `idx_mapping_np` arrays remain synchronized with their PyTorch counterparts. The conditional logic for `use_logit_bias` should be documented clearly since it now encompasses three distinct features (allowed tokens, logit bias, and conditional stop tokens).

---

## 3. [[1/N] Initialize MM components in context managers (A-D)](https://github.com/vllm-project/vllm/pull/32632)


### Base Information

- **PR Number:** #32632
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-19 22:12:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32632/files) (11):**
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/bagel.py`
  - `vllm/model_executor/models/blip2.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/deepseek_vl2.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/interfaces.py`

### Summary

**What changed and why**  
This PR refactors multimodal model initialization by wrapping vision/audio tower components and language model components in context managers (`_mark_tower_model` and `_mark_language_model`). It removes explicit `get_language_model()` methods and consolidates logit computation into the language model. The changes are part of a larger effort (#32631) to standardize component initialization across multimodal models.

**Technical impact**  
The refactoring centralizes component registration through context managers, likely enabling better tracking of modality-specific modules (e.g., for quantization, LoRA, or pipeline parallelism). Removing `get_language_model()` simplifies the interface, and moving `compute_logits` to the language model (as seen in Aria) improves encapsulation. The `TowerMissingLayer` now accepts string modalities for consistency.

**Potential risks**  
If the context managers rely on global state or have side effects (e.g., modifying `vllm_config`), improper nesting or concurrent usage could lead to initialization errors. The removal of `get_language_model()` may break external code that depends on this method. Additionally, the `TowerMissingLayer` change could affect existing code expecting a `set[str]`.

**Key insights**  
This is a systematic cleanup to unify multimodal model initialization. Developers should verify that all models correctly inherit the base class providing `_mark_tower_model` and `_mark_language_model`. Ensure no downstream code calls the removed `get_language_model()` method. The `TowerMissingLayer` update is a minor but breaking change—update any direct instantiations accordingly.

---

## 4. [[Model] Use context managers for encoder- and LM-only mode](https://github.com/vllm-project/vllm/pull/32605)


### Base Information

- **PR Number:** #32605
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-19 19:43:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32605/files) (21):**
  - `examples/online_serving/disaggregated_encoder/README.md`
  - `vllm/config/model.py`
  - `vllm/config/multimodal.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/model_loader/utils.py`
  - `vllm/model_executor/models/adapters.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/opencua.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/step_vl.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces context managers (`_mark_language_model` and `_mark_tower_model`) to handle encoder-only and LM-only modes for multi-modal models, replacing a previous hacky model adapter approach. The CLI flag changes from `--convert mm_encoder_only` to `--mm-encoder-only`. Model developers now wrap component initialization within these context managers, allowing automatic skipping of language or tower models based on configuration.

**Technical impact**  
The architecture simplifies model initialization by removing the need for overriding `get_language_model` and implementing `get_language_model_spec`. The `AutoWeightsLoader` automatically skips weights for disabled components. This change affects all multi-modal model implementations (LLaVA, Qwen-VL, etc.), making their initialization more consistent and reducing boilerplate code.

**Potential risks**  
The deprecated `--convert mm_encoder_only` flag still works but will be removed in v0.15, requiring users to update their CLI commands. There's a risk of improper context manager usage if model developers don't wrap component initialization correctly. The removal of explicit `skip_prefixes` logic in weight loading assumes `AutoWeightsLoader` correctly handles missing layers.

**Key insights**  
Developers must update multi-modal models to use the new context managers instead of manual checks for `get_limit_per_prompt`. The `--mm-encoder-only` flag should be used going forward. This change paves the way for future simplification of `get_mm_mapping` and further unification of multi-modal model interfaces.

---

## 5. [[Model Runner V2] Decouple temperature from penalties](https://github.com/vllm-project/vllm/pull/32629)


### Base Information

- **PR Number:** #32629
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-19 19:13:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32629/files) (3):**
  - `vllm/v1/worker/gpu/sample/gumbel.py`
  - `vllm/v1/worker/gpu/sample/penalties.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`

### Summary

**What changed and why**  
The changes decouple temperature application from penalty application by separating them into distinct operations. Previously, temperature and penalties were applied together in a single kernel (`_penalties_and_temperature_kernel`). Now, temperature is applied via a new dedicated kernel (`_temperature_kernel`), while penalties are handled separately (`_penalties_kernel`). This modularizes the sampling pipeline, allowing temperature and penalties to be applied independently.

**Technical impact**  
This refactoring improves code maintainability and flexibility by isolating concerns. The sampling pipeline now explicitly applies penalties first, then temperature, which may affect performance due to additional kernel launches but provides clearer separation. The gumbel sampling kernel also references the new temperature kernel instead of the combined one, ensuring consistency in temperature handling across different sampling paths.

**Potential risks**  
Introducing an extra kernel launch could slightly increase overhead, especially for small batch sizes. There's a risk of inconsistent behavior if the temperature kernel's early return conditions (`temperature == 0.0 or temperature == 1.0`) don't match the previous logic (`temperature == 0.0` mapped to `1.0`). Additionally, the decoupling assumes no interdependencies between penalty and temperature operations, which should be validated.

**Key insights**  
This change aligns with good software design principles by separating concerns, making the code easier to test and modify. Developers should verify that the new temperature kernel's behavior matches the old combined kernel exactly, particularly for edge cases like zero temperature. Performance profiling is recommended to ensure the added kernel launch doesn't negatively impact throughput.

---

## 6. [[Model Runner V2] Refactor get_cudagraph_and_dp_padding](https://github.com/vllm-project/vllm/pull/32625)


### Base Information

- **PR Number:** #32625
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-19 18:25:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32625/files) (2):**
  - `vllm/v1/worker/gpu/dp_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The refactor moves the `get_cudagraph_and_dp_padding` function from `model_runner.py` to `dp_utils.py` and simplifies its logic. The function now directly returns a tuple `(bool, int, torch.Tensor \| None)` indicating CUDA graph usage, padded token count, and DP tensor, instead of using the `CUDAGraphMode` enum. This centralizes DP and CUDA graph coordination logic.

**Technical impact**  
This change improves code organization by separating DP coordination logic from the model runner. The model runner now delegates the complex decision-making about CUDA graph usage and DP padding to a utility function, making the main execution path cleaner. The interface simplification (boolean vs enum) reduces complexity for callers.

**Potential risks**  
The removal of the `CUDAGraphMode.NONE` path in the utility function's return type could affect other code that might have relied on the enum. The special handling for `num_tokens == 0` and `cudagraph_size = -1` logic requires careful validation to ensure all edge cases (like mixed DP ranks with/without CUDA graphs) are handled correctly.

**Key insights**  
This is a well-structured refactor that follows separation of concerns principles. Developers should note that CUDA graph coordination across DP ranks is now encapsulated in `dp_utils.py`. The boolean return (`use_cudagraph`) is simpler than the previous enum approach, though the TODO about piecewise CUDA graphs remains unaddressed in the execution path.

---

## 7. [[Feat] allow inplace loading lora](https://github.com/vllm-project/vllm/pull/31326)


### Base Information

- **PR Number:** #31326
- **Author:** [Jackmin801](https://github.com/Jackmin801)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-19 18:15:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31326/files) (10):**
  - `docs/features/lora.md`
  - `tests/entrypoints/conftest.py`
  - `tests/entrypoints/openai/test_lora_adapters.py`
  - `tests/lora/conftest.py`
  - `tests/lora/test_llm_with_multi_loras.py`
  - `vllm/entrypoints/openai/models/serving.py`
  - `vllm/entrypoints/serve/lora/api_router.py`
  - `vllm/entrypoints/serve/lora/protocol.py`
  - `vllm/lora/request.py`
  - `vllm/lora/worker_manager.py`

### Summary

**What changed and why**  
This PR introduces in-place LoRA reloading capability, allowing `/v1/load_lora_adapter` to replace an existing LoRA adapter with updated weights while keeping the same name. The primary use case is reinforcement learning training, where policies need to be updated dynamically without interrupting long-running inference requests. A new `load_inplace` parameter controls this behavior.

**Technical impact**  
The changes extend the LoRA loading protocol (`LoadLoRAAdapterRequest` and `LoRARequest`), modify server-side validation to allow duplicate names only when `load_inplace=True`, and update the worker’s `LRUCacheWorkerLoRAManager` to replace adapters in the cache. The system now reuses existing `lora_int_id` for in-place updates, preserving adapter identity while swapping weights.

**Potential risks**  
If `load_inplace=True` is used without proper synchronization, concurrent inference requests might see inconsistent adapter states during replacement. The eviction logic in `add_adapter` temporarily exceeds the cache capacity when loading a new adapter before removing the old one, which could cause memory spikes. There is also a risk of silent failures if the new adapter fails to load after the old one is removed.

**Key insights**  
Developers should ensure that `load_inplace=True` is used only when necessary and with appropriate coordination to avoid race conditions. The validation error messages have been improved to guide users when duplicate names are detected. The feature is well-tested with both OpenAI API and offline LLM scenarios, but performance under high-frequency reloads should be monitored.

---

## 8. [[Model Runner V2] Initialized communication buffer for DP](https://github.com/vllm-project/vllm/pull/32624)


### Base Information

- **PR Number:** #32624
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-19 17:27:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32624/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
Added a call to `prepare_communication_buffer_for_model` in the `load_model` method to ensure communication buffers are properly initialized for the main model and speculator model (if present). This fixes a bug where Data Parallel (DP) + Expert Parallel (EP) configurations in Model Runner V2 would produce incorrect outputs due to missing buffer preparation.

**Technical impact**  
The change ensures that distributed communication buffers are set up immediately after model loading, which is critical for correct tensor parallelism and expert parallelism operations. It also handles the speculator model separately when speculative decoding is enabled, maintaining consistency across all model components.

**Potential risks**  
If `prepare_communication_buffer_for_model` has side effects or is called multiple times elsewhere, it could lead to redundant operations or conflicts. Additionally, the conditional check for `speculator_model` assumes the speculator attribute exists and has a `model` property, which might not hold in all configurations, potentially causing AttributeErrors.

**Key insights**  
This fix is essential for distributed training/inference scenarios using DP+EP. Developers should verify that `prepare_communication_buffer_for_model` is idempotent and consider adding similar initialization in other model-loading paths if needed. Testing should include edge cases where the speculator is absent or configured differently.

---

## 9. [[Attention][MLA] Make FLASHINFER_MLA the default MLA backend on Blackwell, and TRTLLM the default prefill](https://github.com/vllm-project/vllm/pull/32615)


### Base Information

- **PR Number:** #32615
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-19 15:41:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32615/files) (6):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/attention.yaml`
  - `vllm/config/attention.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/platforms/cuda.py`

### Summary

**What changed and why**  
This PR re-applies changes to make FLASHINFER_MLA the default MLA backend on Blackwell GPUs and TRTLLM the default prefill implementation. These changes were previously reverted due to CI failures that have since been fixed. The modifications update configuration defaults and remove environment variable workarounds in CI test pipelines.

**Technical impact**  
The changes alter the default attention backend selection logic for Blackwell GPUs from CUTLASS_MLA to FLASHINFER_MLA, and prioritize TRTLLM prefill over FlashInfer prefill in the MLA implementation. This affects performance characteristics and memory usage patterns for MLA attention operations on supported hardware. CI pipelines are updated to run tests with the new defaults instead of disabling FlashInfer prefill.

**Potential risks**  
The priority change in prefill selection (TRTLLM before FlashInfer) could introduce regressions if TRTLLM has undiscovered edge cases or performance issues. Removing the dependency check between `use_trtllm_ragged_deepseek_prefill` and `use_flashinfer_prefill()` might cause unexpected interactions if both configurations are enabled simultaneously. Platform-specific defaults could lead to inconsistent behavior across different GPU architectures.

**Key insights**  
Developers should verify that TRTLLM prefill implementation is production-ready and thoroughly tested across various model architectures. The logging level changes from `debug_once` to `info_once` will make backend selection more visible in production logs. Ensure compatibility testing covers both Blackwell and non-Blackwell platforms since default behaviors now diverge based on GPU capability family.

---

## 10. [[Model Runner V2] Refactor `dummy_run`](https://github.com/vllm-project/vllm/pull/32533)


### Base Information

- **PR Number:** #32533
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-19 14:50:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32533/files) (3):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The PR refactors the `_dummy_run` method in the model runner to reuse the existing `execute_model` infrastructure instead of maintaining separate dummy run logic. This simplifies the codebase by eliminating duplicate code paths for dummy execution.

**Technical impact**  
The `_dummy_run` method now creates a dummy `SchedulerOutput` and calls `execute_model` with appropriate parameters, consolidating dummy run logic into a single execution path. The `get_cudagraph_size` and `get_cudagraph_and_dp_padding` methods have been modified to accept token distribution data directly rather than requiring a full `SchedulerOutput` object.

**Potential risks**  
The refactored dummy run may not properly handle LoRA warmup (as noted by the "FIXME" comment). There's also a risk that the consolidated execution path could introduce subtle behavioral differences in edge cases, particularly around attention metadata preparation and CUDA graph sizing calculations.

**Key insights**  
This change improves code maintainability by reducing duplication, but developers should verify that the dummy run behavior remains identical for profiling and warmup purposes. The LoRA warmup issue needs to be addressed separately. The parameter changes to CUDA graph methods improve flexibility but require callers to pass token distribution data explicitly.

---

## 11. [feat: spec decode with draft models](https://github.com/vllm-project/vllm/pull/24322)


### Base Information

- **PR Number:** #24322
- **Author:** [tomasruizt](https://github.com/tomasruizt)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-01-19 13:05:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/24322/files) (21):**
  - `examples/offline_inference/spec_decode.py`
  - `examples/online_serving/disaggregated_serving/moriio_toy_proxy_server.py`
  - `tests/v1/e2e/test_spec_decode.py`
  - `tests/v1/worker/test_utils.py`
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/lib/ready_checker.py`
  - `vllm/config/parallel.py`
  - `vllm/config/speculative.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/model_loader/__init__.py`
  - `vllm/model_executor/model_loader/base_loader.py`
  - `vllm/model_executor/model_loader/gguf_loader.py`
  - `vllm/model_executor/model_loader/tensorizer_loader.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/utils.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/spec_decode/draft_model.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
This PR introduces speculative decoding using a separate draft model (e.g., Qwen3-1.7B as draft for Qwen3-32B). The implementation adds a new `DraftModelProposer` class, integrates it with the existing speculative decoding framework, and extends configuration, model loading, and runtime components to support draft-model-based speculation. The goal is to accelerate inference by having a smaller draft model propose tokens that are then verified by the target model, without requiring special trained heads like EAGLE or Medusa.

**Technical impact**  
The changes enable a new speculative decoding method that works with arbitrary model pairs sharing the same vocabulary. Key additions include:
- A `DraftModelProposer` with a Triton kernel (`merge_toks_kernel`) for merging target and draft tokens.
- Configuration validation for draft-model-specific settings (e.g., `draft_tensor_parallel_size`, vocabulary size checks).
- Modified model loading to support draft models with separate `VllmConfig` and namespace prefixes (`draft_model.`).
- Updates to the scheduler, attention metadata utilities, and GPU model runner to handle draft-model speculation.
- Comprehensive tests for correctness, quantization, and tensor parallelism.

**Potential risks**  
- The draft model must have the same vocabulary size as the target model; mismatches cause runtime errors.
- Tensor parallelism for draft and target models must currently be equal due to Torch compile cache conflicts (a known limitation).
- Non-greedy sampling (temperature > 0) may have lower acceptance rates until lossless rejection sampling is implemented (tracked in a separate PR).
- Multimodal models and M-RoPE are not yet supported with draft-model speculation.
- Inter-GPU communication overhead may reduce gains when using tensor parallelism > 1.

**Key insights**  
- Draft-model speculative decoding can significantly improve throughput (up to 2× in benchmarks) by leveraging faster draft models, though it may increase inter-token latency.
- Developers must specify `draft_tensor_parallel_size` (not `tensor_parallel_size`) in the speculative config to avoid validation errors.
- The implementation reuses much of the EAGLE infrastructure, ensuring consistency and reducing maintenance overhead.
- Future optimizations include reducing forward passes by including `next_token_ids` in the first draft model pass and improving TP > 1 performance.

---

## 12. [docs: prefix caching seems quite outdated](https://github.com/vllm-project/vllm/pull/28784)


### Base Information

- **PR Number:** #28784
- **Author:** [longregen](https://github.com/longregen)
- **Merged By:** [russellb](https://github.com/russellb)
- **Merged time:** 2026-01-19 11:49:52
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28784/files) (1):**
  - `docs/design/prefix_caching.md`

### Summary

**What changed and why**  
Updated documentation for prefix caching to reflect current hashing algorithm defaults and options. The changes clarify that SHA256 is now the default (since v0.11) and introduce new hashing algorithm choices (`sha256_cbor`, `xxhash`, `xxhash_cbor`) with their respective trade-offs.

**Technical impact**  
The documentation now accurately describes the available hashing algorithms and their implications for reproducibility, performance, and security. It guides users on selecting algorithms via the `--prefix-caching-hash-algo` CLI flag, ensuring better alignment with the actual implementation.

**Potential risks**  
The note about `xxhash` warns of increased collision risks in multi-tenant setups, which could lead to undefined behavior or data leakage. Users might misinterpret "reproducible" hashing across environments without verifying compatibility of their specific setup.

**Key insights**  
Always use `sha256_cbor` for deterministic caching across different Python versions or deployments. Avoid `xxhash` in security-sensitive, multi-tenant environments unless performance is critical and risks are acceptable. Verify that the chosen algorithm matches the vLLM version in use.

---

## 13. [[BugFix] Fix TRT-LLM NVFP4 DP/EP](https://github.com/vllm-project/vllm/pull/32349)


### Base Information

- **PR Number:** #32349
- **Author:** [jiahanc](https://github.com/jiahanc)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-19 11:32:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32349/files) (4):**
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`

### Summary

**What changed and why**  
This PR fixes a bug where FlashInfer TRT-LLM NVFP4 MoE with data parallelism (DP) and expert parallelism (EP) was broken by a previous change. The issue occurred because the code was checking for a specific quantization class (`ModelOptNvFp4FusedMoE`) and FlashInfer TRT-LLM availability, which failed when `moe_quant_config` was `None`. The fix replaces this hardcoded check with a dynamic property (`do_post_quant_allgather`) and removes the problematic assertion.

**Technical impact**  
The changes decouple the DP/EP allgather logic from specific backend detection, making the code more maintainable and extensible. The quantization layer now exposes a property to indicate when post-quantization allgather is needed, and the fused MoE layer uses this property instead of checking concrete class types. This improves abstraction and reduces coupling between components.

**Potential risks**  
The removal of the assertion `assert self.moe_quant_config is not None` could mask other configuration issues if `do_post_quant_allgather` is incorrectly implemented. There's also a risk that the new property-based approach might not be set correctly for other quantization methods, though the current implementation seems specific to the NVFP4 FlashInfer TRT-LLM backend.

**Key insights**  
The fix demonstrates good software design by replacing type checking with behavior checking via properties. Developers should ensure that any new quantization backends implementing similar DP/EP allgather logic properly define the `do_post_quant_allgather` property. The added test configuration validates the fix works correctly with the specific model and environment settings that triggered the original bug.

---

## 14. [[CI] Add Helion as an optional dependency](https://github.com/vllm-project/vllm/pull/32482)


### Base Information

- **PR Number:** #32482
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-19 11:09:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32482/files) (5):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `setup.py`
  - `tests/kernels/helion/test_helion_available.py`
  - `vllm/utils/import_utils.py`

### Summary

**What changed and why**  
This PR integrates PyTorch Helion as an optional dependency to support future kernel development and testing. It adds a new `helion` extra in `setup.py`, creates a utility function to check for Helion availability, and establishes a dedicated CI test pipeline for Helion-related kernel tests.

**Technical impact**  
The changes enable optional installation of Helion via `pip install vllm[helion]` and introduce a pattern for conditionally importing and testing Helion-based kernels. The new CI jobs ensure Helion functionality is validated on both AMD and NVIDIA GPU hardware, maintaining compatibility with the existing codebase.

**Potential risks**  
The CI jobs assume `helion` is available via PyPI; any installation issues could cause test failures. The conditional import pattern requires developers to consistently use `has_helion()` checks, and missing checks could lead to runtime import errors. The test currently uses a simple kernel; more complex kernels may reveal integration challenges.

**Key insights**  
Developers should follow the provided import pattern to avoid runtime errors when Helion is not installed. The CI setup supports cross-platform testing, but future work should ensure Helion kernels are properly integrated into vLLM's execution path. Consider adding documentation for kernel authors on using Helion within vLLM.

---

## 15. [[BUGFIX] Fix `test_mla_backends.py`. Scale MLA projection weights to prevent numerical instability](https://github.com/vllm-project/vllm/pull/32529)


### Base Information

- **PR Number:** #32529
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-19 10:49:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32529/files) (1):**
  - `tests/v1/attention/test_mla_backends.py`

### Summary

**What changed and why**  
The PR adds scaling to the key and value projection weights (`W_UK` and `W_UV`) in a test for MLA (Multi-Query Latent Attention) backends. The scaling factor is `1/sqrt(kv_lora_rank)`, which normalizes the variance of the projection outputs to approximately 1. This addresses a root cause where unnormalized random weights with high rank (e.g., 512) produced outputs with large magnitudes (~22.6 standard deviation), leading to extreme attention scores and numerical instability during log-sum-exp (LSE) operations.

**Technical impact**  
This change ensures the test operates in a realistic numerical range, preventing spurious failures due to precision loss in attention score computations. It allows for a fair comparison between different attention backends by eliminating artificial numerical instability that masked true backend correctness. The fix is minimal and localized to the test setup, leaving production code unaffected.

**Potential risks**  
The scaling assumes the input to the projection is unit-variance; if the test's input distribution changes, the scaling may need adjustment. There is a slight risk that over-scaling could mask other numerical issues that should be caught, though the chosen factor is mathematically sound for Gaussian random matrices. The fix does not address any potential numerical instability in the LSE implementation itself, only the test inputs that trigger it.

**Key insights**  
This is a proper fix that addresses the statistical properties of random matrix multiplication, ensuring test robustness. Developers should recognize that test weights should be scaled to match expected operational variance to avoid misleading failures. Consider documenting this scaling rationale in the test for future maintainers, and ensure similar scaling is applied in other tests using random projection weights to maintain consistency.

---

## 16. [[CI][amd] Revert NIXL connector change to avoid crash](https://github.com/vllm-project/vllm/pull/32570)


### Base Information

- **PR Number:** #32570
- **Author:** [qli88](https://github.com/qli88)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-19 10:39:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32570/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `docker/Dockerfile.rocm`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
The PR reverts a UCX environment variable change from `UCX_MEM_MMAP_HOOK_MODE=none` to `UCX_RCACHE_MAX_UNRELEASED=1024` to prevent crashes on ROCm platforms. This is a workaround for a memory leak issue when using NIXL, as the original setting breaks NIXL test groups on AMD hardware. A typo in the Dockerfile.rocm is also corrected.

**Technical impact**  
This change modifies the UCX configuration strategy for memory management in distributed KV cache transfers. The new environment variable limits unreleased memory entries in UCX's registration cache, which should mitigate the same memory leak but with better compatibility on ROCm systems. The test configuration is also expanded to include production AMD hardware.

**Potential risks**  
The workaround may not fully resolve the underlying memory leak issue, potentially leading to different performance characteristics or resource exhaustion under heavy load. There's a risk that the new UCX setting could introduce regressions on non-ROCm platforms that were previously stable with the mmap hook approach.

**Key insights**  
This is a temporary compatibility fix while the NIXL/RIXL teams investigate the root cause. Developers should monitor system memory usage in production deployments and be prepared to remove this override once the upstream fix is implemented. The expanded test coverage to production hardware is a positive step for validation.

---

## 17. [support dynamic resolution image encoding for Nemotron Nano VL](https://github.com/vllm-project/vllm/pull/32121)


### Base Information

- **PR Number:** #32121
- **Author:** [netanel-haber](https://github.com/netanel-haber)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-19 10:15:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32121/files) (3):**
  - `vllm/model_executor/models/intern_vit.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/radio.py`

### Summary

**What changed and why**  
This PR adds dynamic image resolution support for Nemotron Nano VL, allowing images to contribute a variable number of tokens based on available context length. It introduces a new `DynamicResolutionImageTiler` that resizes images to fit within token limits while preserving static resolution for videos. The changes extend `radio.py` and `intern_vit.py` to handle concatenated ragged tensors of images with manual attention masking to prevent cross-image attention.

**Technical impact**  
The architecture now supports two image input modes: static (`pixel_values`) and dynamic (`pixel_values_dynamic`). The vision encoder is extended with new classes (`RadioParallelAttention`, `RadioVisionEncoderLayer`, `RadioVisionEncoder`) that accept attention masks to isolate image patches. This enables variable-length image sequences within a single batch, increasing flexibility for multimodal prompts but requiring careful handling of positional embeddings and token extraction.

**Potential risks**  
Dynamic resolution may lead to inconsistent image representations across different context lengths, potentially affecting model performance. The attention masking logic assumes images are non-overlapping in the sequence; incorrect mask construction could allow cross-image attention. Memory fragmentation could occur with highly variable token counts per image. The feature size cache uses `id(image)` which may be unsafe if image objects are reused or modified.

**Key insights**  
Developers must configure `min_num_patches` and `max_num_patches` in the vision config to enable dynamic resolution. The system now requires explicit handling of `imgs_sizes` lists for dynamic inputs. Ensure attention masks are correctly generated for multi-image batches. Consider performance implications of dynamic tiling versus static preprocessing, especially for latency-sensitive applications.

---

## 18. [[Misc] Remove unused ModelKeys](https://github.com/vllm-project/vllm/pull/32608)


### Base Information

- **PR Number:** #32608
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-19 09:34:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32608/files) (1):**
  - `vllm/model_executor/models/module_mapping.py`

### Summary

**What changed and why**  
The PR removes the unused `ModelKeys` dataclass entirely and changes `MultiModelKeys` to no longer inherit from it. This cleanup eliminates dead code that was not being utilized in the codebase.

**Technical impact**  
This change simplifies the module mapping structure by removing an unnecessary abstraction layer. Since `ModelKeys` was not referenced elsewhere, its removal has no functional impact on existing model implementations or their mappings.

**Potential risks**  
Low risk, as the class was unused. However, if any downstream code or future extensions were planning to rely on `ModelKeys`, they would now need to adjust. The inheritance change to `MultiModelKeys` should be verified to ensure no implicit dependencies existed.

**Key insights**  
This is a straightforward code hygiene improvement. Developers should ensure no external scripts or undocumented uses of `ModelKeys` exist. Future additions to `MultiModelKeys` should define fields directly rather than reintroducing unused base classes.

---

## 19. [Add support for LoRA adapters in Nemotron-H models](https://github.com/vllm-project/vllm/pull/30802)


### Base Information

- **PR Number:** #30802
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-19 06:30:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30802/files) (10):**
  - `tests/lora/test_layers.py`
  - `vllm/lora/layers/__init__.py`
  - `vllm/lora/layers/column_parallel_linear.py`
  - `vllm/lora/layers/fused_moe.py`
  - `vllm/lora/lora_weights.py`
  - `vllm/lora/model_manager.py`
  - `vllm/lora/utils.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
This PR adds LoRA adapter support for Nemotron-H models, specifically handling their non-gated MoE architecture and variable-slice linear layers. The changes introduce a new `MergedColumnParallelLinearVariableSliceWithLoRA` class to handle layers with 3+ output slices, modify fused MoE layers to support non-gated MoE, and update LoRA weight packing logic to accommodate the different expert structure.

**Technical impact**  
The implementation extends vLLM's LoRA framework to support Nemotron-H's unique architecture where checkpoints contain single weight tensors that correspond to multiple output slices. The system now properly distinguishes between gated and non-gated MoE configurations, with non-gated MoE using only w1/w2 expert projections instead of w1/w2/w3. The changes maintain backward compatibility while adding new capability for variable-slice linear layers.

**Potential risks**  
The conditional logic for handling 2 vs 3+ slices introduces complexity that could lead to incorrect layer selection in edge cases. The non-gated MoE support excludes gate modules from LoRA adaptation, which may limit fine-tuning capabilities. There's also a risk of incorrect scaling when reusing w1 weights for w3 in non-gated MoE, though the code addresses this with scaling adjustments.

**Key insights**  
Developers should note that Nemotron-H models require special handling due to their non-gated MoE architecture and variable-slice linear layers. The new `MergedColumnParallelLinearVariableSliceWithLoRA` class is specifically designed for cases with 3+ output slices, while existing classes handle 2-slice cases. When working with non-gated MoE models, gate modules are intentionally excluded from LoRA adaptation due to PEFT compatibility issues.

---

## 20. [[Frontend] Score entrypoint support data_1 & data_2 and queries & documents as inputs](https://github.com/vllm-project/vllm/pull/32577)


### Base Information

- **PR Number:** #32577
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-19 06:07:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32577/files) (17):**
  - `docs/serving/openai_compatible_server.md`
  - `examples/offline_inference/basic/score.py`
  - `examples/offline_inference/openai_batch/README.md`
  - `examples/pooling/score/cohere_rerank_client.py`
  - `examples/pooling/score/qwen3_reranker_online.py`
  - `examples/pooling/score/score_api_online.py`
  - `examples/pooling/score/vision_rerank_api_online.py`
  - `examples/pooling/score/vision_score_api_online.py`
  - `tests/entrypoints/openai/test_run_batch.py`
  - `tests/entrypoints/pooling/classify/test_online.py`
  - `tests/entrypoints/pooling/score/test_offline.py`
  - `tests/entrypoints/pooling/score/test_online_score.py`
  - `tests/models/language/pooling_mteb_test/mteb_score_utils.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/run_batch.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/entrypoints/pooling/score/serving.py`

### Summary

**What changed and why**  
The PR introduces new input parameter names (`queries`/`documents` and `data_1`/`data_2`) for the score entrypoint to replace the generic `text_1`/`text_2`, which were misleading for multimodal rerank models. It maintains backward compatibility by supporting all three naming conventions (`text_1`/`text_2`, `data_1`/`data_2`, and `queries`/`documents`) through a union type alias and property mappings.

**Technical impact**  
This change affects the API layer, request validation, and internal handling of score requests. The `ScoreRequest` is now a type alias for three distinct Pydantic models (`ScoreTextRequest`, `ScoreDataRequest`, `ScoreQueriesDocumentsRequest`), each mapping to a common `data_1`/`data_2` interface internally. This ensures consistent processing while expanding the API's expressiveness for multimodal use cases.

**Potential risks**  
The introduction of multiple request types increases complexity in validation and error handling. There is a risk of confusion if clients mix parameter naming conventions incorrectly. Additionally, the changes to `run_batch.py` and validation logic must correctly handle all three request types to avoid runtime errors or silent misprocessing.

**Key insights**  
Developers should adopt `queries`/`documents` as the new standard for clarity, especially in multimodal contexts. The backward compatibility with `text_1`/`text_2` and `data_1`/`data_2` eases migration but adds maintenance overhead. Ensure all client examples and documentation are updated to reflect the preferred naming.

---

## 21. [[Nixl][Bugfix] Track `nixl_num_kv_expired_reqs` metric in Prometheus](https://github.com/vllm-project/vllm/pull/32340)


### Base Information

- **PR Number:** #32340
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-01-19 04:28:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32340/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR adds a new Prometheus metric (`nixl_num_kv_expired_reqs`) to track requests with expired KV blocks, replacing error-prone log parsing for monitoring. It also fixes a bug where `is_empty()` incorrectly discarded time intervals containing only failed requests, delaying their reporting until a successful transfer occurred.

**Technical impact**  
The changes enhance observability by providing structured metrics for KV expiration events and ensure all failure types (expired KV, failed transfers, failed notifications) are reliably reported to Prometheus. The `is_empty()` logic now correctly identifies intervals with any failure metrics as non-empty, preventing data loss in failure-only scenarios.

**Potential risks**  
The metric is tracked only on the P instance, which could create confusion if users expect it on other instances. The `reduce()` method still returns zeroed stats for intervals with only failures (as noted), which might obscure failure patterns in CLI logs, though Prometheus receives the data.

**Key insights**  
Developers should verify the metric's instance-specific behavior aligns with monitoring expectations. The fix to `is_empty()` is critical for accurate failure reporting; ensure similar patterns in other stats classes are reviewed. Consider aligning CLI log reporting with Prometheus to provide consistent visibility across both outputs.

---

## 22. [[CI/Build] Fix dependency conflict between model-hosting-container-standards and starlette](https://github.com/vllm-project/vllm/pull/32560)


### Base Information

- **PR Number:** #32560
- **Author:** [DanielMe](https://github.com/DanielMe)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-19 03:27:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32560/files) (1):**
  - `requirements/test.txt`

### Summary

**What changed and why**  
The PR resolves a dependency conflict by updating pinned versions in `requirements/test.txt`. Specifically, `fastapi` was upgraded from `0.116.1` to `0.128.0` and `starlette` from `0.46.2` to `0.50.0`. This change was necessary because a prior update to `model-hosting-container-standards>=0.1.13` introduced a requirement for `starlette>=0.49.1`, which conflicted with the older pinned versions.

**Technical impact**  
These updates align the test environment dependencies with the common requirements, ensuring consistent dependency resolution across the project. The changes maintain compatibility with `model-hosting-container-standards` while upgrading FastAPI and Starlette to newer, compatible versions, which may include security patches, bug fixes, and new features.

**Potential risks**  
Upgrading FastAPI and Starlette could introduce breaking changes in API behavior or internal interfaces, though the test suite passing mitigates this risk. There is a slight risk of undiscovered incompatibilities in untested code paths or with other indirect dependencies. Additionally, the new `annotated-doc` dependency (added via FastAPI) should be monitored for any unexpected side effects.

**Key insights**  
Dependency conflicts should be resolved by aligning version constraints across requirement files, as demonstrated here. It's crucial to verify that upgrades do not break existing functionality through comprehensive testing, including integration tests for HTTP routing and middleware. Future dependency updates should consider transitive dependencies to prevent similar conflicts.

---

## 23. [[Core] Whisper support `torch.compile`](https://github.com/vllm-project/vllm/pull/30385)


### Base Information

- **PR Number:** #30385
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-19 02:02:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30385/files) (5):**
  - `tests/entrypoints/openai/correctness/test_transcription_api_correctness.py`
  - `vllm/compilation/decorators.py`
  - `vllm/forward_context.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR adds `torch.compile` support for Whisper's decoding step by implementing a mechanism to skip compilation for the first decoder step in encoder-decoder models. The first step computes cross-attention KV caches using encoder outputs, creating a different computational graph than subsequent steps. The solution introduces a `skip_compiled` flag in the forward context to bypass compilation when encoder inputs are present.

**Technical impact**  
The changes extend the existing `torch.compile` infrastructure to handle encoder-decoder models like Whisper. By conditionally skipping compilation for the first decoder step, the system avoids graph recompilation due to varying input shapes, maintaining performance gains from compilation on pure decoding steps. The implementation is minimally invasive, leveraging the forward context and existing compilation decorators.

**Potential risks**  
If the `skip_compiled` flag is incorrectly set or the forward context is unavailable, compilation may be applied to incompatible steps, potentially causing runtime errors or performance degradation. The reliance on `has_encoder_input` logic assumes accurate detection of the first step; edge cases like mixed encoder/decoder batches or dynamic batching could lead to misclassification. External model runners using `_model_forward` may be unaffected due to the updated approach, but changes to the forward context API could still impact them.

**Key insights**  
The PR effectively addresses a common compilation challenge in encoder-decoder models by decoupling the skip logic from the model runner and embedding it in the compilation decorator. Developers should verify that `skip_compiled` is only set when encoder inputs are present and ensure forward context availability in all execution paths. Consider adding validation or logging to confirm the flag's behavior during multi-step decoding.

---

## 24. [[ROCm][CI] Add ROCm attention backend support for EAGLE DP tests](https://github.com/vllm-project/vllm/pull/32363)


### Base Information

- **PR Number:** #32363
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-19 01:57:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32363/files) (1):**
  - `tests/v1/distributed/test_eagle_dp.py`

### Summary

**What changed and why**  
This PR adds ROCm platform support to the EAGLE data parallel tests by parametrizing attention backends based on the detected platform. It introduces platform detection to conditionally select appropriate attention backends (ROCm-specific backends for ROCm, FLASH_ATTN for CUDA) and applies the `VLLM_BATCH_INVARIANT` flag only for CUDA, while marking ROCm tests as expected to fail due to batch invariance limitations.

**Technical impact**  
The changes enable cross-platform testing by dynamically adjusting attention backends and test behavior. This improves test coverage for ROCm without affecting CUDA execution, and the parametrization allows systematic validation of multiple ROCm attention backends. The architecture now cleanly separates platform-specific configurations within a single test.

**Potential risks**  
The conditional application of `VLLM_BATCH_INVARIANT` may lead to inconsistent test behavior between platforms if not properly managed. The `xfail` decorator for ROCm could mask real failures if issues persist beyond the referenced batch invariance problem. Additionally, the hardcoded backend lists might require updates as new backends are added or deprecated.

**Key insights**  
Developers should ensure that any future attention backend additions are reflected in the `ATTN_BACKENDS` lists. The ROCm batch invariance issue (#27433) should be tracked closely to remove the `xfail` once resolved. Consider extracting platform-specific configurations into a shared utility to maintain consistency across other tests.

---

## 25. [[GLM-4.7] GLM Model support for GLM-Lite](https://github.com/vllm-project/vllm/pull/31386)


### Base Information

- **PR Number:** #31386
- **Author:** [zRzRzRzRzRzRzR](https://github.com/zRzRzRzRzRzRzR)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-19 01:18:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31386/files) (9):**
  - `benchmarks/kernels/benchmark_moe.py`
  - `benchmarks/kernels/benchmark_moe_permute_unpermute.py`
  - `docs/features/tool_calling.md`
  - `tests/models/registry.py`
  - `vllm/config/speculative.py`
  - `vllm/model_executor/models/glm4_moe_lite.py`
  - `vllm/model_executor/models/glm4_moe_lite_mtp.py`
  - `vllm/model_executor/models/glm4_moe_mtp.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds support for the GLM-4.7-Flash (GLM-Lite) model to vLLM. The changes include new model implementations (`glm4_moe_lite.py` and `glm4_moe_lite_mtp.py`), updates to benchmarks and documentation, and integration into the model registry and speculative decoding config. The purpose is to enable inference for this new model variant, which likely offers a more efficient or lightweight architecture compared to existing GLM-4 models.

**Technical impact**  
The addition introduces a new model architecture that inherits heavily from existing GLM-4 MoE and DeepSeekV2 components, minimizing code duplication. It integrates with vLLM's pipeline parallelism, speculative decoding (MTP), and benchmarking suites. The model is configured to require Transformers 5.0.0, aligning with upstream Hugging Face support.

**Potential risks**  
The model relies on a Transformers version (`5.0.0.dev`) that may not be stable or widely available, potentially causing compatibility issues. There is also a risk of subtle behavioral differences in the inherited classes (e.g., `Glm4MoeLiteMLP` simply passes through) if the base implementations evolve. The speculative decoding override sets `num_hidden_layers` to 0, which could conflict with other config assumptions.

**Key insights**  
Developers should ensure Transformers 5.0.0 is available before using this model. The implementation effectively reuses existing components, but any changes to parent classes (like `DeepseekV2Attention` or `Glm4MoE`) will propagate to GLM-Lite. The model is marked as `is_available_online=False` in tests, indicating it may not yet be publicly accessible or requires specific setup.

---

## 26. [[CI][Hardware][AMD] Fix test_rotary_embedding_mla_cache_fused](https://github.com/vllm-project/vllm/pull/32408)


### Base Information

- **PR Number:** #32408
- **Author:** [mawong-amd](https://github.com/mawong-amd)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-19 00:25:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32408/files) (1):**
  - `tests/kernels/core/test_rotary_embedding_mla_cache_fused.py`

### Summary

**What changed and why**  
The PR fixes a failing AMD CI test by replacing the Torch-native RoPE reference implementation with a custom HIP-based implementation on ROCm platforms. This addresses numerical differences caused by FP16 multiplication behavior: PyTorch implicitly upcasts to FP32 while custom kernels perform native FP16 operations, leading to divergence amplified by FP8 casting.

**Technical impact**  
The change ensures test consistency by aligning the reference implementation's numerical behavior with the fused custom kernel on AMD hardware. It introduces platform-specific logic (`current_platform.is_rocm()`) but maintains the original Torch-native path for non-ROCm platforms, preserving cross-platform compatibility.

**Potential risks**  
The platform-specific branching adds maintenance complexity and could lead to divergence if future kernel implementations change. There is also a risk of overlooking similar numerical discrepancies in other tests or kernels that rely on FP16 operations with implicit upcasting.

**Key insights**  
Developers should verify that `forward_hip` produces bitwise-identical results to the fused kernel under test. Consider documenting this numerical behavior difference in the codebase, and evaluate whether other tests might require similar adjustments for ROCm compatibility.

---

