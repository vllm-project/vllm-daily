# vLLM Merged PR Report

**Report Date:** 2026-01-22 PST

**Total Merged PRs:** 34

---

## 1. [[CI] Fix mypy for `vllm/v1/structured_output`](https://github.com/vllm-project/vllm/pull/32722)


### Base Information

- **PR Number:** #32722
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 19:55:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32722/files) (18):**
  - `tools/pre_commit/mypy.py`
  - `vllm/reasoning/abs_reasoning_parsers.py`
  - `vllm/reasoning/basic_parsers.py`
  - `vllm/reasoning/deepseek_v3_reasoning_parser.py`
  - `vllm/reasoning/gptoss_reasoning_parser.py`
  - `vllm/reasoning/holo2_reasoning_parser.py`
  - `vllm/reasoning/hunyuan_a13b_reasoning_parser.py`
  - `vllm/reasoning/identity_reasoning_parser.py`
  - `vllm/reasoning/minimax_m2_reasoning_parser.py`
  - `vllm/reasoning/mistral_reasoning_parser.py`
  - `vllm/reasoning/olmo3_reasoning_parser.py`
  - `vllm/reasoning/step3_reasoning_parser.py`
  - `vllm/v1/attention/backends/fa_utils.py`
  - `vllm/v1/attention/backends/mla/aiter_triton_mla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`
  - `vllm/v1/structured_output/__init__.py`
  - `vllm/v1/structured_output/backend_guidance.py`
  - `vllm/v1/structured_output/backend_xgrammar.py`

### Summary

**What changed and why**  
This PR fixes mypy type checking errors across multiple files, primarily by updating type annotations from `list[int]` to `Sequence[int]` in reasoning parser methods and adding type ignores for problematic imports and function calls. The changes ensure consistent type signatures and handle cases where optional parameters could be `None`.

**Technical impact**  
The modifications standardize method signatures in reasoning parsers to accept more flexible sequence types, improving code flexibility while maintaining compatibility. Type ignore comments suppress false positives from mypy in areas where dynamic imports or complex function signatures cause type inference issues, particularly in attention backend implementations.

**Potential risks**  
Using `Sequence[int]` instead of `list[int]` may allow immutable sequences but could break code expecting mutable list operations. The type ignore comments might mask genuine type errors if the underlying code changes. The fix for `backend_tokenizer` access assumes the attribute exists at runtime, which could fail if the tokenizer implementation changes.

**Key insights**  
Developers should ensure reasoning parser implementations work with any `Sequence[int]`, not just lists. The attention backend fixes are workarounds for mypy limitations with conditional imports and dynamic signatures—verify these remain correct as dependencies evolve. Consider whether `ConstantList[int]` should be explicitly handled in type annotations for future clarity.

---

## 2. [[torch.compile] Compile `CustomOp.forward_native` for `SiluAndMul` and `QuantFP8` to avoid raw torch ops inside opaque custom ops](https://github.com/vllm-project/vllm/pull/32806)


### Base Information

- **PR Number:** #32806
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-22 19:52:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32806/files) (7):**
  - `tests/compile/test_silu_mul_quant_fusion.py`
  - `tests/kernels/core/test_activation.py`
  - `vllm/compilation/matcher_utils.py`
  - `vllm/model_executor/custom_op.py`
  - `vllm/model_executor/layers/activation.py`
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`
  - `vllm/model_executor/layers/rotary_embedding/common.py`

### Summary

**What changed and why**  
The changes manually compile `CustomOp.forward_native` for `SiluAndMul` and `QuantFP8` custom ops when they are invoked from within other opaque torch custom ops (e.g., `fused_moe`). This prevents eager PyTorch op execution in performance-critical paths, yielding significant latency improvements (15% for llama4, 3% for DSR1-fp4). The compilation is selectively applied to these two ops to avoid broader compilation issues, serving as a temporary fix until migration to vLLM IR or PluggableLayer.

**Technical impact**  
The `CustomOp` base class now supports a `compile_native` parameter (defaulting to `True` for the targeted ops) that conditionally wraps `forward_native` with `torch.compile`. This ensures compiled execution even when the op is hidden from model-level `torch.compile`. The change is backward-compatible and does not affect ops already visible to model-level compilation, as nested compilation is ignored.

**Potential risks**  
Selectively compiling only two ops creates inconsistency and may lead to performance cliffs for other custom ops used in similar contexts. The reliance on `torch.compile` with `dynamic=True` could introduce overhead or recompilation issues in dynamic scenarios. Additionally, the test modifications (e.g., setting `backend="eager"` or `compile_native=False`) might mask compilation-related failures.

**Key insights**  
This is a targeted performance optimization that addresses a specific inefficiency in opaque custom op chains. Developers should treat this as an interim solution and plan for a more systematic approach, such as migrating all custom ops to vLLM IR. When adding new custom ops, consider whether they require similar compilation handling, and ensure tests properly exercise both compiled and non-compiled paths.

---

## 3. [[BugFix] deepseek_v32_encoding: Replace asserts with proper exceptions](https://github.com/vllm-project/vllm/pull/32884)


### Base Information

- **PR Number:** #32884
- **Author:** [RishabhSaini](https://github.com/RishabhSaini)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-22 19:44:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32884/files) (1):**
  - `vllm/tokenizers/deepseek_v32_encoding.py`

### Summary

**What changed and why**  
The PR replaces Python `assert` statements with explicit exception handling in the DeepSeek v32 tokenizer. Asserts for input validation now raise `ValueError`, while parsing errors raise `RuntimeError`. This change ensures invalid user input returns a 400 Bad Request instead of a 500 Internal Server Error, improving error handling and API behavior.

**Technical impact**  
This modification enhances the robustness of the tokenizer by converting silent assertion failures into explicit, catchable exceptions. It aligns error handling with HTTP status codes, making the system more predictable for clients and easier to debug. The architecture now properly distinguishes between client errors (invalid input) and internal parsing issues.

**Potential risks**  
Some assertions may have been used for internal invariants; replacing them with exceptions could mask programming errors if not carefully reviewed. There's a risk of inconsistent error types if future changes introduce new validation logic. Additionally, the change might affect performance slightly due to the overhead of exception handling in hot paths.

**Key insights**  
Developers should ensure all validation logic is covered by appropriate exception types. Consider adding unit tests for edge cases to verify the new error behavior. Future contributions should maintain the pattern: use `ValueError` for invalid input and `RuntimeError` for unexpected parsing states.

---

## 4. [[Misc] Log vLLM logo when starting server](https://github.com/vllm-project/vllm/pull/32796)


### Base Information

- **PR Number:** #32796
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-22 19:15:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32796/files) (5):**
  - `vllm/entrypoints/grpc_server.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/utils.py`
  - `vllm/envs.py`
  - `vllm/logger.py`

### Summary

**What changed and why**  
Added a new `log_version_and_model` utility function that prints a stylized vLLM logo alongside version and model information during server startup. This replaces previous plain text logging in both gRPC and OpenAI API servers. An environment variable `VLLM_DISABLE_LOG_LOGO` allows disabling the logo display.

**Technical impact**  
Centralizes startup logging logic into a reusable utility, improving consistency across server entry points. The logo rendering adapts to the logger's formatter type (color/monochrome) and respects the existing logging infrastructure. No functional changes to server behavior.

**Potential risks**  
The logo uses ANSI escape codes for color; while there's detection for monochrome formatters, edge cases with custom or third-party formatters could display raw escape sequences. The `current_formatter_type` function assumes a specific handler structure which might not hold if logging configuration is modified externally.

**Key insights**  
This is a non-critical cosmetic enhancement that improves branding visibility. Developers should ensure the logo doesn't interfere with automated log parsing systems—the disable flag provides an escape hatch. The utility function's abstraction makes it easy to extend or modify startup messages consistently.

---

## 5. [[MoE Refactor] Move `select_experts` from `FusedMoEQuantMethod` -> `FusedMoE`](https://github.com/vllm-project/vllm/pull/31996)


### Base Information

- **PR Number:** #31996
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-22 15:21:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31996/files) (22):**
  - `tests/kernels/moe/test_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_method_base.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/router/base_router.py`
  - `vllm/model_executor/layers/fused_moe/router/custom_routing_router.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py`
  - `vllm/model_executor/layers/fused_moe/router/router_factory.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/bitsandbytes.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/experts_int8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gguf.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/ipex_quant.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/moe_wna16.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`

### Summary

**What changed and why**  
The PR refactors MoE (Mixture of Experts) routing logic by moving `select_experts` calls from individual `FusedMoEMethodBase.apply` methods to the unified `FusedMoE` layer. This centralizes routing decisions and introduces a dual-path approach: `apply` now takes precomputed `topk_weights` and `topk_ids`, while `apply_monolithic` (for kernels with combined routing+GEMMs) takes `router_logits`. The `capture` callback is also relocated from the router to the `FusedMoE` layer.

**Technical impact**  
This change decouples routing from kernel execution, simplifying method implementations and enabling more consistent handling of routing across different quantization backends. The introduction of `is_monolithic` property allows the `FusedMoE` layer to dynamically choose between routing-separated and monolithic kernels. The refactor reduces code duplication and aligns with a cleaner separation of concerns.

**Potential risks**  
- The `is_monolithic` property must be correctly implemented in all subclasses; missing or incorrect implementations could lead to runtime errors or performance regressions.
- Edge cases involving `capture` callbacks and expert mapping need careful validation, especially with the `x_orig` hack for FP4 FlashInfer.
- Changes to router return types (indices dtype conversion) could affect downstream operations if not consistently handled.

**Key insights**  
- Developers should ensure new `FusedMoEMethodBase` subclasses properly define `is_monolithic` and implement the appropriate `apply` or `apply_monolithic` method.
- The refactor simplifies future kernel additions by standardizing the interface, but requires thorough testing across all supported backends (e.g., CPU, XPU, CUDA, ROCm).
- Pay close attention to the `capture` logic and any hidden state dependencies (like `x_orig`) to avoid subtle bugs in profiling or expert routing.

---

## 6. [[BugFix] Fix invalid flashinfer_fused_moe_blockscale_fp8 op registration](https://github.com/vllm-project/vllm/pull/32855)


### Base Information

- **PR Number:** #32855
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-22 14:27:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32855/files) (1):**
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`

### Summary

**What changed and why**  
The change fixes an invalid default value in the `flashinfer_fused_moe_blockscale_fp8` function's parameter `routing_method_type`. The default was incorrectly set to an enum (`RoutingMethodType.DeepSeekV3`), causing a registration error. It's now explicitly cast to an `int` to match the parameter's declared type.

**Technical impact**  
This resolves a Torch operation registration failure by ensuring the default argument's type aligns with the function signature. The fix is minimal and localized, affecting only the default value initialization without altering the function's logic or the enum's usage elsewhere.

**Potential risks**  
The explicit cast assumes `RoutingMethodType.DeepSeekV3` is always convertible to an integer. If the enum's underlying value changes or is non-integer, this could lead to runtime errors. Additionally, other similar parameter defaults in the codebase might have the same issue.

**Key insights**  
Always verify that default argument types exactly match the parameter type annotations in Torch operation registrations. Consider adding a linting rule or type-checking step to catch such mismatches early. Review other op registrations for similar enum-to-int conversion issues.

---

## 7. [[Perf] Create TMA-aligned input scale tensor for DeepGemm on Hopper](https://github.com/vllm-project/vllm/pull/32619)


### Base Information

- **PR Number:** #32619
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-22 12:47:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32619/files) (7):**
  - `benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py`
  - `tests/kernels/quant_utils.py`
  - `tests/kernels/quantization/test_block_fp8.py`
  - `tests/kernels/quantization/test_per_token_group_quant.py`
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`
  - `vllm/utils/deep_gemm.py`

### Summary

**What changed and why**  
This PR modifies FP8 quantization to create TMA-aligned input scale tensors for DeepGemm on Hopper GPUs. Previously, DeepGemm would launch a separate `transpose_fp32` kernel to transpose scales into column‑major TMA‑aligned layout when needed. Now, the scale tensor is directly allocated with the correct stride pattern, eliminating the transpose kernel and improving performance.

**Technical impact**  
The changes extend `per_token_group_quant_fp8` to support `tma_aligned_scales`, which creates scales with strides matching DeepGemm’s TMA‑alignment requirements. This avoids a device‑side transpose kernel, reduces kernel launch overhead, and improves end‑to‑end throughput by ~3%. The modification is backward‑compatible—existing calls without the new parameter remain unchanged.

**Potential risks**  
If `tma_aligned_scales=True` is used without `column_major_scales=True`, the stride calculation may produce incorrect layouts. The alignment logic assumes 4‑byte elements (float32); using other element sizes could misalign TMA accesses. Edge cases with non‑standard tensor dimensions (e.g., `dim > 3`) may not be fully handled by the current stride computation.

**Key insights**  
Always pair `tma_aligned_scales=True` with `column_major_scales=True` to ensure correct layout. Verify that `get_tma_aligned_size` uses the appropriate element size if scales are not float32. The performance gain is significant, but ensure all DeepGemm‑compatible paths enable both flags on Hopper.

---

## 8. [[Refactor] Remove unused tpu files](https://github.com/vllm-project/vllm/pull/32610)


### Base Information

- **PR Number:** #32610
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-22 12:35:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32610/files) (4):**
  - `vllm/v1/sample/tpu/__init__.py`
  - `vllm/v1/sample/tpu/metadata.py`
  - `vllm/v1/sample/tpu/sampler.py`
  - `vllm/v1/worker/tpu_worker.py`

### Summary

**What changed and why**  
Removed four unused TPU-related files (`__init__.py`, `metadata.py`, `sampler.py`, `tpu_worker.py`) that were introduced in a previous PR but are no longer referenced in the codebase. The deletion is a cleanup to eliminate dead code and reduce maintenance overhead.

**Technical impact**  
This change reduces codebase size and complexity without affecting functionality, as the files were not imported or used elsewhere. It may simplify future refactoring efforts by removing unused TPU-specific abstractions that could otherwise confuse developers.

**Potential risks**  
If any external or internal dependencies on these files exist (e.g., via dynamic imports or conditional execution paths), their removal could cause runtime errors. Additionally, if TPU support is revived in the future, reimplementing these components might require extra effort.

**Key insights**  
Verify that no tests, scripts, or configuration files reference these modules before merging. Consider adding a comment or documentation note about the removal of TPU sampling support to inform future developers. Ensure the PR description includes confirmation from the original authors (@yaochengji, @NickLucche) that the code is indeed unused.

---

## 9. [Add llmcompressor fp8 kv-cache quant (per-tensor and per-attn_head)](https://github.com/vllm-project/vllm/pull/30141)


### Base Information

- **PR Number:** #30141
- **Author:** [eldarkurtic](https://github.com/eldarkurtic)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-22 12:29:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30141/files) (18):**
  - `csrc/cache_kernels.cu`
  - `docs/features/quantization/quantized_kvcache.md`
  - `tests/kernels/attention/test_cache.py`
  - `tests/quantization/test_compressed_tensors.py`
  - `vllm/attention/layer.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`
  - `vllm/model_executor/model_loader/weight_utils.py`
  - `vllm/model_executor/models/apertus.py`
  - `vllm/model_executor/models/arcee.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/llama_eagle.py`
  - `vllm/model_executor/models/llama_eagle3.py`
  - `vllm/model_executor/models/nemotron_h.py`
  - `vllm/model_executor/models/nemotron_nas.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/flash_attn.py`

### Summary

**What changed and why**  
This PR adds support for loading and running `llm-compressor` models with FP8 KV-cache quantization, extending beyond per-tensor scaling to include per-attention-head quantization. The changes enable finer-grained scaling factors for queries, keys, and values, which can improve accuracy when using calibrated scales from external tools.

**Technical impact**  
The modifications extend kernel-level operations (`reshape_and_cache_flash`) to handle arrays of scales, update quantization utilities to support per-channel broadcasting, and reorganize KV-cache initialization to accommodate both per-tensor and per-head strategies. The Flash Attention v3 backend now supports per-head scales, and tensor model parallelism is considered during scale loading to ensure correct distribution across devices.

**Potential risks**  
Per-head quantization is only supported with the Flash Attention backend and requires Flash Attention v3, limiting compatibility. The changes introduce additional complexity in scale broadcasting and kernel dispatch paths, which could lead to subtle bugs if scales are mismatched. There is also a risk of performance regression if the new branching logic in kernels is not optimized.

**Key insights**  
Developers should ensure that models using per-head quantization are paired with Flash Attention v3. The PR maintains backward compatibility for existing per-tensor quantization, but thorough testing is recommended when switching between scale types. The updated documentation provides clear guidance on calibration methods, emphasizing `llm-compressor` for optimal results.

---

## 10. [[Bugfix][Attention] Explicitly report support for kv_cache_dtype bfloat16](https://github.com/vllm-project/vllm/pull/32795)


### Base Information

- **PR Number:** #32795
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-22 11:05:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32795/files) (13):**
  - `vllm/model_executor/layers/quantization/kv_cache.py`
  - `vllm/platforms/cpu.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/flashinfer.py`
  - `vllm/v1/attention/backends/flex_attention.py`
  - `vllm/v1/attention/backends/mla/cutlass_mla.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/attention/backends/mla/flashinfer_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/triton_mla.py`
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
The PR fixes a bug where attention backends didn't explicitly report support for `bfloat16` kv_cache_dtype, only for `auto`. This caused failures when users explicitly set `--kv-cache-dtype=bfloat16`, as the backend validation incorrectly rejected it. The changes add `"bfloat16"` to the `supported_kv_cache_dtypes` list for all relevant attention backends and update helper functions to distinguish between quantized (FP8) and non-quantized cache dtypes.

**Technical impact**  
The fix ensures that backends supporting bfloat16 KV cache will correctly validate user configurations, allowing explicit `bfloat16` dtype selection. Additionally, the `is_quantized_kv_cache` helper now accurately identifies only FP8 variants as quantized, preventing incorrect handling of bfloat16 cache scaling logic and platform-specific restrictions.

**Potential risks**  
If any backend does not genuinely support bfloat16 KV cache in practice, this change could lead to runtime errors despite passing config validation. The updated logic in `is_quantized_kv_cache` assumes all non-FP8 dtypes are non-quantized; future cache dtype additions may require adjustments. Inconsistent updates across backends could cause validation gaps.

**Key insights**  
Always verify that backend support declarations match actual implementation capabilities. The `is_quantized_kv_cache` function should be the single source of truth for quantization detection—any new cache dtype must be evaluated against it. Consider adding runtime checks or tests to ensure backends handle bfloat16 KV cache correctly.

---

## 11. [[CPU Backend] [Perf] Accelerate tensor-parallel/data-parallel inference across NUMA domains on Arm](https://github.com/vllm-project/vllm/pull/32792)


### Base Information

- **PR Number:** #32792
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-22 10:55:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32792/files) (6):**
  - `cmake/cpu_extension.cmake`
  - `csrc/cpu/cpu_types_arm.hpp`
  - `csrc/cpu/shm.cpp`
  - `csrc/cpu/torch_bindings.cpp`
  - `vllm/distributed/device_communicators/cpu_communicator.py`
  - `vllm/v1/worker/cpu_worker.py`

### Summary

**What changed and why**  
This PR enables vLLM's custom shared memory communicator for Arm architectures to accelerate tensor-parallel/data-parallel inference across NUMA domains. It addresses high synchronization costs in torch.distributed by implementing Arm-specific optimizations including memory model fixes, new vector types, and automatic thread binding.

**Technical impact**  
The changes extend CPU backend support to Arm by adding ASIMD vector types (INT8Vec64) and memory synchronization primitives using atomic operations. The shared memory communicator now works on both x86 and Arm architectures, significantly improving multi-NUMA performance. Build system updates ensure the shm.cpp extension compiles for Arm targets.

**Potential risks**  
The Arm memory model implementation relies on atomic operations which may have different performance characteristics across various Arm microarchitectures. The INT8Vec64 masked store implementation uses a large switch statement that could impact code size and instruction cache efficiency. Platform-specific code paths increase maintenance complexity.

**Key insights**  
Benchmarks show 2x throughput improvement and 4x latency reduction for tp=2 configurations on 2 NUMA domains. Developers should validate performance on diverse Arm hardware since memory ordering semantics differ from x86. The automatic thread binding logic for Arm assumes no SMT, which may need adjustment for future Arm processors with SMT capabilities.

---

## 12. [[CI][Attention] Add more CI dependencies for attention tests](https://github.com/vllm-project/vllm/pull/32487)


### Base Information

- **PR Number:** #32487
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-22 10:44:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32487/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/attention.yaml`

### Summary

**What changed and why**  
This PR adds two new source file dependencies (`vllm/config/attention.py` and `vllm/model_executor/layers/attention`) to the CI pipeline configuration for attention tests. The change ensures that modifications to attention-related configuration and layer code will trigger the relevant CI tests, preventing regressions like the one introduced in PR #32339.

**Technical impact**  
The CI pipeline will now run attention tests whenever changes are made to the core attention configuration or layer implementation files, in addition to the existing v1 attention and test directories. This improves test coverage and ensures that attention-related changes are validated across both H100 and B200 GPU environments.

**Potential risks**  
If the added directories contain many files or frequently changing code, this could increase CI runtime by triggering attention tests more often. There's also a risk of missing other dependencies if the attention system relies on additional modules not yet included in the source file dependencies.

**Key insights**  
This is a preventive measure to catch attention-related regressions earlier in the development cycle. Developers should be aware that changes to attention configuration or layer implementation will now trigger additional CI tests. Consider reviewing whether other test pipelines might need similar dependency updates for comprehensive coverage.

---

## 13. [[Feature] Add --ssl-ciphers CLI argument for TLS cipher control](https://github.com/vllm-project/vllm/pull/30937)


### Base Information

- **PR Number:** #30937
- **Author:** [ricky-chaoju](https://github.com/ricky-chaoju)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-22 09:53:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30937/files) (2):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/cli_args.py`

### Summary

**What changed and why**  
This PR adds a new `--ssl-ciphers` CLI argument to allow users to specify allowed SSL/TLS cipher suites. The change exposes uvicorn's existing `ssl_ciphers` parameter through vLLM's API server interface, providing fine-grained security control for compliance with standards like PCI DSS and HIPAA.

**Technical impact**  
The modification extends the existing SSL/TLS configuration capabilities by adding one new parameter that gets passed through to uvicorn's `serve_http()` function. This maintains architectural consistency with other SSL-related arguments already exposed by vLLM and requires no changes to the underlying HTTP server implementation.

**Potential risks**  
Users might specify insecure or incompatible cipher suites that could weaken security or break TLS connections. The documentation note about "TLS 1.2 and below only" could cause confusion if users expect it to work with TLS 1.3. There's also a risk of improper cipher string formatting causing runtime errors.

**Key insights**  
The implementation correctly follows the existing pattern for SSL parameters. Developers should ensure proper validation of cipher strings and consider adding a warning about TLS 1.3 compatibility. The change is minimal and focused, making it easy to maintain while addressing a legitimate security configuration need.

---

## 14. [Support custom URI schemes and trace handlers for profiler](https://github.com/vllm-project/vllm/pull/32393)


### Base Information

- **PR Number:** #32393
- **Author:** [diviramon](https://github.com/diviramon)
- **Merged By:** [houseroad](https://github.com/houseroad)
- **Merged time:** 2026-01-22 09:45:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32393/files) (3):**
  - `tests/v1/worker/test_gpu_profiler.py`
  - `vllm/config/profiler.py`
  - `vllm/profiler/wrapper.py`

### Summary

**What changed and why**  
The changes enable support for custom URI schemes (e.g., gs://, s3://, hdfs://) for profiler trace output by preventing path normalization for these URIs. Additionally, they allow injection of custom trace handlers via a new `on_trace_ready` parameter in `TorchProfilerWrapper`, enabling platform-specific profiling backends.

**Technical impact**  
The codebase now generalizes URI scheme detection through a new `_is_uri_path()` helper function, replacing hardcoded gs:// checks. This allows any URI scheme to be preserved without absolute path conversion. The profiler wrapper supports optional custom trace handlers, falling back to the default tensorboard handler when none is provided. File I/O operations for profiler output are now skipped for URI paths.

**Potential risks**  
The URI detection logic (`len(scheme) > 1`) may incorrectly classify some edge cases (e.g., "a://" would be considered a URI). There's no validation that custom URI schemes are supported by the underlying storage system. The skip of file writes for URI paths means profiler summary tables won't be saved for cloud storage destinations unless custom handlers implement this functionality.

**Key insights**  
The implementation cleanly separates URI detection from business logic, making it easy to extend. Developers should ensure custom trace handlers properly handle trace export for their specific URI schemes. When using cloud storage paths, note that the profiler's summary table output will only be printed to console (rank 0) but not written to disk unless custom handlers implement this.

---

## 15. [[UX] Default api_server_count to dp_size if not specified](https://github.com/vllm-project/vllm/pull/32525)


### Base Information

- **PR Number:** #32525
- **Author:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-22 09:35:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32525/files) (2):**
  - `vllm/entrypoints/cli/serve.py`
  - `vllm/entrypoints/openai/cli_args.py`

### Summary

**What changed and why**  
The changes introduce intelligent defaulting for `api_server_count` based on data parallelism configuration. When `--api-server-count` is unspecified, it now defaults to `data_parallel_size` for internal load balancing, `data_parallel_size_local` for hybrid LB, or remains 1 for external LB. This prevents API servers from becoming bottlenecks in distributed setups.

**Technical impact**  
This modifies the CLI's behavior to automatically scale API servers with data parallelism, improving throughput in distributed environments. The logic now distinguishes between external, hybrid, and internal load balancing modes, ensuring appropriate defaults for each. The default value was changed from `1` to `None` to trigger the new defaulting logic.

**Potential risks**  
If `data_parallel_size` or `data_parallel_size_local` are incorrectly set or zero, it could lead to invalid `api_server_count` values. The validation for headless mode now raises an error if `api_server_count` is explicitly set, which may break existing scripts that unintentionally combined these flags. The hybrid/external LB detection relies on multiple flags, which could cause confusion if users mix them incorrectly.

**Key insights**  
Developers should be aware that omitting `--api-server-count` now has different implications depending on the parallelism mode. The change promotes best practices but requires careful testing in distributed deployments. Ensure that data parallelism parameters are correctly configured when relying on defaults, and update any scripts that explicitly set `api_server_count` in headless mode.

---

## 16. [[MISC] Add .cursor to .gitignore](https://github.com/vllm-project/vllm/pull/32868)


### Base Information

- **PR Number:** #32868
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 09:27:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32868/files) (1):**
  - `.gitignore`

### Summary

**What changed and why**  
Added `.cursor/` directory to the `.gitignore` file to exclude Cursor IDE configuration and cache files from version control. This prevents IDE-specific files from being tracked in the repository.

**Technical impact**  
This change ensures that Cursor IDE artifacts won't clutter the repository or cause unnecessary merge conflicts. It maintains repository cleanliness by excluding tool-specific files that are irrelevant to the project's source code.

**Potential risks**  
No technical risks since this only affects version control behavior. However, if the team uses Cursor IDE and needs to share certain IDE configurations, they should consider using a separate configuration file that can be committed.

**Key insights**  
This is a standard practice for excluding IDE-specific files. Consider whether any Cursor-specific project settings (like workspace configurations) should be shared via a different mechanism if team consistency is important.

---

## 17. [[Hardware][AMD][CI][Bugfix] Fix regressions from deprecated env vars](https://github.com/vllm-project/vllm/pull/32837)


### Base Information

- **PR Number:** #32837
- **Author:** [mawong-amd](https://github.com/mawong-amd)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 08:59:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32837/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `tests/v1/kv_connector/nixl_integration/config_sweep_accuracy_test.sh`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
This PR fixes regressions on AMD ROCm caused by deprecating environment variables (`VLLM_V1_USE_PREFILL_DECODE_ATTENTION` and `VLLM_ATTENTION_BACKEND`). The changes update CI scripts to set `ROCM_ATTN=1` instead of passing `--attention-backend ROCM_ATTN`, and modify `rocm.py` to safely handle cases where the vLLM config may not be initialized yet.

**Technical impact**  
The fix ensures that the attention backend selection logic gracefully handles uninitialized configurations by using `get_current_vllm_config_or_none()` and checking for `None`. Additionally, CI tests now correctly propagate the `ROCM_ATTN` environment variable to enable the ROCm attention backend, aligning with the new configuration approach.

**Potential risks**  
If `vllm_config` is `None` when `use_prefill_decode_attention` is needed, the ROCM_ATTN backend may not be selected even when required, potentially causing performance regressions or incorrect behavior in specific initialization sequences. The reliance on environment variables in CI scripts could lead to misconfiguration if not consistently set across all test environments.

**Key insights**  
Always use safe accessors like `get_current_vllm_config_or_none()` when retrieving runtime configuration to avoid initialization-order issues. Ensure that environment variable transitions are fully propagated across all CI configurations and test scripts to prevent silent test failures.

---

## 18. [[Bugfix] ModelScope is supported when downloading LORA models.](https://github.com/vllm-project/vllm/pull/32844)


### Base Information

- **PR Number:** #32844
- **Author:** [AuYang261](https://github.com/AuYang261)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-22 08:33:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32844/files) (1):**
  - `vllm/lora/utils.py`

### Summary

**What changed and why**  
The change fixes a bug where the `VLLM_USE_MODELSCOPE=1` environment variable was ignored when downloading LoRA models. Previously, the code always attempted to download from Hugging Face Hub; now it conditionally uses ModelScope's `snapshot_download` when the flag is enabled, aligning LoRA model downloads with the existing behavior for base models.

**Technical impact**  
This modification extends ModelScope support to LoRA model downloads, ensuring consistent repository source handling across the codebase. The function `get_adapter_absolute_path` now dynamically selects the appropriate download function and exception handling based on the environment variable, improving integration with ModelScope without affecting default Hugging Face behavior.

**Potential risks**  
If ModelScope's `snapshot_download` raises exceptions not covered by `HTTPError` or `InvalidParameter`, errors may be mishandled. Additionally, the lambda functions for `download_fn` could obscure stack traces if exceptions occur during their definition. There is also a risk of inconsistent error logging if future changes modify the exception handling structure.

**Key insights**  
The fix correctly centralizes download logic, but consider extracting the download selection into a separate helper function for better testability and clarity. Ensure that all possible exceptions from ModelScope are documented or caught to prevent silent failures. Developers should verify that the ModelScope dependency is properly handled in the project's environment setup.

---

## 19. [Support bge-m3 sparse embeddings and colbert embeddings](https://github.com/vllm-project/vllm/pull/14526)


### Base Information

- **PR Number:** #14526
- **Author:** [maxdebayser](https://github.com/maxdebayser)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-22 07:52:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/14526/files) (9):**
  - `docs/models/pooling_models.md`
  - `tests/models/language/pooling/embed_utils.py`
  - `tests/models/language/pooling/test_bge_m3.py`
  - `tests/models/language/pooling/test_gritlm.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/layers/pooler/special.py`
  - `vllm/model_executor/layers/pooler/tokwise/poolers.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/roberta.py`

### Summary

**What changed and why**  
This PR adds support for BGE-M3 sparse and ColBERT embeddings to vLLM's pooling models. The changes enable loading additional weight files (`sparse_linear.pt` and `colbert_linear.pt`) for the BGE-M3 model, which requires overriding its architecture via `--hf-overrides` due to a mismatch in the Hugging Face config. New pooling tasks (`token_classify` for sparse embeddings and `token_embed` for ColBERT) are introduced, along with a `BOSEOSFilter` to handle token alignment.

**Technical impact**  
The PR extends the pooling framework to support token-wise embeddings and sparse representations, introducing a new `BgeM3EmbeddingModel` class that inherits from `RobertaEmbeddingModel`. It modifies weight loading to handle secondary weight files and adds a dispatch mechanism for different embedding types. The changes are backward-compatible but require explicit configuration for BGE-M3.

**Potential risks**  
The architecture override (`--hf-overrides`) is a workaround that may break if the Hugging Face model configuration changes. Token alignment for sparse embeddings relies on external tokenization, which could lead to mismatches if not handled carefully. The current implementation only returns a single tensor per sequence, limiting multi-embedding returns.

**Key insights**  
Developers must use the `--hf-overrides` flag to load BGE-M3 correctly. Sparse embeddings require post-processing to map tokens to scores via the `/tokenize` endpoint. Consider extending the pooling return type to support multiple embeddings per sequence in the future. The `BOSEOSFilter` is a reusable component for token-based pooling tasks.

---

## 20. [[Misc] Bump opencv-python dependecy version to 4.13](https://github.com/vllm-project/vllm/pull/32668)


### Base Information

- **PR Number:** #32668
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-22 07:51:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32668/files) (5):**
  - `requirements/common.txt`
  - `requirements/nightly_torch_test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/entrypoints/openai/test_translation_validation.py`

### Summary

**What changed and why**  
This PR updates the `opencv-python-headless` dependency from version 4.11.0 to 4.13.0 across multiple requirements files to incorporate security fixes. Additionally, it relaxes version constraints for `genai_perf` and `tritonclient` from exact pins to minimum versions, and updates several transitive dependencies in the locked test environment.

**Technical impact**  
The primary impact is a minor version bump for OpenCV, which may include security patches, bug fixes, and potential API changes. The shift from exact version pins (`==`) to minimum version constraints (`>=`) for `genai_perf` and `tritonclient` increases flexibility but could introduce compatibility risks if newer versions break existing integrations. The test file update adjusts an audio transcription test to include a `repetition_penalty` parameter, likely to align with model-specific requirements.

**Potential risks**  
Updating OpenCV could affect video/image processing modules if there are backward-incompatible changes in the 4.13.0 release. Using minimum version constraints for `genai_perf` and `tritonclient` might lead to unpredictable behavior if untested newer versions are automatically pulled. The test modification assumes the `repetition_penalty` parameter is necessary for the Gemma-3n-E2B-it model, but this change should be validated to ensure it doesn't mask underlying issues.

**Key insights**  
Developers should verify that the OpenCV update doesn't break any video IO functionality and run relevant test suites. The move to minimum version constraints should be accompanied by periodic dependency reviews to catch breaking changes early. The test adjustment highlights the importance of model-specific parameters in API calls, suggesting similar checks may be needed for other models.

---

## 21. [[Cleanup] Move scheduler `get_routed_experts` logic to separate method](https://github.com/vllm-project/vllm/pull/32706)


### Base Information

- **PR Number:** #32706
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-22 07:46:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32706/files) (1):**
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The changes extract the routed experts retrieval logic from the main scheduler flow into a dedicated `_get_routed_experts` method. This refactoring improves code readability by isolating special-case logic. Additionally, the condition for creating an `EngineCoreOutput` is modified to include `stopped` requests, ensuring final output is always returned even when no new tokens are generated.

**Technical impact**  
This refactoring reduces complexity in the main `update_from_output` method by abstracting the routed experts calculation. The architectural change promotes separation of concerns and makes the scheduler's primary control flow easier to understand. The logic for computing slot mappings and retrieving routed experts is now encapsulated in a single, reusable method.

**Potential risks**  
The modified output condition (`or stopped`) could potentially create empty output objects for stopped requests when `new_token_ids`, `pooler_output`, and `kv_transfer_params` are all `None`. This might increase output payload size slightly. The `_get_routed_experts` method assumes `kv_blocks.get_block_ids()` returns a non-empty list, which could cause issues if called on requests without allocated KV cache blocks.

**Key insights**  
The refactoring successfully addresses code readability concerns while maintaining functional equivalence. Developers should verify that the `stopped` condition doesn't create unnecessary overhead in production scenarios. The new private method follows Python naming conventions and provides a clear abstraction boundary for routed experts retrieval logic.

---

## 22. [[torch.compile] Improve Cold Start for MoEs](https://github.com/vllm-project/vllm/pull/32805)


### Base Information

- **PR Number:** #32805
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-22 07:44:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32805/files) (3):**
  - `tests/kernels/moe/test_moe.py`
  - `vllm/forward_context.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`

### Summary

**What changed and why**  
This PR addresses torch.compile cold start performance by removing hard-coded strings from compiled graphs. The custom operators `vllm.moe_forward` and `vllm.moe_forward_shared` previously embedded layer names as string constants, which increased graph compilation time. The solution stores these strings in a reverse-order list within `ForwardContext`, allowing the operators to pop them dynamically during execution.

**Technical impact**  
The changes decouple string constants from the compiled graph, reducing graph specialization overhead and significantly improving torch.compile cold start times (e.g., from 46s to 16s for gpt-oss-120b). The architecture now relies on the execution order of custom operators and assumes torch.compile does not reorder them, introducing a subtle dependency on runtime behavior.

**Potential risks**  
If torch.compile reorders the custom operator calls or if the list `remaining_moe_layers` is mismatched (e.g., due to incorrect initialization or concurrent execution), the layer name resolution will fail. The workaround also assumes a single forward pass context, which may break in edge cases like nested or parallel forward passes.

**Key insights**  
This is a temporary optimization that trades off robustness for performance; the long-term solution should unwrap the custom operator or treat strings as symbolic inputs. Developers must ensure `remaining_moe_layers` is correctly initialized and that tests override it when needed (as shown in the unit test fix). Monitor for any graph reordering issues in future PyTorch versions.

---

## 23. [[Misc][BE] Turn on strict type coverage for vllm/compilation](https://github.com/vllm-project/vllm/pull/31756)


### Base Information

- **PR Number:** #31756
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-22 07:12:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31756/files) (11):**
  - `pyproject.toml`
  - `tests/compile/test_pass_manager.py`
  - `tools/pre_commit/mypy.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/caching.py`
  - `vllm/compilation/collective_fusion.py`
  - `vllm/compilation/compiler_interface.py`
  - `vllm/compilation/decorators.py`
  - `vllm/compilation/matcher_utils.py`
  - `vllm/compilation/piecewise_backend.py`
  - `vllm/compilation/sequence_parallelism.py`

### Summary

**What changed and why**  
This PR enables strict type checking for the `vllm/compilation` module by adding a mypy override in `pyproject.toml` and updating the pre-commit hook to run separate mypy invocations for strict and non-strict paths. The changes include type-safety refactors across the compilation module, such as explicit annotations, boolean casts, and targeted `type: ignore` comments, to satisfy strict typing without altering functional behavior.

**Technical impact**  
The update enforces stricter type safety in the compilation module, which improves code reliability and maintainability. The pre-commit hook now intelligently partitions files into strict and non-strict groups, ensuring that only `vllm/compilation` is subjected to `--strict` checks while other directories follow the existing, more lenient typing rules.

**Potential risks**  
Introducing strict typing may inadvertently mask underlying logic errors if `type: ignore` comments are overused or misplaced. Additionally, the dual mypy invocation strategy could increase pre-commit runtime, especially as the codebase grows, though this is mitigated by only applying strict checks to a limited directory.

**Key insights**  
Developers should ensure that future changes to `vllm/compilation` adhere to strict typing standards, avoiding new `type: ignore` comments unless absolutely necessary. The pattern of using temporary variables for explicit return annotations (e.g., `result: str = ...`) is a recommended practice for maintaining clarity under strict mode.

---

## 24. [[Frontend] Introduce Renderer for processing chat messages (using `ModelConfig`)](https://github.com/vllm-project/vllm/pull/30200)


### Base Information

- **PR Number:** #30200
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 04:44:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30200/files) (48):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `docs/features/reasoning_outputs.md`
  - `tests/entrypoints/openai/test_chat_template.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/entrypoints/openai/test_serving_engine.py`
  - `tests/entrypoints/openai/test_serving_models.py`
  - `tests/entrypoints/openai/test_serving_responses.py`
  - `tests/entrypoints/pooling/score/test_utils.py`
  - `tests/entrypoints/test_chat_utils.py`
  - `tests/renderers/__init__.py`
  - `tests/renderers/test_hf.py`
  - `tests/renderers/test_mistral.py`
  - `tests/test_inputs.py`
  - `tests/v1/engine/test_llm_engine.py`
  - `tests/v1/engine/test_process_multi_modal_uuids.py`
  - `tools/pre_commit/mypy.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/models/serving.py`
  - `vllm/entrypoints/openai/responses/context.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/pooling/__init__.py`
  - `vllm/entrypoints/pooling/classify/serving.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/pooling/score/utils.py`
  - `vllm/entrypoints/serve/tokenize/serving.py`
  - `vllm/entrypoints/utils.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/renderers/__init__.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/mistral.py`
  - `vllm/renderers/protocol.py`
  - `vllm/renderers/registry.py`
  - `vllm/renderers/terratorch.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
This PR introduces a new `Renderer` abstraction to decouple prompt rendering from tokenizers and centralize chat template logic. It defines a `RendererLike` protocol, a registry for lazy registration, and concrete implementations (HF, Mistral, DeepseekV32, Grok2, Terratorch). The renderer is initialized in `InputPreprocessor` and exposed via `EngineClient.renderer`, replacing direct tokenizer initialization and async `get_tokenizer()` calls across the codebase. This refactor aims to simplify the architecture, reduce async overhead, and pave the way for future enhancements like merging completion renderers and deprecating the tokenizer registry.

**Technical impact**  
The changes centralize chat template processing, moving implementation-specific logic (e.g., HF/Mistral template resolution, AST detection, kwargs filtering) into dedicated renderer modules. This reduces coupling between tokenizers and serving paths, making the system more modular and easier to extend. The `InputPreprocessor` now builds a renderer from `ModelConfig`, and all OpenAI endpoints (chat, completions, responses, pooling, tokenize) use the renderer for prompt rendering. The removal of async `get_tokenizer()` calls simplifies code and eliminates potential bottlenecks.

**Potential risks**  
- The refactor touches many files (48 changed), increasing the risk of regression in edge cases, especially for multimodal inputs and tokenizer‑less configurations (`skip_tokenizer_init=True`).  
- The lazy registry approach may introduce subtle import issues if not all renderer dependencies are available at runtime.  
- Changes to error messages (e.g., for `skip_tokenizer_init`) could affect client‑side error handling.  
- The migration of tests to a new `tests/renderers` directory must ensure all critical paths remain covered.

**Key insights**  
- Developers should adopt `engine_client.renderer` and `renderer.get_tokenizer()` instead of the deprecated async `get_tokenizer()`.  
- The renderer abstraction cleanly separates prompt rendering from tokenization, enabling future optimizations like microbatch tokenizer integration.  
- Pay close attention to multimodal data flows, as the refactor changes how `mm_data` and `mm_uuids` are attached to prompts.  
- Ensure that any custom tokenizer integrations are updated to implement the `RendererLike` protocol and registered in `RENDERER_REGISTRY`.

---

## 25. [OffloadingConnector: Support kernel_block_size != block_size](https://github.com/vllm-project/vllm/pull/30692)


### Base Information

- **PR Number:** #30692
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-22 04:30:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30692/files) (7):**
  - `csrc/cache.h`
  - `csrc/cache_kernels.cu`
  - `csrc/torch_bindings.cpp`
  - `tests/kernels/attention/test_cache.py`
  - `tests/v1/kv_offload/test_cpu_gpu.py`
  - `vllm/_custom_ops.py`
  - `vllm/v1/kv_offload/worker/cpu_gpu.py`

### Summary

**What changed and why**  
The PR modifies the `swap_blocks` kernel and offloading connector to support cases where the kernel block size differs from vLLM's logical block size. Previously, `swap_blocks` inferred block size from tensor stride, which assumed equal block sizes. Now, `block_size_in_bytes` is explicitly passed as a parameter, enabling flexible block copying when logical and kernel block sizes differ.

**Technical impact**  
The changes decouple block-size handling from tensor stride, allowing the offloading connector to normalize block-size factors between source and destination tensors. This enables efficient copying when logical block sizes are larger than kernel block sizes, similar to a previous fix for NixlConnector. The CPU↔GPU handler now computes per-tensor `block_size_in_bytes` and handles split K/V tensors via unbinding.

**Potential risks**  
If `block_size_in_bytes` is incorrectly calculated (e.g., mismatched with actual memory layout), data corruption or misalignment could occur. The removal of stride-based inference assumes callers correctly compute block sizes, which may be error-prone. Additionally, the refactored handler logic must ensure all split K/V tensor cases are correctly unbounded and normalized.

**Key insights**  
Developers must ensure `block_size_in_bytes` is accurately derived as `tensor.element_size() * tensor.stride(0) * min_block_size_factor`. The changes align offloading with kernel-block granularity, improving flexibility but requiring careful validation of block-size calculations in all usage contexts. Test updates verify mappings at kernel-block level, which should be referenced for integration.

---

## 26. [[Frontend] add prompt_cache_key for openresponses](https://github.com/vllm-project/vllm/pull/32824)


### Base Information

- **PR Number:** #32824
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 03:34:15
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32824/files) (1):**
  - `vllm/entrypoints/openai/responses/protocol.py`

### Summary

**What changed and why**  
A new optional field `prompt_cache_key` has been added to the `ResponsesRequest` class in the OpenAI responses protocol. This field is intended to support integration with openresponses.org by allowing a key to be specified for reading from or writing to a prompt cache, though the implementation is not yet complete.

**Technical impact**  
This change extends the OpenAI-compatible API request schema without affecting existing functionality, as the field is optional and currently ignored by vLLM. It prepares the frontend for future prompt caching capabilities that could improve performance for repeated prompts.

**Potential risks**  
Since the field is described as unimplemented and ignored, there is a risk of confusion if users expect it to work immediately. Additionally, future implementation must ensure backward compatibility and proper validation to avoid security or performance issues related to cache key handling.

**Key insights**  
Developers should note that this is a forward-looking addition with no current effect. When implementing the cache functionality, thorough testing of cache key uniqueness, expiration, and invalidation will be critical. The description should be updated once the feature is fully implemented to avoid user misunderstanding.

---

## 27. [[CI] refactor release pipeline config into groups](https://github.com/vllm-project/vllm/pull/32833)


### Base Information

- **PR Number:** #32833
- **Author:** [Harry-Chen](https://github.com/Harry-Chen)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-01-22 03:27:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32833/files) (1):**
  - `.buildkite/release-pipeline.yaml`

### Summary

**What changed and why**  
The release pipeline configuration has been refactored to group related steps into logical sections (e.g., "aarch64 + CUDA builds," "x86 + CUDA builds," "Build release images (CUDA 12.9)") without altering the actual job logic. Additionally, the manual approval block (`block-release-image-build-cuda-13-0`) preceding CUDA 13.0 release image builds has been removed as requested.

**Technical impact**  
This reorganization improves readability and maintainability by structuring the pipeline into clear, labeled groups. The removal of the approval block for CUDA 13.0 images means these builds will now run automatically without manual intervention, potentially speeding up the release process.

**Potential risks**  
Removing the approval block eliminates a safety gate, which could lead to unintended releases if the preceding steps have issues. Ensure that the pipeline's dependency graph and any required quality checks are still adequately enforced before the CUDA 13.0 builds execute.

**Key insights**  
The refactoring is purely organizational and should not affect pipeline behavior, but verify that all `depends_on` references and step IDs remain correct. Consider adding automated validation (e.g., integration tests) to compensate for the removed manual approval, especially for release-critical paths.

---

## 28. [[Bugfix] Fix Whisper/encoder-decoder GPU memory leak](https://github.com/vllm-project/vllm/pull/32789)


### Base Information

- **PR Number:** #32789
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 02:50:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32789/files) (2):**
  - `tests/models/multimodal/generation/test_whisper.py`
  - `vllm/v1/core/encoder_cache_manager.py`

### Summary

**What changed and why**  
The fix addresses a GPU memory leak in encoder-decoder models (like Whisper) by modifying the `EncoderDecoderCacheManager`. Previously, encoder cache entries were marked for freeing in the same scheduling step they were allocated, causing the model's execution to miss freeing them. The change introduces a double-buffer mechanism (`allocated` and `to_free` lists) to delay freeing by one step, ensuring entries are only freed after the model has used them.

**Technical impact**  
This change alters the cache management lifecycle for encoder-decoder models, ensuring that encoder outputs persist through the model execution phase before being reclaimed. The architecture now mimics a state transition similar to the standard `EncoderCacheManager`, preventing unbounded growth of the encoder cache and fixing the memory leak described in the issue.

**Potential risks**  
If the double-buffer logic is incorrectly synchronized (e.g., if requests are interleaved or canceled), some entries might not be freed, potentially reintroducing memory issues. The test assumes single-process mode, which may not catch concurrency-related bugs in distributed settings. Additionally, the fix relies on the assumption that `get_freed_mm_hashes` is called consistently each step.

**Key insights**  
The core issue was a race condition between allocation and freeing within a single scheduling step. Developers should verify that the double-buffer approach works correctly under varied scheduling patterns and consider adding stress tests for concurrent requests. The regression test is valuable but should be extended to multi-process scenarios to ensure robustness.

---

## 29. [[Frontend][2/n] Make pooling entrypoints request schema consensus \| ChatRequest](https://github.com/vllm-project/vllm/pull/32574)


### Base Information

- **PR Number:** #32574
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 02:32:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32574/files) (24):**
  - `docs/design/io_processor_plugins.md`
  - `docs/models/pooling_models.md`
  - `docs/models/supported_models.md`
  - `docs/serving/openai_compatible_server.md`
  - `examples/pooling/embed/embed_jina_embeddings_v3_offline.py`
  - `examples/pooling/embed/embed_matryoshka_fy_offline.py`
  - `examples/pooling/embed/embedding_requests_base64_online.py`
  - `examples/pooling/embed/embedding_requests_bytes_online.py`
  - `examples/pooling/embed/openai_embedding_matryoshka_fy_client.py`
  - `examples/pooling/plugin/prithvi_geospatial_mae_online.py`
  - `examples/pooling/pooling/pooling_online.py`
  - `examples/pooling/token_classify/ner_offline.py`
  - `examples/pooling/token_classify/ner_online.py`
  - `examples/pooling/token_embed/jina_embeddings_v4_offline.py`
  - `examples/pooling/token_embed/multi_vector_retrieval_offline.py`
  - `examples/pooling/token_embed/multi_vector_retrieval_online.py`
  - `tests/entrypoints/pooling/classify/test_online.py`
  - `tests/entrypoints/pooling/embed/test_online.py`
  - `tests/entrypoints/pooling/pooling/test_online.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/classify/serving.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`

### Summary

**What changed and why**  
This PR extracts a common `ChatRequestMixin` from pooling entrypoint protocols to standardize chat request handling across classification, embedding, and pooling endpoints. The changes include refactoring protocol definitions, updating example files with consistent naming patterns, and adding comprehensive tests for chat request functionality.

**Technical impact**  
The refactoring creates a shared base class (`ChatRequestMixin`) that centralizes chat-specific parameters and validation logic, reducing code duplication across pooling endpoints. This improves maintainability and ensures consistent behavior for chat-based requests in classification, embedding, and pooling APIs.

**Potential risks**  
The parameter `continue_final_message` may not work correctly with certain models, as noted in test comments. There's also a risk of breaking existing client code if the renamed example files are referenced elsewhere. The validation logic for mutually exclusive parameters (`continue_final_message` and `add_generation_prompt`) must be consistently applied across all endpoints.

**Key insights**  
The PR successfully establishes a unified schema for chat requests in pooling APIs, which is crucial for API consistency. Developers should update any references to renamed example files and ensure that model-specific limitations with `continue_final_message` are documented. The shared validation logic in `ChatRequestMixin` prevents incompatible parameter combinations at the schema level.

---

## 30. [Enable Cross layers KV cache layout at NIXL Connector](https://github.com/vllm-project/vllm/pull/30207)


### Base Information

- **PR Number:** #30207
- **Author:** [liranschour](https://github.com/liranschour)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-22 02:12:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30207/files) (5):**
  - `docs/features/nixl_connector_usage.md`
  - `tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh`
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR enables the NIXL connector to use a new cross-layer KV cache layout where KV cache blocks are contiguous across layers, reducing transfer buffer fragmentation. The implementation adds configuration support, updates topology detection, and modifies memory registration logic to handle the new layout, demonstrating significant performance improvements (2x+ throughput and reduced TTFT).

**Technical impact**  
The changes introduce a new `cross_layers_blocks` mode that alters how KV cache tensors are structured and registered with the NIXL connector. This affects memory layout detection in `TpKVTopology`, compatibility hashing, and buffer registration logic, requiring attention backends to support the contiguous layout. Performance gains come from drastically reducing the number of transfer buffers needed.

**Potential risks**  
- The feature is currently limited to specific attention backends (FLASH_ATTN, FLASHINFER), potentially causing inconsistencies if enabled with unsupported backends.  
- Lazy initialization of `compat_hash` and `kv_topo` could lead to runtime errors if methods assume these are set before `register_kv_caches` is called.  
- The cross-layer layout may introduce subtle bugs in block mapping and handshake logic, especially in heterogeneous TP configurations.

**Key insights**  
- Developers must explicitly enable the feature via `kv_connector_extra_config: {"enable_cross_layers_blocks": "True"}` and ensure compatible attention backends are used.  
- The changes are backward-compatible; existing configurations continue to work without modification.  
- Thorough testing with varied TP sizes and request patterns is recommended to validate correctness across edge cases.

---

## 31. [[Benchmark] Don't default to `temperature==0` in `vllm bench serve`](https://github.com/vllm-project/vllm/pull/32723)


### Base Information

- **PR Number:** #32723
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-22 02:03:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32723/files) (2):**
  - `vllm/benchmarks/lib/endpoint_request_func.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
The changes remove the hardcoded `temperature=0.0` default from benchmark requests in `vllm bench serve`. Previously, if temperature wasn't specified in the command, all requests defaulted to greedy decoding, which didn't reflect true server-side defaults. Now, the server determines the default temperature based on model configuration or API defaults, providing more accurate performance measurements.

**Technical impact**  
Benchmark results will now accurately reflect the performance of a deployment using its actual default sampling parameters. This is important because sampling parameters like top-k and top-p add computational overhead, and greedy decoding (temperature=0) doesn't represent typical generation workloads. The change affects all OpenAI-compatible endpoints (completions, chat completions, and audio).

**Potential risks**  
Benchmark results may become less comparable across runs if server defaults change. Users expecting deterministic greedy decoding performance might see different results unless they explicitly add `--temperature=0`. The warning message only appears when temperature isn't specified, which could be missed if users don't monitor console output.

**Key insights**  
This change improves benchmark accuracy by testing real-world default configurations rather than artificial greedy decoding. Developers should explicitly specify `--temperature=0` if they want to benchmark greedy performance. Consider documenting this behavior change prominently since it affects benchmark result interpretation and comparison.

---

## 32. [[Misc] Replace urllib's `urlparse` with urllib3's `parse_url`](https://github.com/vllm-project/vllm/pull/32746)


### Base Information

- **PR Number:** #32746
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 00:37:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32746/files) (4):**
  - `vllm/connections.py`
  - `vllm/envs.py`
  - `vllm/multimodal/utils.py`
  - `vllm/utils/network_utils.py`

### Summary

**What changed and why**  
The PR replaces Python's standard library `urllib.parse.urlparse` with `urllib3.util.parse_url` across four files for safer and more standardized URL parsing. This change addresses potential inconsistencies and improves URL handling, particularly for edge cases like IPv6 addresses and data URLs.

**Technical impact**  
This migration changes the return type from `urllib.parse.ParseResult` to `urllib3.util.Url`, requiring updates to attribute access patterns. The code now properly handles optional attributes (like `path`, `netloc`) with fallbacks to empty strings and strips leading slashes from data URL media types. It also adds IPv6 address normalization in network utilities.

**Potential risks**  
The main risk is behavioral differences between the two parsing libraries for edge cases or malformed URLs. The change to check `url_spec.scheme` for `None` before calling `.startswith()` introduces a defensive check but could mask issues if `parse_url` returns `None` for scheme unexpectedly. Data URL parsing now requires manual path cleaning which could introduce errors if not consistently applied.

**Key insights**  
Developers should verify that `urllib3` is properly added as a dependency. All URL attribute accesses must now handle potential `None` values. The IPv6 bracket stripping in `split_zmq_path` is a positive improvement for network compatibility. Consider adding tests for IPv6 URLs and malformed URL edge cases to ensure consistent behavior.

---

## 33. [[AMD][ROCm] MoRI EP: a high-performance all2all backend](https://github.com/vllm-project/vllm/pull/28664)


### Base Information

- **PR Number:** #28664
- **Author:** [alexsun07](https://github.com/alexsun07)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-22 00:33:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28664/files) (16):**
  - `tests/kernels/moe/modular_kernel_tools/cli_args.py`
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/modular_kernel_tools/mk_objects.py`
  - `vllm/_aiter_ops.py`
  - `vllm/config/parallel.py`
  - `vllm/distributed/device_communicators/all2all.py`
  - `vllm/distributed/device_communicators/cuda_communicator.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/all2all_utils.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/mori_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/platforms/rocm.py`
  - `vllm/utils/import_utils.py`

### Summary

**What changed and why**  
This PR integrates MoRI-EP, a high-performance all-to-all communication kernel for AMD ROCm, as a new backend in vLLM's fused MoE modular kernel. It introduces two new classes: `MoriPrepareAndFinalize` for quantize-dispatch and combine operations, and `AiterExperts` for expert computation without permute/unpermute steps. The changes enable significant performance improvements (1.33–2.68×) over the naive all-to-all backend, particularly for expert-parallel configurations.

**Technical impact**  
The integration extends vLLM's fused MoE architecture with a new communication path optimized for AMD hardware. It adds a new all-to-all backend option (`"mori"`), updates configuration handling, and introduces platform-specific checks for GPU architectures (gfx942/gfx950). The changes are modular, following the existing design for fused MoE kernels, and conditionally depend on the optional `mori` and `aiter` packages.

**Potential risks**  
- The implementation currently only supports specific GPU architectures (gfx942/gfx950), limiting portability.  
- There are unsupported features: `apply_router_weight_on_input=True` is not allowed, and shared expert fusion is disabled.  
- Conditional imports and runtime checks for optional dependencies (`mori`, `aiter`) could lead to runtime errors if not properly installed.  
- Multi-node configuration requires careful tuning of kernel parameters (e.g., `warp_num_per_block`, `block_num`).

**Key insights**  
- The MoRI backend provides substantial performance gains for expert-parallel workloads, especially at larger EP sizes (e.g., 2.68× speedup at EP16).  
- Developers must ensure the `mori` and `aiter` packages are installed and compatible with the GPU architecture.  
- Configuration validation should be enhanced to gracefully handle unsupported features or missing dependencies.  
- The changes maintain backward compatibility by adding new options without disrupting existing backends.

---

## 34. [[Model] Extend `collect_children` and `no_init_weights` contexts](https://github.com/vllm-project/vllm/pull/32757)


### Base Information

- **PR Number:** #32757
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-22 00:20:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32757/files) (20):**
  - `tests/utils_/test_collection_utils.py`
  - `vllm/model_executor/model_loader/utils.py`
  - `vllm/model_executor/models/adapters.py`
  - `vllm/model_executor/models/bagel.py`
  - `vllm/model_executor/models/chameleon.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/internlm2.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`
  - `vllm/model_executor/models/qwen_vl.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/utils/collection_utils.py`

### Summary

**What changed and why**  
This PR extends the `collect_children` and `no_init_weights` contexts to support multimodal models where both vision and language components exist within the same child module. It introduces a `target` argument to selectively mark specific module types as language or tower components, and adds a `_mark_composite_model` wrapper for convenience. Additionally, it merges `LMMissingLayer` and `TowerMissingLayer` into a unified `StageMissingLayer` class.

**Technical impact**  
The changes enable MM-only and LM-only operation modes for multimodal models by allowing selective initialization of language and vision components. The new `StageMissingLayer` replaces generation-specific modules in pooling adapters, improving memory efficiency when certain model stages are disabled. The architecture now supports more flexible model configurations where components are nested rather than strictly separated.

**Potential risks**  
The refactoring touches multiple model implementations, increasing the risk of regression in multimodal functionality. The caching mechanism in `get_language_model` could potentially return stale references if model structure changes dynamically. The `common_prefix` utility assumes consistent module naming patterns which might not hold for all model architectures.

**Key insights**  
Developers should use the new `_mark_composite_model` wrapper when implementing multimodal models with nested components. The `StageMissingLayer` provides better debugging information with stage names. Ensure that `targets` parameters correctly identify all relevant module types for each modality to avoid missing components during weight loading or initialization.

---

