# vLLM Merged PR Report

**Report Date:** 2026-01-21 PST

**Total Merged PRs:** 33

---

## 1. [[bench] add start_times field to vllm bench serve json result](https://github.com/vllm-project/vllm/pull/32667)


### Base Information

- **PR Number:** #32667
- **Author:** [kebe7jun](https://github.com/kebe7jun)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-21 23:10:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32667/files) (1):**
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
Added a `start_times` field to the JSON results output by the `vllm bench serve` command. This change addresses GitHub issue #32666 by including the start time for each request in the detailed benchmark results.

**Technical impact**  
The modification enhances the benchmarking data collection by exposing request start timestamps alongside existing metrics like TTFT and ITL. This provides additional temporal context for performance analysis without altering the core benchmarking logic or data structure.

**Potential risks**  
Minimal risk as this is purely additive data collection. However, the timestamp format and precision should be documented to ensure consistent interpretation across different systems and use cases.

**Key insights**  
This change improves observability for latency analysis by enabling correlation between request start times and other performance metrics. Developers should verify that the `output.start_time` attribute exists and contains meaningful timestamps across all request types supported by the benchmarking tool.

---

## 2. [[ROCm][CI][Docs] Add comment explaining TRITON_ATTN fallback for ROCm](https://github.com/vllm-project/vllm/pull/32835)


### Base Information

- **PR Number:** #32835
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-21 22:11:10
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32835/files) (1):**
  - `tests/v1/spec_decode/test_acceptance_length.py`

### Summary

**What changed and why**  
A comment was added to clarify that ROCm platforms use Triton as the default attention backend because Flash Attention is not supported. This change addresses a request for documentation to explain the fallback behavior.

**Technical impact**  
This is a documentation-only change that adds clarity to the codebase without altering any functionality. It helps developers understand why `TRITON_ATTN` is returned for ROCm platforms in the `get_available_attention_backends` function.

**Potential risks**  
There are no functional risks since this only adds a comment. However, the comment could become outdated if Flash Attention support is later added for ROCm, requiring an update to maintain accuracy.

**Key insights**  
The comment effectively documents the rationale behind the platform-specific behavior. Developers should ensure such explanatory comments are kept current if the underlying technical constraints change. This practice improves code maintainability and team knowledge sharing.

---

## 3. [[ROCm][CI] Fix AITER test flakiness by using explicit attention backend](https://github.com/vllm-project/vllm/pull/32346)


### Base Information

- **PR Number:** #32346
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-21 21:55:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32346/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `tests/models/language/generation/test_common.py`
  - `vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI325X.json`

### Summary

**What changed and why**  
The PR fixes flaky AITER tests on ROCm by replacing environment variable-based activation with explicit attention backend specification. This addresses state leakage issues that caused numerical divergence when tests run in sequence versus individually. The changes also include minor adjustments to fused MoE kernel configurations and test parameters for ROCm stability.

**Technical impact**  
Using `attention_backend="ROCM_AITER_FA"` directly tests the intended AITER attention backend while avoiding problematic code paths that persisted state across tests. The fused MoE kernel tuning changes (`BLOCK_SIZE_N`, `BLOCK_SIZE_K`, `num_warps`, `kpack`) may affect performance for specific hardware configurations. Test configuration updates ensure batch invariance doesn't interfere with ROCm tests.

**Potential risks**  
The fused MoE kernel parameter changes could introduce performance regressions or numerical differences for the affected configuration (`E=8,N=3584`). Setting `max_num_seqs=1` for ROCm might reduce test coverage for multi-sequence scenarios. The root cause of state leakage in the full AITER stack remains unidentified and could surface elsewhere.

**Key insights**  
Explicit backend specification is more reliable than environment variables for controlling test behavior. The investigation shows that complex stateful systems (like AITER) require careful isolation in CI environments. Developers should verify that kernel parameter changes maintain correctness across all supported ROCm hardware variants.

---

## 4. [[ROCm][CI] Lower Acceptance Len Threshold For test_draft_model_quantization](https://github.com/vllm-project/vllm/pull/32731)


### Base Information

- **PR Number:** #32731
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-21 21:47:34
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32731/files) (1):**
  - `tests/v1/e2e/test_spec_decode.py`

### Summary

**What changed and why**  
The change lowers the expected acceptance length threshold from 3.9 to 3.8 in a test for speculative decoding with draft model quantization. This adjustment addresses a consistent test failure on AMD CI where the observed acceptance length (approximately 3.89) fell just below the original threshold of 3.9.

**Technical impact**  
This modification relaxes the test's tolerance for acceptance length, making it less sensitive to minor performance variations across hardware platforms (AMD vs. NVIDIA). It ensures the test passes on AMD CI while maintaining validation of speculative decoding behavior, though it slightly reduces the strictness of the performance requirement.

**Potential risks**  
Overly relaxing the threshold could mask genuine performance regressions or hardware-specific issues in speculative decoding. If the acceptance length continues to drift downward on any platform, the test may no longer catch meaningful degradation. Additionally, this change does not address the root cause of the hardware discrepancy.

**Key insights**  
The adjustment is a pragmatic fix for CI stability but highlights the need for platform-aware testing or more robust tolerance mechanisms. Consider monitoring acceptance length trends over time to ensure the threshold remains appropriate. For future similar tests, evaluate whether dynamic thresholds or hardware-specific baselines would be more maintainable.

---

## 5. [Upgrade transformers-4.57.5](https://github.com/vllm-project/vllm/pull/32287)


### Base Information

- **PR Number:** #32287
- **Author:** [huydhn](https://github.com/huydhn)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-21 21:19:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32287/files) (4):**
  - `requirements/nightly_torch_test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/models/multimodal/pooling/test_jinavl_reranker.py`

### Summary

**What changed and why**  
The PR upgrades the transformers dependency from version 4.57.3 to 4.57.5 to address a CI issue referenced in the HuggingFace transformers repository. Additionally, it adds conditional skips for five test cases specifically when transformers 4.57.5 is detected, citing a known issue in the transformers library.

**Technical impact**  
Updating the dependency may resolve underlying CI failures, but the added test skips indicate that transformers 4.57.5 introduces regressions or bugs affecting the JinAvL reranker model tests. This creates a temporary workaround to maintain CI stability while awaiting a fix in a future transformers release.

**Potential risks**  
Skipping multiple test cases reduces test coverage for the JinAvL reranker under transformers 4.57.5, potentially masking integration issues. If the skipped tests are not re-enabled after a transformers update, regressions could go undetected. There is also a risk that the fix for the original CI issue might introduce subtle incompatibilities elsewhere.

**Key insights**  
The changes are a pragmatic stopgap to unblock CI, but developers should monitor the linked transformers issue (#43295) and remove the skips once a fix is released. Consider adding a TODO comment or tracking ticket to ensure the skips are not forgotten. This approach highlights the importance of pinning dependencies to specific versions when dealing with upstream instability.

---

## 6. [[Llama.py -> mistral.py] Extract mistral-only relevant code into separate file](https://github.com/vllm-project/vllm/pull/32780)


### Base Information

- **PR Number:** #32780
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-21 21:14:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32780/files) (3):**
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/mistral.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
The PR extracts Mistral-specific code from `llama.py` into a new `mistral.py` file. This refactoring removes Mistral-only features like Llama4 scaling and weight remapping logic from the base Llama implementation, addressing concerns about unwanted dependencies and improving code clarity.

**Technical impact**  
The changes create a cleaner separation between Llama and Mistral model implementations. The base `LlamaAttention` class no longer contains Mistral-specific logic, and a new `MistralAttention` class inherits from it to add Llama4 scaling. The model registry now points `MistralForCausalLM` to the new implementation instead of the Llama one.

**Potential risks**  
There's a risk of breaking existing Mistral model loading since weight remapping logic was moved. The quantization scaling fusion logic in `MistralDecoderLayer` introduces new dependencies that could affect FP8 quantization support. The inheritance hierarchy adds complexity that might make debugging more challenging.

**Key insights**  
This refactoring improves maintainability by separating concerns, but developers should verify Mistral model loading still works correctly with different quantization formats. The pattern of extracting model-specific logic into separate files could be applied to other model variants in the future. The changes demonstrate good architectural practice by reducing coupling between model implementations.

---

## 7. [[FlashMLA] Update FlashMLA to expose new arguments](https://github.com/vllm-project/vllm/pull/32810)


### Base Information

- **PR Number:** #32810
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-21 21:02:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32810/files) (8):**
  - `.gitignore`
  - `cmake/external_projects/flashmla.cmake`
  - `setup.py`
  - `tests/v1/attention/test_sparse_mla_backends.py`
  - `vllm/third_party/flashmla/__init__.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/ops/flashmla.py`

### Summary

**What changed and why**  
This PR updates the FlashMLA integration to expose new kernel arguments and reorganizes the interface. Key changes include updating the FlashMLA git tag, vendoring the interface file with a torch-ops shim, adding FP8-specific metadata and kernel functions, and restructuring the metadata handling to use a `FlashMLASchedMeta` dataclass.

**Technical impact**  
The changes introduce a more modular architecture by separating FP8 and non-FP8 kernel paths, removing the backward kernel for SM100 dense prefill, and centralizing scheduler metadata. The vendored interface now uses `torch.ops._flashmla_C` directly, improving integration with PyTorch's operator system. This enables better support for FP8 KV caches and sparse attention workflows.

**Potential risks**  
Removing the SM100 dense backward kernel (`fmha_cutlass_bwd_sm100.cu`) may break gradient computations for certain configurations. The conditional logic for FP8 vs. non-FP8 paths increases complexity and could lead to runtime errors if metadata mismatches occur. Additionally, the updated git tag may introduce untested behavior or regressions.

**Key insights**  
Developers should verify that all FP8 and sparse attention use cases are thoroughly tested, especially with CUDA 12.9+. The removal of the backward kernel requires checking that no training pipelines rely on it. The new `FlashMLASchedMeta` abstraction simplifies metadata management but must be consistently applied across both dense and sparse backends.

---

## 8. [[ROCm][CI] fix get_valid_backends](https://github.com/vllm-project/vllm/pull/32787)


### Base Information

- **PR Number:** #32787
- **Author:** [divakar-amd](https://github.com/divakar-amd)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-21 20:27:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32787/files) (1):**
  - `tests/v1/spec_decode/test_acceptance_length.py`

### Summary

**What changed and why**  
The fix addresses an issue where `get_valid_backends` was incorrectly returning `None` due to the `__getattr__` override in `interface.py`, which returns `None` for missing attributes instead of raising `AttributeError`. The change modifies the check to inspect the platform class directly rather than relying on `hasattr` on the instance.

**Technical impact**  
This change ensures that the attention backend detection logic correctly distinguishes between platforms that genuinely implement `get_valid_backends` and those where it's missing. It adds explicit handling for ROCm platforms by defaulting to "TRITON_ATTN" when the method is absent, while maintaining "FLASH_ATTN" as the default for other platforms.

**Potential risks**  
The fix assumes that all ROCm platforms without `get_valid_backends` should use "TRITON_ATTN", which might not hold true for all ROCm configurations or future platform implementations. Additionally, the PR description notes that further debugging is ongoing, suggesting this may be a partial fix with potential underlying issues in the cudagraph refactor.

**Key insights**  
Developers should verify that the assumption about ROCm's default attention backend aligns with actual platform capabilities. The use of `getattr(current_platform.__class__, ...)` is a robust pattern for checking method existence when `__getattr__` is overridden. Consider whether this pattern should be standardized elsewhere in the codebase to avoid similar issues.

---

## 9. [[Docs] Remove outdated async_scheduling limitation with speculative decoding](https://github.com/vllm-project/vllm/pull/32775)


### Base Information

- **PR Number:** #32775
- **Author:** [ikaadil](https://github.com/ikaadil)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-21 20:19:25
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32775/files) (1):**
  - `vllm/config/scheduler.py`

### Summary

**What changed and why**  
Removed outdated documentation stating that async scheduling is incompatible with speculative decoding. This change reflects that async scheduling now works with speculative decoding after the implementation in PR #31998.

**Technical impact**  
The documentation now accurately reflects the current system capabilities, preventing confusion about feature compatibility. No functional changes were made to the scheduler configuration or behavior.

**Potential risks**  
If other undocumented limitations still exist with async scheduling (such as pipeline parallelism), developers might assume full compatibility. The removal could lead to incorrect assumptions if the documentation isn't comprehensive elsewhere.

**Key insights**  
Always keep documentation synchronized with feature implementations to prevent misleading constraints. Consider verifying if other mentioned limitations (like pipeline parallelism) still apply and update accordingly. This is a good example of maintaining documentation hygiene.

---

## 10. [Cleanup some huggingface_hub-related stuff](https://github.com/vllm-project/vllm/pull/32788)


### Base Information

- **PR Number:** #32788
- **Author:** [Wauplin](https://github.com/Wauplin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-21 19:38:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32788/files) (3):**
  - `vllm/lora/utils.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/repo_utils.py`

### Summary

**What changed and why**  
This PR simplifies Hugging Face Hub integration by removing redundant error handling and token management logic. The changes eliminate duplicate exception catching for `EntryNotFoundError` and `RepositoryNotFoundError` (already covered by `HfHubHTTPError`), remove a custom `_get_hf_token` utility in favor of Hugging Face Hub's built-in token resolution, and add clarifying comments about caching behavior.

**Technical impact**  
The codebase becomes more maintainable by relying on the official `huggingface_hub` SDK's error hierarchy and token resolution logic. Removing the custom token function ensures consistent authentication behavior (including support for `hf auth login`), while the exception simplification reduces code complexity without changing functionality.

**Potential risks**  
Low risk, as the changes primarily remove redundant code. However, developers should verify that all token-dependent operations still work correctly in environments where `HF_TOKEN` is set but not trimmed (previously handled by `_get_hf_token`). The exception consolidation assumes `HfHubHTTPError` reliably catches all relevant subclasses.

**Key insights**  
These changes align vLLM with Hugging Face Hub best practices. Developers should use `huggingface_hub.get_token()` directly if token retrieval is needed. The added comment in `file_exists` clarifies an intentional deviation from the SDK's default implementation for performance reasons (caching and retry logic).

---

## 11. [[EC Connector] Optimize remote cache check in scheduler](https://github.com/vllm-project/vllm/pull/32585)


### Base Information

- **PR Number:** #32585
- **Author:** [knlnguyen1802](https://github.com/knlnguyen1802)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-21 19:30:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32585/files) (5):**
  - `tests/v1/core/test_scheduler.py`
  - `tests/v1/ec_connector/unit/test_ec_example_connector.py`
  - `vllm/distributed/ec_transfer/ec_connector/base.py`
  - `vllm/distributed/ec_transfer/ec_connector/example_connector.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
This PR optimizes remote cache checking in the EC Connector scheduler by modifying `_try_schedule_encoder_inputs` to check cache existence per multimodal media item only when it is planned for scheduling. Previously, the scheduler checked remote cache for all steps, even during decode phases, which impacted TPOT (Time Per Output Token).

**Technical impact**  
The change replaces the batch `has_caches(request)` call with per-item `has_cache_item(identifier)` checks, reducing unnecessary remote lookups. This shifts cache checking from a request-level pre-check to an on-demand, item-level verification, aligning cache queries with actual scheduling needs and improving scheduler efficiency.

**Potential risks**  
Introducing per-item remote checks could increase latency if many items are scheduled simultaneously, though this is mitigated by checking only planned items. The refactor changes the EC connector interface (`has_caches` → `has_cache_item`), requiring updates in all connector implementations and tests, which increases the risk of integration errors if not fully adopted.

**Key insights**  
The optimization reduces redundant remote cache checks, directly improving TPOT. Developers must ensure all EC connector implementations implement the new `has_cache_item` interface. Testing should verify that per-item checks do not introduce performance regressions in high-concurrency scenarios.

---

## 12. [[Bugfix] Fix potential EAGLE spec decode segfault during graph capture](https://github.com/vllm-project/vllm/pull/32818)


### Base Information

- **PR Number:** #32818
- **Author:** [mawong-amd](https://github.com/mawong-amd)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-21 19:11:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32818/files) (1):**
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
The PR restores the original conditional logic around CUDA graph dispatch in the `dummy_run` function of `SpecDecodeBaseProposer`. After a refactor, the `use_cudagraphs` parameter was left unused, causing CUDA graph dispatch to occur incorrectly when it should be disabled. This was leading to segfaults during CUDA graph capture with EAGLE speculative decoding on multi-GPU setups.

**Technical impact**  
This fix ensures CUDA graph dispatch only occurs when explicitly enabled via `use_cudagraphs`. When disabled, it defaults to `CUDAGraphMode.NONE` and uses the padded token count directly, preventing unintended graph capture attempts that caused segmentation faults, particularly on MI300X with tensor parallelism > 1.

**Potential risks**  
The main risk is that other code paths might have similar issues from the same refactor. The fix assumes `num_tokens_dp_padded` is valid when `use_cudagraphs=False`, which should hold but depends on correct padding logic. Edge cases around tensor parallelism and padding could still surface if the padding calculations have bugs.

**Key insights**  
Always validate that refactored code preserves all original conditional logic, especially around performance-critical features like CUDA graphs. The segfault manifested only under specific hardware/configurations (MI300X, TP>1), highlighting the importance of testing across diverse environments. Developers should audit similar dispatch logic changes from PR #30143 to prevent related issues.

---

## 13. [[Deprecation] Remove deprecated environment variables](https://github.com/vllm-project/vllm/pull/32812)


### Base Information

- **PR Number:** #32812
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-21 18:25:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32812/files) (6):**
  - `.buildkite/test-amd.yaml`
  - `tests/v1/spec_decode/test_acceptance_length.py`
  - `vllm/config/attention.py`
  - `vllm/envs.py`
  - `vllm/platforms/rocm.py`
  - `vllm/usage/usage_lib.py`

### Summary

**What changed and why**  
This PR removes deprecated environment variables related to attention configuration that were slated for removal in v0.14.0. The changes eliminate environment variable support for attention backend selection and related settings, replacing them with explicit configuration via command-line arguments or programmatic configuration.

**Technical impact**  
The removal fundamentally changes how attention configuration is managed. Environment variables like `VLLM_ATTENTION_BACKEND` no longer work, requiring users to migrate to the `--attention-config.backend` command-line argument or `AttentionConfig(backend=...)` programmatic approach. The attention configuration system now relies solely on the structured `AttentionConfig` dataclass.

**Potential risks**  
Breaking changes for users or scripts still relying on the deprecated environment variables. The ROCM platform code now imports `get_current_vllm_config()` at runtime, which could potentially cause circular import issues if not carefully managed. Test configurations and build scripts need to be updated to use the new parameter format.

**Key insights**  
This is a clean-up of technical debt that improves configuration consistency. Developers must update any scripts, tests, or deployment configurations that used these environment variables. The migration path is straightforward: replace environment variables with explicit configuration arguments. Pay special attention to the ROCM platform changes to ensure proper attention backend selection logic.

---

## 14. [[Model Runner V2] Do not error on attention backends](https://github.com/vllm-project/vllm/pull/32820)


### Base Information

- **PR Number:** #32820
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-21 17:02:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32820/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
Removed validation code that restricted attention backends to only "FLASH_ATTN", "FLASHINFER", and "FLASHINFER_MLA". This change allows the model runner to accept any attention backend without raising a `NotImplementedError`.

**Technical impact**  
The system becomes more flexible by supporting additional or custom attention backends without requiring code modifications. However, it removes a safety check that ensured only tested/validated backends were used, potentially allowing untested backends to proceed.

**Potential risks**  
Untested attention backends may cause runtime errors, performance degradation, or incorrect outputs. The removal of validation could lead to silent failures if an incompatible backend is used, making debugging more difficult.

**Key insights**  
Consider adding logging or warnings when unsupported backends are detected to aid debugging. Ensure comprehensive testing of any new backends before deployment, and document the change to inform developers about reduced validation.

---

## 15. [[Model Runner V2] Refactor Prompt Logprobs](https://github.com/vllm-project/vllm/pull/32811)


### Base Information

- **PR Number:** #32811
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-21 15:12:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32811/files) (4):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/logprob.py`
  - `vllm/v1/worker/gpu/sample/prompt_logprob.py`
  - `vllm/v1/worker/gpu/states.py`

### Summary

**What changed and why**  
The changes refactor prompt logprobs computation by extracting it from `model_runner.py` into a dedicated `PromptLogprobsWorker` class. This moves logprobs-related logic and state management out of the main model runner and request states, centralizing it in a separate module.

**Technical impact**  
This improves code modularity and separation of concerns. The `PromptLogprobsWorker` now manages its own state (e.g., `uses_prompt_logprobs`, `in_progress_prompt_logprobs`), reducing coupling with `RequestStates`. The computation uses a new Triton kernel (`_prompt_logprobs_token_ids_kernel`) for efficient token ID gathering, and the chunked logprobs calculation is now reusable.

**Potential risks**  
The Triton kernel introduces a new dependency and potential device compatibility issues. State management is now split between `PromptLogprobsWorker` and `RequestStates`, which could lead to inconsistencies if not properly synchronized (e.g., during request removal). The kernel’s block size (1024) and chunk size are hardcoded, which may not be optimal for all hardware or prompt lengths.

**Key insights**  
This refactoring aligns with single-responsibility principles, making the model runner cleaner. Developers should ensure the `PromptLogprobsWorker` lifecycle matches request states exactly, especially for edge cases like preemption. The kernel optimization is promising but requires validation across different GPU architectures. Consider making chunk/block sizes configurable for performance tuning.

---

## 16. [[Kernel] Add topk_sigmoid kernel](https://github.com/vllm-project/vllm/pull/31246)


### Base Information

- **PR Number:** #31246
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-21 14:49:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31246/files) (13):**
  - `benchmarks/kernels/benchmark_fused_topk.py`
  - `csrc/moe/moe_ops.h`
  - `csrc/moe/topk_softmax_kernels.cu`
  - `csrc/moe/torch_bindings.cpp`
  - `tests/kernels/moe/test_fused_topk.py`
  - `tests/model_executor/test_enabled_custom_ops.py`
  - `vllm/_aiter_ops.py`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_router.py`
  - `vllm/model_executor/layers/fused_moe/router/router_factory.py`
  - `vllm/model_executor/models/minimax_m2.py`

### Summary

**What changed and why**  
This PR adds a `topk_sigmoid` kernel to support sigmoid-based scoring in fused MoE routing, extending beyond the existing softmax-only support. It also adds optional expert bias for selection, where biased scores are used for expert choice but unbiased scores are returned as weights. The changes enable the MiniMax-M2 model to use the fused kernel instead of a slower grouped-topk workaround, improving throughput by 11%.

**Technical impact**  
The kernel implementation is unified under a single `topkGating` template that handles both softmax and sigmoid via a `ScoringFunc` enum. The Python API is extended with `ops.topk_sigmoid` and enhanced `ops.topk_softmax` (both supporting bias). ROCm leverages AITER’s `topk_sigmoid` where available, falling back to a torch implementation for bias. Routing configuration now permits sigmoid in non-grouped paths, and the MiniMax-M2 model is updated to use the fused kernel directly.

**Potential risks**  
The bias handling introduces a subtle behavior: biased scores are used for expert selection, but unbiased scores are returned as weights—this must be consistently documented and tested. ROCm’s AITER `topk_sigmoid` lacks bias support, forcing a fallback path that could affect performance or correctness if not properly isolated. The refactored `topkGating` kernel increases complexity; any regressions in softmax routing need careful validation.

**Key insights**  
Developers should note the new `scoring_func` parameter in routing APIs and the optional `e_score_correction_bias`. For ROCm, the bias path uses a torch fallback, which may have performance implications. The changes are well-tested with new unit tests and benchmarks, but integration should verify that all existing softmax-based models remain unaffected. The performance gain for MiniMax-M2 is significant, justifying the added complexity.

---

## 17. [[Misc] Add Helion version check to collect_env](https://github.com/vllm-project/vllm/pull/32797)


### Base Information

- **PR Number:** #32797
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-21 13:54:47
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32797/files) (1):**
  - `vllm/collect_env.py`

### Summary

**What changed and why**  
Added "helion" to both the `DEFAULT_PIP_PACKAGES` and `DEFAULT_PIP_PATTERNS` sets in the environment collection script. This ensures the `collect_env` utility will detect and report the installed version of the Helion package when generating system diagnostics.

**Technical impact**  
The change integrates Helion into the existing environment reporting framework. When users run `collect_env.py`, the script will now automatically check for the Helion package via pip and include its version in the output, improving the completeness of diagnostic information for systems using Helion.

**Potential risks**  
The primary risk is minimal: if the Helion package is not installed, the script will simply report it as not found, which is the standard behavior for other optional packages in the list. There is a low risk of the package name being incorrect or conflicting with an existing package in the PyPI ecosystem.

**Key insights**  
This is a straightforward maintenance update that follows existing patterns. Developers should ensure the PyPI package name is accurately "helion" and consider if similar updates are needed for any other optional, version-sensitive dependencies used in the project.

---

## 18. [[ModelRunner V2] Don't pin reused flashinfer tensors](https://github.com/vllm-project/vllm/pull/32799)


### Base Information

- **PR Number:** #32799
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-21 13:17:43
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32799/files) (1):**
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
The change disables pinning of reused CPU buffers in FlashInfer attention backend when using ModelRunnerV2. This prevents a race condition where async GPU copies from step N could overlap with CPU buffer updates in step N+1 due to missing explicit synchronization in ModelRunnerV2.

**Technical impact**  
When `VLLM_USE_V2_MODEL_RUNNER` is enabled, CPU buffers will no longer use pinned memory, potentially reducing memory transfer performance but ensuring correctness. The existing pinning behavior is preserved for ModelRunnerV1 to maintain optimal performance where synchronization is properly handled.

**Potential risks**  
Disabling pinned memory may introduce a slight performance degradation for GPU memory transfers in ModelRunnerV2 due to slower pageable memory copies. If future changes add synchronization to ModelRunnerV2, this workaround might become unnecessary but could be overlooked, leaving suboptimal performance.

**Key insights**  
This is a defensive fix prioritizing correctness over performance for ModelRunnerV2. Developers should be aware that any performance improvements to ModelRunnerV2 should consider revisiting this pinning logic. The conditional check cleanly separates V1/V2 behavior without affecting other parts of the system.

---

## 19. [[ROCm] fix import for on_gfx9](https://github.com/vllm-project/vllm/pull/32783)


### Base Information

- **PR Number:** #32783
- **Author:** [divakar-amd](https://github.com/divakar-amd)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-21 10:41:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32783/files) (2):**
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The PR fixes import issues for ROCm GFX9 devices by directly importing `on_gfx9` from `vllm.platforms.rocm` instead of accessing it via `p.rocm.on_gfx9()`. This change ensures the function is available when checking device support for FP8 quantization in fused MoE layers.

**Technical impact**  
This update resolves a potential `AttributeError` when `p.rocm` is accessed on non-ROCm platforms or when the `rocm` attribute is missing. It decouples the GFX9 check from the platform abstraction layer, making the dependency explicit and avoiding runtime attribute lookups.

**Potential risks**  
If `vllm.platforms.rocm` is not importable on non-ROCm systems (e.g., due to missing ROCm dependencies), the conditional import could still cause import errors. Additionally, the change introduces slight code duplication across two files, which could lead to maintenance overhead if further modifications are needed.

**Key insights**  
Direct imports improve clarity and avoid indirect attribute access, but consider centralizing the GFX9 check logic to reduce duplication. Verify that the `vllm.platforms.rocm` module is safely importable in all environments, or wrap the import in a try-except block for robustness.

---

## 20. [Add missing import of fused_topk to benchmark_moe](https://github.com/vllm-project/vllm/pull/32784)


### Base Information

- **PR Number:** #32784
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-21 10:30:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32784/files) (1):**
  - `benchmarks/kernels/benchmark_moe.py`

### Summary

**What changed and why**  
This PR fixes a `NameError` in the MoE benchmark script by adding the missing import for `fused_topk` from `vllm.model_executor.layers.fused_moe`. Additionally, it conditionally initializes the `deep_gemm_experts` variable only when `use_deep_gemm` is True, preventing unnecessary kernel creation.

**Technical impact**  
The changes ensure the benchmark script can execute successfully by resolving the undefined function error. The conditional initialization of `deep_gemm_experts` optimizes resource usage by avoiding the creation of a fused MoE kernel when it's not needed, which is a minor performance improvement for the benchmarking process.

**Potential risks**  
If `use_deep_gemm` is True but the conditional block fails to execute due to an uninitialized variable or logic error, the `deep_gemm_experts` variable could remain `None`, potentially causing downstream issues. The PR description lacks verification that all benchmark configurations (e.g., different `--tp-size` values) were tested with the conditional logic.

**Key insights**  
Always verify imports are present for all referenced functions. The refactoring to conditionally create `deep_gemm_experts` is a good practice for efficiency, but ensure the condition (`use_deep_gemm`) is properly defined and tested in all relevant code paths. Consider adding a comment explaining why the conditional initialization is beneficial.

---

## 21. [[Model Runner V2] Minor refactor for `compute_slot_mappings`](https://github.com/vllm-project/vllm/pull/32794)


### Base Information

- **PR Number:** #32794
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-21 10:24:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32794/files) (3):**
  - `vllm/v1/worker/gpu/block_table.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`

### Summary

**What changed and why**  
The PR refactors the `compute_slot_mappings` function to use `self.block_table_ptrs` and a new `idx_mapping` parameter instead of `self.input_block_table_ptrs`. This aligns the function with a more consistent indexing scheme, where `idx_mapping` maps batch indices to request state indices for block table lookups.

**Technical impact**  
The changes affect how slot mappings are computed for KV cache groups by shifting from direct request indexing to indirect indexing via `idx_mapping`. This improves flexibility in handling request reordering or batching scenarios. The kernel parameter `BLOCK_SIZE` is renamed to `TRITON_BLOCK_SIZE` for clarity, and `page_sizes` is renamed to `block_sizes` to better reflect its purpose.

**Potential risks**  
If `idx_mapping` is incorrectly sized or contains out-of-bounds indices, it could lead to memory access violations or incorrect slot mappings. The reliance on `idx_mapping` assumes it is properly populated by the caller, which adds a dependency that must be validated. Additionally, the padding logic for CUDA graphs remains unchanged but must still ensure no out-of-bounds writes.

**Key insights**  
Developers should ensure `idx_mapping` is correctly initialized and matches the expected request count before calling `compute_slot_mappings`. The renaming of constants and variables improves code readability but requires updates in any downstream references. This change is part of a broader effort to standardize indexing in the model runner, so consistency across related functions should be verified.

---

## 22. [[Misc] Omit "disable NCCL for DP sync" startup log when not applicable](https://github.com/vllm-project/vllm/pull/32707)


### Base Information

- **PR Number:** #32707
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-21 09:03:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32707/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The change adds a conditional check to suppress a startup log message when it's not applicable. Previously, the log would always appear when async scheduling was enabled, but it's only relevant for Data Parallel (DP) with Mixture of Experts (MoE) configurations. This reduces log noise for users not using DP+MoE.

**Technical impact**  
This modification refines the logging behavior without altering the core functionality. The `disable_nccl_for_dp_synchronization` configuration logic remains unchanged—it's still automatically set to `True` with async scheduling and `False` otherwise. Only the informational log is now conditionally displayed.

**Potential risks**  
The conditional logic depends on `self.model_config` being `None` or checking `is_moe`. If `model_config` is unexpectedly `None` in a DP+MoE scenario, the log might still be incorrectly suppressed. Additionally, any future changes to the initialization order of these config attributes could affect this check.

**Key insights**  
This is a clean improvement to user experience by eliminating unnecessary log messages. Developers should ensure the `model_config` is properly initialized before this `__post_init__` call. Consider adding a comment explaining why this specific DP+MoE combination triggers the log for future maintainability.

---

## 23. [Bump Flashinfer to v0.6.1](https://github.com/vllm-project/vllm/pull/30993)


### Base Information

- **PR Number:** #30993
- **Author:** [elvischenv](https://github.com/elvischenv)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-21 08:49:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30993/files) (12):**
  - `docker/Dockerfile`
  - `docker/Dockerfile.nightly_torch`
  - `docker/versions.json`
  - `requirements/cuda.txt`
  - `tests/kernels/moe/test_ocp_mx_moe.py`
  - `tests/v1/sample/test_topk_topp_sampler.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/trtllm_moe.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
This PR upgrades FlashInfer from v0.5.3 to v0.6.1 across Dockerfiles, version files, and requirements. The primary API change is the removal of the `tile_tokens_dim` parameter from all TRTLLM MoE kernels, which simplifies kernel calls by eliminating dynamic tile-size calculations.

**Technical impact**  
The update removes the `calculate_tile_tokens_dim` utility and its usage across MoE implementations, reducing code complexity. Additionally, attention backends now pass `o_data_type` through FlashInfer prefill/decode wrappers, and the fast-plan decode call is adjusted to conditionally handle backend-specific arguments (e.g., for `fa2`).

**Potential risks**  
A test (`test_flashinfer_sampler`) has been temporarily skipped due to failures, indicating potential compatibility issues with the new FlashInfer version. The removal of tile-size logic may affect MoE kernel performance or correctness if the new version handles token distribution differently. Developers should verify MoE behavior under varying token loads.

**Key insights**  
Ensure all MoE tests pass after the upgrade, particularly for MXFP4 and FP8 quantization paths. Review the skipped sampler test to determine if it's a regression or a test-specific issue. The changes are largely mechanical but require validation of attention and MoE performance across different model configurations.

---

## 24. [[PluggableLayer][1/N] Define PluggableLayer (Fix ci)](https://github.com/vllm-project/vllm/pull/32744)


### Base Information

- **PR Number:** #32744
- **Author:** [whx-sjtu](https://github.com/whx-sjtu)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-21 08:38:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32744/files) (7):**
  - `benchmarks/kernels/benchmark_activation.py`
  - `docs/design/custom_op.md`
  - `tests/kernels/utils.py`
  - `tests/model_executor/test_enabled_custom_ops.py`
  - `vllm/config/compilation.py`
  - `vllm/model_executor/custom_op.py`
  - `vllm/model_executor/layers/mla.py`

### Summary

**What changed and why**  
This PR introduces a new `PluggableLayer` base class alongside the existing `CustomOp` system, and migrates the MLA (Multi-Head Latent Attention) layer to use it. The changes reapplies a previously reverted implementation (#32331) to enable a pluggable layer architecture as per RFC #23786, allowing out-of-tree (OOT) replacements of entire layer classes rather than per-platform forward method dispatch.

**Technical impact**  
The `PluggableLayer` class provides a module-composing abstraction that supports OOT replacement at instantiation time via a global registry (`op_registry` and `op_registry_oot`). This decouples layer initialization and composition from platform-specific dispatch, simplifying the extension mechanism for complex layers like MLA. The refactoring also consolidates registry access by moving `op_registry` and `op_registry_oot` to module-level dictionaries, making them shared across both `CustomOp` and `PluggableLayer`.

**Potential risks**  
- The `__new__` method in `PluggableLayer` relies on a `name` attribute; if a subclass isn't properly decorated with `@PluggableLayer.register`, instantiation will fail.  
- Since `op_registry` is now a module-level variable, there’s a risk of namespace collisions between `CustomOp` and `PluggableLayer` registrations if naming isn’t carefully managed.  
- The MLA layer’s `forward_native` was renamed to `forward`, which may affect any existing OOT implementations that depend on the old method name.

**Key insights**  
- Developers should use `PluggableLayer` for stateful, module-composing layers that require full-class replacement, while `CustomOp` remains suitable for lightweight, per-platform dispatchable operations.  
- Ensure all pluggable layers are decorated with `@PluggableLayer.register` to avoid instantiation errors.  
- When migrating layers to `PluggableLayer`, verify that OOT replacements are updated to match the new class structure and method signatures.

---

## 25. [[Quantization][Deprecation] Remove RTN](https://github.com/vllm-project/vllm/pull/32697)


### Base Information

- **PR Number:** #32697
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-21 08:34:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32697/files) (4):**
  - `tests/quantization/test_rtn.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/rtn.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils.py`

### Summary

**What changed and why**  
The PR removes RTN (Round-To-Nearest) quantization support from vLLM. This follows a deprecation notice in version 0.14, indicating RTN is being phased out as a quantization method.

**Technical impact**  
RTN quantization is completely removed from the codebase: its configuration class, linear/moe method implementations, utility functions, and associated tests are deleted. The quantization registry no longer lists "rtn" as a supported method, breaking any existing code or configurations that depend on RTN quantization.

**Potential risks**  
Users who have models quantized with RTN or rely on RTN in their deployment pipelines will experience failures. The removal may break backward compatibility for saved models or configurations that specify `quantization="rtn"`. No migration path or alternative is provided in this change.

**Key insights**  
This is a breaking change that requires users to transition to other quantization methods before upgrading. Developers should update documentation and release notes to clearly communicate this removal. Consider whether any migration utilities or warnings could help users transition more smoothly.

---

## 26. [[ROCm][Deepseekv3.2] Refactor Sparse Indexer as CustomOp](https://github.com/vllm-project/vllm/pull/29287)


### Base Information

- **PR Number:** #29287
- **Author:** [ganyi1996ppo](https://github.com/ganyi1996ppo)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-21 07:16:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29287/files) (8):**
  - `vllm/_aiter_ops.py`
  - `vllm/config/compilation.py`
  - `vllm/model_executor/layers/sparse_attn_indexer.py`
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/platforms/rocm.py`
  - `vllm/v1/attention/backends/mla/indexer.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/ops/rocm_aiter_mla_sparse.py`

### Summary

**What changed and why**  
This PR refactors the `SparseAttnIndexer` into a `CustomOp` to optimize DeepseekV3.2 performance on AMD devices. The change separates heavy kernels (e.g., `fp8_mqa_logits`, `fp8_paged_mqa_logits`) into a platform-specific custom operation, enabling tailored implementations for different hardware backends. It also integrates ROCm-specific optimizations like `mla_decode_fwd` and adds new Triton kernels for dynamic shape handling and preshuffle layout support.

**Technical impact**  
The refactoring decouples the sparse attention indexer logic from the model implementation, improving modularity and enabling backend-specific optimizations. The new custom op registration allows ROCm to use its own implementation (`rocm_aiter_sparse_attn_indexer`) while maintaining a fallback to the CUDA path. This enhances performance portability across platforms and facilitates future optimizations without modifying core model code.

**Potential risks**  
- The custom op approach may introduce platform-specific bugs if implementations diverge significantly.  
- New Triton kernels (`fetch_ragged_layout`, `_indexer_k_quant_and_cache_kernel`, etc.) require thorough testing for edge cases, especially with dynamic shapes and padding scenarios.  
- The ROCm-specific changes assume compatibility with existing CUDA workflows; any discrepancies in kernel behavior could affect model accuracy or performance.

**Key insights**  
- Developers should verify that the custom op is correctly registered and dispatched on the target platform (e.g., ROCm vs. CUDA).  
- The addition of `token_to_seq` metadata in `DeepseekV32IndexerPrefillChunkMetadata` is critical for the new Triton kernels; ensure it is properly populated during prefill.  
- Performance testing on both CUDA and ROCm is essential to validate optimizations and avoid regressions.

---

## 27. [[Quantization][Deprecation] Deprecate HQQ](https://github.com/vllm-project/vllm/pull/32681)


### Base Information

- **PR Number:** #32681
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-21 06:32:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32681/files) (8):**
  - `csrc/quantization/gptq_marlin/generate_kernels.py`
  - `tests/kernels/quantization/test_marlin_gemm.py`
  - `tests/weight_loading/models.txt`
  - `vllm/lora/layers/base_linear.py`
  - `vllm/lora/layers/utils.py`
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/hqq_marlin.py`

### Summary

**What changed and why**  
This PR deprecates HQQ (Half-Quadratic Quantization) support from the codebase following the release of vLLM 0.14. All HQQ-related code—including the quantization method implementation, kernel generation, tests, and model loading references—has been removed.

**Technical impact**  
The removal eliminates HQQ as a supported quantization method. Users relying on HQQ-quantized models will no longer be able to load or run them. The codebase is simplified by deleting the `HQQMarlinMethod` class, configuration, and associated utilities, reducing maintenance overhead.

**Potential risks**  
Existing users with HQQ-quantized models will encounter errors when attempting to load them. There is a risk of breaking workflows or pipelines that depend on HQQ, especially if no migration path or deprecation warning was provided prior to removal.

**Key insights**  
Developers should update documentation and release notes to clearly indicate HQQ is no longer supported. Consider providing guidance on alternative quantization methods (e.g., GPTQ Marlin) for affected users. Ensure any remaining references in configuration files or examples are cleaned up to avoid confusion.

---

## 28. [[Quantization][Deprecation] Remove `DeepSpeedFp8`](https://github.com/vllm-project/vllm/pull/32679)


### Base Information

- **PR Number:** #32679
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-21 06:32:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32679/files) (5):**
  - `examples/offline_inference/load_sharded_state.py`
  - `examples/offline_inference/save_sharded_state.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/deepspeedfp.py`
  - `vllm/model_executor/models/arctic.py`

### Summary

**What changed and why**  
This PR removes the DeepSpeedFP quantization implementation from vLLM as part of a deprecation process following the release of version 0.14. The changes delete the DeepSpeedFP-specific code, configuration entries, and example references, preparing for its complete removal in version 0.15.

**Technical impact**  
The removal eliminates support for DeepSpeedFP (fp6/fp8) quantization, affecting users who relied on this method. The codebase no longer imports or registers the `DeepSpeedFPConfig` and `DeepSpeedFPLinearMethod`, and the Arctic model's MoE layers now use standard parameters instead of quantized ones. This simplifies the quantization layer registry and reduces maintenance overhead.

**Potential risks**  
Existing models quantized with DeepSpeedFP will become incompatible, potentially breaking workflows. The Arctic model's MoE layers lose optimized selective dequantization logic, which could impact performance for certain token-to-expert ratios. There is also a risk if any downstream code still references the removed `deepspeedfp` string or classes.

**Key insights**  
Developers should migrate from DeepSpeedFP to alternative quantization methods (e.g., `fp8`) before upgrading to vLLM 0.15. The Arctic model changes revert to non-quantized weights, which may require performance evaluation. Ensure all configuration files and scripts are updated to remove `quantization="deepspeedfp"` references.

---

## 29. [Support nccl fp8 communication](https://github.com/vllm-project/vllm/pull/32760)


### Base Information

- **PR Number:** #32760
- **Author:** [amirkl94](https://github.com/amirkl94)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-21 05:31:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32760/files) (3):**
  - `vllm/distributed/device_communicators/pynccl_wrapper.py`
  - `vllm/model_executor/layers/fused_moe/prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/shared_fused_moe.py`

### Summary

**What changed and why**  
This PR adds support for NCCL FP8 (float8_e4m3fn) communication by extending the NCCL data type enumeration and mapping logic. It also removes FP8-specific workarounds in fused MoE layers that are no longer needed, and adjusts a conditional for overlapped execution.

**Technical impact**  
The changes enable direct NCCL communication using FP8 data types, eliminating the need for int8 view conversions in fused MoE operations. This improves performance and simplifies the code by leveraging native NCCL FP8 support. The conditional adjustment may affect when overlapped execution is enabled for certain MoE configurations.

**Potential risks**  
The modified conditional for overlapped execution includes a TODO comment questioning its correctness, which could lead to unintended behavior if the condition is inaccurate. Removing the int8 view workaround assumes NCCL FP8 support is fully functional and compatible across all target environments, which may not hold true on older systems or with specific hardware.

**Key insights**  
Developers should verify that the NCCL version in use supports the `ncclFloat8e4m3` data type. The TODO comment in `shared_fused_moe.py` should be addressed promptly to ensure the overlapped execution logic is correct. Testing should focus on FP8 communication paths and MoE performance to confirm no regressions.

---

## 30. [[MoE Refactor] Oracle Select FP8+NVFP4 Kernels In Priority](https://github.com/vllm-project/vllm/pull/32414)


### Base Information

- **PR Number:** #32414
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-21 05:22:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32414/files) (82):**
  - `.buildkite/test-pipeline.yaml`
  - `benchmarks/kernels/benchmark_cutlass_moe_fp8.py`
  - `benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py`
  - `benchmarks/kernels/benchmark_grouped_gemm_cutlass.py`
  - `benchmarks/kernels/benchmark_moe.py`
  - `docs/design/moe_kernel_features.md`
  - `tests/compile/test_fusion_attn.py`
  - `tests/compile/test_silu_mul_quant_fusion.py`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Llama-4-Scout-Fp8-ModelOpt-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-vllm-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-vllm-cutlass.yaml`
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/modular_kernel_tools/mk_objects.py`
  - `tests/kernels/moe/test_batched_deepgemm.py`
  - `tests/kernels/moe/test_block_fp8.py`
  - `tests/kernels/moe/test_cutlass_moe.py`
  - `tests/kernels/moe/test_deepep_deepgemm_moe.py`
  - `tests/kernels/moe/test_deepep_moe.py`
  - `tests/kernels/moe/test_deepgemm.py`
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/kernels/moe/test_flashinfer_moe.py`
  - `tests/kernels/moe/test_modular_oai_triton_moe.py`
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/moe/test_nvfp4_moe.py`
  - `tests/kernels/moe/test_pplx_cutlass_moe.py`
  - `tests/kernels/moe/test_pplx_moe.py`
  - `tests/kernels/moe/test_routing.py`
  - `tests/kernels/moe/test_triton_moe_no_act_mul.py`
  - `tests/kernels/moe/utils.py`
  - `tools/vllm-rocm/pin_rocm_dependencies.py`
  - `vllm/compilation/activation_quant_fusion.py`
  - `vllm/compilation/fusion.py`
  - `vllm/compilation/fusion_attn.py`
  - `vllm/compilation/matcher_utils.py`
  - `vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/fallback.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/router/custom_routing_router.py`
  - `vllm/model_executor/layers/fused_moe/router/grouped_topk_router.py`
  - `vllm/model_executor/layers/fused_moe/router/router_factory.py`
  - `vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`
  - `vllm/model_executor/models/qwen3_moe.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/warmup/deep_gemm_warmup.py`
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
This PR refactors MoE (Mixture of Experts) kernel selection and initialization to introduce a priority-based oracle system. The changes standardize kernel interfaces, add feature registration, and enable automatic kernel selection across hardware and model architectures. This allows for better code reuse, clearer error messages at initialization, and support for kernels like TRTLLM.

**Technical impact**  
The refactor introduces a unified `FusedMoEConfig` that encapsulates MoE layer parameters, replacing scattered arguments. Kernels now expose static methods (e.g., `_supports_quant_scheme`, `_supports_activation`) to declare their capabilities, enabling the oracle to validate compatibility early. The modular kernel framework is extended to support both standard and batched activation formats, with initialization logic consolidated. This shifts runtime failures to initialization-time checks and simplifies kernel integration.

**Potential risks**  
- The large number of file changes (82 files) increases the risk of regression, especially in edge cases like expert-parallel configurations or mixed quantization schemes.  
- Some kernels (e.g., `NaiveBatchedExperts`, `TritonWNA16Experts`) have placeholder support methods that raise `NotImplementedError`; these need to be completed before they can be used with the oracle.  
- The removal of the `allow_deep_gemm` parameter and direct `deep_gemm_moe_fp8` calls could break existing workflows that rely on explicit kernel selection.

**Key insights**  
- Developers must ensure new kernels implement all required static support methods (`_supports_*`) to be discoverable by the oracle.  
- The `FusedMoEConfig` is now central to kernel construction; any new kernel parameters should be integrated there.  
- Testing should focus on the oracle’s priority logic and fallback behavior, especially for heterogeneous hardware environments.

---

## 31. [[bugfix] Aria model](https://github.com/vllm-project/vllm/pull/32727)


### Base Information

- **PR Number:** #32727
- **Author:** [divakar-amd](https://github.com/divakar-amd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-21 05:11:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32727/files) (1):**
  - `vllm/model_executor/models/aria.py`

### Summary

**What changed and why**  
This PR fixes missing `lm_head` and `logits_processor` components in the Aria model implementation. The issue occurred because the Aria model's `language_model` is an `AriaTextModel` (extending `LlamaModel`), not `LlamaForCausalLM`, so it lacks these essential components for logit computation.

**Technical impact**  
The changes add `ParallelLMHead` and `LogitsProcessor` layers directly to the Aria model class, enabling proper logit computation during inference. This restores the model's ability to generate token predictions by correctly processing hidden states through the vocabulary projection layer.

**Potential risks**  
The fix assumes the `logit_scale` configuration parameter exists (defaulting to 1.0), which could cause issues if the config structure differs from expectations. Additionally, the weight loading mechanism for the new `lm_head` layer must be verified to ensure proper initialization from pretrained weights.

**Key insights**  
This fix highlights the importance of understanding inheritance hierarchies when refactoring model architectures. Developers should ensure that model components requiring specific parent class functionality are either properly inherited or explicitly implemented. The solution demonstrates a clean pattern for adding missing components while maintaining compatibility with the existing weight loading system.

---

## 32. [[Model] Add Eagle2.5-8B Vision-Language Model support](https://github.com/vllm-project/vllm/pull/32456)


### Base Information

- **PR Number:** #32456
- **Author:** [George-Polya](https://github.com/George-Polya)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-21 01:39:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32456/files) (5):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/eagle2_5_vl.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds support for NVIDIA's Eagle2.5-8B vision-language model to the vLLM codebase. The changes include a new model implementation file (`eagle2_5_vl.py`) that integrates the SigLIP vision encoder with a Qwen2-7B language model using an MLP projection layer, along with updates to documentation, example scripts, and model registries to enable offline inference and OpenAI-compatible serving.

**Technical impact**  
The addition extends vLLM's multimodal capabilities by supporting a new architecture that uses dynamic image tiling (1-12 tiles) with 256 tokens per tile. The implementation leverages existing `BaseInternVLProcessor` infrastructure for multimodal processing, ensuring consistency with other vision-language models in the codebase. The model supports data-parallel tensor parallelism for the vision encoder and integrates with vLLM's quantization and LoRA systems.

**Potential risks**  
The model requires `trust_remote_code=True` due to custom Hugging Face implementations, which may introduce security or compatibility concerns. Dynamic image tiling and pixel-shuffle downsampling could have edge cases with extreme image dimensions or aspect ratios. The fallback logic for obtaining the image token ID relies on a hardcoded token (`<IMG_CONTEXT>` ID: 151667), which may not be robust if tokenizer vocabularies change.

**Key insights**  
Developers should note that the model uses a Qwen2-based tokenizer with specific stop tokens (`<\|endoftext\|>`, `<\|im_start\|>`, `<\|im_end\|>`). The implementation disables the vision encoder's pooling head to preserve all patch tokens, which is critical for the projection layer. When serving, ensure `--limit-mm-per-prompt image=1` is set to match the example configuration, as the model currently supports only one image per prompt.

---

## 33. [[Bugfix] Force using spawn multiprocess method when it's the WSL platform](https://github.com/vllm-project/vllm/pull/32749)


### Base Information

- **PR Number:** #32749
- **Author:** [jasonyanwenl](https://github.com/jasonyanwenl)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-21 01:35:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32749/files) (1):**
  - `vllm/utils/system_utils.py`

### Summary

**What changed and why**  
Added a check to force the `spawn` multiprocessing start method when running on WSL (Windows Subsystem for Linux). This addresses an NVML initialization failure that occurs because NVML is incompatible with the `fork` method in WSL environments.

**Technical impact**  
The change ensures that multiprocessing in WSL always uses `spawn`, preventing NVML-related crashes during engine startup. This aligns with existing logic that forces `spawn` for other specific conditions (e.g., CUDA/XPU initialization).

**Potential risks**  
Using `spawn` may slightly increase process startup time compared to `fork`. Additionally, if other platform-specific incompatibilities arise (e.g., with other virtualization or container environments), they may require similar handling.

**Key insights**  
This fix is a targeted workaround for a known WSL limitation. Developers should be aware that `spawn` is now the default for WSL, which may affect performance-sensitive multiprocessing scenarios. Consider extending this pattern if similar issues emerge on other platforms.

---

