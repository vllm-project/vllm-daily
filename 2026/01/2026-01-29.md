# vLLM Merged PR Report

**Report Date:** 2026-01-29 PST

**Total Merged PRs:** 42

---

## 1. [[model] Add support for openPangu7B-VL](https://github.com/vllm-project/vllm/pull/32449)


### Base Information

- **PR Number:** #32449
- **Author:** [hujiaxin0](https://github.com/hujiaxin0)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 23:54:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32449/files) (9):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `examples/offline_inference/vision_language_multi_image.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/layers/rotary_embedding/__init__.py`
  - `vllm/model_executor/layers/rotary_embedding/mrope_interleaved.py`
  - `vllm/model_executor/models/openpangu.py`
  - `vllm/model_executor/models/openpangu_vl.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds support for the openPangu-VL-7B vision-language model by implementing a new model class `OpenPanguVLForConditionalGeneration`. It includes the necessary rotary embedding logic for multimodal inputs, vision encoder components, and integration into the existing vLLM infrastructure (registry, examples, documentation).

**Technical impact**  
The changes extend vLLM's multimodal capabilities with a new model architecture that supports both image and video inputs. A key addition is the `MRotaryEmbeddingInterleaved` class, which handles interleaved rotary embeddings for multimodal sections. The implementation follows patterns established by other VL models (e.g., Qwen2.5-VL) and integrates with the existing multimodal processing pipeline.

**Potential risks**  
The custom rotary embedding logic (`MRotaryEmbeddingInterleaved`) introduces complexity and may have edge cases, especially with varying section lengths (2D vs 3D). The model requires `trust_remote_code=True` and `enforce_eager=True`, which could affect performance or compatibility. There is also a risk of incomplete testing, as the PR description lacks test results.

**Key insights**  
Developers should note the model's specific prompt formatting with `[unused*]` tokens and the need for interleaved rotary embeddings. The implementation assumes familiarity with existing multimodal patterns in vLLM. Ensure thorough testing of image/video inputs and validate the rotary embedding behavior across different input configurations.

---

## 2. [Explicitly set `return_dict` for `apply_chat_template`](https://github.com/vllm-project/vllm/pull/33372)


### Base Information

- **PR Number:** #33372
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 23:27:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33372/files) (11):**
  - `examples/offline_inference/prompt_embed_inference.py`
  - `examples/online_serving/prompt_embed_inference_with_openai_client.py`
  - `examples/online_serving/token_generation_client.py`
  - `tests/entrypoints/openai/test_serving_tokens.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/tokenizers/grok2.py`
  - `vllm/transformers_utils/processors/hunyuan_vl.py`
  - `vllm/transformers_utils/processors/qwen3_asr.py`

### Summary

**What changed and why**  
This PR explicitly sets the `return_dict` parameter for `apply_chat_template` calls to manage a breaking change in Transformers v5, where the default changed from `False` to `True`. External examples and tests now use `return_dict=True` to align with the new Transformers behavior, while internal vLLM code uses `return_dict=False` to maintain compatibility with existing interfaces.

**Technical impact**  
The changes ensure consistent behavior across the codebase: external users will experience the new Transformers v5 default (dictionary output), while internal vLLM components continue to receive token IDs directly. This prevents disruptions to internal APIs that expect token IDs rather than dictionaries.

**Potential risks**  
If any internal calls to `apply_chat_template` are missed, they may inadvertently receive dictionaries instead of token IDs, leading to runtime errors. Additionally, external code that does not update to handle dictionary outputs (e.g., by accessing `.input_ids`) could break when upgrading to Transformers v5.

**Key insights**  
Developers should ensure all `apply_chat_template` calls explicitly set `return_dict` to avoid ambiguity. For external usage, always access `.input_ids` when `return_dict=True`. Consider adding a lint rule or test to enforce explicit `return_dict` values in future contributions.

---

## 3. [[CI] Enable mypy import following for `vllm/spec_decode`](https://github.com/vllm-project/vllm/pull/33282)


### Base Information

- **PR Number:** #33282
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 22:43:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33282/files) (5):**
  - `tools/pre_commit/mypy.py`
  - `vllm/v1/spec_decode/draft_model.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/spec_decode/medusa.py`
  - `vllm/v1/spec_decode/suffix_decoding.py`

### Summary

**What changed and why**  
This PR enables mypy import following for the `vllm/v1/spec_decode` module by removing it from the exclusion list in the mypy configuration. The changes primarily involve adding assertions to handle cases where `speculative_config` could be `None` and fixing type-related errors through more conservative attribute access and type ignores.

**Technical impact**  
The codebase now has improved type safety for the speculative decoding module. By adding explicit `assert` statements for `speculative_config` being non-null, the code clarifies assumptions and prevents potential runtime errors. The changes also modify attribute access patterns to be safer (using `getattr` and checking for `None`) and add type ignores for abstract type handling in `get_layers_from_vllm_config`.

**Potential risks**  
The added assertions could potentially fail if `speculative_config` is legitimately `None` in some code paths not covered by tests. The `# type: ignore[type-abstract]` comments might mask underlying type system issues that should be properly resolved. The more conservative `getattr` approach for `target_language_model` assumes a specific structure that might not hold for all model architectures.

**Key insights**  
This is a positive step toward better type checking coverage. Developers should ensure all code paths leading to these modified functions properly initialize `speculative_config`. The type ignores should be revisited later to find more type-safe solutions. The attribute access pattern change in `eagle.py` (lines 1170-1175) provides a good template for safer attribute checking that could be applied elsewhere.

---

## 4. [Move decode context parallel validationn to `ParallelConfig`](https://github.com/vllm-project/vllm/pull/33239)


### Base Information

- **PR Number:** #33239
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-01-29 22:18:41
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33239/files) (2):**
  - `vllm/config/parallel.py`
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
The validation logic for decode context parallel (DCP) configuration has been moved from `EngineArgs` in `arg_utils.py` to the `_validate_parallel_config` method in `ParallelConfig`. This ensures the check runs whenever `ParallelConfig` or `EngineArgs` is instantiated, centralizing validation.

**Technical impact**  
This change improves code organization by placing DCP validation within the appropriate configuration class, making the validation more reusable and consistent. It also replaces an `assert` statement with a proper `ValueError`, providing clearer error handling.

**Potential risks**  
If `_validate_parallel_config` is not called in all relevant code paths, the validation could be bypassed. Additionally, moving from an `assert` to a `ValueError` changes error behavior in production (asserts can be disabled), which is generally positive but should be verified.

**Key insights**  
Centralizing validation logic in `ParallelConfig` enhances maintainability and ensures consistent enforcement of constraints. Developers should ensure any new uses of `ParallelConfig` or `EngineArgs` trigger the validator, and consider similar refactoring for other scattered validation checks.

---

## 5. [[CI][AMD] Skip 4 GPUs testgroup ray tests](https://github.com/vllm-project/vllm/pull/33305)


### Base Information

- **PR Number:** #33305
- **Author:** [rjrock](https://github.com/rjrock)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-29 21:39:53
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33305/files) (2):**
  - `examples/offline_inference/rlhf_colocate.py`
  - `tests/distributed/test_utils.py`

### Summary

**What changed and why**  
Two test files were modified to skip specific tests in AMD/ROCm CI environments. The changes add conditional checks that exit early (`rlhf_colocate.py`) or skip tests (`test_utils.py`) when running on ROCm platforms, addressing test failures due to Ray's incompatibility with vLLM on ROCm.

**Technical impact**  
These changes prevent test failures in AMD CI by conditionally bypassing problematic tests. The modifications are minimal and localized, ensuring other platforms remain unaffected. However, they reduce test coverage for ROCm, potentially masking underlying compatibility issues between Ray and vLLM on AMD hardware.

**Potential risks**  
Skipping tests may hide regressions or compatibility problems in ROCm environments. The early exit in `rlhf_colocate.py` uses `sys.exit(0)`, which could be abrupt if the script is part of a larger test suite. Additionally, the condition `torch.version.hip is not None` may not cover all ROCm detection scenarios.

**Key insights**  
Consider using `pytest.skip()` consistently for better integration with test frameworks. Document the rationale for skipping these tests and track the underlying Ray-ROCm compatibility issue. Ensure future tests include ROCm-specific implementations or workarounds rather than broad skips to maintain coverage.

---

## 6. [[Models] Refactor Kimi-K2.5 weight loading](https://github.com/vllm-project/vllm/pull/33346)


### Base Information

- **PR Number:** #33346
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-29 21:31:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33346/files) (2):**
  - `vllm/model_executor/models/kimi_k25.py`
  - `vllm/model_executor/models/kimi_k25_vit.py`

### Summary

**What changed and why**  
This PR refactors the Kimi-K2.5 model's weight loading mechanism to align with previous architectural refactoring. The main change replaces a custom, complex weight loading implementation with a standardized `AutoWeightsLoader` and `WeightsMapper` approach, simplifying the model initialization and weight mapping logic.

**Technical impact**  
The refactoring significantly reduces code complexity by eliminating 174 lines of custom weight loading logic. It standardizes the model's architecture by using `init_vllm_registered_model` for language model initialization and delegates weight loading responsibilities to shared utilities. This improves maintainability and consistency with other models in the codebase.

**Potential risks**  
The weight mapping changes (`mm_projector.proj.0` → `mm_projector.linear_1`) must be verified against the actual checkpoint structure. The removal of pipeline parallelism (PP) specific logic like `PPMissingLayer` and `get_pp_group` checks could affect distributed training scenarios if not properly handled elsewhere in the codebase.

**Key insights**  
The refactoring demonstrates good architectural alignment by adopting shared utilities for model initialization and weight loading. Developers should ensure the weight mapping accurately reflects the checkpoint structure and verify that pipeline parallelism support is maintained through the new initialization pattern. The simplified code structure will make future maintenance and debugging easier.

---

## 7. [[BugFix] Disable async scheduling for Mamba prefix caching](https://github.com/vllm-project/vllm/pull/33352)


### Base Information

- **PR Number:** #33352
- **Author:** [peakcrosser7](https://github.com/peakcrosser7)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-29 20:40:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33352/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
This PR adds validation logic to disable async scheduling when Mamba prefix caching is enabled. Since v0.14.0, async scheduling is enabled by default, but it's incompatible with Mamba's prefix caching modes ("all" and "align"). The changes prevent runtime errors by raising an error when explicitly enabled and issuing a warning when auto-enabled.

**Technical impact**  
The modification affects configuration validation in `vllm/config/vllm.py`, specifically the `__post_init__` method. It introduces a new compatibility check that influences the scheduler's behavior based on the Mamba cache mode. This ensures the system avoids unsupported configurations that could lead to incorrect execution or crashes.

**Potential risks**  
If the validation logic is too restrictive, it might inadvertently block valid future configurations if Mamba prefix caching gains async support. There's also a risk that the warning might be missed during deployment, leading to confusion about why async scheduling is disabled. The check only occurs at initialization, so runtime changes to cache mode won't be caught.

**Key insights**  
Developers should ensure Mamba models with prefix caching are explicitly configured with `async_scheduling=False`. The warning system helps with visibility, but documentation should be updated to clarify this incompatibility. Consider adding a test to validate the interaction between these features and monitor for future compatibility improvements.

---

## 8. [Fix `tie_word_embeddings` for multimodal models in Transformers v5](https://github.com/vllm-project/vllm/pull/33359)


### Base Information

- **PR Number:** #33359
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-01-29 19:37:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33359/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
Added logic to propagate the `tie_word_embeddings` configuration from the multimodal model's Hugging Face config to its text sub-config. This ensures that when vLLM loads Transformers v5 multimodal models, the language model component correctly receives the embedding tying setting, which is essential for proper weight sharing between the embedding and LM head layers.

**Technical impact**  
The change modifies the `with_hf_config` method to conditionally copy `tie_word_embeddings` from the top-level Hugging Face config to the text sub-config for multimodal models. This aligns vLLM's internal configuration handling with Transformers v5's architectural pattern, where embedding tying is defined at the outer multimodal model level rather than within the nested language model config.

**Potential risks**  
If the `get_text_config()` method returns `None` or lacks the expected attribute, the assignment could fail. Additionally, this fix assumes that all multimodal models in Transformers v5 follow the described pattern; models with different nesting structures might not be handled correctly. There is also a risk of overwriting an existing `tie_word_embeddings` value in the text config if one is already present.

**Key insights**  
This is a targeted fix for compatibility with Transformers v5's updated config hierarchy. Developers should verify that `get_text_config()` is implemented and returns a valid config object for all supported multimodal models. Consider adding a fallback or warning if the text config is inaccessible, and ensure that the logic does not interfere with non-multimodal models.

---

## 9. [[Model][Multimodal] Add explicit MusicFlamingo adapter](https://github.com/vllm-project/vllm/pull/32696)


### Base Information

- **PR Number:** #32696
- **Author:** [WangHaoyuuu](https://github.com/WangHaoyuuu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 19:01:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32696/files) (6):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/audio_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/musicflamingo.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
Added an explicit MusicFlamingo adapter that reuses AudioFlamingo3's architecture while handling MusicFlamingo-specific configurations and processors. This ensures proper type checks and processor loading for MusicFlamingo checkpoints, which share the same underlying model but use different identifiers.

**Technical impact**  
The changes extend the multimodal registry to recognize MusicFlamingo as a distinct model type, while maintaining code reuse through inheritance. Compatibility fixes in the encoder layer ensure forward passes work with both AudioFlamingo3 and MusicFlamingo checkpoints, and the adapter safely falls back to AudioFlamingo3 components when MusicFlamingo classes are unavailable.

**Potential risks**  
The optional dependency handling (`try-except`) could mask import errors if transformers' MusicFlamingo module has breaking changes. The dummy `freqs` parameter added to the encoder may cause confusion if not properly documented. There's also a risk of subtle incompatibilities if MusicFlamingo's processor diverges significantly from AudioFlamingo3's in future releases.

**Key insights**  
This is a clean pattern for supporting model variants with shared architectures. Developers should ensure the fallback logic is robust and monitor for upstream changes in the transformers library. The encoder signature fix highlights the importance of maintaining compatibility across related model families.

---

## 10. [[Docs] Adding links and intro to Speculators and LLM Compressor](https://github.com/vllm-project/vllm/pull/32849)


### Base Information

- **PR Number:** #32849
- **Author:** [aireilly](https://github.com/aireilly)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-01-29 14:12:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32849/files) (5):**
  - `docs/features/README.md`
  - `docs/features/quantization/README.md`
  - `docs/features/quantization/llm_compressor.md`
  - `docs/features/spec_decode/README.md`
  - `docs/features/spec_decode/speculators.md`

### Summary

**What changed and why**  
This PR updates documentation to introduce and link to two new vLLM-related projects: LLM Compressor (for quantization) and Speculators (for speculative decoding). It adds overview pages for both projects and updates existing references to point to the new documentation structure.

**Technical impact**  
The changes improve documentation discoverability by creating dedicated pages for these projects and updating cross-references. The spec_decode links now point to a README directory instead of a single file, suggesting better organization. No functional code changes are made—this is purely documentation enhancement.

**Potential risks**  
Broken links could occur if the referenced paths (like `spec_decode/README.md`) don't exist or are moved. The added project pages assume users will navigate to external repositories, which might lead to confusion if those projects are not yet stable or well-documented.

**Key insights**  
These updates signal vLLM's growing ecosystem with dedicated tools for quantization and speculative decoding. Developers should note the new resources for model optimization and latency reduction. Ensure all hyperlinks are validated during documentation builds to maintain integrity.

---

## 11. [[Bugfix] Enable Triton MoE for FP8 per-tensor dynamic](https://github.com/vllm-project/vllm/pull/33300)


### Base Information

- **PR Number:** #33300
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-29 12:15:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33300/files) (2):**
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The changes add support for FP8 per-tensor dynamic quantization schemes in Triton's MoE backend. Specifically, they register two new quantization scheme pairs: `(kFp8StaticTensorSym, kFp8DynamicTokenSym)` and `(kFp8StaticTensorSym, kFp8DynamicTensorSym)` to the `SUPPORTED_W_A_FP8` lists. This fixes a bug where Triton MoE was incorrectly excluded for models using FP8 per-tensor dynamic scaling, causing fallback to a less optimal backend.

**Technical impact**  
These modifications enable the Triton MoE kernel to be selected when running MoE models with FP8 per-tensor dynamic quantization. This improves performance for such configurations by allowing the use of a specialized, optimized backend instead of defaulting to a generic fallback like Marlin. The change is minimal and only affects the backend selection logic.

**Potential risks**  
The risk is low as this only expands the allowlist of supported quantization schemes. However, it assumes the Triton MoE kernel implementation already correctly handles these specific FP8 dynamic schemes. If the underlying kernel lacks full support, runtime errors or numerical inaccuracies could occur. The change should be validated with the mentioned test model `marksverdhei/GLM-4.7-Flash-FP8`.

**Key insights**  
This is a straightforward configuration fix that aligns the support registration with actual kernel capabilities. Developers should ensure comprehensive testing across different MoE model architectures and batch sizes to confirm the Triton backend functions correctly with the newly enabled schemes. The fix highlights the importance of keeping backend support lists synchronized with kernel feature development.

---

## 12. [[release] Minor fixes to release annotation and wheel upload](https://github.com/vllm-project/vllm/pull/33129)


### Base Information

- **PR Number:** #33129
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-29 12:09:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33129/files) (3):**
  - `.buildkite/release-pipeline.yaml`
  - `.buildkite/scripts/annotate-release.sh`
  - `.buildkite/scripts/upload-release-wheels-pypi.sh`

### Summary

**What changed and why**  
This PR implements several fixes and improvements to the release pipeline. It adds support for CUDA 13.0 Docker images, corrects a `twine` command syntax that was causing errors, changes the ROCm image repository naming, and separates GitHub release automation from PyPI uploads by removing GitHub-related logic and renaming the wheel upload script to be PyPI-specific.

**Technical impact**  
The changes streamline the release process by separating concerns: PyPI uploads are now handled independently from GitHub releases. The pipeline now supports additional CUDA 13.0 variants for both wheels and Docker images, and uses corrected `twine` command syntax. The ROCm images are now pushed to a dedicated repository (`vllm-openai-rocm`) instead of using tags on the main repository.

**Potential risks**  
Removing GitHub release automation could lead to manual errors if release managers forget to create GitHub releases. The new CUDA 13.0 support introduces additional complexity that must be tested thoroughly. The script renaming requires corresponding pipeline configuration updates to avoid broken references.

**Key insights**  
The separation of PyPI and GitHub release steps is a good architectural decision that improves maintainability. Developers should ensure the new `upload-release-wheels-pypi.sh` script is properly referenced in all pipeline configurations. The addition of CUDA 13.0 support expands compatibility but requires validation that all build artifacts are correctly generated and uploaded.

---

## 13. [Add Triton fused MoE config for B200 (Nemotron Nano)](https://github.com/vllm-project/vllm/pull/32804)


### Base Information

- **PR Number:** #32804
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-29 11:21:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32804/files) (1):**
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_B200.json`

### Summary

**What changed and why**  
Added a Triton fused MoE configuration file (`E=128,N=1856,device_name=NVIDIA_B200.json`) to optimize Mixture-of-Experts (MoE) kernel performance for the NVIDIA Nemotron Nano 30B model on B200 GPUs. This resolves a warning about using suboptimal default configurations and provides tuned parameters for various batch sizes.

**Technical impact**  
The configuration file enables vLLM to use optimized kernel parameters (e.g., block sizes, warps, stages) for MoE layers on B200 hardware, improving inference throughput. This change is backward-compatible—the system falls back to defaults if the file is absent—and directly enhances performance for the specified model and GPU combination.

**Potential risks**  
The configuration is specific to the B200 GPU and the Nemotron Nano model (E=128, N=1856); using it with other hardware or model architectures may lead to suboptimal performance or errors. Additionally, the tuning was performed with a limited set of batch sizes; extreme batch sizes outside the tested range might not benefit equally.

**Key insights**  
Developers should ensure this configuration is only applied to compatible B200 deployments. Consider extending similar tuning to other GPU models or MoE configurations to prevent warnings and performance gaps. The performance gains (6–12% in tests) justify including such hardware-specific optimizations in the codebase.

---

## 14. [[Bugfix][Kernel] Fix negative memory offset in GDN Triton kernel](https://github.com/vllm-project/vllm/pull/33326)


### Base Information

- **PR Number:** #33326
- **Author:** [CarstyYou](https://github.com/CarstyYou)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-01-29 10:40:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33326/files) (1):**
  - `vllm/model_executor/layers/fla/ops/fused_recurrent.py`

### Summary

**What changed and why**  
Added boundary checks in the Triton kernel `fused_recurrent_gated_delta_rule_fwd_kernel` to skip processing when `ssm_state_indices` contains `PAD_SLOT_ID = -1`. This prevents negative memory offsets that caused illegal CUDA memory access when using CUDA Graphs with MTP speculative decoding on Qwen3-Next models.

**Technical impact**  
The fix ensures the kernel safely handles padded slot indices, aligning with how Flash Attention’s cache kernels manage invalid indices. This resolves crashes under CUDA Graph replay and maintains functional correctness for speculative decoding workflows.

**Potential risks**  
If other kernels or components rely on similar state indices without validation, analogous issues may surface. The conditional skip could subtly affect performance if many padded indices are processed, though impact is likely minimal.

**Key insights**  
Always validate indices from external data structures (like `ssm_state_indices`) in GPU kernels to prevent out-of-bounds memory access. This pattern should be reviewed in related attention or state-management kernels to ensure consistency. The fix is minimal and mirrors established practices in the codebase.

---

## 15. [[NVIDIA] [feat] Integrate flashinfer Trtllmgen bf16 moe](https://github.com/vllm-project/vllm/pull/32954)


### Base Information

- **PR Number:** #32954
- **Author:** [Linda-Stadter](https://github.com/Linda-Stadter)
- **Merged By:** [pavanimajety](https://github.com/pavanimajety)
- **Merged time:** 2026-01-29 10:00:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32954/files) (5):**
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR integrates FlashInfer's TRTLLM BF16 MoE kernel into vLLM, adding support for BF16 unquantized Mixture of Experts models. The changes include a new kernel implementation, backend selection logic, weight reordering utilities, and integration with the existing fused MoE infrastructure. This is a rebased version of PR #28238 with accuracy issues resolved.

**Technical impact**  
The integration adds a new high-performance BF16 MoE backend that can be enabled via the `VLLM_USE_FLASHINFER_MOE_FP16` environment variable. The system now supports multiple MoE backends (TRTLLM, CUTLASS, AITER, TRITON) with automatic selection based on hardware capabilities and configuration. Weight tensors are automatically reordered to match FlashInfer's block layout during model loading.

**Potential risks**  
The weight reordering logic in `process_weights_after_loading` modifies weight tensors in-place, which could affect model portability or checkpoint compatibility. The kernel has specific routing method limitations (excludes `TopKTokens` and `TopKTokensV2`). There's also a risk of performance regression if the environment variable isn't properly set, as indicated by the logging about available but disabled backends.

**Key insights**  
Developers should enable `VLLM_USE_FLASHINFER_MOE_FP16=1` for optimal BF16 MoE performance on supported NVIDIA GPUs. The implementation treats the TRTLLM backend as monolithic (bypassing modular kernel), which simplifies integration but may affect future extensibility. Weight layout transformations are critical for kernel compatibility and should be validated when loading different model checkpoints.

---

## 16. [[BUGFIX][XPU] fix memory check after XPU reuse GPU_worker](https://github.com/vllm-project/vllm/pull/33358)


### Base Information

- **PR Number:** #33358
- **Author:** [xuechendi](https://github.com/xuechendi)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-29 09:56:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33358/files) (1):**
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The change modifies an assertion in `gpu_worker.py` from strict inequality (`>`) to non-strict inequality (`>=`). This addresses a bug where XPU memory profiling reports identical free memory before and after profiling, causing an assertion failure. The fix allows the initial free memory to be equal to or greater than the profiled free memory.

**Technical impact**  
This adjustment relaxes a memory consistency check, accommodating scenarios where GPU/XPU memory usage remains unchanged during profiling. It prevents unnecessary assertion failures in environments where memory is stable or shared processes don't fluctuate, improving robustness for XPU and similar hardware.

**Potential risks**  
The relaxed check may mask legitimate profiling issues where memory should decrease (e.g., due to leaks or external allocations). It could also allow silent failures if memory increases unexpectedly, potentially hiding environment inconsistencies or profiling errors.

**Key insights**  
The fix is minimal and targeted, but developers should ensure memory profiling assumptions remain valid. Consider adding logging for equal-memory cases to aid debugging. Verify that this change doesn't affect other GPU backends where strict inequality is still required.

---

## 17. [[Chore] Move `MediaConnector` to `vllm.multimodal.media`](https://github.com/vllm-project/vllm/pull/33324)


### Base Information

- **PR Number:** #33324
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 08:54:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33324/files) (8):**
  - `tests/multimodal/test_utils.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/media/__init__.py`
  - `vllm/multimodal/media/connector.py`
  - `vllm/multimodal/utils.py`
  - `vllm/transformers_utils/tokenizer.py`

### Summary

**What changed and why**  
This PR moves the `MediaConnector` class and `MEDIA_CONNECTOR_REGISTRY` from `vllm.multimodal.utils` to a new `vllm.multimodal.media.connector` module to resolve circular import issues missed in a previous change (#32406). It also adds a missing `AttributeError` raise in `vllm.transformers_utils.tokenizer.__getattr__` and updates import paths across the codebase accordingly.

**Technical impact**  
The refactoring improves modularity by consolidating media-related functionality (connector, registry, and media I/O classes) under the `vllm.multimodal.media` package. This reduces coupling and clarifies the separation between core utilities and media-specific logic. The changes maintain backward compatibility via deprecation warnings for moved symbols.

**Potential risks**  
- The deprecation warning for `MEDIA_CONNECTOR_REGISTRY` may cause noise in logs until fully removed in v0.17.  
- Any external code relying on the old import paths may break if not updated, though warnings provide a migration path.  
- The new `AttributeError` addition could affect error handling in tokenizer-related code if `__getattr__` was previously falling through silently.

**Key insights**  
- The restructuring enhances code organization and mitigates circular import problems, which is critical for maintainability.  
- Developers should update imports to use `vllm.multimodal.media` for media connector components.  
- Ensure all references to the moved symbols are updated in dependent code to avoid future breakage when deprecated paths are removed.

---

## 18. [[ez] Delete torch25_custom_graph_pass](https://github.com/vllm-project/vllm/pull/33287)


### Base Information

- **PR Number:** #33287
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-29 08:47:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33287/files) (1):**
  - `vllm/compilation/torch25_custom_graph_pass.py`

### Summary

**What changed and why**  
The file `torch25_custom_graph_pass.py` has been entirely removed. This class was a compatibility shim that emulated the Torch 2.6 `CustomGraphPass` interface for older PyTorch versions (<2.6), primarily to support pickling for inductor code caching. The removal likely indicates that support for PyTorch versions prior to 2.6 is being dropped, or the compatibility layer is no longer needed.

**Technical impact**  
This change simplifies the codebase by eliminating a legacy abstraction. Projects using vLLM will now require PyTorch 2.6 or higher if they rely on custom graph passes, as the backward compatibility mechanism is removed. The removal may affect any downstream code that imported or subclassed `Torch25CustomGraphPass`.

**Potential risks**  
If any existing code still depends on this class (e.g., subclasses or imports), it will break immediately. Additionally, users on PyTorch <2.6 may encounter errors when using features that previously leveraged this compatibility layer. The change assumes that all users have migrated to PyTorch 2.6+.

**Key insights**  
This is a cleanup step that aligns the codebase with newer PyTorch versions. Developers should verify their PyTorch version is ≥2.6 and update any custom graph pass implementations to use the native `CustomGraphPass` interface from Torch 2.6. Consider adding a version check or clear error message if PyTorch <2.6 is detected to guide users.

---

## 19. [[Bugfix] Fix broken GLM-OCR initialization](https://github.com/vllm-project/vllm/pull/33350)


### Base Information

- **PR Number:** #33350
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-29 07:56:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33350/files) (1):**
  - `vllm/model_executor/models/glm_ocr.py`

### Summary

**What changed and why**  
The change fixes a runtime `NameError` by converting `GlmOcrVisionConfig` from a direct type reference to a forward reference (string). This resolves an import issue where `GlmOcrVisionConfig` was only available during type checking but not at runtime when the class is defined.

**Technical impact**  
This ensures the GLM-OCR model initializes correctly without breaking existing functionality. The forward reference defers type resolution until runtime, aligning with Python's late-binding behavior for circular or conditional imports.

**Potential risks**  
Minimal risk, as the change is syntactical and doesn't alter logic. However, if other parts of the codebase rely on runtime type introspection of `vision_config`, they may need similar updates. The fix assumes the actual `GlmOcrVisionConfig` class is importable when the `__init__` method is executed.

**Key insights**  
Always verify that type annotations used at class definition are resolvable at runtime, especially in complex module structures. Consider using `from __future__ import annotations` to make all annotations forward references by default, preventing similar issues elsewhere.

---

## 20. [[Multimodal] Simplify MM input definitions](https://github.com/vllm-project/vllm/pull/33331)


### Base Information

- **PR Number:** #33331
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-29 05:32:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33331/files) (17):**
  - `tests/distributed/test_shm_storage.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/multimodal/test_cache.py`
  - `tests/v1/core/test_kv_cache_utils.py`
  - `tests/v1/core/test_prefix_caching.py`
  - `tests/v1/core/test_priority_scheduler_random.py`
  - `tests/v1/core/test_scheduler.py`
  - `tests/v1/core/utils.py`
  - `tests/v1/streaming_input/test_gpu_model_runner_streaming.py`
  - `tests/v1/streaming_input/test_scheduler_streaming.py`
  - `tests/v1/test_serial_utils.py`
  - `vllm/multimodal/cache.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/utils.py`
  - `vllm/v1/serial_utils.py`
  - `vllm/v1/worker/gpu/mm/encoder_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The PR simplifies multimodal input definitions by removing the `modality` and `key` fields from `MultiModalFieldElem`. Instead, `MultiModalKwargsItem` is now constructed directly from dictionaries, making the relationship between keys and field elements clearer. The `group_mm_kwargs_by_modality` function now expects a list of `(modality, MultiModalKwargsItem)` tuples since modality information is no longer stored in the field elements.

**Technical impact**  
This change reduces redundancy in data structures and aligns construction patterns with dictionary-based APIs. The modality is now passed explicitly where needed (e.g., in grouping and caching), simplifying the internal representation. Serialization/deserialization logic is updated to reflect the removal of the two fields, and all related tests are adjusted accordingly.

**Potential risks**  
If any code still relies on the removed `modality` or `key` attributes of `MultiModalFieldElem`, it will break. The change to `group_mm_kwargs_by_modality` requires callers to provide modality explicitly, which could cause issues if not updated everywhere. Serialization changes may affect compatibility with previously serialized data.

**Key insights**  
The refactor improves clarity by decoupling modality from field elements and using dictionaries for construction. Developers should ensure all calls to `group_mm_kwargs_by_modality` are updated to pass `(modality, item)` tuples. Review dependent code for any direct access to the removed fields, and verify serialization/deserialization in distributed contexts.

---

## 21. [[Backport] [Kimi-K2.5] Replace torch.cuda with current_platform for d…](https://github.com/vllm-project/vllm/pull/33320)


### Base Information

- **PR Number:** #33320
- **Author:** [flyrae](https://github.com/flyrae)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 04:29:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33320/files) (1):**
  - `vllm/model_executor/models/kimi_k25.py`

### Summary

**What changed and why**  
The change replaces `torch.cuda.current_device()` with `current_platform.current_device()` in the `KimiK25ForConditionalGeneration` class initialization. This enables the model to run on non-CUDA hardware platforms (like NPU or ROCm) supported by vLLM, improving platform compatibility.

**Technical impact**  
This modification abstracts device detection away from CUDA-specific APIs, aligning with vLLM's multi-platform architecture. It ensures the model initializes correctly across different backends without requiring CUDA, which is essential for heterogeneous hardware deployment.

**Potential risks**  
If `current_platform` is not properly initialized or lacks a `current_device()` implementation for a specific backend, the model may fail to load. Additionally, any downstream code that implicitly assumes CUDA device semantics could encounter compatibility issues.

**Key insights**  
This is a strategic change that enhances vLLM's hardware agnosticism. Developers should verify that `current_platform` is robustly implemented for all target platforms and consider similar updates elsewhere in the codebase to maintain consistency. Testing across all supported backends is crucial.

---

## 22. [[Intel GPU] refine xpu worker](https://github.com/vllm-project/vllm/pull/32894)


### Base Information

- **PR Number:** #32894
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-29 04:26:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32894/files) (2):**
  - `vllm/platforms/__init__.py`
  - `vllm/v1/worker/xpu_worker.py`

### Summary

**What changed and why**  
This PR refactors XPU platform initialization and worker code to align with GPU implementation patterns. The changes remove legacy memory profiling logic from `xpu_worker.py` and simplify platform detection in `__init__.py`, while disabling distributed communication when xccl is unavailable.

**Technical impact**  
The XPU worker now inherits memory management and initialization workflows from the GPU worker base class, improving code consistency. Platform initialization logic is streamlined with clearer conditional paths for xccl availability. Memory profiling is replaced with a standardized memory snapshot approach used by other platforms.

**Potential risks**  
Removing the custom `xpu_get_mem_info()` method could affect memory reporting accuracy for client GPUs if the base implementation doesn't handle non-torch allocations properly. The warning when xccl is unavailable may impact distributed training functionality. Disabled UTs need follow-up to ensure full test coverage.

**Key insights**  
The refactoring improves code maintainability by reducing platform-specific divergence. Developers should verify memory calculations work correctly across all Intel GPU types (data center vs client). The distributed backend fallback behavior requires monitoring to ensure graceful degradation when xccl is unavailable.

---

## 23. [[Models] Qwen3-ASR](https://github.com/vllm-project/vllm/pull/33312)


### Base Information

- **PR Number:** #33312
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 03:27:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33312/files) (9):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/audio_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/qwen3_asr.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/qwen3_asr.py`
  - `vllm/transformers_utils/processors/qwen3_asr.py`

### Summary

**What changed and why**  
This PR adds support for the Qwen3-ASR model series, an automatic speech recognition (ASR) model. The changes include a new model implementation, configuration, processor, and updates to documentation and examples to integrate it into the vLLM framework.

**Technical impact**  
The addition introduces a new multimodal model class `Qwen3ASRForConditionalGeneration` that inherits from existing Qwen3 and multimodal infrastructure, reusing components like `Qwen3OmniMoeAudioEncoder` and `Qwen3OmniMoeThinkerMultiModalProcessor`. It extends vLLM's ASR capabilities, enabling audio transcription with proper token replacement and feature extraction.

**Potential risks**  
The model reuses audio processing logic from Qwen2.5-Omni, which may introduce subtle differences in audio handling compared to the original Qwen3-ASR. The `is_available_online=False` flag in registry tests suggests potential offline or compatibility issues. Additionally, hardcoded audio token handling and feature length calculations could be error-prone if the model's preprocessing changes.

**Key insights**  
Developers should note that this model follows the existing multimodal pattern in vLLM, leveraging shared audio encoders and processors. Ensure audio inputs are properly formatted and tokenized using the new `Qwen3ASRProcessor`. The model supports transcription with language detection via `ISO639_1_SUPPORTED_LANGS`. Integration appears consistent with vLLM's architecture, but thorough testing with diverse audio inputs is recommended.

---

## 24. [[Bugfix][CPU] Fix thread num for shared memory communication](https://github.com/vllm-project/vllm/pull/33317)


### Base Information

- **PR Number:** #33317
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-29 03:26:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33317/files) (3):**
  - `csrc/cpu/shm.cpp`
  - `csrc/cpu/torch_bindings.cpp`
  - `vllm/distributed/device_communicators/cpu_communicator.py`

### Summary

**What changed and why**  
The PR fixes a bug where CPU shared memory communication fails when different processes have varying thread counts. Previously, `SHMManager` used `omp_get_max_threads()` locally, which could differ across nodes. Now, the thread count is passed explicitly from Python after a distributed `all_reduce(MIN)` to ensure consistency.

**Technical impact**  
This change modifies the `SHMManager` constructor to accept an explicit `thread_num` parameter, which is determined by taking the minimum thread count across all processes in the device group. This ensures all nodes use the same thread count for shared memory operations, preventing mismatches in memory allocation or access patterns.

**Potential risks**  
Using the minimum thread count may underutilize CPUs with higher core counts, potentially reducing performance. If the `all_reduce` operation fails or returns an invalid value, it could lead to incorrect initialization. There is also a risk of silent regression if future changes reintroduce implicit thread count detection.

**Key insights**  
Always synchronize configuration parameters across distributed processes to avoid inconsistent state. Consider adding validation to ensure `thread_num` is positive and within reasonable bounds. Document this behavior to clarify that performance may be limited by the least capable node in the group.

---

## 25. [[Voxtral] Streaming example](https://github.com/vllm-project/vllm/pull/33042)


### Base Information

- **PR Number:** #33042
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-29 03:22:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33042/files) (5):**
  - `requirements/common.txt`
  - `requirements/nightly_torch_test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/models/multimodal/generation/test_voxtral_streaming.py`

### Summary

**What changed and why**  
This PR updates the `mistral_common` dependency from version 1.8.8 to 1.9.0 across multiple requirement files and adds a comprehensive test for the new streaming generator API for the Voxtral model. The test demonstrates both synchronous and asynchronous streaming capabilities, validating the integration with vLLM's updated streaming interface.

**Technical impact**  
The changes enable testing of real-time audio streaming with Voxtral, leveraging the new `StreamingInput` API and asynchronous generation. The test fixtures are refactored to use shared configuration, and the new `RealTimeAudioInput` class simulates online audio streaming by chunking audio data with proper look-ahead and look-back buffers. This validates the end-to-end pipeline for streaming multimodal inputs.

**Potential risks**  
Upgrading `mistral_common` could introduce breaking changes if the new version modifies audio encoding or tokenization behavior. The streaming test relies on post-processing text normalization to match expected outputs, which may be fragile if the model's streaming behavior changes. Additionally, the test is currently skipped (`@pytest.mark.skip`), so it won't run in CI until enabled, potentially hiding integration issues.

**Key insights**  
The PR effectively showcases the new streaming API with a well-structured test that covers both synchronous and asynchronous use cases. Developers should ensure the `mistral_common` upgrade is compatible with existing workflows and consider enabling the test in CI once Voxtral streaming is publicly available. The `RealTimeAudioInput` class serves as a valuable reference for implementing real-time audio streaming in production applications.

---

## 26. [[Quantization][Refactor]  use platform dict to choose kernel](https://github.com/vllm-project/vllm/pull/33130)


### Base Information

- **PR Number:** #33130
- **Author:** [zufangzhu](https://github.com/zufangzhu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 02:44:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33130/files) (1):**
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py`

### Summary

**What changed and why**  
The code refactors kernel selection for mixed precision linear operations by organizing available kernels into a platform-specific dictionary instead of a single priority list. This change ensures that only kernels compatible with the current platform are considered during selection, improving efficiency and preventing unnecessary compatibility checks.

**Technical impact**  
This change introduces a more structured approach to kernel selection by explicitly mapping kernels to their supported platforms (CUDA, ROCM, XPU, CPU). The architecture now cleanly separates platform-specific kernel implementations, making it easier to maintain and extend support for new platforms without affecting unrelated code paths.

**Potential risks**  
If the current platform is not found in the dictionary (e.g., due to a new platform being added), this will raise a KeyError during kernel selection. Additionally, the removal of Dynamic4bitLinearKernel and CPUWNA16LinearKernel from the CUDA list could affect fallback behavior if other CUDA kernels fail, though this appears intentional based on the platform mapping.

**Key insights**  
The refactor improves code organization and platform isolation, but developers should ensure that all supported platforms are explicitly listed in the dictionary. Consider adding a default fallback or validation to handle unexpected platform enums gracefully. This change also makes it clearer which kernels are available per platform, aiding future development and debugging.

---

## 27. [[Bug Fix] Handle variable-length tensors in MultiModalFlatField batching](https://github.com/vllm-project/vllm/pull/31751)


### Base Information

- **PR Number:** #31751
- **Author:** [AndriiPasternak31](https://github.com/AndriiPasternak31)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 02:43:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31751/files) (2):**
  - `tests/models/multimodal/generation/test_ultravox.py`
  - `vllm/multimodal/inputs.py`

### Summary

**What changed and why**  
The fix addresses a crash when batching variable-length audio tensors in `MultiModalFlatField._reduce_data()`. Previously, when tensors had incompatible shapes on non-concat dimensions (e.g., different audio durations), the code incorrectly flattened tensors into individual rows. Now, it returns the batch as-is, allowing models like Ultravox to pad and concatenate tensors appropriately.

**Technical impact**  
This change modifies the batching logic to handle variable-length tensors by using a slice-assign approach: it creates a zero-initialized output tensor with max dimensions across the batch, then assigns each input tensor to its proper position. This preserves tensor structure and enables correct concatenation along the specified dimension, fixing the crash for concurrent requests with different audio lengths.

**Potential risks**  
The slice-assign method may increase memory usage temporarily due to allocating a zero tensor of max dimensions. Edge cases include tensors with more than three dimensions or mismatched dtypes/devices, which could lead to errors. Additionally, the fix assumes `dim` is correctly specified; incorrect `dim` values might still cause shape mismatches.

**Key insights**  
Developers should ensure that models using this batching logic implement proper padding (e.g., `pad_and_concat_to_dim3`) for variable-length inputs. The fix maintains backward compatibility for same-length tensors while adding robustness for variable lengths. Consider validating tensor properties (dtype, device) before slice-assign to prevent silent failures.

---

## 28. [[BugFix] Async Eplb fix potential race condition](https://github.com/vllm-project/vllm/pull/32881)


### Base Information

- **PR Number:** #32881
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-29 02:31:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32881/files) (2):**
  - `vllm/distributed/eplb/async_worker.py`
  - `vllm/distributed/eplb/eplb_state.py`

### Summary

**What changed and why**  
Added synchronization to prevent a race condition in async Expert Parallel Load Balancing (EPLB). The fix introduces a CUDA event (`buffer_consumed_event`) that signals when the main thread finishes copying weights from intermediate buffers, ensuring the async thread waits before reusing those buffers for the next layer's transfer.

**Technical impact**  
This change enforces proper ordering between the main thread's weight consumption and the async thread's buffer preparation. It adds a lightweight CUDA event-based barrier, maintaining pipeline efficiency while preventing data corruption when the main thread's copy operations overlap with async buffer writes.

**Potential risks**  
If `buffer_consumed_event` is not properly reset to `None` after waiting, subsequent transfers might stall. The fix currently relies on the async thread clearing the event, which could fail if exceptions occur. Additionally, any future removal of the `.cpu()` synchronization in the async thread (as noted) would expose this race without the fix.

**Key insights**  
The fix is minimal and targeted, using CUDA events for low-overhead synchronization. Developers should ensure all code paths correctly manage the event lifecycle. Consider adding a timeout or fallback for event waits to handle edge cases, and validate the fix under high-load conditions to confirm no performance regression.

---

## 29. [[fix] tesdt mcp_tool_calling_streaming with a more complex math question](https://github.com/vllm-project/vllm/pull/32769)


### Base Information

- **PR Number:** #32769
- **Author:** [daniel-salib](https://github.com/daniel-salib)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 02:25:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32769/files) (1):**
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`

### Summary

**What changed and why**  
The change modifies a test case by replacing a simple multiplication question (13 * 24) with a more complex one (123 * 456). This ensures the Python tool is consistently triggered during testing, addressing an issue where simpler math questions sometimes fail to activate the tool.

**Technical impact**  
This update improves test reliability by using a calculation that reliably exercises the tool-calling logic. It does not affect production code or system behavior, only the test's ability to consistently validate the MCP tool streaming functionality.

**Potential risks**  
There is minimal risk since this is a test-only change. However, if the test's purpose is to verify tool triggering thresholds, the new value may mask underlying issues where the tool should also work with simpler queries.

**Key insights**  
Developers should ensure test inputs reliably trigger the intended code paths. Consider adding additional test cases for both simple and complex queries to comprehensively validate tool-calling behavior across different scenarios.

---

## 30. [[Chore] Remove `use_data_parallel` kwargs from ViT implementation](https://github.com/vllm-project/vllm/pull/33310)


### Base Information

- **PR Number:** #33310
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 02:20:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33310/files) (9):**
  - `vllm/model_executor/models/idefics2_vision_model.py`
  - `vllm/model_executor/models/intern_vit.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/step_vl.py`

### Summary

**What changed and why**  
Removed the `use_data_parallel` parameter from ViT (Vision Transformer) model constructors across multiple vision-language models (e.g., Idefics2, InternVL, MiniCPMV, Step3-VL). Instead, the data-parallel decision is now centralized via a new `is_vit_use_data_parallel()` function imported from the `vision` module. This simplifies the API and ensures consistent data-parallel behavior across all ViT implementations.

**Technical impact**  
The changes reduce parameter propagation through the model initialization chain, making the codebase cleaner and easier to maintain. Data-parallel configuration is now determined dynamically at runtime via a single function, which may affect how tensor parallelism is applied—especially in models like InternVL where TP is conditionally disabled based on head count divisibility.

**Potential risks**  
If `is_vit_use_data_parallel()` has unexpected side effects or returns inconsistent values across different model components, it could lead to mismatched parallelism strategies (e.g., some layers using TP while others do not). The removal of explicit parameters also makes it harder to trace data-parallel behavior during debugging. Additionally, any existing code that passed `use_data_parallel` explicitly will now break.

**Key insights**  
This refactor centralizes parallelism control, promoting consistency but introducing a hidden dependency on the `vision` module. Developers should verify that `is_vit_use_data_parallel()` is thoroughly tested and its behavior is well-documented. When updating or extending ViT models, ensure that the function’s logic aligns with the intended parallelism strategy for all supported hardware configurations.

---

## 31. [[Misc] Cleanup Kimi-K2.5's vision chunk modality entrypoints](https://github.com/vllm-project/vllm/pull/33157)


### Base Information

- **PR Number:** #33157
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 01:46:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33157/files) (7):**
  - `tests/entrypoints/test_chat_utils.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/registry.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/multimodal/video.py`
  - `vllm/renderers/hf.py`

### Summary

**What changed and why**  
This PR cleans up Kimi-K2.5's vision chunk implementation by adding comprehensive test coverage and moving UUID reconstruction functions from chat utilities to the renderer module. The changes address technical debt from an urgent initial implementation by improving code organization and test coverage.

**Technical impact**  
The refactoring centralizes vision chunk UUID handling logic in the renderer module where it's actually used, improving separation of concerns. Test coverage for vision chunk modality is significantly enhanced with 531 lines of new tests covering various image/video scenarios. The removal of the IdentityVideoLoader suggests video processing is now handled through standard pipelines.

**Potential risks**  
The PR introduces test skips for Kimi-K2.5 in processing tests due to known offline inference issues with vision chunks, which could mask real problems. The substantial refactoring of UUID handling logic could introduce subtle bugs in multimodal request processing, particularly around video chunk UUID generation and placeholder replacement.

**Key insights**  
Developers should verify that the moved UUID reconstruction functions maintain identical behavior in their new location. The comprehensive test suite provides good regression protection, but the skipped tests indicate underlying issues that need resolution. The cleanup improves maintainability but requires careful validation of multimodal processing pipelines, especially for video content.

---

## 32. [Bugfix: Pass router logits dtype in nemotron shared experts](https://github.com/vllm-project/vllm/pull/32669)


### Base Information

- **PR Number:** #32669
- **Author:** [amirkl94](https://github.com/amirkl94)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-29 01:36:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32669/files) (1):**
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
The fix addresses a dtype mismatch error in the Nemotron model's MoE layer when using data parallelism with flashinfer cutlass MoE kernels. The issue occurred because `router_logits_dtype` was not being passed to the MoE layer constructor, causing an assertion failure between `torch.bfloat16` and `torch.float32`. The change ensures consistency by explicitly setting `router_logits_dtype` to `torch.float32` and passing it to both the `ReplicatedLinear` gate and the MoE layer.

**Technical impact**  
This change ensures that the router logits dtype is consistently defined and propagated through the MoE layer initialization, aligning with the requirements introduced in a previous PR. It resolves the dtype assertion error in distributed training scenarios (`dp > 1`) when using specific MoE kernels, maintaining compatibility with the updated MoE layer interface.

**Potential risks**  
Hardcoding `router_logits_dtype = torch.float32` may introduce rigidity if future configurations require different dtypes (e.g., mixed precision training). Additionally, the fix assumes that `torch.float32` is always appropriate for the gate parameters and router logits, which could conflict with model configurations optimized for other precisions like bfloat16.

**Key insights**  
Developers should verify that the dtype alignment is consistent across all MoE-related components, especially when using distributed training or custom kernels. Consider making `router_logits_dtype` configurable rather than hardcoded to enhance flexibility. Ensure similar updates are applied to other model architectures using shared MoE layers to prevent analogous issues.

---

## 33. [Make `mypy` opt-out instead of opt-in](https://github.com/vllm-project/vllm/pull/33205)


### Base Information

- **PR Number:** #33205
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 01:12:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33205/files) (11):**
  - `tools/pre_commit/mypy.py`
  - `vllm/_aiter_ops.py`
  - `vllm/config/compilation.py`
  - `vllm/config/utils.py`
  - `vllm/env_override.py`
  - `vllm/envs.py`
  - `vllm/forward_context.py`
  - `vllm/logger.py`
  - `vllm/logprobs.py`
  - `vllm/profiler/utils.py`
  - `vllm/v1/kv_cache_interface.py`

### Summary

**What changed and why**  
The PR changes the mypy pre-commit script from an opt-in to an opt-out model. Previously, new directories had to be explicitly added to a `FILES` list to be type-checked; now all `vllm/` files are checked by default, and specific directories are excluded via an expanded `SEPARATE_GROUPS` list (renamed to function as an exclude list). This ensures new code is automatically covered by mypy without manual updates.

**Technical impact**  
The change simplifies maintenance by eliminating the need to update the `FILES` list when adding new modules. The logic in `group_files()` is inverted: it now includes all `vllm/` files by default and only excludes those in `SEPARATE_GROUPS` or `EXCLUDE`. Several type annotations and minor fixes were applied to previously untested files to resolve mypy errors uncovered by the broader coverage.

**Potential risks**  
The expanded coverage may initially reveal hidden type errors in excluded directories (now listed in `SEPARATE_GROUPS`), potentially causing CI failures if those exclusions are removed prematurely. The `# type: ignore` comments and `None` assignments (e.g., `head_size_v: int = None`) could mask underlying issues that need proper resolution.

**Key insights**  
Developers should prioritize fixing mypy errors in the newly listed excluded directories to eventually remove them from `SEPARATE_GROUPS`. The shift to opt-out improves code quality enforcement but requires vigilance to ensure type safety across the entire codebase. Future PRs should avoid adding new exclusions without justification.

---

## 34. [[Misc] Remove missed `pad_for_cudagraph`](https://github.com/vllm-project/vllm/pull/33283)


### Base Information

- **PR Number:** #33283
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-29 01:12:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33283/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
Removed the `pad_for_cudagraph` method from the `vllm/config/vllm.py` file. This change follows up on a previous PR (#30143) that eliminated the `pad_for_cudagraph` functionality, ensuring no leftover references remain in the codebase.

**Technical impact**  
The removal eliminates a helper method that calculated padded batch sizes for CUDA graph capture. This simplifies the configuration class and aligns with the broader removal of CUDA graph padding logic, reducing code maintenance overhead.

**Potential risks**  
If any other part of the codebase still calls `pad_for_cudagraph`, it will result in an `AttributeError`. Developers should verify that all dependencies on this method were addressed in the earlier PR to prevent runtime failures.

**Key insights**  
This is a clean-up change that reinforces the removal of deprecated functionality. Ensure thorough testing to confirm no hidden usages exist, and consider adding a deprecation warning in future changes if similar legacy methods are phased out gradually.

---

## 35. [[Doc] Update outdated link to Ray documentation](https://github.com/vllm-project/vllm/pull/32660)


### Base Information

- **PR Number:** #32660
- **Author:** [graftim](https://github.com/graftim)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-29 00:56:06
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32660/files) (1):**
  - `docs/serving/openai_compatible_server.md`

### Summary

**What changed and why**  
Updated a broken link in the documentation from `https://docs.ray.io/en/latest/serve/llm/serving-llms.html` to `https://docs.ray.io/en/latest/serve/llm/index.html`. The change ensures the link points to the current Ray Serve LLM documentation page.

**Technical impact**  
This is a documentation-only change with no impact on code functionality, system behavior, or architecture. It improves the accuracy and usability of the documentation for users seeking Ray Serve LLM resources.

**Potential risks**  
Minimal risk since it only affects a hyperlink. However, if the new URL is incorrect or becomes outdated in the future, users may encounter a 404 error or be directed to irrelevant content.

**Key insights**  
Always verify that external documentation links are current and functional. Consider implementing automated link checking in CI/CD pipelines to catch broken links proactively. For documentation PRs, ensure the change aligns with the target site's structure to avoid future obsolescence.

---

## 36. [Adding optional speculator tests for larger models](https://github.com/vllm-project/vllm/pull/32943)


### Base Information

- **PR Number:** #32943
- **Author:** [shanjiaz](https://github.com/shanjiaz)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 00:54:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32943/files) (2):**
  - `.buildkite/test-pipeline.yaml`
  - `tests/v1/spec_decode/test_acceptance_length.py`

### Summary

**What changed and why**  
This PR adds support for testing the Qwen3-30B-MOE-VL-Eagle3 speculator model by creating an optional CI job for large model tests on A100 GPUs. The changes modify the test configuration to exclude slow tests from the standard spec_decode suite while adding a separate optional job for comprehensive large model testing.

**Technical impact**  
The test pipeline now separates standard spec_decode tests from large model acceptance tests, preventing resource-intensive tests from running on every PR. The test configuration system has been enhanced with model-specific tolerance settings and pytest marks, allowing more flexible test management. The Qwen3-30B-MOE-VL model test uses a higher tolerance (15%) for per-position acceptance length validation due to small absolute values.

**Potential risks**  
The higher tolerance (15% vs default) for the Qwen3 model's per-position acceptance length could mask subtle regressions. The optional test job might not run consistently since it's marked as optional in CI, potentially delaying detection of issues with large models. There's a risk that developers might inadvertently add slow tests to the standard suite, impacting CI performance.

**Key insights**  
The separation of large model tests into optional CI jobs is a good practice for balancing test coverage with CI efficiency. The enhanced model configuration system with custom tolerances and marks provides better test customization. Developers should ensure new large model tests use the `slow_test` mark and appropriate tolerance settings, and monitor that optional tests run regularly to maintain coverage.

---

## 37. [[PluggableLayer][2/N] Apply PluggableLayer to linear layers](https://github.com/vllm-project/vllm/pull/33152)


### Base Information

- **PR Number:** #33152
- **Author:** [whx-sjtu](https://github.com/whx-sjtu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 00:53:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33152/files) (1):**
  - `vllm/model_executor/layers/linear.py`

### Summary

**What changed and why**  
The PR changes the base class for linear layers from `CustomOp` to `PluggableLayer` and updates all corresponding decorator registrations. This is part of a broader effort to refactor the codebase to use a pluggable layer architecture, as outlined in the linked issue.

**Technical impact**  
This change centralizes linear layer registration under the `PluggableLayer` abstraction, improving modularity and consistency across the model executor. It ensures that linear layers (replicated, column-parallel, and row-parallel) are now managed through a unified pluggable interface, which may facilitate easier extensions and custom implementations in the future.

**Potential risks**  
If `PluggableLayer` does not fully implement all methods or behaviors previously provided by `CustomOp`, there could be runtime errors or regressions in layer functionality. Additionally, any downstream code that directly references `CustomOp` for linear layers may break unless it also adapts to the new base class.

**Key insights**  
The change is a straightforward substitution that aligns with architectural goals, but thorough testing is essential to verify that all linear layer behaviors remain intact. Developers should ensure that `PluggableLayer` is a proper superset or equivalent of `CustomOp` and update any related documentation or examples accordingly.

---

## 38. [support returning tokenids in responses api](https://github.com/vllm-project/vllm/pull/33212)


### Base Information

- **PR Number:** #33212
- **Author:** [cmunley1](https://github.com/cmunley1)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 00:52:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33212/files) (3):**
  - `tests/entrypoints/openai/test_return_tokens_as_ids.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`

### Summary

**What changed and why**  
This PR adds support for returning token IDs in the OpenAI-compatible Responses API when `return_tokens_as_token_ids` is enabled. The changes modify the token decoding logic to use a shared helper method that conditionally returns token IDs as strings (prefixed with "token_id:") instead of decoded text.

**Technical impact**  
The implementation centralizes token representation logic by reusing the existing `_get_decoded_token` method from the base OpenAI serving class. This ensures consistency between the standard OpenAI API and the newer Responses API. The tokenizer decode call is also fixed to accept a list argument.

**Potential risks**  
If `skip_tokenizer_init=True` is set, token decoding may fail when token IDs are requested, as the tokenizer won't be available. The test mock assumes a simple tokenizer decode, which might not reflect real-world tokenizer behavior (e.g., handling special tokens).

**Key insights**  
Developers should verify that the `return_tokens_as_token_ids` flag is properly propagated in all relevant API endpoints. The change promotes code reuse but requires careful testing to ensure the token ID formatting is consistent across different API versions and edge cases.

---

## 39. [[BugFix] Fix EPLB fail for MoeFP4 model with Marlin backend](https://github.com/vllm-project/vllm/pull/33262)


### Base Information

- **PR Number:** #33262
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 00:52:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33262/files) (1):**
  - `vllm/model_executor/utils.py`

### Summary

**What changed and why**  
The PR modifies the `replace_parameter` utility function to explicitly handle `None` values for the `new_data` argument. Previously, passing `None` would create an empty CPU parameter, which caused an EPLB (Expert Parallel Load Balancing) error when using the Marlin MOE FP4 backend without activation scales. The fix ensures that when `new_data` is `None`, the parameter is set to `None` instead of creating a parameter tensor.

**Technical impact**  
This change allows the Marlin MOE FP4 backend to work correctly with EPLB enabled when activation scales are not used. It prevents the runtime error `"No backend type associated with device type cpu"` that occurred during expert weight rearrangement in distributed all-gather operations. The fix is minimal and only affects the specific case where `new_data` is `None`.

**Potential risks**  
If other parts of the codebase rely on `replace_parameter` always returning a `torch.nn.Parameter` object (even if empty), setting the attribute to `None` could lead to `AttributeError` or `TypeError` when accessing the parameter. Additionally, there may be downstream logic that assumes parameters are always tensors, which could break if `None` is introduced unexpectedly.

**Key insights**  
The fix correctly addresses the root cause by avoiding the creation of an invalid CPU tensor. Developers should verify that any code calling `replace_parameter` can handle `None` parameters. Consider adding a comment or type hint update in the function signature to make this behavior explicit. Also, ensure that the Marlin backend consistently expects `None` for unused activation scales to prevent similar issues in other contexts.

---

## 40. [[Doc]: fixing multiple typos in diverse files](https://github.com/vllm-project/vllm/pull/33256)


### Base Information

- **PR Number:** #33256
- **Author:** [didier-durand](https://github.com/didier-durand)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 00:52:03
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33256/files) (14):**
  - `.buildkite/scripts/generate-nightly-index.py`
  - `csrc/cpu/cpu_wna16.cpp`
  - `csrc/cpu/torch_bindings.cpp`
  - `docs/design/custom_op.md`
  - `docs/design/debug_vllm_compile.md`
  - `docs/design/plugin_system.md`
  - `docs/models/pooling_models.md`
  - `docs/serving/context_parallel_deployment.md`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/model_executor/layers/fused_moe/shared_fused_moe.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py`
  - `vllm/utils/system_utils.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
This PR fixes multiple typos across 14 different files in the codebase. The changes are purely cosmetic, correcting spelling errors and grammatical issues in documentation, comments, and code strings. The purpose is to improve the overall quality and professionalism of the repository's written content.

**Technical impact**  
These changes have no technical impact on system behavior, performance, or functionality. They only affect human-readable text in documentation, comments, and log/error messages. All existing tests continue to pass as expected.

**Potential risks**  
The risk is minimal since these are text-only corrections. However, one change in `csrc/cpu/torch_bindings.cpp` fixes a parameter name from `sheduler_metadata` to `scheduler_metadata`, which could potentially affect external integrations if they rely on the exact string literal. This should be verified for backward compatibility.

**Key insights**  
This is a straightforward maintenance PR that improves code quality. Developers should note the parameter name correction in the CPU attention kernel binding, as it's the only change that could have any external implications. All other changes are safe cosmetic improvements that enhance readability without affecting functionality.

---

## 41. [[Bugfix] Fix Qwen3-VL-Reranker load.](https://github.com/vllm-project/vllm/pull/33298)


### Base Information

- **PR Number:** #33298
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-29 00:42:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33298/files) (6):**
  - `examples/pooling/score/vision_rerank_api_online.py`
  - `examples/pooling/score/vision_score_api_online.py`
  - `tests/entrypoints/pooling/classify/test_online_vision.py`
  - `tests/entrypoints/pooling/score/test_online_score_vision.py`
  - `tests/entrypoints/test_utils.py`
  - `vllm/model_executor/models/adapters.py`

### Summary

**What changed and why**  
This PR fixes the Qwen3-VL-Reranker model loading by updating adapter weight-loading logic to correctly handle vision-language models with a `score` head. It also refactors example scripts and adds comprehensive tests for the vision reranker/scorer API endpoints. The changes ensure proper detection of VLM models that have a scoring head and adjust weight assignment accordingly.

**Technical impact**  
The adapter loading functions (`load_weights_using_from_2_way_softmax` and `load_weights_no_post_processing`) now conditionally check for the presence of a `score` attribute on the language model, preventing incorrect weight assignment. Example scripts are simplified by using shared utility functions (`encode_image_url`, `fetch_image`) and expanded to test multiple input formats (text, image URL, base64). A new test suite validates the scoring API with various document types.

**Potential risks**  
If other VLM models lack the `score` attribute but are incorrectly flagged as having one, weight loading could fail. The refactored examples assume the utility functions are always available and may break if those dependencies change. The test additions increase coverage but also add execution time and dependency on external image URLs.

**Key insights**  
Developers should verify that any new VLM reranker models follow the same pattern (i.e., have a `score` attribute on the language model). The centralized image encoding utilities improve maintainability and should be used for all similar multimodal examples. The expanded test coverage is critical for ensuring API compatibility across different input formats.

---

## 42. [[CI/Build][BugFix] fix cuda/compat loading order issue in docker build](https://github.com/vllm-project/vllm/pull/33116)


### Base Information

- **PR Number:** #33116
- **Author:** [wpc](https://github.com/wpc)
- **Merged By:** [houseroad](https://github.com/houseroad)
- **Merged time:** 2026-01-29 00:19:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33116/files) (1):**
  - `docker/Dockerfile`

### Summary

**What changed and why**  
The PR changes the CUDA compatibility library configuration filename from `00-cuda-compat.conf` to `cuda-compat.conf` in the Dockerfile. This removes the `00-` prefix that was causing the compatibility library path to be loaded with higher priority than standard CUDA libraries, which led to CUDA initialization errors on systems with newer drivers that don't require compatibility libraries.

**Technical impact**  
By renaming the file, the CUDA compatibility library path is now loaded in alphabetical order after other CUDA library configurations (e.g., `000_cuda.conf`). This ensures that when a system has a driver version that supports the installed CUDA version natively, the standard CUDA libraries are used instead of the compatibility libraries, preventing erroneous CUDA initialization failures.

**Potential risks**  
If the compatibility library is genuinely required (e.g., on older drivers), the renamed file may still load too late if other configuration files override its path, though testing shows it works. There is also a risk that future additions of configuration files with names like `aaa-cuda.conf` could again affect loading order, but this is unlikely.

**Key insights**  
Library loading order in Linux (via `ldconfig`) is sensitive to filename alphabetical sorting. Prefixes like `00-` can unintentionally force priority. Developers should be cautious when naming configuration files that affect library paths and consider the impact of lexical ordering on system behavior. The fix is minimal and low-risk, as confirmed by tests on both newer (GB200) and older (H100) driver setups.

---

