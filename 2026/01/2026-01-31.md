# vLLM Merged PR Report

**Report Date:** 2026-01-31 PST

**Total Merged PRs:** 29

---

## 1. [Change defaults for vllm bench startup](https://github.com/vllm-project/vllm/pull/33489)


### Base Information

- **PR Number:** #33489
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 23:46:01
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33489/files) (1):**
  - `vllm/benchmarks/startup.py`

### Summary

**What changed and why**  
The PR reduces the default iteration counts for the `vllm bench startup` command's cold, warmup, and warm phases from (5, 3, 5) to (3, 1, 3). This change addresses the issue that the original defaults were too large, causing the benchmark to take an impractical amount of time to complete.

**Technical impact**  
This change makes the startup benchmark more practical and user-friendly by default, significantly reducing its runtime. It does not alter the benchmark's core logic or measurement methodology, only the number of samples collected for each phase, which may slightly affect the statistical confidence of the results.

**Potential risks**  
The primary risk is increased variance in benchmark results due to fewer samples, potentially making performance regressions or improvements harder to detect. Users relying on the default values for precise, production-level benchmarking may now need to explicitly set higher iteration counts.

**Key insights**  
This is a sensible usability improvement. Developers should be aware that for rigorous performance testing, they should override these new defaults with higher values appropriate for their required confidence level. The change highlights the importance of balancing benchmark accuracy with practical runtime.

---

## 2. [fix: only include Authorization header when OPENAI_API_KEY is set](https://github.com/vllm-project/vllm/pull/33488)


### Base Information

- **PR Number:** #33488
- **Author:** [zack041](https://github.com/zack041)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 23:35:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33488/files) (1):**
  - `vllm/benchmarks/lib/endpoint_request_func.py`

### Summary

**What changed and why**  
The fix addresses a bug where benchmark requests would fail with 401 Unauthorized when `OPENAI_API_KEY` is not set. Previously, the code always included an `Authorization: Bearer None` header, even when no API key was configured. Now, a new helper function `_get_headers()` conditionally adds the Authorization header only if `OPENAI_API_KEY` is present in the environment.

**Technical impact**  
This change centralizes header construction and ensures that the Authorization header is omitted when no API key is set, allowing requests to succeed against servers that don't require authentication. It also standardizes Content-Type header handling across multiple request functions, reducing code duplication.

**Potential risks**  
If downstream servers rely on the presence of any Authorization header (even with "Bearer None"), they may now reject requests. Additionally, the change assumes that an empty string API key should also skip the header, which is correct but should be validated against server expectations.

**Key insights**  
The refactor improves maintainability by consolidating header logic. Developers should ensure that any new request functions use `_get_headers()` and verify that their deployment environments correctly handle optional authentication. Consider adding a configuration flag if explicit control over header inclusion is needed in the future.

---

## 3. [[Models]: lfm2_siglip2 return intermediate encoder layers](https://github.com/vllm-project/vllm/pull/33370)


### Base Information

- **PR Number:** #33370
- **Author:** [lalo](https://github.com/lalo)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-31 22:17:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33370/files) (1):**
  - `vllm/model_executor/models/lfm2_siglip2.py`

### Summary

**What changed and why**  
This PR adds support for returning intermediate encoder layer outputs from Siglip2 vision models. The changes introduce a `select_layers` parameter to `Siglip2Model` and `Siglip2VisionTransformer` that allows VLM architectures to access hidden states from specific encoder layers instead of just the final output. This is particularly useful for vision-language models that benefit from multi-scale feature representations.

**Technical impact**  
The implementation modifies the encoder to optionally collect all hidden states via `return_all_hidden_states`, then uses a shared utility function `resolve_visual_encoder_outputs` to select and potentially concatenate requested layers. The architecture now supports partial encoder instantiation through `num_hidden_layers_override` and conditional post-layer normalization based on whether the full encoder depth is used. The weight loading logic has been updated to handle these optional components.

**Potential risks**  
The weight loading logic now skips layers when `num_hidden_layers_override` is set, which could cause silent failures if the checkpoint doesn't match the expected architecture. The conditional `post_layernorm` (which may be `None`) requires careful handling in downstream code. Negative indexing in `select_layers` could cause confusion if users aren't aware of the `max_possible_layers` context.

**Key insights**  
The implementation wisely reuses the existing `resolve_visual_encoder_outputs` utility, promoting consistency across vision models. Developers should ensure that when using `num_hidden_layers_override`, the corresponding checkpoint contains weights for the requested layers. The concatenation of multiple selected layers creates a new dimension that downstream components must handle appropriately.

---

## 4. [[Critical] Revert #33110](https://github.com/vllm-project/vllm/pull/33500)


### Base Information

- **PR Number:** #33500
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 21:06:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33500/files) (1):**
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This PR reverts commit 27cb2f678f7567b14a13a2f7e085137dca1a4129, which introduced a call to `get_max_tokens_per_item_by_modality` during the initialization of `InputProcessor`. The change caused startup hangs for multimodal models, and while lazy initialization could resolve the hang, it would negatively impact the latency of the first request. The revert removes the problematic initialization and associated encoder cache validation to restore stable startup behavior.

**Technical impact**  
The removal eliminates the pre-allocation and validation of multimodal encoder cache sizes during `InputProcessor` initialization. This reverts the system to a state where encoder cache limits are not enforced at startup or during prompt validation, potentially avoiding the hang but also removing safeguards against oversized multimodal inputs relative to cache capacity.

**Potential risks**  
Without the encoder cache size validation, users may encounter runtime errors or performance degradation if multimodal inputs exceed the available cache, as the system will no longer provide early error messages. Additionally, the underlying issue causing the hang during initialization remains unaddressed, which could affect future attempts to implement similar functionality.

**Key insights**  
Developers should treat this revert as a temporary measure and prioritize finding a robust solution for initializing multimodal encoder caches without blocking startup. Consider asynchronous or background initialization strategies that do not impact first-request latency. Ensure any future implementation includes proper error handling and validation to maintain system reliability.

---

## 5. [[Bugfix] Fix inconsistent handling of cache reset](https://github.com/vllm-project/vllm/pull/33481)


### Base Information

- **PR Number:** #33481
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 20:23:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33481/files) (7):**
  - `docs/benchmarking/sweeps.md`
  - `vllm/benchmarks/sweep/server.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/worker/gpu/mm/encoder_runner.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR fixes inconsistent cache reset handling across the codebase. It ensures that `vllm bench sweep` and `pause_generation` properly call all necessary cache reset endpoints, removes a duplicate `reset_mm_cache` method in the OpenAI serving layer, and forwards cache reset calls to the encoder model runner in Model Runner V2. Documentation is updated to reflect the comprehensive cache reset approach.

**Technical impact**  
The changes standardize cache reset behavior by ensuring all relevant components (`/reset_prefix_cache`, `/reset_mm_cache`, `/reset_encoder_cache`) are called during benchmark sweeps and pause operations. The architecture now properly delegates cache reset responsibilities: the OpenAI API layer uses `EngineClient.reset_mm_cache`, while Model Runner V2 correctly propagates reset calls to the encoder runner. This prevents stale embeddings from being reused after model updates.

**Potential risks**  
The `reset_mm_cache` implementation in `encoder_runner.py` contains a TODO comment and currently only passes, which may indicate incomplete functionality for multi-modal budget management. There's a risk that the benchmark sweep's blanket approach of calling all `/reset_*_cache` endpoints could become problematic if future cache types require different reset conditions or sequencing.

**Key insights**  
Developers should note that cache reset behavior is now more consistent but requires monitoring the TODO in encoder runner. The documentation update correctly reflects that all cache reset endpoints are called during benchmarks. When adding new cache types, ensure they follow the established pattern and are included in the `VLLM_RESET_CACHE_ENDPOINTS` list if appropriate for benchmark sweeps.

---

## 6. [pin LMCache to v0.3.9 or greater with vLLM v0.15.0](https://github.com/vllm-project/vllm/pull/33440)


### Base Information

- **PR Number:** #33440
- **Author:** [Gregory-Pereira](https://github.com/Gregory-Pereira)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-31 19:50:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33440/files) (1):**
  - `requirements/kv_connectors.txt`

### Summary

**What changed and why**  
The PR pins LMCache to version 0.3.9 or greater in the `requirements/kv_connectors.txt` file. This change ensures compatibility with vLLM v0.15.0, specifically addressing runtime issues in the KV transfer path due to API mismatches in older LMCache versions (e.g., 0.3.8). The update prevents users from inadvertently installing an incompatible version that imports but fails at runtime.

**Technical impact**  
This version constraint enforces a minimum compatible LMCache version, ensuring the v1 LMCache KV connector functions correctly with vLLM v0.15.0. It eliminates silent failures by preventing installations of LMCache 0.3.8, which lacks necessary adapter changes (e.g., missing `cdiv` import from `vllm.utils`). The change maintains backward compatibility with newer LMCache releases (≥0.3.9).

**Potential risks**  
If future LMCache releases introduce breaking changes, the loose constraint (`>= 0.3.9`) could allow incompatible versions to be installed. Additionally, users with existing environments may need to manually upgrade LMCache if pinned to 0.3.8, potentially disrupting workflows. The fix does not address broader version compatibility documentation gaps between LMCache and vLLM.

**Key insights**  
Always pin minimum versions for dependencies with known API incompatibilities to prevent runtime errors. Consider adding a version compatibility matrix in documentation to clarify supported pairings. For future maintenance, monitor LMCache releases and test against vLLM to proactively adjust constraints if needed.

---

## 7. [[ROCm][CI] Update huggingface-hub pin](https://github.com/vllm-project/vllm/pull/33492)


### Base Information

- **PR Number:** #33492
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-31 18:51:55
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33492/files) (1):**
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
Added a version pin for `huggingface-hub==0.36.0` to the ROCm CI test requirements file. This is a follow-up action to ensure consistency with a previous change made for another CI environment, likely to prevent unexpected test failures due to incompatible library updates.

**Technical impact**  
This change locks the Hugging Face Hub library to a specific version (0.36.0) within the ROCm testing pipeline. It ensures deterministic builds and test results by eliminating variability introduced by automatic updates to newer, potentially breaking versions of the library.

**Potential risks**  
The primary risk is version drift and dependency staleness. If this pin is not updated periodically, the CI environment may fall behind security patches, bug fixes, or new features required by other dependencies. There is also a risk of creating inconsistency if other requirement files (e.g., for CUDA) are pinned to a different version.

**Key insights**  
This is a standard and necessary practice for CI stability. The team should establish a documented process for periodically reviewing and updating these version pins. Consider using a single, centralized version constraint file for shared dependencies like `huggingface-hub` to maintain consistency across all CI configurations (ROCm, CUDA, etc.).

---

## 8. [[Refactor] Make Renderer an abstract class](https://github.com/vllm-project/vllm/pull/33479)


### Base Information

- **PR Number:** #33479
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-31 18:36:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33479/files) (12):**
  - `vllm/engine/protocol.py`
  - `vllm/renderers/__init__.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/mistral.py`
  - `vllm/renderers/protocol.py`
  - `vllm/renderers/registry.py`
  - `vllm/renderers/terratorch.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
The PR refactors the renderer interface from a protocol (`RendererLike`) to an abstract base class (`BaseRenderer`). This change consolidates common functionality like `config` storage and `_async_tokenizer` lazy initialization into the base class, eliminating repetitive code in concrete renderer implementations.

**Technical impact**  
Concrete renderer classes now inherit from `BaseRenderer` instead of implementing a protocol. They no longer need to explicitly store `self.config` or call `super().__init__()` without arguments. The abstract class provides default implementations for common methods while marking `from_config`, `tokenizer`, and `render_messages` as abstract, ensuring required implementations.

**Potential risks**  
The lazy initialization of `_async_tokenizer` now uses `None` check instead of `hasattr`, which could cause issues if subclasses override `__init__` without calling `super().__init__(config)`. Type hints have changed from `RendererLike` to `BaseRenderer` throughout the codebase, which could affect external integrations that depend on the protocol type.

**Key insights**  
This refactor improves code maintainability by reducing duplication across renderer implementations. Developers should ensure all concrete renderers properly call `super().__init__(config)` and implement the three abstract methods. The change is mostly structural with minimal functional impact, but thorough testing of all renderer types is recommended.

---

## 9. [fix: Add SM120 (RTX Blackwell) support for FlashInfer CUTLASS NVFP4 MoE kernels](https://github.com/vllm-project/vllm/pull/33417)


### Base Information

- **PR Number:** #33417
- **Author:** [renehonig](https://github.com/renehonig)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 14:06:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33417/files) (6):**
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py`

### Summary

**What changed and why**  
This PR adds SM120 (RTX Blackwell workstation GPUs) support to NVFP4 MoE kernel selection logic. The issue was that SM12.0 devices were incorrectly identified as unsupported because the device capability family check for SM100 (family 10) didn't match SM120 (family 12). The fix extends the checks to include families 110 and 120 alongside existing 100-family support.

**Technical impact**  
The changes enable NVFP4-quantized MoE models to run on RTX Blackwell GPUs (compute capability 12.0) by updating kernel availability checks across four backend files. Additionally, redundant helper functions and a support detection module were removed, simplifying the codebase and consolidating device capability logic.

**Potential risks**  
While the fix addresses the immediate regression, there's a risk that future GPU architectures (e.g., SM130) may require similar updates. The removal of `nvfp4_moe_support.py` could affect any external code that depended on its public API, though this appears internal. Also, the `flashinfer_trtllm_moe.py` change only removes a comment, which is safe but should be verified for any hidden implications.

**Key insights**  
Developers should note that device capability families are now explicitly checked for 100, 110, and 120 across NVFP4 MoE backends. The cleanup of unused code reduces maintenance overhead, but ensure no downstream dependencies exist on the removed modules. Consider adding a more future-proof architecture check (e.g., >=SM100) if the kernel support is consistent across future Blackwell families.

---

## 10. [[Misc] Fix flashinfer related tests](https://github.com/vllm-project/vllm/pull/33462)


### Base Information

- **PR Number:** #33462
- **Author:** [esmeetu](https://github.com/esmeetu)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-31 13:10:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33462/files) (5):**
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py`
  - `tests/kernels/quantization/test_fp8_quant.py`
  - `vllm/model_executor/layers/quantization/utils/nvfp4_utils.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR fixes CUDA errors from FlashInfer TRTLLM kernels on Blackwell GPUs by adding support for the CUDNN backend in NVFP4 quantization tests and utilities. The changes update test configurations, seed initialization methods, and backend handling logic to include CUDNN alongside existing CUTLASS and TRTLLM backends.

**Technical impact**  
The modifications enable proper testing and execution of NVFP4 quantization with the CUDNN backend, expanding backend compatibility. The seed initialization changes standardize random number generation across tests by replacing `current_platform.seed_everything()` with `set_random_seed()`, improving test consistency.

**Potential risks**  
The addition of CUDNN backend support may introduce new dependencies or compatibility issues with different CUDA/cuDNN versions. There's a risk that the TRTLLM kernel errors on Blackwell aren't fully resolved if the underlying FlashInfer issue persists, though this PR mitigates by providing alternative backend options.

**Key insights**  
Developers should verify that the CUDNN backend is properly configured in their environment before using NVFP4 quantization. The standardized seed initialization improves test reproducibility. Consider monitoring FlashInfer updates for permanent fixes to the TRTLLM kernel issues on Blackwell architecture.

---

## 11. [Fix grammar](https://github.com/vllm-project/vllm/pull/33121)


### Base Information

- **PR Number:** #33121
- **Author:** [smashyalts](https://github.com/smashyalts)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 09:59:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33121/files) (1):**
  - `vllm/v1/worker/cpu_worker.py`

### Summary

**What changed and why**  
The change fixes a grammatical error in an assertion error message, replacing "No enough" with "Not enough." This correction improves clarity and professionalism in user-facing error messages.

**Technical impact**  
This is a minor, non-functional change that only affects the error message text. It does not impact system behavior, logic, or performance, as it is purely a string correction within a runtime assertion.

**Potential risks**  
There is minimal risk since this is a cosmetic change. However, any downstream scripts or tests that parse or match the exact error message string may break if they rely on the previous phrasing.

**Key insights**  
Always ensure error messages are grammatically correct and clear, as they directly impact user and developer experience. For future changes, consider updating any related documentation or tests that might depend on specific error message content.

---

## 12. [[Bugfix]: Fix display errors in TORCH_CHECK messages](https://github.com/vllm-project/vllm/pull/32942)


### Base Information

- **PR Number:** #32942
- **Author:** [lingebeng](https://github.com/lingebeng)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 09:48:49
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32942/files) (6):**
  - `csrc/cpu/sgl-kernels/gemm.cpp`
  - `csrc/cpu/sgl-kernels/gemm_fp8.cpp`
  - `csrc/cpu/sgl-kernels/gemm_int8.cpp`
  - `csrc/cpu/sgl-kernels/moe.cpp`
  - `csrc/cpu/sgl-kernels/moe_int8.cpp`
  - `csrc/moe/marlin_moe_wna16/ops.cu`

### Summary

**What changed and why**  
Fixed incorrect string literals in TORCH_CHECK error messages across six files. The changes replace hardcoded `"nb_size"` strings with the actual `nb_size` variable in GEMM kernel error messages, and correct a tensor dimension check from `size(0)` to `size(1)` in a CUDA MOE operation.

**Technical impact**  
These changes ensure error messages display the actual runtime values of `nb_size` when unexpected block sizes are encountered, improving debugging clarity. The tensor dimension fix aligns the check with the intended dimension (column size) for bias tensors, preventing incorrect validation logic.

**Potential risks**  
Low risk, as the changes only affect error reporting and validation logic. However, the tensor dimension fix could expose previously hidden bugs if `b_bias` tensors had incorrect shapes that were masked by the wrong index check.

**Key insights**  
Always validate error messages to ensure they reference actual runtime variables rather than hardcoded strings. For tensor dimension checks, verify indices match the intended semantic dimension (e.g., `size(1)` for columns). Consider adding static assertions or unit tests for tensor shape expectations.

---

## 13. [[Misc] support collect_env for endpoint /server_info](https://github.com/vllm-project/vllm/pull/33246)


### Base Information

- **PR Number:** #33246
- **Author:** [muma378](https://github.com/muma378)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-31 09:43:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33246/files) (1):**
  - `vllm/entrypoints/serve/instrumentator/server_info.py`

### Summary

**What changed and why**  
The PR adds system environment collection to the `/server_info` endpoint by importing `get_env_info()` from `vllm.collect_env`. This change addresses GitHub issue #32402, which requested enhanced server information for debugging and monitoring purposes.

**Technical impact**  
The endpoint now returns a `system_env` field containing cached system environment details (e.g., hardware, software versions). The use of `functools.lru_cache` and `asyncio.to_thread` ensures efficient, non‑blocking retrieval of environment data, maintaining server responsiveness.

**Potential risks**  
Caching the environment info with `maxsize=1` means changes to the system environment during runtime won't be reflected until the server restarts. Additionally, `get_env_info()` might expose sensitive system details; ensure this is acceptable for the endpoint's intended use (development mode only).

**Key insights**  
This is a well‑implemented feature that leverages existing utilities (`collect_env`) and async patterns. Developers should verify that the endpoint remains restricted to development mode (`VLLM_SERVER_DEV_MODE=1`) to avoid unintentional information disclosure in production.

---

## 14. [Update `huggingface-hub` pin for the last time before Transformers v5](https://github.com/vllm-project/vllm/pull/33473)


### Base Information

- **PR Number:** #33473
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 09:14:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33473/files) (1):**
  - `requirements/test.txt`

### Summary

**What changed and why**  
The change updates the pinned version of `huggingface-hub` from 0.34.3 to 0.36.0 in the test requirements file. This is intended to incorporate bug fixes to improve CI reliability ahead of a planned migration to Transformers v5, which will require `huggingface-hub>=1`.

**Technical impact**  
This update introduces minor version changes that likely include bug fixes and possibly minor feature additions. It ensures the test environment aligns more closely with the upcoming major dependency shift, reducing potential integration issues when transitioning to Transformers v5.

**Potential risks**  
While minor version updates are generally safe, there is a risk of introducing subtle behavioral changes or new bugs that could affect CI stability. The update could also inadvertently affect downstream dependencies like `accelerate` or `datasets` if they have specific compatibility constraints with the new version.

**Key insights**  
Developers should verify that the new version does not break existing tests or integrations. This change is a proactive step to smooth the transition to Transformers v5, but thorough testing is recommended to confirm CI reliability improvements. Consider documenting any observed changes in CI behavior post-update.

---

## 15. [[Refactor] Move MM data parsing outside processor](https://github.com/vllm-project/vllm/pull/33408)


### Base Information

- **PR Number:** #33408
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-31 08:46:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33408/files) (43):**
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/multimodal/processing/test_gemma3.py`
  - `tests/models/multimodal/processing/test_glm4_1v.py`
  - `tests/models/multimodal/processing/test_h2ovl.py`
  - `tests/models/multimodal/processing/test_idefics3.py`
  - `tests/models/multimodal/processing/test_internvl.py`
  - `tests/models/multimodal/processing/test_llama4.py`
  - `tests/models/multimodal/processing/test_llava_next.py`
  - `tests/models/multimodal/processing/test_llava_onevision.py`
  - `tests/models/multimodal/processing/test_minimax_vl_01.py`
  - `tests/models/multimodal/processing/test_nemotron_vl.py`
  - `tests/models/multimodal/processing/test_phi3v.py`
  - `tests/models/multimodal/processing/test_phi4mm.py`
  - `tests/models/multimodal/processing/test_qwen2_vl.py`
  - `tests/models/multimodal/processing/test_qwen3_omni.py`
  - `tests/models/multimodal/processing/test_smolvlm.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/multimodal/processing/test_transformers.py`
  - `tests/multimodal/test_processing.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/minicpmo.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/multimodal.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/dummy_inputs.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The PR moves multimodal data parsing from `BaseMultiModalProcessor._to_mm_items` to `BaseProcessingInfo.parse_mm_items`. This refactor decouples data parsing from the processor, enabling parsing to occur outside the processor (e.g., in input preprocessing or validation stages). The change updates all call sites to use the new method and passes parsed `mm_items` instead of raw `mm_data` to the processor's `apply` method.

**Technical impact**  
This change improves separation of concerns by making data parsing a responsibility of `BaseProcessingInfo` rather than the processor. It affects the processor's API—`apply` now expects `mm_items` instead of `mm_data`—requiring updates across all processor implementations, test files, and integration points. The architecture now supports pre-parsed multimodal data, which can facilitate caching, validation, and reuse of parsed items.

**Potential risks**  
- Inconsistent validation could occur if `parse_mm_data` is called with `validate=False` in some places but not others.  
- The refactor touches many files (43 modified), increasing the risk of missing an update or introducing subtle bugs in edge cases.  
- Changes to processor subclasses (e.g., `CLIP`, `SigLIP`, `Terratorch`) must ensure they handle `mm_items` correctly, especially where custom logic existed for `mm_data`.

**Key insights**  
- Developers must now call `processor.info.parse_mm_data(mm_data)` before passing data to `processor.apply`.  
- The `validate` parameter in `parse_mm_data` allows skipping validation when needed (e.g., in dummy input generation).  
- Ensure all new processor implementations adopt the `mm_items` parameter and avoid relying on the old `_to_mm_items` method.

---

## 16. [[Deprecation] Remove deprecated items related to pooling](https://github.com/vllm-project/vllm/pull/33477)


### Base Information

- **PR Number:** #33477
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 08:44:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33477/files) (8):**
  - `docs/models/pooling_models.md`
  - `vllm/config/model.py`
  - `vllm/config/pooler.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/pooling_params.py`

### Summary

**What changed and why**  
This PR removes deprecated pooling-related items scheduled for v0.15, including the `softmax`, `activation`, and `normalize` parameters, consolidating them under `use_activation`. It also deprecates the `normalize` parameter in the online Embedding API (to be removed in v0.17) and removes the `reward` and `mm_encoder_only` conversion types.

**Technical impact**  
The changes simplify the pooling configuration by unifying activation control under `use_activation`, reducing code complexity. The removal of deprecated conversion types (`reward`, `mm_encoder_only`) and pooling parameters streamlines the codebase, aligning with the planned v0.15 cleanup. Backward compatibility is maintained for `normalize` in the Embedding API via warnings and automatic mapping to `use_activation`.

**Potential risks**  
Users still relying on the deprecated `normalize` parameter in the Embedding API will see warnings and need to migrate to `use_activation` before v0.17. The removal of `reward` and `mm_encoder_only` conversion types may break existing workflows if not updated to `embed` or `--mm-encoder-only` flag usage. Edge cases where `softmax`/`activation` were explicitly set to `False` need verification that `use_activation` defaults (`True` in most cases) don’t alter behavior.

**Key insights**  
Developers should update any code using `normalize`, `softmax`, or `activation` to `use_activation` immediately. For Embedding API calls, replace `normalize` with `use_activation` to avoid future breakage. Ensure conversion types are updated: use `--convert embed` instead of `reward` and `--mm-encoder-only` instead of `mm_encoder_only`. Review pooling model configurations to confirm activation behavior remains consistent post-migration.

---

## 17. [[Bugfix] Early-reject requests with MM data longer than encode cache capacity](https://github.com/vllm-project/vllm/pull/33110)


### Base Information

- **PR Number:** #33110
- **Author:** [YunzhuLu](https://github.com/YunzhuLu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 08:41:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33110/files) (1):**
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
Added validation in `InputProcessor._validate_model_input()` to check if any multimodal item's embedding length exceeds the pre-allocated encoder cache capacity. This prevents an infinite retry loop in the V1 engine scheduler when processing oversized multimodal inputs, replacing a hang with an immediate 400 error.

**Technical impact**  
The change introduces early rejection of invalid requests at the input validation stage, preventing wasted CPU cycles and GPU idle time. It calculates the encoder cache size during initialization for multimodal-capable models and validates each multimodal item against this budget before scheduling attempts begin.

**Potential risks**  
The validation only triggers for decoder prompts with multimodal inputs, potentially missing similar issues in other prompt types. The error message references `--limit-mm-per-prompt` flag, but the PR description mentions `--max-num-batched-tokens` - this inconsistency could confuse users. There's also a risk of false positives if cache size calculations change dynamically.

**Key insights**  
This is a critical fix that transforms a silent hang into a clear error response, greatly improving debuggability. Developers should ensure consistent error messaging across all documentation. Consider extending similar validation to other prompt types if multimodal inputs can appear elsewhere. The initialization logic correctly skips when `skip_tokenizer_init=True`, maintaining compatibility with existing configurations.

---

## 18. [Support clear mm and encoder cache](https://github.com/vllm-project/vllm/pull/33452)


### Base Information

- **PR Number:** #33452
- **Author:** [jma99fb](https://github.com/jma99fb)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-31 07:22:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33452/files) (15):**
  - `docs/usage/security.md`
  - `tests/v1/core/test_encoder_cache_manager.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/serve/cache/api_router.py`
  - `vllm/v1/core/encoder_cache_manager.py`
  - `vllm/v1/core/sched/interface.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/executor/abstract.py`
  - `vllm/v1/metrics/stats.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
Added support for resetting the encoder cache via a new `/reset_encoder_cache` API endpoint. This allows clearing cached vision embeddings when model weights are updated, preventing stale embeddings from being reused. The change includes new `reset()` methods in cache managers, updates across the engine stack, and comprehensive unit tests.

**Technical impact**  
The encoder cache can now be programmatically invalidated, ensuring consistency when model weights change. The reset operation clears both logical state (scheduler’s cache manager) and physical storage (GPU model runner’s cache). Encoder cache usage is also exposed in scheduler stats for monitoring.

**Potential risks**  
Resetting the encoder cache while requests are in progress may lead to desynchronized internal caches, as noted in the warning. The API currently does not verify successful reset completion, which could mask failures. Edge cases with concurrent resets and allocations are not explicitly handled.

**Key insights**  
This feature is critical for maintaining correctness in multimodal workflows with dynamic model updates. Developers should call reset only when no requests are pending to avoid cache inconsistencies. Consider adding validation to confirm cache clearance and implementing safeguards against concurrent modifications.

---

## 19. [[BugFix][Router Replay] Capture Logical Experts with EPLB](https://github.com/vllm-project/vllm/pull/33013)


### Base Information

- **PR Number:** #33013
- **Author:** [HollowMan6](https://github.com/HollowMan6)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-31 07:12:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33013/files) (4):**
  - `tests/model_executor/test_routed_experts_capture.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/router/base_router.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The fix addresses two issues: 1) routed experts capture was broken because `RoutedExpertsCapturer.create()` ran after model construction, missing the binding in `FusedMoE.__init__`; 2) when EPLB is enabled, vLLM maps logical expert IDs to physical IDs, but Megatron replay requires logical IDs. The solution moves capture binding to `GPUModelRunner` and captures logical IDs in `BaseRouter` before EPLB mapping.

**Technical impact**  
Capture logic is now lazily bound via `GPUModelRunner._bind_routed_experts_capturer()`, ensuring it runs after model initialization. The `BaseRouter` captures logical IDs pre-EPLB mapping, making replay compatible with Megatron. Both chunked and non-chunked forward paths use the same capture helper, ensuring consistency.

**Potential risks**  
If `GPUModelRunner._bind_routed_experts_capturer()` is called before model modules are fully initialized, capture may still fail. The closure in `_capture_fn` captures `layer_id` and `capturer` by value, but if these objects are mutated, it could lead to stale references. The fix assumes all relevant `FusedMoE` modules are in `static_forward_context`.

**Key insights**  
Always bind capture callbacks after model construction to avoid timing issues. Capture logical IDs before any ID mapping (like EPLB) to ensure downstream compatibility. Use a shared capture helper in the router to maintain consistency across different forward paths. The added unit tests validate both pre-EPLB capture and binding behavior.

---

## 20. [[fix][torch.compile] Fix cold-start compilation time increase by adding kv cache update to splitting ops](https://github.com/vllm-project/vllm/pull/33441)


### Base Information

- **PR Number:** #33441
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 06:48:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33441/files) (4):**
  - `tests/compile/test_cold_start.py`
  - `tests/compile/test_graph_partition.py`
  - `vllm/compilation/backends.py`
  - `vllm/config/compilation.py`

### Summary

**What changed and why**  
This PR fixes a cold-start compilation time regression by adding `unified_kv_cache_update` to the splitting operations list, ensuring it's grouped with `unified_attention` in the same subgraph. This reduces piecewise backend overhead and prevents redundant graph compilations. A test validates that consecutive splitting operations are correctly merged.

**Technical impact**  
The change improves cold-start compilation times significantly (e.g., from 87.56s to 22.03s for llama3-70b) by optimizing graph partitioning. It modifies the splitting logic to keep consecutive splitting ops together, reducing the number of subgraphs and leveraging caching more effectively. The fix is conditional, applying only when `use_inductor_graph_partition` is disabled.

**Potential risks**  
Excluding `unified_kv_cache_update` from CUDA graphs may impact performance in scenarios where cache updates are critical, though current tests show no regression. The logic for grouping consecutive splitting ops assumes `node.next` exists, which could fail if splitting ops are at the graph's end. Changes to splitting behavior might affect other models or compilation modes not covered in tests.

**Key insights**  
The fix is a targeted workaround for a specific performance regression; consider a more robust solution for handling string parameters in Inductor long-term. Ensure the consecutive-op grouping logic handles edge cases, such as final nodes in the graph. Monitor performance across diverse models to confirm no hidden regressions, especially for CUDA graph modes.

---

## 21. [[Doc] Update plugin deprecation notices](https://github.com/vllm-project/vllm/pull/33476)


### Base Information

- **PR Number:** #33476
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-31 06:48:28
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33476/files) (1):**
  - `docs/design/plugin_system.md`

### Summary

**What changed and why**  
Updated the deprecation notice for the `seed_everything` platform interface from "will be removed in v0.15.0 or later" to "has been removed in v0.16.0." This corrects documentation that was missed in a previous update (PR #33362).

**Technical impact**  
The change is purely documentation and has no functional impact on the codebase. It accurately reflects the current state of the API, informing users that the deprecated interface is no longer available as of a specific version.

**Potential risks**  
There is minimal risk. The primary concern is ensuring the version number (v0.16.0) is correct and consistent with the actual release timeline. An incorrect version could mislead users about API availability.

**Key insights**  
This is a routine documentation maintenance update. Developers should verify that the stated removal version (v0.16.0) aligns with the project's release history. Such documentation accuracy is crucial for user migration and avoiding confusion.

---

## 22. [support return prompt token ids in responses](https://github.com/vllm-project/vllm/pull/33378)


### Base Information

- **PR Number:** #33378
- **Author:** [cmunley1](https://github.com/cmunley1)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 06:04:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33378/files) (1):**
  - `vllm/entrypoints/openai/responses/protocol.py`

### Summary

**What changed and why**  
The PR updates documentation for the `return_messages` parameter in the OpenAI responses API protocol. The change removes outdated references to "gpt-oss only" and clarifies that the feature is supported for non-background requests.

**Technical impact**  
This is a documentation-only change that corrects misleading information about feature availability. It ensures API users have accurate expectations regarding the `return_messages` parameter's support scope, reducing potential confusion.

**Potential risks**  
No technical risks since only documentation strings were modified. However, the change could lead users to expect broader support than intended if the feature remains limited to specific request types not fully documented.

**Key insights**  
Always keep API documentation synchronized with actual implementation constraints. Consider whether additional clarifications about "non-background" request types are needed to prevent misinterpretation.

---

## 23. [[Misc] Replace deprecated interface seed_everything](https://github.com/vllm-project/vllm/pull/33474)


### Base Information

- **PR Number:** #33474
- **Author:** [esmeetu](https://github.com/esmeetu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 05:38:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33474/files) (2):**
  - `benchmarks/kernels/cpu/benchmark_cpu_attn.py`
  - `benchmarks/kernels/cpu/benchmark_cpu_fused_moe.py`

### Summary

**What changed and why**  
The PR replaces deprecated `current_platform.seed_everything(seed)` calls with `set_random_seed(seed)` from `vllm.utils.torch_utils` in two CPU benchmark scripts. This aligns with deprecation of the platform-specific seeding interface in favor of a centralized torch utility.

**Technical impact**  
The changes maintain identical seeding behavior while removing a dependency on `current_platform` for seeding. This simplifies the codebase by using a single, standardized seeding method across the project, improving consistency and reducing platform-specific abstractions in benchmark code.

**Potential risks**  
Low risk, as the change is a straightforward replacement of one seeding function with another that should provide equivalent functionality. However, if `set_random_seed` has different underlying behavior (e.g., seeding additional libraries), it could slightly alter benchmark reproducibility. The limited scope (only two benchmark files) minimizes broader impact.

**Key insights**  
This is a maintenance update reflecting an internal API deprecation. Developers should adopt `set_random_seed` for all future seeding needs and update other usages of `current_platform.seed_everything` to ensure consistency. The change also highlights the project's move toward decoupling platform-specific utilities where possible.

---

## 24. [[Bugfix] Fix incompatibility between #33372 and #32863](https://github.com/vllm-project/vllm/pull/33475)


### Base Information

- **PR Number:** #33475
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 05:21:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33475/files) (4):**
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/params.py`

### Summary

**What changed and why**  
This PR fixes a `kwargs` not defined error caused by a merge conflict between PRs #33372 and #32863. The issue occurred because #32863 removed the `kwargs` parameter from certain renderer functions, but #33372 added code that attempted to modify `kwargs["return_dict"]`. The fix removes the problematic `kwargs["return_dict"] = False` assignments from three renderer files and instead sets `return_dict=False` centrally in the `get_apply_chat_template_kwargs` method.

**Technical impact**  
The changes centralize the `return_dict=False` configuration into the `ChatTemplateParams` class, making the system more maintainable and eliminating the runtime error. This ensures all chat template calls consistently use `return_dict=False` without relying on mutable keyword argument modifications in individual renderer implementations.

**Potential risks**  
If other parts of the codebase were depending on the `kwargs` dictionary being modified in-place by the renderer functions, those dependencies could break. Additionally, any future changes to the `get_apply_chat_template_kwargs` method would affect all renderers uniformly, which could introduce widespread issues if not carefully tested.

**Key insights**  
This fix demonstrates the importance of coordinating changes between related PRs to avoid merge conflicts that break functionality. The solution improves code organization by moving configuration to a single source of truth. Developers should ensure that parameter handling is consistent across all renderer implementations and consider adding tests to catch similar integration issues.

---

## 25. [[ez] Add structured torch.compile logs](https://github.com/vllm-project/vllm/pull/33213)


### Base Information

- **PR Number:** #33213
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-31 05:00:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33213/files) (3):**
  - `tests/compile/test_structured_logging.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/piecewise_backend.py`

### Summary

**What changed and why**  
This PR adds structured logging for torch.compile operations in vLLM to improve visualization in tlparse (Torch Trace). The changes introduce logging of compilation configuration, piecewise graph splits, and submodule compilation events to provide better observability into the compilation process.

**Technical impact**  
The implementation adds trace_structured calls at key compilation stages: configuration logging in the backend, graph splitting visualization, and per-submodule compilation tracking. This creates a structured event stream that tlparse can consume to display compilation metadata, graph structures, and timing information in a more organized format.

**Potential risks**  
The additional logging calls could impact performance during compilation, particularly for models with many submodules. There's also a risk of log file bloat if not properly managed, though the code includes mechanisms to log each graph only once. The test relies on mocking trace_structured, which might not capture all real-world edge cases.

**Key insights**  
The logging follows a consistent pattern with clear metadata and payload separation. Developers should ensure the logging doesn't affect production performance and consider adding log level controls. The test provides good coverage but should be expanded to validate log content, not just presence. Future work could include log aggregation and analysis tools.

---

## 26. [[Frontend] Use new Renderer for Completions and Tokenize API](https://github.com/vllm-project/vllm/pull/32863)


### Base Information

- **PR Number:** #32863
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-31 04:51:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32863/files) (64):**
  - `examples/online_serving/prompt_embed_inference_with_openai_client.py`
  - `tests/engine/test_short_mm_context.py`
  - `tests/entrypoints/llm/test_chat.py`
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_completion_with_prompt_embeds.py`
  - `tests/entrypoints/openai/test_embedding_shape_validation.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `tests/entrypoints/openai/test_prompt_validation.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/entrypoints/pooling/basic/test_truncation.py`
  - `tests/entrypoints/pooling/classify/test_online.py`
  - `tests/entrypoints/pooling/score/test_online_score.py`
  - `tests/entrypoints/test_renderer.py`
  - `tests/models/language/pooling/test_mm_classifier_conversion.py`
  - `tests/models/language/pooling/test_truncation_control.py`
  - `tests/models/language/pooling_mteb_test/mteb_embed_utils.py`
  - `tests/models/language/pooling_mteb_test/mteb_score_utils.py`
  - `tests/renderers/test_completions.py`
  - `tests/renderers/test_mistral.py`
  - `tests/renderers/test_sparse_tensor_validation.py`
  - `tests/test_inputs.py`
  - `vllm/config/model.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/protocol.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/context.py`
  - `vllm/entrypoints/openai/responses/protocol.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/classify/serving.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/renderer.py`
  - `vllm/entrypoints/serve/disagg/protocol.py`
  - `vllm/entrypoints/serve/disagg/serving.py`
  - `vllm/entrypoints/serve/tokenize/protocol.py`
  - `vllm/entrypoints/serve/tokenize/serving.py`
  - `vllm/entrypoints/utils.py`
  - `vllm/inputs/data.py`
  - `vllm/inputs/parse.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/renderers/__init__.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/embed_utils.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/mistral.py`
  - `vllm/renderers/params.py`
  - `vllm/renderers/protocol.py`
  - `vllm/renderers/terratorch.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/utils/async_utils.py`
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
This PR migrates completions and tokenization logic to the new Renderer API, removing the legacy `CompletionRenderer` and consolidating preprocessing steps. Key changes include adding `render_completions` and `tokenize_prompt` methods to Renderer, introducing `ChatParams` and `TokenizeParams` to reduce argument passing, and deprecating `truncate_prompt_tokens` in favor of `tokenization_kwargs`. The goal is to unify input processing across APIs and pave the way for integrating IO Processor and multimodal handling into Renderer.

**Technical impact**  
The changes centralize prompt rendering and tokenization within the Renderer, simplifying the serving engine and entrypoints. By replacing scattered preprocessing logic with structured params objects, the codebase becomes more maintainable and consistent. The removal of `CompletionRenderer` and associated utilities reduces duplication, while the new async tokenization backend (`AsyncMicrobatchTokenizer`) improves efficiency for concurrent requests.

**Potential risks**  
- Deprecated parameters (`truncate_prompt_tokens`) may cause confusion if users miss migration warnings.  
- The refactor touches many files (64 modified), increasing the risk of regression in edge cases like multimodal inputs or pooling APIs.  
- Changes to error messages (e.g., context length validation) could affect client-side error handling.  
- The shift to `tokenization_kwargs` requires updates to downstream code that relied on the old truncation API.

**Key insights**  
- Developers should adopt `ChatParams`/`TokenizeParams` for new features and migrate existing code to use `tokenization_kwargs` instead of deprecated truncation parameters.  
- The Renderer is now the single source of truth for input processing; future work (like IO Processor integration) should build on this foundation.  
- Thorough testing is essential, especially for multimodal and pooling endpoints, due to the wide scope of changes.

---

## 27. [[perf] v1/spec_decode: skip softmax for all-greedy rejection sampling](https://github.com/vllm-project/vllm/pull/32852)


### Base Information

- **PR Number:** #32852
- **Author:** [caozuoba](https://github.com/caozuoba)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-01-31 01:51:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32852/files) (1):**
  - `vllm/v1/sample/rejection_sampler.py`

### Summary

**What changed and why**  
The PR optimizes the v1 speculative decoding rejection sampler by skipping the full softmax computation when the entire batch uses greedy sampling (`sampling_metadata.all_greedy`). Instead of computing `target_probs = target_logits.softmax()`, it directly uses `target_logits.argmax()` for greedy paths, since `argmax(softmax(logits)) == argmax(logits)`. This reduces unnecessary compute and memory overhead while preserving behavior.

**Technical impact**  
This change improves performance by eliminating the softmax operation for all-greedy batches, which reduces both computational cost and memory bandwidth usage. The rejection sampling logic now conditionally computes softmax only when needed (i.e., for non-greedy sampling), and the `target_logits` tensor is passed directly to the rejection sampling kernel for greedy cases.

**Potential risks**  
If the `all_greedy` flag is incorrectly set (e.g., due to bugs in sampling metadata), the softmax may be incorrectly skipped for non-greedy requests, leading to incorrect sampling behavior. Additionally, the `target_probs` tensor is now computed later in the function, so any downstream code expecting it earlier could break, though the current usage appears safe.

**Key insights**  
This is a targeted optimization that leverages mathematical equivalence to avoid expensive operations. Developers should ensure the `all_greedy` flag is accurately maintained and verify that no other code paths depend on early softmax computation. The performance gains (~3% throughput improvement) are significant for greedy-heavy workloads, making this a valuable change for production deployments.

---

## 28. [[ROCM] Enable aiter attn backend for qwen3-next model](https://github.com/vllm-project/vllm/pull/32492)


### Base Information

- **PR Number:** #32492
- **Author:** [jennyyyyzhen](https://github.com/jennyyyyzhen)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-31 01:03:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32492/files) (2):**
  - `docs/design/attention_backends.md`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
Updated the ROCM_AITER_FA attention backend to explicitly support block sizes of 16 and 32 instead of a generic multiple-of-16 constraint. This fixes an issue where Qwen3-Next (which uses a non-standard block size of 544) incorrectly attempted to use the aiter backend, which doesn't actually support large block sizes.

**Technical impact**  
The change decouples kernel block size support from page block size alignment, ensuring the backend only claims compatibility with block sizes it truly supports. This prevents runtime failures when models with unusual block sizes (like 544) are paired with the aiter backend.

**Potential risks**  
If other models rely on the previous multiple-of-16 behavior for block sizes other than 16 or 32, they may no longer use the aiter backend, potentially affecting performance. The documentation update must stay synchronized with the code to avoid confusion about supported configurations.

**Key insights**  
Always validate that `get_supported_kernel_block_sizes` accurately reflects the backend's true capabilities, especially when dealing with non-standard model architectures. This fix highlights the importance of precise constraints over generic multiples to prevent runtime mismatches.

---

## 29. [[BugFix] Add synchronize in CutlassW4A8LinearKernel to ensure data is ready for use.](https://github.com/vllm-project/vllm/pull/33078)


### Base Information

- **PR Number:** #33078
- **Author:** [ayrnb](https://github.com/ayrnb)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-31 00:14:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33078/files) (1):**
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass.py`

### Summary

**What changed and why**  
A single `torch.cuda.synchronize()` call was added in `CutlassW4A8LinearKernel.process_weights_after_loading` after `convert_packed_uint4b8_to_signed_int4_inplace`. This ensures GPU operations complete before subsequent layout permutation and encoding steps, fixing incorrect results in w4afp8 models during in-place modifications.

**Technical impact**  
The synchronization enforces proper GPU execution ordering for weight transformation pipelines. Without it, asynchronous kernel execution could allow later operations to read stale or partially modified data, leading to silent numerical errors in quantized linear layers.

**Potential risks**  
Adding synchronization may introduce a minor performance penalty during model loading, though this is a one-time cost. If other similar transformation pipelines exist in the codebase, they may harbor the same latent bug and require audit. Overuse of synchronization could eventually impact startup latency.

**Key insights**  
This fix highlights a subtle CUDA synchronization requirement for in-place GPU data transformations. Developers should audit other weight-loading or in-place modification paths for similar issues. Consider adding a comment explaining why synchronization is necessary here to prevent future removal during optimizations.

---

