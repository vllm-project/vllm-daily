# vLLM Merged PR Report

**Report Date:** 2026-01-25 PST

**Total Merged PRs:** 12

---

## 1. [[Refactor] Use data parser for matching data items to multi-modal UUIDs](https://github.com/vllm-project/vllm/pull/32955)


### Base Information

- **PR Number:** #32955
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-25 23:00:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32955/files) (14):**
  - `docs/features/multimodal_inputs.md`
  - `examples/pooling/plugin/prithvi_geospatial_mae_offline.py`
  - `tests/entrypoints/openai/test_vision_embeds.py`
  - `tests/entrypoints/test_chat_utils.py`
  - `tests/models/multimodal/pooling/test_prithvi_mae.py`
  - `tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py`
  - `tests/v1/engine/test_process_multi_modal_uuids.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/pooling/score/utils.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/utils/__init__.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This refactor centralizes UUID validation and data parsing logic for multi-modal inputs. Key changes include moving UUID validation from the `LLM` class to `InputProcessor`, using a data parser to determine item counts consistently across validation and UUID generation, and updating the `MultiModalDataTracker` to correctly handle concatenated embeddings. Documentation was revised to clarify embedding shapes, and fixes were applied to Terratorch and Qwen3-VL implementations.

**Technical impact**  
The changes improve code organization by separating concerns—validation logic is now consolidated in `InputProcessor`, making the system more maintainable. Using the data parser ensures consistent item counting for both validation and UUID generation, reducing edge-case bugs. The refactor also standardizes how embeddings are processed across different models, particularly for complex cases like Qwen3-VL with spatial merge considerations.

**Potential risks**  
There is a risk of breaking backward compatibility for users relying on the old embedding shape assumptions, as the documentation now emphasizes model-specific shapes. The changes to UUID validation could introduce stricter checks that reject previously accepted inputs. Additionally, the refactor touches multiple components (e.g., Terratorch field handling), which may affect niche use cases or custom integrations.

**Key insights**  
Developers should review the updated documentation for embedding shape requirements, as they are now model-dependent. The consolidation of validation logic reduces duplication but requires thorough testing of multi-modal inputs, especially for edge cases like mixed raw and embedded data. Ensure that any custom multi-modal processors align with the new data parsing approach to avoid runtime errors.

---

## 2. [Set splitk=1 for fused-moe-lora expand kernel](https://github.com/vllm-project/vllm/pull/32882)


### Base Information

- **PR Number:** #32882
- **Author:** [dcmaddix](https://github.com/dcmaddix)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-25 22:52:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32882/files) (1):**
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`

### Summary

**What changed and why**  
The change modifies the fused-moe-lora expand kernel to hardcode `SPLIT_K` to 1, removing the previously passed `split_k` variable. This aligns the expand kernel's behavior with the dense LoRA expand kernel, where splitK optimization is unnecessary because the inner K dimension is not the largest dimension in the GEMM operation.

**Technical impact**  
This simplifies the kernel configuration by eliminating a tunable parameter for the expand operation, ensuring consistency with the dense LoRA implementation. It may slightly reduce kernel launch overhead by avoiding unnecessary splitK computations, but performance impact is expected to be neutral or positive since splitK optimization is not beneficial for this kernel type.

**Potential risks**  
If the assumption about the inner K dimension size is incorrect for certain model architectures or configurations, forcing `SPLIT_K=1` could theoretically degrade performance. However, this risk appears minimal given the established pattern in the dense LoRA expand kernel and the explicit rationale about dimension sizes.

**Key insights**  
Developers should note that splitK optimization is only valuable when the inner K dimension dominates the GEMM; this change reinforces that architectural understanding. Future kernel optimizations should continue to distinguish between shrink and expand operations based on dimension characteristics rather than applying uniform tuning strategies.

---

## 3. [[Model Runner V2] Add LoRAState to consolidate lora logic](https://github.com/vllm-project/vllm/pull/33062)


### Base Information

- **PR Number:** #33062
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-25 22:21:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33062/files) (3):**
  - `vllm/v1/worker/gpu/lora_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/states.py`

### Summary

**What changed and why**  
This PR introduces a new `LoraState` class to consolidate LoRA-related logic that was previously scattered within the `RequestState` class. The changes extract LoRA management (tracking active requests, generating mapping inputs) into a dedicated module, simplifying the core request state management.

**Technical impact**  
The refactoring separates concerns by moving LoRA-specific functionality out of the general-purpose `RequestState` class. This improves modularity and makes the codebase more maintainable. The `RequestState` class becomes simpler by removing the `extra_data` dictionary and LoRA-specific arrays/methods.

**Potential risks**  
There's a risk of incomplete request cleanup if `lora_state.remove_request()` isn't called consistently alongside other cleanup operations. The `make_lora_inputs` method now accesses `self.lora_requests` separately from the main request tracking, which could lead to synchronization issues if request lifecycle management isn't perfectly coordinated.

**Key insights**  
This is a positive architectural improvement that follows the single responsibility principle. Developers should ensure that all request lifecycle operations (add/remove/finish) consistently update both the main request state and the LoRA state. Consider adding integration tests to verify LoRA functionality works correctly after this refactoring.

---

## 4. [[Tests] Remove Duplicates](https://github.com/vllm-project/vllm/pull/33032)


### Base Information

- **PR Number:** #33032
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-25 21:23:54
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33032/files) (3):**
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass-dp-ep.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass-dp-ep.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/config-b200.txt`

### Summary

**What changed and why**  
Two YAML configuration files for Qwen3-30B-A3B-NvFp4 models were removed from the `moe-refactor` test directory, along with their references in the `config-b200.txt` file. The changes eliminate duplicate configurations that were specifically for data-parallel and expert-parallel (DP/EP) setups, as noted in the PR description.

**Technical impact**  
This cleanup reduces test configuration redundancy and simplifies the test suite. The removed files were likely duplicates of existing configurations with only minor differences in model source paths (`RedHatAI` vs `nvidia`) and parallelization flags. The remaining configurations continue to cover the necessary test scenarios without duplication.

**Potential risks**  
If the DP/EP configurations were intended for specific parallelization testing scenarios, their removal might leave gaps in coverage for those particular setups. Additionally, any external scripts or CI pipelines that directly referenced these removed file paths will now fail unless updated.

**Key insights**  
This is a straightforward cleanup that improves maintainability. Developers should verify that the remaining configurations adequately cover all required test cases, especially for data-parallel and expert-parallel execution modes. Any automated processes using the removed files should be updated to reference the appropriate alternative configurations.

---

## 5. [[StepVL] support close img patch](https://github.com/vllm-project/vllm/pull/32923)


### Base Information

- **PR Number:** #32923
- **Author:** [ltd0924](https://github.com/ltd0924)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-25 20:56:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32923/files) (1):**
  - `vllm/model_executor/models/step3_vl.py`

### Summary

**What changed and why**  
The changes introduce a configurable `enable_patch` parameter to the `ImagePatcher` class, allowing patch extraction to be disabled via a Hugging Face config override. This is intended for video understanding tasks where only key frames should be processed without patch operations.

**Technical impact**  
When `enable_patch` is set to `False`, the patcher returns zero patches and bypasses patch-related computations, effectively disabling the patch extraction pipeline. This modifies the model's image processing behavior dynamically based on configuration, without altering the core architecture.

**Potential risks**  
If `enable_patch` is incorrectly set or the config override fails silently, the model may unexpectedly skip patch extraction, potentially degrading performance for tasks that rely on patches. The condition `long < 728` (changed from `long <= 728`) could slightly alter window size calculations for images exactly 728 pixels long.

**Key insights**  
Developers should ensure the `hf-overrides` parameter is correctly passed when serving the model. The change is backward-compatible (defaults to `True`), but testing with both enabled and disabled patches is recommended to validate behavior for different use cases.

---

## 6. [[CI] Fix MHA attention test failure (AttributeError when model_config is None in ViT attention backend)](https://github.com/vllm-project/vllm/pull/33033)


### Base Information

- **PR Number:** #33033
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-25 19:49:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33033/files) (1):**
  - `vllm/model_executor/models/vision.py`

### Summary

**What changed and why**  
The fix adds null checks for `model_config` before accessing its `multimodal_config` attribute in two functions. This prevents `AttributeError` when `model_config` is `None`, which occurs during tests using the `default_vllm_config` fixture.

**Technical impact**  
This change ensures the vision attention backend functions gracefully handle missing configuration, maintaining backward compatibility for test environments. The system behavior now correctly falls back to default values when multimodal configuration is unavailable.

**Potential risks**  
The fix assumes `None` is an acceptable default for `multimodal_config`, which may mask deeper configuration issues in production. Edge cases where `model_config` exists but lacks `multimodal_config` attribute remain unhandled.

**Key insights**  
Always validate nested attribute access when dealing with optional configurations. Consider using `getattr()` with chaining for cleaner null-safe access. Review similar patterns elsewhere in the codebase to prevent related issues.

---

## 7. [[Model Runner V2] Minor simplification for finish_requests](https://github.com/vllm-project/vllm/pull/33048)


### Base Information

- **PR Number:** #33048
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-25 18:35:02
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33048/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The code simplifies request cleanup logic by unifying the handling of finished and preempted requests. Instead of separately processing preempted requests, it now merges them with finished requests into a single set before performing cleanup operations.

**Technical impact**  
This reduces code duplication and consolidates request termination logic into a single loop. The change maintains the same cleanup operations (removing requests from req_states, encoder_runner, and prompt_logprobs_worker) but processes all terminated requests together rather than in separate loops.

**Potential risks**  
The union operation assumes both sets contain unique request IDs, which should be valid but could cause issues if there were duplicates. The conditional check changes from `is not None` to boolean evaluation, which could behave differently if preempted_req_ids is an empty collection (though likely intended).

**Key insights**  
The simplification improves maintainability by eliminating redundant code. Developers should verify that preempted_req_ids and finished_req_ids are always disjoint sets to prevent duplicate processing. The boolean evaluation change is cleaner but worth noting for future debugging.

---

## 8. [[Model Runner V2] Fix slot_mapping after #25954](https://github.com/vllm-project/vllm/pull/33046)


### Base Information

- **PR Number:** #33046
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-25 18:29:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33046/files) (5):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py`

### Summary

**What changed and why**  
This PR fixes a bug introduced in #25954 that broke Model Runner V2 by correcting how `slot_mapping` is handled across the codebase. The issue was that `slot_mapping` was being processed inconsistently—sometimes passed as a raw tensor and sometimes as a per-layer dictionary. The changes ensure `slot_mapping` is consistently transformed into a per-layer dictionary (`slot_mappings_by_layer`) via `build_slot_mappings_by_layer()` before being used in model execution contexts.

**Technical impact**  
The modifications standardize the slot mapping data flow: `prepare_inputs_to_capture()` now returns `slot_mappings_by_layer` directly, and `InputBatch` includes a new `slot_mappings` field to store this per-layer mapping. This ensures compatibility with the `set_forward_context` and model execution paths, particularly for CUDA graph capture and speculative decoding (Eagle), where the per-layer format is required.

**Potential risks**  
If `build_slot_mappings_by_layer` is called incorrectly or with mismatched `kv_cache_config` parameters, it could lead to runtime errors or incorrect cache behavior. The changes also introduce new dependencies (e.g., `slot_mappings` in `InputBatch`), so any code instantiating `InputBatch` must now provide this field or use the updated `make_dummy()` method.

**Key insights**  
Developers should ensure that `slot_mappings` is always transformed to the per-layer dictionary format before being passed to execution contexts. The `InputBatch` structure now explicitly tracks this mapping, improving clarity. When extending or modifying related code, maintain consistency by using `build_slot_mappings_by_layer()` and updating all call sites, including dummy/profile runs.

---

## 9. [[Bugfix][VLM] Fix transformers backend embed_multimodal for Qwen2.5-VL profiling](https://github.com/vllm-project/vllm/pull/32969)


### Base Information

- **PR Number:** #32969
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-25 16:34:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32969/files) (1):**
  - `vllm/model_executor/models/transformers/multimodal.py`

### Summary

**What changed and why**  
The fix addresses a `RuntimeError` in the transformers backend's multimodal embedding logic when profiling Qwen2.5-VL models. The issue occurred because dummy encoder outputs during memory profiling had mismatched dimensions when split using `num_image_patches`. The solution refactors the tensor reshaping and splitting to handle varying input dimensions and edge cases like profiling data.

**Technical impact**  
This change improves robustness by supporting different vision embedding formats (e.g., Qwen2.5-VL's 2D tensors vs. Idefics3's expanded tokens) and gracefully handling dummy data during profiling. It replaces hard-coded assumptions with dynamic size calculations and adds fallback logic for mismatched tensor sizes, ensuring compatibility across multimodal models.

**Potential risks**  
The padding/truncation logic for mismatched sizes during profiling could introduce subtle artifacts if dummy data differs significantly from real data. Additionally, the `repeat` operation for small tensors may unintentionally duplicate embeddings, though this is limited to profiling scenarios. Edge cases with empty embeddings (`total_tokens == 0`) now raise explicit errors but could disrupt error handling flows.

**Key insights**  
Developers should verify that the dynamic size matching logic works for all supported multimodal models beyond Qwen2.5-VL. The fix highlights the importance of testing memory profiling paths with realistic dummy data. Consider adding unit tests for the new size-mismatch branches to prevent regressions.

---

## 10. [[Model] Use mm_position to compute mrope positions for Qwen2.5-Omni](https://github.com/vllm-project/vllm/pull/32772)


### Base Information

- **PR Number:** #32772
- **Author:** [Etelis](https://github.com/Etelis)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-25 04:15:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32772/files) (3):**
  - `examples/offline_inference/qwen2_5_omni/only_thinker.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
Refactored the Qwen2.5-Omni model's mrope position computation to use `mm_position.offset` directly instead of token-by-token iteration. This aligns with the pattern established in PR #32126 for Qwen2-VL models and improves efficiency by leveraging offset-based iteration across all three modalities (audio, image, video).

**Technical impact**  
The changes introduce a new iterator (`iter_mm_features`) that processes multimodal features by their positional offsets, eliminating the previous while-loop approach. This simplifies the position calculation logic and ensures consistent handling of interleaved audio-video content when `use_audio_in_video=True`. The refactoring also adds proper masking for overlapping embeddings in the GPU model runner.

**Potential risks**  
The audio-video pairing logic assumes matching order between video and audio features, which could fail if the input data isn't properly synchronized. The OR masking for overlapping embeddings in `gpu_model_runner.py` might cause unintended behavior if multiple modalities claim the same token positions incorrectly. Edge cases with malformed multimodal data could lead to offset calculation errors.

**Key insights**  
Developers should verify that multimodal inputs maintain consistent ordering between audio and video features when using `use_audio_in_video`. The new iterator pattern improves code maintainability but requires careful validation of offset calculations. The masking change in the GPU runner is critical for correct embedding placement in overlapping scenarios.

---

## 11. [[Doc] Add Qwen2.5 models to batch invariance tested models](https://github.com/vllm-project/vllm/pull/33016)


### Base Information

- **PR Number:** #33016
- **Author:** [ZhanqiuHu](https://github.com/ZhanqiuHu)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-25 01:20:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33016/files) (1):**
  - `docs/features/batch_invariance.md`

### Summary

**What changed and why**  
A single-line documentation update adds the Qwen2.5 model family (six instruct models from 0.5B to 32B parameters) to the validated batch invariance models list. This change fulfills a request from issue #27433 to expand the list of tested models.

**Technical impact**  
The change is purely documentation and does not affect code, architecture, or runtime behavior. It informs users that these Qwen2.5 models have been validated for batch invariance across different tensor parallelism (TP) sizes and attention backends (FLASH_ATTN and FLASHINFER).

**Potential risks**  
No technical risks exist, as this is a documentation-only change. However, if the underlying batch invariance tests were incomplete or the models behave differently under untested configurations (e.g., different hardware, software versions), users might encounter unexpected issues.

**Key insights**  
The update enhances the documentation's accuracy and utility by reflecting recent testing efforts. Developers should ensure that any future changes to batch invariance behavior or model support are promptly documented, and consider automating documentation updates from test results to maintain consistency.

---

## 12. [[BugFix]  Add env variable to control PDL in LoRA](https://github.com/vllm-project/vllm/pull/32836)


### Base Information

- **PR Number:** #32836
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-25 00:32:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32836/files) (2):**
  - `vllm/envs.py`
  - `vllm/lora/ops/triton_ops/utils.py`

### Summary

**What changed and why**  
A new environment variable `VLLM_LORA_DISABLE_PDL` has been introduced to control Programmatic Dependent Launch (PDL) support for LoRA operations. This serves as a temporary workaround to prevent Triton compilation failures on SM100 GPUs where enabling PDL with LoRA causes issues, addressing specific bug reports (#30872 and #32424).

**Technical impact**  
The changes modify the `supports_pdl` function to conditionally disable PDL based on the environment variable, overriding the default capability check for SM90+ GPUs. This allows users to bypass PDL on affected hardware without altering code, maintaining backward compatibility while providing an escape hatch for compilation errors.

**Potential risks**  
If the environment variable is incorrectly set (e.g., left enabled on SM100 GPUs), Triton compilation failures may persist. Additionally, disabling PDL on supported GPUs could impact performance optimizations for LoRA operations, though this is a trade-off for stability. The fix is temporary and may require a more permanent solution for SM100 compatibility.

**Key insights**  
Developers should set `VLLM_LORA_DISABLE_PDL=1` when using SM100 GPUs with LoRA to avoid Triton errors. This workaround highlights a hardware-specific limitation; future updates should ideally resolve the root cause rather than relying on environment variables. Ensure documentation or deployment scripts reflect this variable for affected environments.

---

