# vLLM Merged PR Report

**Report Date:** 2026-01-13 PST

**Total Merged PRs:** 34

---

## 1. [[Model] Re-implement Qwen3Omni Audio Encoder](https://github.com/vllm-project/vllm/pull/32167)


### Base Information

- **PR Number:** #32167
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-13 23:40:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32167/files) (1):**
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
The PR re-implements the Qwen3-Omni audio encoder using vLLM primitives, replacing the original Hugging Face-based implementation. This includes new modules like `SinusoidsPositionEmbedding`, `Qwen3OmniMoeAudioAttention` (built on `MMEncoderAttention` and `QKVParallelLinear`), and a full transformer encoder stack. The goal is to improve performance through vectorization and better integration with vLLM's attention backends, achieving roughly 10% speedup at high batch sizes.

**Technical impact**  
The changes integrate the audio encoder directly into vLLM's model execution framework, enabling tensor parallelism and leveraging vLLM's optimized attention mechanisms. This removes the explicit dependency on `flash_attn` and aligns the encoder with vLLM's multimodal architecture. The encoder now uses chunked processing and fp16-safe clamping, improving numerical stability and scalability.

**Potential risks**  
The removal of explicit `flash_attn` dependency assumes vLLM's attention backend selection is robust, which could affect performance or correctness if the backend is misconfigured. The fp16 clamping may introduce subtle numerical differences in edge cases. Additionally, the weight mapping for stacked `qkv` requires careful validation to ensure compatibility with pre-trained checkpoints.

**Key insights**  
Developers should verify that the new encoder maintains output parity with the original implementation, especially for audio-only inference scenarios. The performance gains are notable for high batch sizes, but testing across diverse audio inputs and batch configurations is recommended. Future work should address the noted memory profiling bug and enable data parallelism for both audio and vision encoders.

---

## 2. [Add Molmo2 multimodal model support](https://github.com/vllm-project/vllm/pull/30997)


### Base Information

- **PR Number:** #30997
- **Author:** [sangho-vision](https://github.com/sangho-vision)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-13 23:33:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30997/files) (11):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `examples/offline_inference/vision_language_multi_image.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/registry.py`
  - `tests/models/test_initialization.py`
  - `vllm/config/model.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/multimodal/processing.py`
  - `vllm/multimodal/video.py`

### Summary

**What changed and why**  
This PR adds comprehensive support for the Molmo2 multimodal model family to vLLM, enabling image, multi-image, and video understanding. It introduces a new model implementation (`Molmo2ForConditionalGeneration`), registers it in the multimodal registry, adds a Molmo2-specific video loader backend, and updates documentation and examples. The changes fulfill a feature request to integrate this open vision-language model from Allen AI.

**Technical impact**  
The integration extends vLLM's multimodal capabilities to handle Molmo2's unique architecture, which uses a prefix language modeling formulation requiring TritonAttention-based implementation for correct attention patterns. The new video backend supports frame sampling via OpenCV, and the model implementation includes vision backbone, projector, and text stack components. This adds a new model family to the supported ecosystem with specific handling for multimodal embeddings and token replacement.

**Potential risks**  
Molmo2's prefix LM formulation depends on recent TritonAttention changes (PRs #30386, #30974), so incomplete integration of those dependencies could cause attention errors. The video loader introduces new dependencies (`decord`, `PyAV`, or `torchcodec`) that may affect deployment environments. Edge cases in multi-token placeholder replacement (`PromptUpdateDetails.select_token_ids`) could lead to incorrect embedding mappings if token IDs are mismatched.

**Key insights**  
Developers must ensure TritonAttention support is fully integrated before using Molmo2. The model requires setting `VLLM_VIDEO_LOADER_BACKEND="molmo2"` for video inputs and careful configuration of `max_num_batched_tokens` due to its 36K context length. Testing should validate simultaneous image and video inputs, as the PR includes a rule to avoid processing them together in tests.

---

## 3. [[Refactor] [8/N] to simplify the vLLM openai responsesapi_serving architecture](https://github.com/vllm-project/vllm/pull/32260)


### Base Information

- **PR Number:** #32260
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-13 23:26:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32260/files) (21):**
  - `tests/entrypoints/openai/responses/test_function_call_parsing.py`
  - `tests/entrypoints/openai/test_protocol.py`
  - `tests/entrypoints/openai/test_serving_responses.py`
  - `vllm/entrypoints/context.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`
  - `vllm/entrypoints/openai/parser/responses_parser.py`
  - `vllm/entrypoints/openai/responses/__init__.py`
  - `vllm/entrypoints/openai/responses/api_router.py`
  - `vllm/entrypoints/openai/responses/protocol.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/responses_utils.py`
  - `vllm/reasoning/abs_reasoning_parsers.py`
  - `vllm/reasoning/basic_parsers.py`
  - `vllm/reasoning/minimax_m2_reasoning_parser.py`
  - `vllm/reasoning/mistral_reasoning_parser.py`
  - `vllm/reasoning/olmo3_reasoning_parser.py`
  - `vllm/reasoning/qwen3_reasoning_parser.py`
  - `vllm/tool_parsers/abstract_tool_parser.py`

### Summary

**What changed and why**  
This PR refactors the vLLM OpenAI Responses API architecture by extracting related components into a dedicated `responses` module. The main changes include moving the `ResponsesRequest` protocol and associated classes from `engine/protocol.py` to a new `responses/protocol.py`, creating a separate API router (`responses/api_router.py`), and updating import paths across the codebase. This simplifies the monolithic `engine/protocol.py` and decouples the Responses API logic from other engine components.

**Technical impact**  
The refactoring improves code organization by separating concerns—Responses API-specific protocols, routing, and serving logic are now modular. This reduces the complexity of `engine/protocol.py` (which lost over 500 lines) and enhances maintainability. The changes are largely structural, with no functional alterations to the API behavior, as evidenced by updated import paths in tests and core files.

**Potential risks**  
While the changes are primarily structural, there is a risk of missed import updates in less-tested modules or edge cases, which could lead to runtime errors. The creation of a new module also introduces a new dependency structure that must be carefully managed to avoid circular imports. Additionally, any downstream code relying on the old import paths will break unless updated.

**Key insights**  
This refactoring is a positive step toward a more modular and scalable architecture. Developers should verify that all import paths are correctly updated, especially in external or plugin code. The separation aligns with single-responsibility principles and will facilitate future enhancements to the Responses API. Ensure comprehensive testing to catch any import-related issues before deployment.

---

## 4. [[Docs] Add docs about OOT Quantization Plugins](https://github.com/vllm-project/vllm/pull/32035)


### Base Information

- **PR Number:** #32035
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-13 23:25:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32035/files) (2):**
  - `docs/features/quantization/README.md`
  - `vllm/model_executor/layers/quantization/__init__.py`

### Summary

**What changed and why**  
This PR adds comprehensive documentation for creating out-of-tree (OOT) quantization plugins in vLLM, enabling users to implement custom quantization methods without modifying the core codebase. It also exports `register_quantization_config` from the quantization module's public API to support plugin registration.

**Technical impact**  
The changes extend vLLM's extensibility by formalizing a plugin architecture for quantization. Developers can now implement custom `QuantizationConfig`, `QuantizeMethodBase`, and `FusedMoEMethodBase` subclasses and register them via decorators, allowing vLLM to dynamically support new quantization schemes. This promotes modularity and reduces friction for integrating experimental or hardware-specific quantization techniques.

**Potential risks**  
Improper implementation of required abstract methods (e.g., `get_quant_method`) could lead to runtime errors or skipped quantization. The documentation assumes familiarity with vLLM's internal layer types (e.g., `LinearBase`, `FusedMoE`), which may pose a learning curve. Additionally, plugin registration relies on Python entry points or explicit imports, which could cause conflicts if multiple plugins use the same quantization name.

**Key insights**  
This documentation fills a critical gap for advanced users and researchers needing custom quantization support. Developers should thoroughly test plugins with target hardware and model architectures, especially for MoE layers. The exposure of `register_quantization_config` in `__init__.py` is a minimal but essential API change that ensures backward compatibility while enabling the plugin ecosystem.

---

## 5. [AMD CI Test - unskip moe_sum test and moe_align_block_size tests](https://github.com/vllm-project/vllm/pull/32039)


### Base Information

- **PR Number:** #32039
- **Author:** [hongxiayang](https://github.com/hongxiayang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 23:25:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32039/files) (3):**
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/moe/test_moe_align_block_size.py`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
Removed ROCm-specific skip decorators from three Mixture of Experts (MoE) kernel tests and added an `import_kernels()` method to the `RocmPlatform` class. These changes enable the previously skipped MoE tests to run on AMD ROCm platforms by ensuring the required `_moe_C` extension is loaded automatically, matching the behavior of other platforms.

**Technical impact**  
The addition of `import_kernels()` ensures ROCm-specific CUDA extensions (like `vllm._rocm_C`) are imported, which registers the `torch.ops._moe_C` operations needed for MoE kernels. This aligns ROCm’s extension loading with the existing patterns in `Platform` and `XPUPlatform`, allowing MoE functionality to be exercised in CI and production on AMD hardware.

**Potential risks**  
If the `vllm._rocm_C` extension is missing or fails to import silently (due to `contextlib.suppress`), MoE operations may still fail at runtime with unclear errors. Additionally, unskipping tests without validating performance or correctness across all ROCm GPU architectures could expose hardware-specific issues not caught by CI.

**Key insights**  
Ensure the `_rocm_C` extension is properly built and distributed for all target ROCm environments. Consider adding a warning or log if the import fails to aid debugging. Verify that other MoE-related tests and kernels work correctly on ROCm, as this fix only addresses the immediate loading issue for the unskipped tests.

---

## 6. [[misc] Remove is_torch_equal_or_newer(2.4) cases](https://github.com/vllm-project/vllm/pull/32296)


### Base Information

- **PR Number:** #32296
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 23:22:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32296/files) (4):**
  - `vllm/compilation/decorators.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/utils/torch_utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR removes version checks for PyTorch 2.4+ by eliminating the `supports_dynamo()` and `supports_custom_op()` helper functions and their conditional usage. The rationale is that vLLM now uses PyTorch 2.9, making these backward compatibility guards unnecessary.

**Technical impact**  
The codebase is simplified by removing conditional logic that previously gated Dynamo compilation and custom op registration based on PyTorch version. This reduces runtime checks and makes the code more straightforward, assuming all environments now meet the minimum version requirement. The removal of `supports_dynamo()` from compilation conditions means Dynamo compilation will always be attempted if enabled, which could affect environments with older PyTorch versions.

**Potential risks**  
If any deployment still uses PyTorch versions older than 2.4, this change could cause runtime errors. Specifically, custom op registration may fail on older versions lacking `torch.library.custom_op`, and Dynamo compilation could encounter unsupported features. The removal of the assertion in `direct_register_custom_op()` also eliminates a helpful error message for CUDA-like platforms with outdated PyTorch.

**Key insights**  
This cleanup is appropriate for the project's current PyTorch 2.9 baseline. However, ensure CI/CD and deployment pipelines enforce the minimum PyTorch version. Consider adding a version check during initialization to fail fast with a clear message if an older PyTorch version is detected, rather than allowing obscure runtime failures.

---

## 7. [[Build] Relax anthropic version pin from ==0.71.0 to >=0.71.0](https://github.com/vllm-project/vllm/pull/32289)


### Base Information

- **PR Number:** #32289
- **Author:** [dsfaccini](https://github.com/dsfaccini)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 23:21:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32289/files) (1):**
  - `requirements/common.txt`

### Summary

**What changed and why**  
The change relaxes the `anthropic` package version constraint from an exact pin (`==0.71.0`) to a minimum version (`>=0.71.0`). This resolves dependency conflicts with other packages (like `pydantic-ai`) that require newer versions of `anthropic`, enabling compatibility with the broader ecosystem.

**Technical impact**  
This modification allows the system to use any `anthropic` version from 0.71.0 onward, increasing flexibility in dependency resolution. It may reduce installation conflicts and facilitate integration with tools that depend on newer `anthropic` releases, though it introduces variability in the exact version used across environments.

**Potential risks**  
If newer `anthropic` versions introduce breaking changes or incompatibilities with the existing codebase, unexpected behavior could occur. There is also a risk of subtle issues if the code relies on undocumented or deprecated features specific to version 0.71.0.

**Key insights**  
Consider adding an upper bound (e.g., `<1.0.0`) if semantic versioning is followed, to guard against major breaking changes. Ensure thorough testing with newer `anthropic` versions to validate compatibility, and monitor for any regressions in functionality that depends on this package.

---

## 8. [[ROCm][CI] Handle missing vision_config in Isaac model attention patch](https://github.com/vllm-project/vllm/pull/32281)


### Base Information

- **PR Number:** #32281
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-13 23:21:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32281/files) (1):**
  - `tests/models/multimodal/generation/vlm_utils/model_utils.py`

### Summary

**What changed and why**  
Added try-except error handling around the `patch_hf_vision_attn_for_rocm` call to catch `AttributeError` when an Isaac model's attention layer lacks a `vision_config` attribute. This prevents test failures for models like Isaac-0.1 that use `Siglip2VariableLengthAttention`, while logging a warning for transparency.

**Technical impact**  
The change makes the test runner more robust by allowing it to proceed when the ROCm-specific patch cannot be applied, accommodating model variants with different attention implementations. It maintains backward compatibility and follows a "fail informatively" approach by warning rather than silently ignoring errors.

**Potential risks**  
The warning could be overlooked in logs, potentially masking other `AttributeError` cases unrelated to `vision_config`. There's also a risk that models missing the patch may exhibit undefined behavior on ROCm platforms, though the impact is limited to specific hardware configurations.

**Key insights**  
This is a pragmatic workaround for model API inconsistencies, but a long-term fix should involve standardizing the attention layer interface in upstream models. Developers should monitor warnings to ensure missing patches don't affect test validity, and consider conditional patching based on model version or attributes to reduce reliance on exception handling.

---

## 9. [Consolidate Intel Quantization Toolkit Integration in vLLM](https://github.com/vllm-project/vllm/pull/31716)


### Base Information

- **PR Number:** #31716
- **Author:** [yiliu30](https://github.com/yiliu30)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-13 23:11:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31716/files) (10):**
  - `docs/features/quantization/README.md`
  - `docs/features/quantization/auto_round.md`
  - `docs/features/quantization/inc.md`
  - `tests/quantization/test_auto_round.py`
  - `vllm/config/model.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/auto_round.py`
  - `vllm/model_executor/layers/quantization/inc.py`
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
This PR consolidates Intel quantization support by removing the standalone AutoRound implementation and routing all Intel quantization through the Intel Neural Compressor (INC) framework. The `auto-round` quantization method now maps to `INCConfig`, and the dedicated `auto_round.py` file has been deleted. The `inc.py` implementation has been expanded to support weight-only quantization schemes (W4A16/W8A16), multiple backends (GPTQ/AWQ/Marlin/IPEX), per-layer configurations, and fp16/bf16 activations.

**Technical impact**  
The changes simplify the quantization architecture by eliminating duplicate code paths and creating a unified interface for Intel quantization. Model loading logic is streamlined by removing special CPU loading for "online" quantization and treating only `gguf` as config-less. The quantization registry now returns `INCConfig` for `auto-round` requests, and documentation has been consolidated to reflect the unified approach.

**Potential risks**  
Existing users of `auto-round` quantization may need to update their configurations since the implementation now routes through INC. The removal of special CPU loading for online quantization could affect performance or compatibility for certain hardware configurations. There's also a risk that the expanded INC implementation may introduce bugs in previously stable AutoRound functionality.

**Key insights**  
Developers should update any code referencing `AutoRoundConfig` to use `INCConfig` instead. The documentation changes provide clearer guidance for Intel quantization usage, but developers should verify that their specific quantization recipes are supported in the consolidated implementation. The addition of `inc` to the override probe order in model configuration ensures backward compatibility for `auto-round` checkpoints.

---

## 10. [[ROCm][CI] Disable Async Scheduling For Qwen3-Next-80B-A3B-Instruct MTP Async EPLB Accuracy Test](https://github.com/vllm-project/vllm/pull/32275)


### Base Information

- **PR Number:** #32275
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-13 21:29:42
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32275/files) (1):**
  - `.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh`

### Summary

**What changed and why**  
The change disables async scheduling specifically for ROCm platforms in the Qwen3-Next-80B-A3B-Instruct MTP Async EPLB accuracy test. This is a temporary workaround because enabling async scheduling by default (via PR #31998) caused the test to hang after processing 140 prompts on AMD CI. The fix adds a `--no-async-scheduling` flag only for ROCm while leaving other platforms unaffected.

**Technical impact**  
This modification localizes the impact to AMD CI by conditionally disabling async scheduling via a platform-specific argument. It ensures the test can proceed without hangs while the root cause is investigated. The change does not affect CUDA or other platforms, maintaining existing behavior elsewhere.

**Potential risks**  
The workaround may mask underlying issues with async scheduling on ROCm, potentially delaying resolution of the actual bug. There is also a risk that other ROCm tests or configurations could encounter similar hangs if they rely on async scheduling. Additionally, disabling async scheduling could affect performance or latency for ROCm in this test scenario.

**Key insights**  
This is a targeted CI fix to unblock testing, not a permanent solution. Developers should prioritize investigating the async scheduling bug on ROCm to re-enable the feature. Consider adding logging or diagnostics to capture the hang's root cause. Ensure any future fixes are validated across both ROCm and CUDA platforms to maintain consistency.

---

## 11. [[Model Runner V2] Refactor Sampler](https://github.com/vllm-project/vllm/pull/32245)


### Base Information

- **PR Number:** #32245
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-13 17:58:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32245/files) (7):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/metadata.py`
  - `vllm/v1/worker/gpu/sample/penalties.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`
  - `vllm/v1/worker/gpu/sample/states.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/states.py`

### Summary

**What changed and why**  
This PR refactors the sampling system by moving sampling-related states (temperature, top_p, penalties, etc.) from `RequestState` to a new `Sampler` class and removes the `SamplingMetadata` data class. The goal is to consolidate sampling logic and state management within the `Sampler`, simplifying the `RequestState` and improving modularity.

**Technical impact**  
The changes centralize sampling state management in the `Sampler`, which now owns `SamplingStates`, `PenaltiesState`, and `LogitBiasState`. This reduces the complexity of `RequestState` and decouples sampling logic from request lifecycle management. The removal of `SamplingMetadata` eliminates a temporary data structure, passing required parameters directly to the sampler.

**Potential risks**  
The refactor introduces new state management classes (`SamplingStates`, `PenaltiesState`) that must be correctly synchronized with request addition/removal. There is a risk of missed updates if `add_request` or `apply_staged_writes` calls are omitted. The `output_bin_counts` tensor remains large (GBs) and could cause memory pressure.

**Key insights**  
Developers should ensure that all sampling parameters are propagated through the new `Sampler.add_request` method. The `apply_staged_writes` pattern is critical for correctness; any new request handling must include these calls. Consider future optimization for the `output_bin_counts` memory usage, as noted in the TODO.

---

## 12. [[Kernel][Performance] Enable smaller Scaling Factor tiling for NVFP4 small-batch decoding](https://github.com/vllm-project/vllm/pull/30885)


### Base Information

- **PR Number:** #30885
- **Author:** [LopezCastroRoberto](https://github.com/LopezCastroRoberto)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 15:22:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30885/files) (9):**
  - `.buildkite/test-pipeline.yaml`
  - `tests/kernels/quantization/nvfp4_utils.py`
  - `tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py`
  - `tests/models/quantization/test_nvfp4.py`
  - `vllm/_custom_ops.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR introduces a new NVFP4 backend variant (`trtllm_8x4_sf`) that uses smaller 8x4 scaling-factor tiling to optimize small-batch decoding workloads. The changes include modifications to quantization utilities, kernel operations, and model execution layers to automatically select this backend when `VLLM_NVFP4_GEMM_BACKEND=flashinfer-trtllm` and batch size ≤ 32, delivering 25–35% higher throughput for small batches.

**Technical impact**  
The changes extend the FlashInfer integration with a new layout option, modify the `scaled_fp4_quant` function to accept a backend parameter for layout selection, and update linear layer implementations to use the appropriate quantization path. This creates a performance-optimized codepath specifically for small-concurrency scenarios without affecting larger batch operations.

**Potential risks**  
The performance gains are highly specific to small batch sizes (≤32), and using this backend for larger batches could degrade performance as shown in the benchmark data. There's a risk of incorrect backend selection if the batch size detection logic fails or if environment variables are misconfigured. The changes also introduce additional complexity in quantization layout handling.

**Key insights**  
Developers should note this is an opt-in optimization requiring explicit environment variable configuration. The backend should only be used for small-batch decoding scenarios, as it becomes suboptimal beyond batch size 32. The implementation maintains backward compatibility by automatically selecting the appropriate layout based on input dimensions and backend configuration.

---

## 13. [[Improvement] Persist CUDA compat libraries paths to prevent reset on `apt-get`](https://github.com/vllm-project/vllm/pull/30784)


### Base Information

- **PR Number:** #30784
- **Author:** [emricksini-h](https://github.com/emricksini-h)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 14:35:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30784/files) (1):**
  - `docker/Dockerfile`

### Summary

**What changed and why**  
The Dockerfile was modified to persist CUDA compatibility library paths by writing them to `/etc/ld.so.conf.d/00-cuda-compat.conf` before running `ldconfig`, instead of only transiently updating the cache. This ensures the configuration survives subsequent `apt-get` operations that might reset the linker cache.

**Technical impact**  
This change makes the CUDA compatibility layer resilient to package management activities, preventing silent fallbacks to host driver versions. It improves container reliability and performance consistency by maintaining the intended CUDA version across image extensions and debugging sessions.

**Potential risks**  
If the CUDA version variable (`CUDA_VERSION`) changes in derived images or future builds, the persisted path may become incorrect. There is also a minor risk of file conflicts if other processes modify `ld.so.conf.d`, though the `00-` prefix helps prioritize this configuration.

**Key insights**  
Always persist system-level configurations like library paths in Docker to avoid transient state issues. Consider validating the CUDA path dynamically if version flexibility is needed, and ensure similar patterns are applied consistently across all relevant Dockerfile stages.

---

## 14. [Add mergify label job for "bug" in PR titles](https://github.com/vllm-project/vllm/pull/31980)


### Base Information

- **PR Number:** #31980
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 14:28:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31980/files) (1):**
  - `.github/mergify.yml`

### Summary

**What changed and why**  
A new Mergify rule named "label-bug" has been added to automatically apply a `bug` label to pull requests. The rule triggers when a PR title contains the case-insensitive words "bug" or "bugfix," provided the PR is not already labeled as "stale."

**Technical impact**  
This change automates the labeling of bug-related PRs within the GitHub workflow, improving issue triage and project management. It integrates seamlessly with the existing Mergify configuration, adding a new automated step to the PR processing pipeline without affecting other rules.

**Potential risks**  
The regex pattern may incorrectly label PRs with titles like "debug" or "bugbear" that are not necessarily bug fixes. There is also a risk of the rule not firing if the title uses synonyms like "fix," "defect," or "issue" without the specified keywords, leading to inconsistent labeling.

**Key insights**  
This automation enhances workflow efficiency but should be monitored for false positives. Consider expanding the condition list to include common synonyms for bugs to improve coverage. Ensure the team is aware of this new automated labeling to prevent confusion in the triage process.

---

## 15. [[Build] Add scripts for cherry-picking and trigger build](https://github.com/vllm-project/vllm/pull/32282)


### Base Information

- **PR Number:** #32282
- **Author:** [simon-mo](https://github.com/simon-mo)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-13 13:21:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32282/files) (2):**
  - `.buildkite/scripts/cherry-pick-from-milestone.sh`
  - `.buildkite/scripts/trigger-ci-build.sh`

### Summary

**What changed and why**  
Two new Bash scripts were added to automate release and CI processes. `cherry-pick-from-milestone.sh` identifies and cherry-picks commits from a GitHub milestone that are missing from the current branch, while `trigger-ci-build.sh` triggers a Buildkite CI build with specific environment variables (RUN_ALL=1, NIGHTLY=1). Both scripts include dry-run modes and comprehensive error handling.

**Technical impact**  
These scripts introduce automation for release management and CI triggering, reducing manual effort and potential human error. They integrate with existing tools (Git, GitHub CLI, Buildkite CLI) and follow best practices with clear usage instructions, color-coded output, and prerequisite checks. The changes are additive and isolated to the `.buildkite/scripts/` directory, minimizing impact on the core codebase.

**Potential risks**  
The cherry-pick script assumes PRs are merged via merge commits and may skip PRs with null SHAs. It also depends on external tools (gh, git) being properly configured and authenticated. The build-trigger script requires the commit to exist on a remote branch, which could fail for local-only commits. Both scripts use `set -euo pipefail`, which may cause abrupt exits if unhandled edge cases arise (e.g., network issues during API calls).

**Key insights**  
These scripts are well-structured with safety-first defaults (dry-run mode) and helpful error messages. Developers should ensure the required CLIs (gh, bk) are installed and authenticated before use. Consider adding integration tests or documenting these scripts in the team’s release/CI playbooks to ensure consistent adoption. For the cherry-pick script, verify milestone naming conventions align with GitHub’s actual milestone titles.

---

## 16. [[Misc] Add In-Container restart capability through supervisord for sagemaker entrypoint](https://github.com/vllm-project/vllm/pull/28502)


### Base Information

- **PR Number:** #28502
- **Author:** [HappyAmazonian](https://github.com/HappyAmazonian)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-13 13:06:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28502/files) (2):**
  - `examples/online_serving/sagemaker-entrypoint.sh`
  - `requirements/common.txt`

### Summary

**What changed and why**  
The PR adds supervisor-based process auto-restart capability to the SageMaker entrypoint by wrapping the `vllm serve` command with `standard-supervisor`. This ensures the server restarts automatically within the container if it exits unexpectedly. The feature is enabled by default but can be disabled via the `PROCESS_AUTO_RECOVERY=false` environment variable.

**Technical impact**  
The change introduces a dependency on `model-hosting-container-standards>=0.1.13` and replaces direct execution with a supervisor-managed process. This adds resilience to the SageMaker deployment by automatically recovering from crashes, but also introduces an additional layer of process management that may affect signal handling and logging.

**Potential risks**  
Supervisor may interfere with graceful shutdown signals (e.g., SIGTERM) if not configured properly, potentially leading to delayed or incomplete termination. The default-enabled auto-restart could mask underlying issues (e.g., memory leaks) by continuously restarting a failing process. Environment variable conflicts may arise if users already have custom supervisor configurations.

**Key insights**  
Developers should verify that supervisor’s signal propagation aligns with vLLM’s shutdown logic. Consider adding documentation about the `PROCESS_AUTO_RECOVERY` toggle and supervisor configuration options. Monitor resource usage in long-running deployments to ensure restarts don’t accumulate resource leaks.

---

## 17. [fix(rocm): Use refresh_env_variables() for rocm_aiter_ops in test_moe](https://github.com/vllm-project/vllm/pull/31711)


### Base Information

- **PR Number:** #31711
- **Author:** [rabi](https://github.com/rabi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-13 11:11:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31711/files) (1):**
  - `tests/kernels/moe/test_moe.py`

### Summary

**What changed and why**  
The PR replaces `importlib.reload()` with `rocm_aiter_ops.refresh_env_variables()` to properly update class-level environment variables in ROCm AITER operations. It also explicitly sets the `VLLM_ROCM_USE_AITER` environment variable based on the test parameter to ensure consistent test behavior independent of external environment state.

**Technical impact**  
This change improves test reliability by ensuring environment variables are correctly refreshed without the side effects of module reloading. The explicit environment variable setting eliminates test flakiness caused by external environment contamination, making test behavior deterministic based on the `use_rocm_aiter` parameter.

**Potential risks**  
The `refresh_env_variables()` method must be properly implemented in `rocm_aiter_ops` to read environment variables at call time. If this method doesn't exist or has different behavior than expected, tests could fail or produce incorrect results. The consolidation of the float32 skip condition could potentially miss edge cases if the logic was previously more complex.

**Key insights**  
The change demonstrates a cleaner approach to managing environment-dependent behavior in tests. Developers should prefer dedicated refresh methods over module reloading when available, as they're more targeted and avoid unintended side effects. The explicit parameter-based environment setup pattern shown here is a best practice for making tests deterministic and isolated.

---

## 18. [[Perf] Optimize grouped topk kernel, 1.2%~2% E2E Throughput improvement](https://github.com/vllm-project/vllm/pull/32058)


### Base Information

- **PR Number:** #32058
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-13 10:58:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32058/files) (2):**
  - `csrc/moe/grouped_topk_kernels.cu`
  - `tests/models/utils.py`

### Summary

**What changed and why**  
The PR fuses the two-step grouped top-k kernel into a single CUDA kernel (`grouped_topk_fused_kernel`), eliminating the intermediate `group_scores` tensor and reducing global memory reads/writes. This optimization targets MoE (Mixture of Experts) routing, specifically improving the performance of expert selection during inference.

**Technical impact**  
The change reduces kernel launch overhead and memory traffic by combining per-group scoring and final top-k selection into one pass. It modifies the kernel launch configuration to use one block per token and one warp per expert group, leveraging dynamic shared memory for WarpSelect staging. The API is simplified by removing the `group_scores` parameter and associated temporary allocations.

**Potential risks**  
The fused kernel relies on correct shared memory layout alignment and warp synchronization; any misalignment could lead to data races or incorrect results. The removal of intermediate buffers assumes the new kernel handles all edge cases (e.g., invalid scores, group bounds) previously validated in separate steps. Tightened input checks may reject previously accepted configurations.

**Key insights**  
The optimization yields measurable throughput gains (1.2–2% E2E) as shown in benchmarks, making it a valuable performance improvement for MoE workloads. Developers should verify that all MoE model configurations (including custom expert counts) pass the updated validation checks. The changes to test utilities ensure compatibility with models like Kimi that use different parameter names (`num_expert_group`, `num_experts_per_token`).

---

## 19. [Fix CUDA 13 wheel installation doc](https://github.com/vllm-project/vllm/pull/32276)


### Base Information

- **PR Number:** #32276
- **Author:** [dmitry-tokarev-nv](https://github.com/dmitry-tokarev-nv)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-13 10:48:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32276/files) (1):**
  - `docs/getting_started/installation/gpu.cuda.inc.md`

### Summary

**What changed and why**  
Updated the CUDA 13 wheel installation documentation to correctly reference the manylinux_2_35 wheels (instead of manylinux_2_31) and to dynamically determine the CPU architecture (x86_64 or aarch64) for the wheel filename. This fixes the installation instructions for v0.13.0+ releases, which now ship wheels built against glibc 2.35 for both architectures.

**Technical impact**  
The change ensures the installation command dynamically constructs the correct wheel URL based on the system's CPU architecture (`uname -m`). This corrects a broken link for aarch64 systems and future-proofs the command for multi-architecture deployments. The documentation now accurately reflects the actual wheel naming scheme used in releases.

**Potential risks**  
The script assumes `uname -m` returns exactly `x86_64` or `aarch64`, which is standard but could fail on exotic or future architectures. There is also a dependency on the external `curl` and `jq` commands being available, though this was already present. The change hardcodes `manylinux_2_35`, which may need updating if the glibc version changes in a future release.

**Key insights**  
This is a necessary documentation fix that aligns the install guide with the actual release artifacts. Developers should verify their target system's architecture and glibc version compatibility. Consider adding a fallback or architecture validation if broader platform support is needed. The fix is minimal and correct for the stated purpose.

---

## 20. [[responseAPI] support partial message generation](https://github.com/vllm-project/vllm/pull/32100)


### Base Information

- **PR Number:** #32100
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [houseroad](https://github.com/houseroad)
- **Merged time:** 2026-01-13 10:41:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32100/files) (3):**
  - `tests/entrypoints/test_responses_utils.py`
  - `vllm/entrypoints/openai/serving_responses.py`
  - `vllm/entrypoints/responses_utils.py`

### Summary

**What changed and why**  
Added support for partial message generation in the Responses API, enabling users to provide incomplete assistant messages (with `in_progress` or `incomplete` status) and have the model continue from that point. This aligns with Anthropic's Messages API behavior and is implemented via a new `should_continue_final_message` utility that detects continuation scenarios and adjusts chat template parameters accordingly.

**Technical impact**  
The changes introduce a new continuation detection mechanism that influences prompt construction. When continuation is triggered, `add_generation_prompt=False` and `continue_final_message=True` are passed to the chat template, altering the generated prompt to append to the existing assistant message rather than starting a new turn. This affects the underlying model's generation behavior and output formatting.

**Potential risks**  
- Edge cases where `status` fields are missing or malformed may lead to incorrect continuation detection.  
- The logic only considers the last item in the input list, which could be problematic if intermediate messages have `in_progress` status.  
- Compatibility risks with chat templates that may not fully support the `continue_final_message` parameter.

**Key insights**  
- Continuation is exclusively determined by the last input item's status and type, ensuring predictable behavior.  
- Developers should ensure that `status` fields are correctly set in both structured objects and dictionary inputs to avoid unintended behavior.  
- The implementation maintains backward compatibility by defaulting to standard generation when no continuation is needed.

---

## 21. [[EPLB][Cleanup] Remove `is_async_enabled` from `EplbModelState`](https://github.com/vllm-project/vllm/pull/32050)


### Base Information

- **PR Number:** #32050
- **Author:** [SageMoore](https://github.com/SageMoore)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-13 10:19:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32050/files) (2):**
  - `vllm/distributed/eplb/async_worker.py`
  - `vllm/distributed/eplb/eplb_state.py`

### Summary

**What changed and why**  
This PR removes the redundant `is_async_enabled` flag from `EplbModelState` and consolidates async state management into the single `is_async` flag in `EplbState`. The change eliminates duplication that could lead to inconsistencies and simplifies async logic by ensuring all async checks reference the same source of truth. Additionally, it tightens the async worker API by making `cuda_stream` a required parameter and adds assertions to enforce async state validity.

**Technical impact**  
The codebase now has a single source of truth for async EPLB state, reducing complexity and potential bugs. Async behavior is consistently gated by `EplbState.is_async`, and the removal of per-model async flags streamlines conditionals in `step()` and `rearrange()`. The async worker now explicitly requires a `cuda_stream` and validates async state upfront, improving robustness.

**Potential risks**  
If any code paths still rely on the removed `is_async_enabled` variable (e.g., in other modules or configurations), they could break. The new `assert state.is_async` in async paths may raise errors if async workflows are incorrectly initialized. The removal of per-model async checks assumes all models in a state share the same async configuration, which should be validated.

**Key insights**  
Consolidating state flags reduces maintenance overhead and prevents subtle bugs. Developers should ensure all async-related configurations are set at the `EplbState` level. The added assertions provide immediate feedback for misconfigured async usage, aiding debugging. Future changes should avoid reintroducing redundant state variables.

---

## 22. [[Trivial] Remove duplicate enable_mfu_metrics](https://github.com/vllm-project/vllm/pull/32246)


### Base Information

- **PR Number:** #32246
- **Author:** [markmc](https://github.com/markmc)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-13 09:09:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32246/files) (1):**
  - `vllm/config/observability.py`

### Summary

**What changed and why**  
Removed a duplicate `enable_mfu_metrics` configuration field from the observability configuration class. This field was likely introduced accidentally during a previous merge conflict resolution (PR #29105).

**Technical impact**  
This change eliminates a redundant configuration parameter, ensuring the configuration schema remains clean and consistent. It prevents potential confusion or errors if the duplicate field had conflicting values with another instance elsewhere in the codebase.

**Potential risks**  
Low risk, as this appears to be a straightforward cleanup. However, ensure no other parts of the codebase depend on this specific field instance—though unlikely, references to it could now cause AttributeErrors.

**Key insights**  
Always verify merge conflict resolutions to avoid introducing duplicates or inconsistencies. Consider adding a linter or test to detect duplicate class attributes in configuration files to prevent similar issues in the future.

---

## 23. [[4/N][Attention] Move MLA common to model_executor](https://github.com/vllm-project/vllm/pull/32060)


### Base Information

- **PR Number:** #32060
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 09:08:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32060/files) (14):**
  - `tests/v1/attention/test_mla_backends.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py`
  - `vllm/logging_utils/formatter.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/v1/attention/backends/mla/cutlass_mla.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/attention/backends/mla/flashinfer_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/backends/mla/triton_mla.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
This PR relocates MLA (Multi-Head Latent Attention) common utilities from `vllm/v1/attention/backends/mla/common.py` to `vllm/model_executor/layers/attention/mla_attention.py`. The change is part of a larger refactoring effort (#31919) to centralize shared MLA logic under the `model_executor` directory, improving code organization and reducing duplication across attention backends.

**Technical impact**  
The refactor consolidates MLA-related classes (`MLACommonBackend`, `MLACommonMetadata`, `QueryLenSupport`, etc.) into a single module, making them easier to maintain and import. All MLA backends (e.g., `cutlass_mla`, `flashmla`), distributed KV connectors, and spec decode components now reference the new location. This structural change enhances modularity and aligns with the project's architectural layering.

**Potential risks**  
While primarily a code move, there is a risk of missed import updates in less frequently used code paths or third-party extensions. The PR relies on CI for validation, but manual verification of all dependent modules is advisable to prevent runtime import errors. Additionally, any downstream code that directly referenced the old path will break unless updated.

**Key insights**  
Developers should update any local or external code that imports MLA common utilities from the old path. The change simplifies future MLA feature development by centralizing shared logic. Ensure all team members are aware of the new import location to avoid inconsistencies in future contributions.

---

## 24. [nixl_connector: export UCX_MEM_MMAP_HOOK_MODE=none to avoid a UCX memory leak](https://github.com/vllm-project/vllm/pull/32181)


### Base Information

- **PR Number:** #32181
- **Author:** [hasB4K](https://github.com/hasB4K)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-13 08:21:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32181/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
The change adds environment variable configuration to mitigate a UCX memory leak when using NIXL. Before importing NIXL bindings, it checks if `UCX_MEM_MMAP_HOOK_MODE` is unset and sets it to `'none'`, which prevents UCX from installing mmap hooks that cause the leak. If NIXL was already imported, it logs a warning since the fix cannot be applied retroactively.

**Technical impact**  
This proactively addresses a known memory leak in UCX when NIXL is used, particularly for certain models. The fix is applied at module import time, ensuring the environment variable is set before any UCX initialization occurs through NIXL. It maintains backward compatibility by only modifying the environment if the variable isn't already set and preserves existing import logic for ROCm vs. CUDA paths.

**Potential risks**  
If NIXL or RIXL modules were imported earlier (e.g., by other code), the warning is logged but the leak may still occur, requiring manual intervention. There's a minor risk of interfering with other UCX-dependent components that rely on mmap hooks, though setting the variable only when unset minimizes this. The fix assumes the leak is specific to UCX's mmap hooks and that disabling them is safe for NIXL usage.

**Key insights**  
Developers should ensure NIXL imports happen after this module loads to guarantee the fix applies. Consider documenting this requirement or adding checks in other entry points that might import NIXL earlier. Monitoring memory usage in deployments using NIXL is still advised to confirm the mitigation works. The change is low-risk and focused, but team awareness of the UCX leak context is helpful for troubleshooting.

---

## 25. [[BugFix] [KVConnector] Fix KV events for LMCache connector](https://github.com/vllm-project/vllm/pull/32169)


### Base Information

- **PR Number:** #32169
- **Author:** [hickeyma](https://github.com/hickeyma)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-13 07:50:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32169/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py`

### Summary

**What changed and why**  
The fix modifies the LMCache connector to handle KV cache events that do not yet support the `lora_name` attribute. Previously, accessing `e.lora_name` directly caused an `AttributeError` when LMCache events omitted this field. The change uses `getattr(e, "lora_name", None)` to safely retrieve the attribute, defaulting to `None` if it is missing.

**Technical impact**  
This ensures compatibility between vLLM's KV event processing and the current LMCache event schema, preventing runtime crashes when KV events are enabled. The connector now gracefully handles both present and absent `lora_name` attributes, maintaining backward compatibility while aligning with LMCache's evolving API.

**Potential risks**  
If LMCache later introduces `lora_name` support, the `getattr` approach will still work but may mask schema mismatches if the attribute type changes. Additionally, other event attributes could be missing in future LMCache updates, requiring similar defensive checks to avoid new `AttributeError` instances.

**Key insights**  
Always validate external API dependencies defensively, especially when integrating with third-party services like LMCache. Consider adding a broader validation layer for event schemas to proactively catch compatibility issues. This fix is minimal and targeted, but similar patterns should be applied to other event attributes to future-proof the connector.

---

## 26. [[Refactor] [7/N] to simplify the vLLM lora serving architecture](https://github.com/vllm-project/vllm/pull/32251)


### Base Information

- **PR Number:** #32251
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-13 07:37:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32251/files) (5):**
  - `tests/entrypoints/openai/test_serving_models.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/serving_models.py`
  - `vllm/entrypoints/serve/lora/api_router.py`
  - `vllm/entrypoints/serve/lora/protocol.py`

### Summary

**What changed and why**  
This PR centralizes LoRA-related protocol definitions by moving `LoadLoRAAdapterRequest` and `UnloadLoRAAdapterRequest` Pydantic models from `vllm/entrypoints/openai/engine/protocol.py` to a new dedicated file `vllm/entrypoints/serve/lora/protocol.py`. Import statements in dependent files are updated accordingly to reference the new location.

**Technical impact**  
The change improves code organization by separating LoRA-specific protocol definitions from general OpenAI engine protocols. This creates a cleaner architectural separation where LoRA serving logic has its own protocol module, making the codebase more modular and maintainable. The functionality remains unchanged as only import paths are modified.

**Potential risks**  
There's minimal risk since this is purely a refactoring of import statements and file organization. However, any missed import updates could cause runtime errors. The changes affect multiple entry points (OpenAI serving, models API, and test files), so thorough testing of LoRA loading/unloading functionality is recommended.

**Key insights**  
This refactoring is part of a larger effort to simplify vLLM's LoRA serving architecture (indicated by "[7/N]" in the PR title). Developers should ensure all LoRA-related protocol imports now reference the new dedicated module. The PR description lacks test details, so manual verification of LoRA operations across all modified entry points is advised before merging.

---

## 27. [[Refactor] Remove `MultiModalProfiler`](https://github.com/vllm-project/vllm/pull/32254)


### Base Information

- **PR Number:** #32254
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-13 07:10:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32254/files) (4):**
  - `tests/models/multimodal/processing/test_mllama4.py`
  - `tests/multimodal/test_processing.py`
  - `vllm/multimodal/profiling.py`
  - `vllm/multimodal/registry.py`

### Summary

**What changed and why**  
The PR removes the `MultiModalProfiler` class, moving its two key methods (`get_decoder_dummy_data` and `get_mm_max_tokens`) to `DummyInputsBuilder` and `MultiModalRegistry` respectively. This simplifies the architecture by eliminating an unnecessary abstraction layer.

**Technical impact**  
The changes streamline the multi-modal profiling logic by directly exposing functionality through the processor's `dummy_inputs` builder and the registry. This reduces indirection and makes the flow more linear—profiling now directly uses processor components without an intermediate profiler wrapper.

**Potential risks**  
There is a risk of breaking external code that depended on the `MultiModalProfiler` interface. Additionally, the refactored `get_mm_max_tokens` logic in `MultiModalRegistry` must correctly replicate the original behavior, especially around `mm_counts` handling and `max_tokens_per_item` fallback.

**Key insights**  
This refactor improves code clarity by removing redundant abstraction. Developers should update any references to `MultiModalProfiler` to use the processor's `dummy_inputs` or the registry directly. Ensure thorough testing of multi-modal model initialization and profiling paths to confirm no regression in token calculation or dummy data generation.

---

## 28. [[6/N][Attention] Move utils to more appropriate locations](https://github.com/vllm-project/vllm/pull/32215)


### Base Information

- **PR Number:** #32215
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-13 05:38:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32215/files) (14):**
  - `tests/v1/attention/test_attention_splitting.py`
  - `vllm/model_executor/layers/attention/chunked_local_attention.py`
  - `vllm/model_executor/layers/attention/cross_attention.py`
  - `vllm/model_executor/layers/attention/encoder_only_attention.py`
  - `vllm/model_executor/layers/attention/static_sink_attention.py`
  - `vllm/model_executor/models/whisper_utils.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/ubatch_utils.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
This PR reorganizes utility functions to improve code organization and ownership. Functions related to attention backend subclassing (`subclass_attention_backend`, `subclass_attention_backend_with_overrides`) were moved from `v1/attention/backends/utils.py` to `v1/attention/backend.py`, while functions for splitting attention metadata across microbatches (`_make_metadata_with_slice`, `slice_query_start_locs`, `split_attn_metadata`) were moved to `v1/worker/ubatch_utils.py`. This creates clearer logical grouping: backend utilities stay with backend definitions, and ubatch utilities stay with ubatch logic.

**Technical impact**  
The changes improve code maintainability by grouping related functionality together. Import statements were updated across 14 files to reflect the new locations. The `split_decodes_and_prefills` function remains in its original location since it's more closely tied to attention backend operations. A new test file was added to ensure proper coverage of the moved functions.

**Potential risks**  
Import changes could break downstream code that hasn't been updated, though the PR appears comprehensive. The tensor cloning operations in `_make_metadata_with_slice` could impact CUDA graph compatibility as noted in the comments. There's a risk of circular imports if the new locations create dependency cycles between modules.

**Key insights**  
This is part of a larger refactoring effort (step 6 of #31919) to improve code organization. Developers should verify all imports are updated in their local codebases. The separation creates cleaner boundaries between attention backend logic and worker-specific ubatch operations, which should make future modifications easier to reason about.

---

## 29. [[Refactor] [6/N] to simplify the vLLM openai chat_completion serving architecture](https://github.com/vllm-project/vllm/pull/32240)


### Base Information

- **PR Number:** #32240
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-13 05:01:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32240/files) (128):**
  - `tests/entrypoints/openai/responses/test_errors.py`
  - `tests/entrypoints/openai/responses/test_function_call_parsing.py`
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_chat_template.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_gptoss_structural_tags_integration.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `tests/entrypoints/openai/test_protocol.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/entrypoints/openai/test_serving_chat_stream_harmony.py`
  - `tests/entrypoints/openai/test_serving_engine.py`
  - `tests/entrypoints/openai/test_serving_models.py`
  - `tests/entrypoints/openai/test_serving_responses.py`
  - `tests/entrypoints/openai/tool_parsers/test_gigachat3_tool_parser.py`
  - `tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py`
  - `tests/entrypoints/openai/tool_parsers/test_hunyuan_a13b_tool_parser.py`
  - `tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py`
  - `tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py`
  - `tests/entrypoints/openai/tool_parsers/test_olmo3_tool_parser.py`
  - `tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py`
  - `tests/entrypoints/openai/tool_parsers/utils.py`
  - `tests/entrypoints/openai/utils.py`
  - `tests/reasoning/test_base_thinking_reasoning_parser.py`
  - `tests/reasoning/test_deepseekv3_reasoning_parser.py`
  - `tests/reasoning/utils.py`
  - `tests/tool_parsers/test_ernie45_moe_tool_parser.py`
  - `tests/tool_parsers/test_functiongemma_tool_parser.py`
  - `tests/tool_parsers/test_glm4_moe_tool_parser.py`
  - `tests/tool_parsers/test_jamba_tool_parser.py`
  - `tests/tool_parsers/test_kimi_k2_tool_parser.py`
  - `tests/tool_parsers/test_minimax_tool_parser.py`
  - `tests/tool_parsers/test_mistral_tool_parser.py`
  - `tests/tool_parsers/test_openai_tool_parser.py`
  - `tests/tool_parsers/test_qwen3coder_tool_parser.py`
  - `tests/tool_parsers/test_seed_oss_tool_parser.py`
  - `tests/tool_parsers/test_xlam_tool_parser.py`
  - `tests/tool_use/test_chat_completion_request_validations.py`
  - `tests/tool_use/test_tool_choice_required.py`
  - `tests/v1/engine/test_async_llm.py`
  - `vllm/entrypoints/anthropic/serving_messages.py`
  - `vllm/entrypoints/context.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/chat_completion/__init__.py`
  - `vllm/entrypoints/openai/chat_completion/api_router.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/chat_completion/stream_harmony.py`
  - `vllm/entrypoints/openai/engine/__init__.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`
  - `vllm/entrypoints/openai/parser/responses_parser.py`
  - `vllm/entrypoints/openai/run_batch.py`
  - `vllm/entrypoints/openai/serving_completion.py`
  - `vllm/entrypoints/openai/serving_models.py`
  - `vllm/entrypoints/openai/serving_responses.py`
  - `vllm/entrypoints/openai/serving_transcription.py`
  - `vllm/entrypoints/openai/speech_to_text.py`
  - `vllm/entrypoints/openai/utils.py`
  - `vllm/entrypoints/pooling/classify/api_router.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/classify/serving.py`
  - `vllm/entrypoints/pooling/embed/api_router.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/api_router.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/pooling/score/api_router.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/responses_utils.py`
  - `vllm/entrypoints/sagemaker/routes.py`
  - `vllm/entrypoints/serve/disagg/api_router.py`
  - `vllm/entrypoints/serve/disagg/protocol.py`
  - `vllm/entrypoints/serve/disagg/serving.py`
  - `vllm/entrypoints/serve/elastic_ep/api_router.py`
  - `vllm/entrypoints/serve/lora/api_router.py`
  - `vllm/entrypoints/serve/tokenize/api_router.py`
  - `vllm/entrypoints/serve/tokenize/protocol.py`
  - `vllm/entrypoints/serve/tokenize/serving.py`
  - `vllm/entrypoints/utils.py`
  - `vllm/reasoning/abs_reasoning_parsers.py`
  - `vllm/reasoning/basic_parsers.py`
  - `vllm/reasoning/deepseek_r1_reasoning_parser.py`
  - `vllm/reasoning/deepseek_v3_reasoning_parser.py`
  - `vllm/reasoning/ernie45_reasoning_parser.py`
  - `vllm/reasoning/gptoss_reasoning_parser.py`
  - `vllm/reasoning/granite_reasoning_parser.py`
  - `vllm/reasoning/holo2_reasoning_parser.py`
  - `vllm/reasoning/hunyuan_a13b_reasoning_parser.py`
  - `vllm/reasoning/identity_reasoning_parser.py`
  - `vllm/reasoning/minimax_m2_reasoning_parser.py`
  - `vllm/reasoning/mistral_reasoning_parser.py`
  - `vllm/reasoning/olmo3_reasoning_parser.py`
  - `vllm/reasoning/qwen3_reasoning_parser.py`
  - `vllm/reasoning/step3_reasoning_parser.py`
  - `vllm/tokenizers/mistral.py`
  - `vllm/tool_parsers/abstract_tool_parser.py`
  - `vllm/tool_parsers/deepseekv31_tool_parser.py`
  - `vllm/tool_parsers/deepseekv32_tool_parser.py`
  - `vllm/tool_parsers/deepseekv3_tool_parser.py`
  - `vllm/tool_parsers/ernie45_tool_parser.py`
  - `vllm/tool_parsers/functiongemma_tool_parser.py`
  - `vllm/tool_parsers/gigachat3_tool_parser.py`
  - `vllm/tool_parsers/glm4_moe_tool_parser.py`
  - `vllm/tool_parsers/granite_20b_fc_tool_parser.py`
  - `vllm/tool_parsers/granite_tool_parser.py`
  - `vllm/tool_parsers/hermes_tool_parser.py`
  - `vllm/tool_parsers/hunyuan_a13b_tool_parser.py`
  - `vllm/tool_parsers/internlm2_tool_parser.py`
  - `vllm/tool_parsers/jamba_tool_parser.py`
  - `vllm/tool_parsers/kimi_k2_tool_parser.py`
  - `vllm/tool_parsers/llama4_pythonic_tool_parser.py`
  - `vllm/tool_parsers/llama_tool_parser.py`
  - `vllm/tool_parsers/minimax_m2_tool_parser.py`
  - `vllm/tool_parsers/minimax_tool_parser.py`
  - `vllm/tool_parsers/mistral_tool_parser.py`
  - `vllm/tool_parsers/olmo3_tool_parser.py`
  - `vllm/tool_parsers/openai_tool_parser.py`
  - `vllm/tool_parsers/phi4mini_tool_parser.py`
  - `vllm/tool_parsers/pythonic_tool_parser.py`
  - `vllm/tool_parsers/qwen3coder_tool_parser.py`
  - `vllm/tool_parsers/qwen3xml_tool_parser.py`
  - `vllm/tool_parsers/seed_oss_tool_parser.py`
  - `vllm/tool_parsers/step3_tool_parser.py`
  - `vllm/tool_parsers/utils.py`
  - `vllm/tool_parsers/xlam_tool_parser.py`

### Summary

**What changed and why**  
This PR refactors the OpenAI chat completion serving architecture by splitting the monolithic `vllm/entrypoints/openai/protocol.py` into modular components. The main change is the creation of a new `chat_completion` subdirectory containing dedicated protocol and serving modules, while moving shared protocol definitions to an `engine` subdirectory. This is part 6 of a series aimed at simplifying the architecture.

**Technical impact**  
The refactoring improves code organization by separating chat‑completion‑specific logic from general‑purpose protocol definitions. This reduces coupling and makes the codebase more maintainable. The changes are largely structural—renaming imports and moving files—so functional behavior should remain unchanged. However, the split introduces new import paths that all dependent modules must now use.

**Potential risks**  
- Import errors may arise if any module still references the old `protocol.py` path. The PR description notes that compatibility with previous imports is a TODO item.  
- The large number of file changes (128 files) increases the risk of missing an update in a less‑frequently used module.  
- The refactoring is part of a multi‑step series; intermediate states could break integrations that depend on the exact structure.

**Key insights**  
- Developers must update all imports related to chat completion protocols to use the new `vllm.entrypoints.openai.chat_completion.protocol` path.  
- The `engine/protocol.py` now holds shared definitions (e.g., `ErrorResponse`, `UsageInfo`), promoting reuse across different serving endpoints.  
- Verify that the TODO items in the PR description—especially completion_serving, responses_serving, and transcription_serving splits—are addressed in subsequent PRs to avoid partial migration.

---

## 30. [[Quantization] fix: overflow with static per-tensor scaling](https://github.com/vllm-project/vllm/pull/29867)


### Base Information

- **PR Number:** #29867
- **Author:** [mickaelseznec](https://github.com/mickaelseznec)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-13 04:56:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29867/files) (2):**
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`
  - `vllm/v1/attention/backends/mla/common.py`

### Summary

**What changed and why**  
This PR fixes an overflow issue in FP8 weight dequantization when using static per-tensor scaling with the `eye` method. The solution centralizes dequantization logic by adding `get_and_maybe_dequant_weights` in `quant_utils.py`, which handles FP8 tensor/block scales directly and only falls back to the `eye`-based path as a generic case. The MLA attention backend is updated to use this utility, removing duplicate ad-hoc code.

**Technical impact**  
The changes consolidate weight dequantization into a single utility, improving maintainability and ensuring safe handling of FP8 static scaling. The `scaled_dequantize` function now returns a single tensor, simplifying its API. The MLA backend now relies on the centralized logic, reducing code duplication and ensuring consistent behavior across quantization methods.

**Potential risks**  
The `eye`-based fallback remains O(N³) and is intended for offline use only; misuse in online scenarios could cause performance issues. There is a risk of regression if the new utility does not correctly handle all quantization variants (e.g., Marlin or other non-FP8 methods). The removal of custom logic in MLA may affect edge cases not covered by the generic utility.

**Key insights**  
Developers should use `get_and_maybe_dequant_weights` for all weight dequantization needs to ensure safe handling of FP8 scaling. The `eye` fallback should be avoided in performance-critical paths. Testing should verify that all quantization configurations (including block and per-tensor scaling) work correctly with the new utility.

---

## 31. [[Docs] Nixl Usage recommend `fail` kv_load_failure_policy](https://github.com/vllm-project/vllm/pull/32198)


### Base Information

- **PR Number:** #32198
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-01-13 04:51:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32198/files) (1):**
  - `docs/features/nixl_connector_usage.md`

### Summary

**What changed and why**  
This documentation update adds `"kv_load_failure_policy":"fail"` to all NixlConnector example configurations and introduces a new section explaining the policy. The change emphasizes that `fail` is the recommended setting over the default `recompute` to avoid performance degradation when KV cache transfers fail.

**Technical impact**  
No code changes were made, so system behavior remains unchanged. However, the documentation now guides users toward a configuration that prevents decode instances from inefficiently recomputing prefilled KV cache blocks, which could otherwise cause latency spikes and resource contention.

**Potential risks**  
Users who rely on the default `recompute` behavior without reading the updated docs may continue to experience performance jitter. Additionally, if network reliability issues cause frequent KV load failures, setting `fail` could increase request error rates unless the underlying transfer reliability is improved.

**Key insights**  
Always configure `kv_load_failure_policy="fail"` in production to maintain consistent decode performance and avoid prefill work interfering with decode-optimized instances. This aligns with the disaggregated architecture's goal of separating prefill and decode workloads. Ensure network stability between prefiller and decoder instances to minimize failures.

---

## 32. [[Bugfix] Replace `PoolingParams.normalize` with `use_activation`](https://github.com/vllm-project/vllm/pull/32243)


### Base Information

- **PR Number:** #32243
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-13 02:45:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32243/files) (21):**
  - `examples/pooling/embed/openai_embedding_long_text/README.md`
  - `examples/pooling/embed/openai_embedding_long_text/client.py`
  - `examples/pooling/embed/openai_embedding_long_text/service.sh`
  - `tests/entrypoints/pooling/embed/test_offline.py`
  - `tests/entrypoints/pooling/embed/test_online_long_text.py`
  - `tests/entrypoints/pooling/score/test_online_score.py`
  - `tests/model_executor/test_model_load_with_params.py`
  - `tests/models/language/pooling/test_pooler_config_init_behaviour.py`
  - `tests/test_config.py`
  - `tests/test_pooling_params.py`
  - `vllm/config/pooler.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/model_executor/layers/pooler/seqwise/heads.py`
  - `vllm/model_executor/layers/pooler/seqwise/poolers.py`
  - `vllm/model_executor/layers/pooler/tokwise/heads.py`
  - `vllm/model_executor/layers/pooler/tokwise/poolers.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/pooling_params.py`
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
This PR replaces the deprecated `normalize` parameter with `use_activation` across the codebase to fix test failures introduced by a previous change (#32148). The change standardizes parameter naming for activation functions in pooling operations, deprecating `normalize` for embedding tasks and unifying it with the `use_activation` parameter used for classification tasks.

**Technical impact**  
The changes affect pooling configuration handling across embedding, classification, and scoring tasks. The `PoolingParams` class now uses `use_activation` as the unified parameter, with `normalize`, `softmax`, and `activation` marked as deprecated. Backward compatibility is maintained through the `get_use_activation()` helper function that translates deprecated parameters while emitting deprecation warnings.

**Potential risks**  
The parameter renaming could break external integrations that still use the deprecated `normalize` parameter, though warnings are provided. There's a risk of confusion during the transition period where both parameter names exist in documentation and code examples. The test changes show some logic restructuring in scoring tests that could introduce subtle behavioral differences if not thoroughly validated.

**Key insights**  
Developers should immediately update their code to use `use_activation` instead of `normalize` for embedding tasks. The deprecation warnings indicate removal in v0.15, so timely migration is essential. Pay special attention to the scoring test logic changes, which now handle activation parameters differently for cross-encoder vs. other models.

---

## 33. [[Refactor] Remove `get_encoder_dummy_data`](https://github.com/vllm-project/vllm/pull/32241)


### Base Information

- **PR Number:** #32241
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-13 01:21:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32241/files) (6):**
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/processing.py`
  - `vllm/multimodal/profiling.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This refactor removes the unused `get_encoder_dummy_data` method and related infrastructure from the multimodal profiling system, as it's not needed for V1. The `pad_dummy_encoder_prompt` property is migrated to `skip_prompt_length_check` on the `BaseProcessingInfo` class, allowing length check skipping without instantiating a full processor.

**Technical impact**  
The changes simplify the multimodal architecture by removing dead code paths for encoder dummy data generation. The new `skip_prompt_length_check` property provides a cleaner abstraction for models like Whisper and Nemotron that need to bypass prompt length validation due to encoder prompt padding requirements. This reduces processor instantiation overhead during input validation.

**Potential risks**  
The removal assumes `get_encoder_dummy_data` is truly unused in V1—any remaining dependencies could cause runtime errors. The refactored length check logic now relies on `skip_prompt_length_check` being correctly implemented by all multimodal models; missing implementations could cause validation failures for models that previously relied on the old property.

**Key insights**  
This is a clean-up PR that reduces code complexity while maintaining functionality. Developers should verify no other code paths depend on the removed methods, particularly in profiling or testing contexts. The new property-based approach is more efficient and follows better separation of concerns between processing logic and validation.

---

## 34. [[Model] Use mm_position to compute mrope positions for Qwen2-VL/2.5-VL](https://github.com/vllm-project/vllm/pull/32126)


### Base Information

- **PR Number:** #32126
- **Author:** [YunzhuLu](https://github.com/YunzhuLu)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-13 01:04:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32126/files) (2):**
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`

### Summary

**What changed and why**  
The PR optimizes M-RoPE position computation for Qwen2-VL and Qwen2.5-VL models by leveraging pre‑parsed multimodal features (`mm_features`) and replacing token‑scanning logic with vectorized NumPy operations. A new helper `iter_mm_grid_thw` extracts grid dimensions and temporal scaling factors directly from `mm_features` using `mm_position.offset`, simplifying the position‑id generation.

**Technical impact**  
The changes replace sequential token‑based scanning and manual torch index construction with offset‑based segmentation and NumPy vectorization (`np.indices`, `np.broadcast_to`). This reduces algorithmic complexity, improves performance, and ensures consistent handling of image and video modalities. The final positions are converted back to torch tensors, maintaining compatibility with downstream components.

**Potential risks**  
Introducing NumPy adds a dependency that may affect environments where NumPy is not available or where GPU‑to‑CPU data transfer becomes a bottleneck. The assumption that `mm_features` are sorted by `offset` could break if inputs are unordered. Additionally, the integer casting after temporal scaling (`t_factor`) might cause precision loss if `t_factor` is not integer‑valued.

**Key insights**  
The refactor significantly cleans up the code by centralizing grid‑iteration logic and eliminating redundant token‑matching loops. Developers should verify that `mm_features` are always provided in the expected order and that `t_factor` scaling produces correct integer indices. Consider adding a fallback or warning for non‑integer `t_factor` values to avoid silent errors.

---

