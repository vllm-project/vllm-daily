# vLLM Merged PR Report

**Report Date:** 2026-01-27 PST

**Total Merged PRs:** 35

---

## 1. [[XPU]disable test_acceptance_length UT](https://github.com/vllm-project/vllm/pull/33226)


### Base Information

- **PR Number:** #33226
- **Author:** [yma11](https://github.com/yma11)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-01-27 23:24:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33226/files) (1):**
  - `.buildkite/scripts/hardware_ci/run-xpu-test.sh`

### Summary

**What changed and why**  
The PR disables the `test_acceptance_length.py` unit test in the XPU CI pipeline by adding it to the `--ignore` list in the test execution script. This is a temporary workaround because the test fails on the PVC platform due to a Triton kernel compilation error related to excessive scratch space usage in the `sample_recovered_tokens_kernel`.

**Technical impact**  
This change only affects the XPU CI test suite, preventing a specific failing test from running. It does not modify any production code, so system behavior remains unchanged. The test will continue to run on other platforms (e.g., GPU) unless they also hit the scratch space limit.

**Potential risks**  
Disabling the test means that any regressions related to acceptance length functionality on XPU hardware will go undetected. If the underlying kernel issue is not resolved, it could affect production performance or correctness when using spec-decode on XPU. There is also a risk that the test may be forgotten and remain disabled indefinitely.

**Key insights**  
This is a temporary fix that should be accompanied by a tracking issue to address the root cause—either optimizing the kernel's scratch memory usage or increasing the platform's limit. Developers should verify if similar issues exist in other spec-decode tests and consider adding a skip decorator in the test file itself for better visibility.

---

## 2. [[Docs] Simplify CPU x86 Docker build documentation](https://github.com/vllm-project/vllm/pull/33071)


### Base Information

- **PR Number:** #33071
- **Author:** [maryamtahhan](https://github.com/maryamtahhan)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-27 22:37:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33071/files) (1):**
  - `docs/getting_started/installation/cpu.x86.inc.md`

### Summary

**What changed and why**  
The PR simplifies the x86 CPU Docker build documentation by streamlining the presentation of build commands and reducing verbosity. It consolidates all available build arguments into a single command block upfront, condenses the auto-detection explanation, and pares down the examples to three essential scenarios: default auto-detection, AVX512 cross-compilation, and AVX2 cross-compilation.

**Technical impact**  
This is a documentation-only change with no impact on the actual build system, codebase, or runtime behavior. The modifications improve readability and reduce cognitive load for users by presenting the information more concisely, which should lead to better developer experience when configuring CPU-specific builds.

**Potential risks**  
The simplification removes some explanatory context, such as the explicit mention that cross-compilation is for building "on systems that don't have the target platforms ISA." This could slightly increase the chance of user misunderstanding, though the core information is preserved. The removal of tables and detailed descriptions is generally safe for this technical audience.

**Key insights**  
The documentation now follows a more direct, reference-style format that prioritizes the essential command syntax and common use cases. Developers should appreciate the cleaner structure, but should be aware that the auto-detection note is now more concise. When cross-compiling, they must explicitly set the relevant `VLLM_CPU_*` build arguments to `true`.

---

## 3. [[ROCm] Enabling forward_includes_kv_cache on ROCm MHA backends](https://github.com/vllm-project/vllm/pull/33106)


### Base Information

- **PR Number:** #33106
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-27 22:36:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33106/files) (3):**
  - `vllm/v1/attention/backends/rocm_aiter_unified_attn.py`
  - `vllm/v1/attention/backends/rocm_attn.py`
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
The changes enable `forward_includes_kv_cache_update` support for three ROCm MHA backends (`RocmAiterUnifiedAttention`, `RocmAttention`, and `TritonAttention`). This involves refactoring the KV cache update logic from the `forward` method into a separate `do_kv_cache_update` method, aligning with a broader architectural change (PR #32335) to decouple cache updates from attention computation.

**Technical impact**  
These modifications standardize the backend interface by introducing a dedicated method for KV cache updates, improving code modularity and maintainability. The attention computation (`forward`) now focuses solely on the attention operation, while cache management is handled separately, enabling more flexible scheduling and potential optimizations. The changes also ensure consistency across different backends by adopting a uniform pattern.

**Potential risks**  
There is a risk of introducing regressions if the `do_kv_cache_update` method is not called correctly in the new execution flow, especially for edge cases like cross-attention or KV cache sharing. Additionally, the FP8 dtype handling in `TritonAttention` requires careful validation, as noted in the comments about unsupported explicit casts for certain data types.

**Key insights**  
Developers should verify that all callers of these backends now invoke `do_kv_cache_update` appropriately. The refactoring enhances separation of concerns but necessitates thorough integration testing to ensure cache updates occur in the correct order and context. Special attention should be paid to the `kv_sharing_target_layer_name` logic and FP8 support in Triton kernels.

---

## 4. [Adds FunAudioChat multimodal audio model support (#2)](https://github.com/vllm-project/vllm/pull/33058)


### Base Information

- **PR Number:** #33058
- **Author:** [nemoramo](https://github.com/nemoramo)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-27 21:18:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33058/files) (8):**
  - `examples/offline_inference/audio_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/funaudiochat.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/multimodal/audio.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/funaudiochat.py`

### Summary

**What changed and why**  
This PR adds support for the FunAudioChat multimodal audio-to-text model in vLLM. It implements the model architecture, registers it in vLLM's model registry, enhances audio processing with resampling and fallback handling, and provides an offline inference example. The changes enable loading and running FunAudioChat models through vLLM's engine.

**Technical impact**  
The integration extends vLLM's multimodal capabilities to include audio‑to‑text models. It introduces a new model executor (`funaudiochat.py`) that handles continuous and discrete audio encoders, integrates with vLLM's multimodal processing pipeline, and adds configuration shims for Transformers compatibility. The audio processor now supports flexible input formats (raw waveforms or (audio, sampling_rate) tuples) and includes validation to avoid unsupported WAV formats.

**Potential risks**  
- The model relies on optional dependencies (e.g., `flash_attn` for long‑audio performance); missing dependencies may cause runtime errors.  
- The configuration uses a temporary shim (`funaudiochat.py` in configs) because Transformers lacks native support; this may need updates when Transformers adds official support.  
- Audio resampling logic includes a tolerance check (`math.isclose`), which could introduce subtle bugs if the tolerance is too strict or loose for certain sampling rates.

**Key insights**  
- Developers must provide a local model path via `--model` when using FunAudioChat, as no public Hugging Face repo exists yet.  
- The implementation reuses vLLM's existing multimodal infrastructure (e.g., `MMEncoderAttention`, `MultiModalDataParser`), ensuring consistency with other multimodal models.  
- The weight‑loading logic handles GPTQ‑style biases correctly by skipping missing bias parameters, which is important for quantization compatibility.

---

## 5. [[Bugfix] Lazy import NgramProposer in GPU model runner](https://github.com/vllm-project/vllm/pull/32821)


### Base Information

- **PR Number:** #32821
- **Author:** [22quinn](https://github.com/22quinn)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-27 21:07:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32821/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The change removes the eager import of `NgramProposer` from `gpu_model_runner.py` and replaces it with lazy imports inside the conditional blocks where it's actually used. This fixes a runtime error that occurred during module initialization when `NgramProposer` triggered LLVM initialization via Numba in environments where such initialization is deprecated or unsupported.

**Technical impact**  
This modification reduces the module's startup dependencies and prevents the Numba/LLVM initialization from occurring unless the `ngram` speculative decoding method is explicitly used. It maintains the same runtime behavior for `ngram`-based speculative decoding while avoiding side effects in other execution paths.

**Potential risks**  
If the lazy import fails (e.g., due to missing dependencies or environment issues), the error will now occur later at runtime when `ngram` decoding is attempted, rather than at module load time. This could make debugging slightly more context-dependent. Additionally, the `# noqa: F823` comment suppresses a type-checking error, which might obscure real issues if the import logic changes.

**Key insights**  
Lazy imports are an effective pattern for optional dependencies or components with heavy initialization costs. However, ensure that any runtime import errors are properly caught and logged. Consider adding a runtime check or fallback mechanism if `NgramProposer` becomes unavailable in production environments.

---

## 6. [Don't use `min_pixels`/`max_pixels` from Qwen2VL's processor](https://github.com/vllm-project/vllm/pull/33208)


### Base Information

- **PR Number:** #33208
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-27 21:02:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33208/files) (1):**
  - `vllm/model_executor/models/qwen2_vl.py`

### Summary

**What changed and why**  
The PR updates the Qwen2VL model to use the new image processor attribute names (`size["shortest_edge"]` and `size["longest_edge"]`) instead of the deprecated `min_pixels` and `max_pixels`. This change aligns the code with Transformers v5, where the old attributes have been removed.

**Technical impact**  
This change ensures compatibility with the latest Transformers library version and prevents runtime errors when accessing removed attributes. The image resizing logic remains functionally equivalent, as the new attributes serve the same purpose for determining image dimensions.

**Potential risks**  
If any downstream code or configurations still reference the old attribute names, they may break. The removal of the fallback logic (`image_processor.max_pixels or image_processor.size["longest_edge"]`) assumes `size["longest_edge"]` is always present, which could cause issues if the image processor configuration is incomplete.

**Key insights**  
Developers should verify that all image processor configurations in use provide the `size` dictionary with both `"shortest_edge"` and `"longest_edge"` keys. This is a straightforward but necessary update to maintain compatibility with Transformers v5.

---

## 7. [Add flake8-implicit-str-concat rules to Ruff](https://github.com/vllm-project/vllm/pull/33191)


### Base Information

- **PR Number:** #33191
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-27 20:56:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33191/files) (16):**
  - `.buildkite/performance-benchmarks/scripts/convert-results-json-to-markdown.py`
  - `csrc/quantization/machete/generate.py`
  - `examples/offline_inference/automatic_prefix_caching.py`
  - `examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py`
  - `pyproject.toml`
  - `tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py`
  - `tests/tool_parsers/test_deepseekv31_tool_parser.py`
  - `tests/utils.py`
  - `vllm/benchmarks/datasets.py`
  - `vllm/compilation/wrapper.py`
  - `vllm/entrypoints/openai/translations/speech_to_text.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py`
  - `vllm/reasoning/olmo3_reasoning_parser.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/core/single_type_kv_cache_manager.py`
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
This PR adds the `flake8-implicit-str-concat` (ISC) rules to the Ruff linter configuration. The changes update string concatenations across 15 files to use implicit concatenation (adjacent string literals) instead of explicit `+` operators, improving code style consistency and readability.

**Technical impact**  
The addition of ISC rules enforces a consistent string concatenation style throughout the codebase. This does not affect runtime behavior but standardizes code formatting, making the code easier to read and maintain. The changes are purely stylistic and do not alter any functional logic.

**Potential risks**  
There is minimal risk as the changes are syntactically equivalent and only modify string literal formatting. However, care should be taken to ensure that multi-line string concatenations remain logically correct, especially in complex formatted strings or those involving escape sequences.

**Key insights**  
Developers should adopt implicit string concatenation for adjacent literals in new code. This change aligns with Python's native support for implicit concatenation and reduces visual clutter. Reviewers should verify that concatenated strings across multiple lines maintain intended formatting, particularly in log messages and error strings.

---

## 8. [Relax protobuf library version constraints](https://github.com/vllm-project/vllm/pull/33202)


### Base Information

- **PR Number:** #33202
- **Author:** [jeffreywang-anyscale](https://github.com/jeffreywang-anyscale)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-27 20:15:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33202/files) (3):**
  - `pyproject.toml`
  - `requirements/build.txt`
  - `requirements/common.txt`

### Summary

**What changed and why**  
The PR removes strict version constraints (`>=` specifiers) for `protobuf`, `grpcio`, `grpcio-tools`, and `grpcio-reflection` from three dependency files. This change aims to resolve version conflicts with other libraries in the Ray ecosystem, such as TensorFlow, which require older protobuf versions, by allowing more flexible version resolution.

**Technical impact**  
By relaxing version pins, the dependency resolver (e.g., pip or uv) can select compatible versions that satisfy both vLLM's requirements and those of downstream dependencies. This improves interoperability in complex environments like Ray but shifts the burden of version compatibility to the resolver and runtime environment.

**Potential risks**  
If the resolver selects an older, incompatible version (e.g., protobuf < 6.30.0), it could break functionality that depends on newer features, such as the gRPC server or LlamaTokenizer. Additionally, inconsistent version resolution across different environments may lead to hard-to-reproduce issues.

**Key insights**  
While this change enhances flexibility, consider adding a lower-bound constraint (e.g., `protobuf >= 3.20.3`) to prevent installation of unsupported older versions. Document the minimum compatible versions clearly, and monitor for regressions in gRPC-related features during integration testing.

---

## 9. [[ROCm][CI] Add TORCH_NCCL_BLOCKING_WAIT For Distributed Tests (A100)](https://github.com/vllm-project/vllm/pull/32891)


### Base Information

- **PR Number:** #32891
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-27 19:32:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32891/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
Added `TORCH_NCCL_BLOCKING_WAIT=1` environment variable to the Distributed Tests (A100) CI step. This is a temporary workaround for a known HIP runtime bug causing flaky test failures, as documented in ROCm/hip#3876. The change mirrors a previous fix applied to other distributed test groups.

**Technical impact**  
This change forces NCCL operations to use blocking waits instead of asynchronous behavior, which masks the underlying HIP bug by ensuring synchronization. It only affects the specific CI test step for AMD A100 distributed tests, leaving production code and other test configurations unchanged.

**Potential risks**  
The workaround may introduce performance overhead in CI tests due to synchronous waiting. There's a risk of forgetting to remove this workaround once the upstream HIP bug is fixed, as it's a manual TODO. If the bug persists longer than expected, it could mask other synchronization issues.

**Key insights**  
This is a targeted, justified workaround for a known upstream issue. The clear TODO comment with issue tracking link is excellent for maintainability. Developers should monitor the linked HIP issue for fixes and promptly remove this workaround when the root cause is resolved to restore optimal test performance.

---

## 10. [[Feature]: Container image WORKDIR consistency](https://github.com/vllm-project/vllm/pull/33159)


### Base Information

- **PR Number:** #33159
- **Author:** [SouthWest7](https://github.com/SouthWest7)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-27 19:06:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33159/files) (1):**
  - `docker/Dockerfile.cpu`

### Summary

**What changed and why**  
This PR standardizes the WORKDIR path in the CPU Dockerfile from `/workspace` to `/vllm-workspace` to align with the GPU Dockerfile. The change ensures consistency across CPU and GPU container variants, simplifying the process for users building derived images that support both architectures.

**Technical impact**  
The modification updates all WORKDIR instructions and related mount paths in the multi-stage Dockerfile to use `/vllm-workspace`. This eliminates the previous symlink workaround (`ln -sf /workspace /vllm-workspace`) and ensures uniform directory structure across container builds, improving maintainability and reducing potential path-related confusion.

**Potential risks**  
If any external scripts or CI/CD pipelines rely on the old `/workspace` path, they may break unless updated. Additionally, the removal of the symlink could affect any existing workflows that depended on it for backward compatibility, though the change is intentional to enforce consistency.

**Key insights**  
Developers should verify that all dependent tooling and documentation references are updated to use `/vllm-workspace`. This alignment reduces cognitive overhead and technical debt, making the Dockerfiles easier to maintain and extend. Ensure any downstream image builds or automation scripts are adjusted accordingly.

---

## 11. [[Docs] Use definition lists for CLI reference docs](https://github.com/vllm-project/vllm/pull/33186)


### Base Information

- **PR Number:** #33186
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-27 18:22:48
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33186/files) (2):**
  - `docs/mkdocs/hooks/generate_argparse.py`
  - `mkdocs.yaml`

### Summary

**What changed and why**  
The PR modifies CLI reference documentation generation to use definition lists instead of plain markdown formatting. This change aligns the documentation style with the previous sphinx-argparse output and improves visual structure by indenting argument details like choices, help text, and defaults under each CLI option heading.

**Technical impact**  
The changes affect only documentation rendering by enabling the `def_list` markdown extension and updating the Python generator to output definition list syntax (`:` prefix for content). This maintains the same information while improving readability through consistent indentation, without impacting any runtime code or functionality.

**Potential risks**  
If the `def_list` extension is not supported in all documentation deployment environments, formatting may break or render incorrectly. Additionally, the removal of separate handling for `choices` and `metavar` could obscure edge cases where their semantic differences matter, though currently they are treated identically.

**Key insights**  
This is a documentation-only change that enhances maintainability and visual consistency. Developers should verify the rendered output in the deployed docs to ensure definition lists display as intended. The consolidation of `choices` and `metavar` logic simplifies the code but warrants awareness if future changes require distinguishing between them.

---

## 12. [[docs] Improve tlparse section](https://github.com/vllm-project/vllm/pull/33211)


### Base Information

- **PR Number:** #33211
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-27 18:07:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33211/files) (1):**
  - `docs/design/debug_vllm_compile.md`

### Summary

**What changed and why**  
Updated documentation to clarify how to generate and use torch.compile logs with tlparse. The changes emphasize setting the `TORCH_TRACE` environment variable to capture logs in a directory and specify that log files (not just pieces) should be shared for bug reports.

**Technical impact**  
Improves user guidance by providing clearer instructions for enabling tracing and specifying the exact log file to analyze (`rank_0_log_file` instead of ambiguous `the_first_log_file`). No functional changes to the codebase.

**Potential risks**  
Users might misinterpret "rank_0_log_file" in single-rank environments, though it remains accurate. The emphasis on sending entire log files could raise privacy/security concerns if logs contain sensitive data.

**Key insights**  
Documentation now better aligns with actual tool usage. Recommend adding a note about sanitizing logs before sharing and clarifying that rank numbering starts at 0. The updates enhance reproducibility for debugging torch.compile issues.

---

## 13. [[CI] minor fixes to pipeline generator and tests](https://github.com/vllm-project/vllm/pull/33151)


### Base Information

- **PR Number:** #33151
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-27 17:04:03
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33151/files) (4):**
  - `.buildkite/.pipeline_gen_v2`
  - `.buildkite/image_build/image_build.sh`
  - `.buildkite/image_build/image_build.yaml`
  - `.buildkite/test_areas/distributed.yaml`

### Summary

**What changed and why**  
This PR implements minor CI pipeline fixes including renaming `VLLM_BAKE_FILE` to `VLLM_BAKE_FILE_PATH` for clarity, fixing argument order in the main branch image build command, updating a hardcoded image tag to use the `$IMAGE_TAG` variable for 2-node tests, and adding a stamp file to mark readiness for pipeline generation V2.

**Technical impact**  
The changes improve CI script robustness by ensuring correct variable naming consistency, fixing a missing argument that could cause main branch builds to fail, and making test configurations more maintainable by replacing hardcoded image references with variables. The stamp file addition enables future pipeline migration tracking.

**Potential risks**  
The argument fix in `image_build.yaml` changes the parameter order, which could break if the script's parameter handling differs between branch conditions. The hardcoded image tag replacement assumes `$IMAGE_TAG` is properly set in the test environment. Missing validation for the new `VLLM_BAKE_FILE_PATH` default value could cause issues if the file doesn't exist.

**Key insights**  
Developers should verify the argument order change works correctly for both main and non-main branches. The variable renaming improves code clarity but requires updating any external documentation referencing the old variable names. The stamp file indicates upcoming pipeline infrastructure changes that teams should prepare for.

---

## 14. [[Model Runner V2] Use a different stream for grammar bitmask h2d copy](https://github.com/vllm-project/vllm/pull/33059)


### Base Information

- **PR Number:** #33059
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-27 16:37:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33059/files) (2):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/structured_outputs.py`

### Summary

**What changed and why**  
The changes optimize grammar bitmask processing by introducing asynchronous GPU memory copies using a dedicated CUDA stream. This allows the host-to-device (H2D) copy of grammar bitmasks to overlap with model forward passes, improving GPU utilization and reducing latency.

**Technical impact**  
The `StructuredOutputsWorker` now uses device-resident tensors instead of `UvaBufferPool`, with a separate `copy_stream` for asynchronous transfers. This changes memory management from UVM-based to explicit GPU memory allocation, enabling better overlap of data transfers with compute operations in the main execution stream.

**Potential risks**  
The asynchronous nature introduces synchronization complexity - improper stream synchronization could cause data races or use of incomplete data. The explicit `wait_stream` calls must be correctly ordered to prevent accessing buffers before copies complete. Device memory usage increases since tensors are pre-allocated on GPU rather than using UVM.

**Key insights**  
This is a performance optimization that requires careful stream synchronization. Developers should verify the synchronization logic maintains correctness across all execution paths. Consider adding validation that the async copies complete successfully before kernel launches, and monitor GPU memory usage since pre-allocated device tensors may increase memory pressure.

---

## 15. [Add attention benchmarking tools](https://github.com/vllm-project/vllm/pull/26835)


### Base Information

- **PR Number:** #26835
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-27 16:09:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/26835/files) (12):**
  - `benchmarks/attention_benchmarks/README.md`
  - `benchmarks/attention_benchmarks/__init__.py`
  - `benchmarks/attention_benchmarks/batch_spec.py`
  - `benchmarks/attention_benchmarks/benchmark.py`
  - `benchmarks/attention_benchmarks/common.py`
  - `benchmarks/attention_benchmarks/configs/mla_decode.yaml`
  - `benchmarks/attention_benchmarks/configs/mla_mixed_batch.yaml`
  - `benchmarks/attention_benchmarks/configs/reorder_threshold.yaml`
  - `benchmarks/attention_benchmarks/configs/speculative_decode.yaml`
  - `benchmarks/attention_benchmarks/configs/standard_attention.yaml`
  - `benchmarks/attention_benchmarks/mla_runner.py`
  - `benchmarks/attention_benchmarks/runner.py`

### Summary

**What changed and why**  
A comprehensive attention benchmarking suite was added to enable performance testing and parameter tuning for vLLM attention backends. The suite supports both standard attention (Flash/Triton/FlashInfer) and MLA (Multi‑Query Latent Attention) backends, with a flexible batch‑specification grammar and pre‑configured YAML configurations for common use cases like speculative decoding and mixed‑batch workloads.

**Technical impact**  
This introduces a new, self‑contained benchmarking module (`benchmarks/attention_benchmarks/`) that does not affect production code. It provides a unified CLI and Python API for comparing backend performance, sweeping tuning parameters (e.g., `num_kv_splits`, `reorder_batch_threshold`), and generating CSV/JSON reports. The architecture uses mock objects and minimal vLLM configuration to avoid dependencies on full model loading.

**Potential risks**  
- The suite relies on internal vLLM APIs (e.g., `AttentionLayerBase`, metadata builders) that may change in future releases, potentially breaking benchmarks.  
- Some MLA backends require specific GPU architectures (e.g., Hopper for FlashAttn MLA), which could lead to runtime errors if hardware requirements are not met.  
- The mock configuration may not perfectly replicate real‑world conditions, especially for memory‑bound scenarios.

**Key insights**  
- This toolset is essential for systematic backend selection and hyperparameter optimization, as evidenced by its use in prior tuning PRs (#26846, #27363, #27368).  
- Developers should leverage the YAML configs for reproducible studies and use the CLI’s `--sweep-param` for automated parameter searches.  
- Ensure hardware compatibility (SM90+ for Hopper‑only backends) and consider adding integration tests to catch API drift.

---

## 16. [[torch.compile] Speed up MOE handling in forward_context](https://github.com/vllm-project/vllm/pull/33184)


### Base Information

- **PR Number:** #33184
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-27 15:17:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33184/files) (4):**
  - `tests/kernels/moe/test_moe.py`
  - `vllm/config/compilation.py`
  - `vllm/forward_context.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`

### Summary

**What changed and why**  
The changes optimize MOE (Mixture of Experts) layer handling during torch.compile by precomputing all MOE layer names during model initialization instead of recomputing them on every forward pass. Additionally, the implementation shifts from popping strings from a list to using a counter-based index to retrieve layer names, reducing overhead.

**Technical impact**  
This reduces per-forward-pass computational overhead by eliminating repeated layer name collection and list manipulation. The architecture now stores MOE layer names in `CompilationConfig.static_all_moe_layers` during initialization and accesses them via an index in `ForwardContext`, improving performance during compiled execution.

**Potential risks**  
If the order of MOE layer execution changes dynamically or custom operators are reordered by torch.compile, the counter-based approach may fail. The assertion in `get_layer_from_name` assumes a fixed execution order, which could break if the compilation behavior changes. Additionally, any mismatch between the stored layer names and actual layers could cause runtime errors.

**Key insights**  
Precomputing static data during initialization is a effective optimization for compilation scenarios. Developers should ensure that the MOE layer execution order remains consistent with initialization order. Consider adding validation to catch order mismatches early, and monitor torch.compile updates that might affect operator ordering assumptions.

---

## 17. [[Perf] Optimize dcp allocate tensor](https://github.com/vllm-project/vllm/pull/33102)


### Base Information

- **PR Number:** #33102
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-27 14:24:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33102/files) (1):**
  - `vllm/v1/attention/ops/common.py`

### Summary

**What changed and why**  
The PR optimizes tensor allocation in distributed checkpointing by eliminating an unnecessary intermediate tensor allocation. Instead of creating an empty tensor and then using `view_as()` after `all_gather()`, it directly reshapes the gathered tensor using `reshape()` with the target shape.

**Technical impact**  
This reduces memory allocation overhead by avoiding the creation of a temporary tensor. The change maintains the same tensor shape and data layout while improving performance through more efficient memory operations. The functional behavior remains identical as both approaches produce tensors with identical dimensions.

**Potential risks**  
The main risk is that `reshape()` may return a view instead of a copy in some cases, though with `all_gather()` output this should be safe. There's a minor risk if `cp_attn_lse.shape` contains unexpected dimensions, but the shape calculation logic remains unchanged.

**Key insights**  
This is a clean optimization that follows PyTorch best practices by reducing unnecessary allocations. Developers should note that `reshape()` is generally preferred over `view()` when the tensor might not be contiguous, though `contiguous()` is already called. Similar patterns could be applied elsewhere in the codebase for consistent optimization.

---

## 18. [[Bugfix] Fix display error (inconsistent with context)](https://github.com/vllm-project/vllm/pull/33020)


### Base Information

- **PR Number:** #33020
- **Author:** [lingebeng](https://github.com/lingebeng)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-27 12:33:30
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33020/files) (1):**
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
The changes remove references to "ARM" from two logger.info messages in the `_set_default_chunked_prefill_and_prefix_caching_args` function. This fixes a display inconsistency because ARM support was previously removed in PR #29193, but the log messages were not updated accordingly.

**Technical impact**  
These are purely cosmetic changes that correct misleading log output. The functionality remains unchanged—chunked prefill and prefix caching are still disabled for POWER, S390X, and RISC-V CPU architectures, but no longer incorrectly state they are unsupported for ARM.

**Potential risks**  
The risk is minimal since only log messages are modified. However, if the code still contains other hidden references to ARM handling (e.g., in conditionals or comments), those could cause confusion or bugs. Additionally, future developers might misinterpret the log history if they encounter old logs mentioning ARM.

**Key insights**  
Always update all related documentation, logs, and comments when removing features to maintain consistency. Consider performing a broader search for "ARM" in the codebase to ensure no other outdated references exist. This fix highlights the importance of comprehensive cleanup during deprecation.

---

## 19. [Enabling "2 node" distributed tests in the AMD CI pipeline.](https://github.com/vllm-project/vllm/pull/32719)


### Base Information

- **PR Number:** #32719
- **Author:** [Alexei-V-Ivanov-AMD](https://github.com/Alexei-V-Ivanov-AMD)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-27 11:13:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32719/files) (2):**
  - `.buildkite/scripts/hardware_ci/run-amd-test.sh`
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
This PR enables 2-node distributed testing in the AMD CI pipeline by adding multi-node test execution logic to the run script and updating the pipeline configuration. The changes introduce a new cleanup function for Docker networks, parse multi-node commands from the pipeline, and execute them via a new multi-node test script.

**Technical impact**  
The pipeline now supports distributed tests across two nodes with four GPUs total, using a custom script (`run-multi-node-test.sh`) to orchestrate multi-container execution. The test configuration is updated to remove grep filters that could mask test failures, ensuring full test output is captured.

**Potential risks**  
The bash command parsing logic is fragile—it relies on specific formatting of the `commands` string and uses complex regex matching. If the pipeline command format changes, the parsing may break. Additionally, the cleanup function assumes container names follow a specific pattern (`node${node}`), which could lead to orphaned resources if naming conventions differ.

**Key insights**  
Developers should verify that `run-multi-node-test.sh` exists and is compatible with the new orchestration logic. The regex-based parsing should be hardened or replaced with a more robust method (e.g., using structured input). Ensure the `cleanup_network` function is called in all error paths to prevent resource leaks.

---

## 20. [[Attention] Use `has_flashinfer` helper](https://github.com/vllm-project/vllm/pull/33177)


### Base Information

- **PR Number:** #33177
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-27 10:33:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33177/files) (1):**
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
This PR replaces a local `flashinfer_available()` function with the centralized `has_flashinfer()` helper from `vllm.utils.flashinfer`. The change removes duplicate FlashInfer availability checking logic and standardizes on a single implementation.

**Technical impact**  
The codebase now uses a consistent method for detecting FlashInfer availability across different modules. This improves maintainability by eliminating code duplication and ensures uniform behavior for FlashInfer-related feature flags in MLA attention operations.

**Potential risks**  
The centralized `has_flashinfer()` function might have different caching behavior or implementation details than the removed local version. If `has_flashinfer()` has side effects or different performance characteristics, it could subtly affect the initialization or runtime behavior of attention mechanisms.

**Key insights**  
This is a positive refactoring that follows DRY principles. Developers should verify that `has_flashinfer()` provides equivalent functionality and performance. Future FlashInfer-related changes only need to be made in one location, reducing maintenance overhead and potential inconsistencies.

---

## 21. [feature: support eagle3 for HunyuanVL & Hunyuan](https://github.com/vllm-project/vllm/pull/33035)


### Base Information

- **PR Number:** #33035
- **Author:** [irisliu10](https://github.com/irisliu10)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-27 09:55:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33035/files) (4):**
  - `vllm/config/speculative.py`
  - `vllm/model_executor/models/hunyuan_v1.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
Added Eagle3 speculative decoding support for HunyuanVL and Hunyuan models by extending the supported model list, implementing the `SupportsEagle3` interface with auxiliary hidden state layer methods, and handling XD-RoPE positional encodings in the Eagle3 draft manager.

**Technical impact**  
Enables speculative decoding acceleration for Hunyuan models, potentially doubling throughput as shown in test results. The changes integrate with the existing Eagle3 framework by adding model-specific hooks for intermediate hidden states and ensuring compatibility with multimodal positional encodings (XD-RoPE).

**Potential risks**  
Inconsistent handling of XD-RoPE when draft and target models differ (`draft_uses_xdrope_dim == 0`) could cause positional encoding mismatches. The auxiliary layer selection (fixed indices 2, mid, last-3) may not be optimal for all Hunyuan variants. Multimodal token index configuration assumes `image_token_id` mapping.

**Key insights**  
Verify XD-RoPE alignment between draft/target models during initialization. Consider making auxiliary layer configurable rather than hardcoded. Ensure image token indices are correctly propagated for all multimodal model types.

---

## 22. [[Doc] Improve serve parameter documentation with meaningful defaults](https://github.com/vllm-project/vllm/pull/33082)


### Base Information

- **PR Number:** #33082
- **Author:** [karanb192](https://github.com/karanb192)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-01-27 09:19:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33082/files) (1):**
  - `docs/mkdocs/hooks/generate_argparse.py`

### Summary

**What changed and why**  
The change modifies the documentation generator to skip displaying "Default: None" for command-line arguments in the `vllm serve` documentation. This is because a `None` default typically indicates that the actual default value is determined at runtime (e.g., through auto-detection) rather than being a static `None`, so showing it is misleading to users.

**Technical impact**  
This is a documentation-only change that affects the output of the `MarkdownFormatter.add_arguments()` method. Arguments with meaningful defaults (strings, numbers, booleans) continue to be displayed, while those with `default=None` will now omit the default line entirely, making the generated docs clearer.

**Potential risks**  
If any argument legitimately has `None` as a meaningful, static default (rather than a placeholder for runtime detection), its documentation will now incorrectly hide that information. The change assumes all `None` defaults are placeholders, which could be incorrect if the codebase uses `None` differently elsewhere.

**Key insights**  
The fix correctly addresses user confusion about unhelpful "None" defaults in docs. Reviewers should verify that no serve parameters actually rely on `None` as a valid, user-relevant default value. Consider adding a comment or configuration to handle special cases if `None` ever represents a meaningful default in the future.

---

## 23. [Support compress-tensors with nvfp4 or fp8 weights and modelopt with nvfp4 weights on Turing](https://github.com/vllm-project/vllm/pull/33076)


### Base Information

- **PR Number:** #33076
- **Author:** [ir1ka](https://github.com/ir1ka)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-27 08:04:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33076/files) (3):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`

### Summary

**What changed and why**  
The changes enable support for nvfp4 and fp8 weight quantization on Turing GPUs (compute capability 75) by lowering the minimum capability requirement from 80 to 75 for compressed-tensors schemes, and fixing a backend selection condition for modelopt nvfp4 to check for device capability 100 before using flashinfer.

**Technical impact**  
These modifications allow vLLM to load and run models quantized with nvfp4 and fp8 weights on Turing architecture GPUs (e.g., RTX 2080 Ti). The system will now correctly recognize these quantization schemes as supported instead of rejecting them, and will use the appropriate computational backend (like cutlass) for these GPUs.

**Potential risks**  
Lowering the minimum capability could expose the code to GPUs with compute capability below 75, which may lack necessary hardware features, though the PR description indicates testing was successful on Turing. The change to the modelopt backend condition assumes flashinfer is only suitable for capability 100+ devices; if this assumption is incorrect for some configurations, it could affect performance or compatibility.

**Key insights**  
The changes are minimal and targeted, directly addressing the reported issue. Developers should ensure comprehensive testing across various Turing GPU models to confirm stability. Future work might involve documenting this extended GPU support and considering if similar capability adjustments are needed elsewhere in the quantization codebase.

---

## 24. [[BugFix] Fix P/D with non-MoE DP](https://github.com/vllm-project/vllm/pull/33037)


### Base Information

- **PR Number:** #33037
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-27 08:03:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33037/files) (2):**
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/utils.py`

### Summary

**What changed and why**  
The fix moves the logic for appending DP rank suffixes to `kv_transfer_config.engine_id` earlier in the initialization flow. Previously, this assignment occurred in `_init_data_parallel`, but now it happens immediately after process forking in the engine core initialization, before DP rank/size are reset for dense models. This ensures unique engine IDs are set before rank information is lost in non-MoE DP scenarios.

**Technical impact**  
This change guarantees that each data-parallel rank in dense (non-MoE) models receives a distinct `engine_id` for KV transfer configuration. The adjustment prevents potential collisions or misrouting of KV cache transfers between DP ranks, which could occur if engine IDs were duplicated after rank reset.

**Potential risks**  
If the DP rank suffix logic is applied multiple times (e.g., in both `core.py` and `utils.py`), it could create malformed engine IDs like `id_dp0_dp0`. Additionally, the fix assumes `local_dp_rank` is correctly defined at the new location, which depends on proper initialization order.

**Key insights**  
Developers should verify that `local_dp_rank` is accessible in the early initialization context and ensure no duplicate suffix appplications occur. The change highlights the importance of initializing distributed identifiers before rank modifications, especially in hybrid parallelization setups.

---

## 25. [Support heterogeneous NemotronHPuzzle model](https://github.com/vllm-project/vllm/pull/32549)


### Base Information

- **PR Number:** #32549
- **Author:** [danielafrimi](https://github.com/danielafrimi)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-27 07:55:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32549/files) (5):**
  - `tests/models/registry.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/nemotron_h.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`

### Summary

**What changed and why**  
This PR adds support for `NemotronHPuzzleForCausalLM`, a heterogeneous variant of NemotronH where layers can have different configurations (expert counts, sliding windows, etc.). Changes include registering the new model, handling per-layer configurations in the model implementation, and updating expert-count detection to support varying expert numbers across layers.

**Technical impact**  
The modifications enable heterogeneous model architectures within the existing NemotronH framework by introducing dynamic per-layer configuration retrieval. The model now checks for a `get_nemotron_h_config_for_layer` method and uses `block_configs` to determine the maximum number of experts across layers, ensuring compatibility with weight loading and inference.

**Potential risks**  
If `block_configs` is missing or malformed in a heterogeneous model, expert detection may fail, leading to incorrect weight mapping. The reliance on `getattr` for optional attributes could mask configuration errors. Additionally, the per-layer sliding window attribute may not be properly validated, potentially causing attention mechanism issues.

**Key insights**  
Developers should ensure that any heterogeneous model provides a valid `block_configs` structure and implements `get_nemotron_h_config_for_layer` if per-layer differences exist. The expert-mapping logic now uses the maximum expert count across layers, which is critical for correct weight loading but may allocate unnecessary memory if expert counts vary significantly.

---

## 26. [[LoRA][Spec Decode] Support LoRA for Nemotron-H MTP models](https://github.com/vllm-project/vllm/pull/32265)


### Base Information

- **PR Number:** #32265
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-01-27 07:53:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32265/files) (4):**
  - `vllm/lora/lora_model.py`
  - `vllm/lora/worker_manager.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
This PR adds LoRA support for Nemotron-H models with Multi-Token Prediction (MTP) by introducing a mechanism to skip MTP layers during LoRA weight loading. The changes add a `skip_prefixes` parameter to LoRA loading methods and implement it in the Nemotron-H model to exclude weights prefixed with "mtp.".

**Technical impact**  
The changes introduce a generic `lora_skip_prefixes` attribute in the model interface, allowing any model to define prefixes for modules that should be excluded from LoRA loading. This maintains LoRA compatibility for models with non-standard layers (like MTP) that aren't used during inference, preventing errors from missing weight mappings.

**Potential risks**  
If the skip logic incorrectly matches module names, it could unintentionally exclude valid LoRA weights. The current implementation checks for prefixes both at the start of names and with a preceding dot, which might not cover all naming patterns. There's also a risk that other models implementing this feature might define incorrect prefixes.

**Key insights**  
The solution elegantly extends existing LoRA infrastructure without modifying core logic. Developers adding LoRA support to new models with inference-irrelevant layers should set `lora_skip_prefixes` appropriately. The prefix matching logic should be reviewed when applied to other model architectures to ensure it correctly identifies target modules.

---

## 27. [[Frontend] Cleanup api server](https://github.com/vllm-project/vllm/pull/33158)


### Base Information

- **PR Number:** #33158
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-27 07:18:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33158/files) (5):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/basic/__init__.py`
  - `vllm/entrypoints/openai/basic/api_router.py`
  - `vllm/entrypoints/openai/server_utils.py`
  - `vllm/entrypoints/sagemaker/api_router.py`

### Summary

**What changed and why**  
This PR refactors the OpenAI API server by extracting common utilities, middleware, and basic API routes from `api_server.py` into two new dedicated modules: `api_router.py` and `server_utils.py`. The primary goal is to improve code modularity and maintainability, making the main server file cleaner and more focused on application assembly and configuration.

**Technical impact**  
The refactoring significantly reduces the size of `api_server.py` (from 424 lines removed to 23 lines added) by moving non-core logic to separate modules. This creates a clearer separation of concerns: `api_router.py` handles basic API endpoints like `/load` and `/version`, while `server_utils.py` contains middleware, logging utilities, and response handling. The architecture now follows better modular design principles, making individual components easier to test and maintain.

**Potential risks**  
There's a risk of breaking changes if the extraction wasn't complete or if there are hidden dependencies between the moved code and the remaining server logic. The SSEDecoder implementation in `server_utils.py` appears truncated in the diff (ends with "full_content = full_c"), which could indicate incomplete code. Additionally, changes to import paths (like in `sagemaker/api_router.py`) need careful verification to ensure all references are updated correctly.

**Key insights**  
This is a well-structured refactoring that follows good software engineering practices. Developers should verify the SSEDecoder implementation is complete and test all API endpoints thoroughly. The new modular structure will make future enhancements easier, but team members need to be aware of the new file organization when making changes to server functionality.

---

## 28. [[Metrics][MFU] Fix UnembedMetrics FLOP overcounting for prefill (#33045)](https://github.com/vllm-project/vllm/pull/33045)


### Base Information

- **PR Number:** #33045
- **Author:** [omkhalil](https://github.com/omkhalil)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-01-27 07:16:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33045/files) (1):**
  - `vllm/v1/metrics/perf.py`

### Summary

**What changed and why**  
The fix addresses a FLOP overcounting issue in the UnembedMetrics class where `total_num_tokens()` was incorrectly used for unembedding (LM head) layer calculations. For autoregressive generation, logits computation is only needed for the last token in prefill requests, not all tokens. A new `num_logits_tokens()` method was added to correctly count tokens requiring logits, and three metric calculation methods were updated to use this new method.

**Technical impact**  
This correction ensures accurate FLOP and memory traffic calculations for the unembedding layer, particularly distinguishing between prefill (last token only) and decode (all tokens) phases. The change affects performance profiling metrics but not the actual model execution or output, making profiling data more representative of true computational costs.

**Potential risks**  
If other parts of the codebase rely on the original FLOP counting logic for non-profiling purposes (e.g., scheduling or resource estimation), they may need similar updates. The fix assumes a clear separation between prefill and decode phases; hybrid or non-autoregressive use cases might require further adjustments.

**Key insights**  
Always verify that performance metrics align with the actual computational graph—especially for conditional operations like last-token logits. Consider adding a comment in the code linking this fix to the autoregressive assumption to prevent future misuse. Review other metric calculations to ensure similar phase-specific logic is correctly applied.

---

## 29. [[Bugfix] Fix DeepseekV32 `AssertionError: num_kv_heads == 1`](https://github.com/vllm-project/vllm/pull/33090)


### Base Information

- **PR Number:** #33090
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-27 07:03:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33090/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`

### Summary

**What changed and why**  
The change fixes a hardcoded `num_kv_heads` value from 4 to 1 in a test utility function. This resolves an `AssertionError` for the DeepseekV32 model, which expects a single key-value head in certain distributed KV transfer scenarios.

**Technical impact**  
This adjustment ensures the KV cache shape detection logic aligns with models using a single KV head, preventing runtime assertion failures during distributed inference. The change is minimal and localized, affecting only the test shape used for dimension analysis.

**Potential risks**  
If other models or configurations assume `num_kv_heads=4` in this utility, they might encounter similar assertion errors. The fix addresses a specific case but doesn’t generalize the utility to handle variable KV head counts dynamically.

**Key insights**  
Hardcoding test parameters can lead to model-specific failures. Consider extending the utility to infer `num_kv_heads` from the actual model configuration or making it configurable to avoid future issues with diverse architectures.

---

## 30. [[5/N][Attention] Finish eliminating `vllm/attention` folder](https://github.com/vllm-project/vllm/pull/32064)


### Base Information

- **PR Number:** #32064
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-27 07:02:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32064/files) (151):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/kernels.yaml`
  - `.github/CODEOWNERS`
  - `docs/contributing/model/basic.md`
  - `docs/design/custom_op.md`
  - `tests/compile/test_fusion_attn.py`
  - `tests/compile/test_qk_norm_rope_fusion.py`
  - `tests/kernels/attention/test_attention.py`
  - `tests/kernels/attention/test_mha_attn.py`
  - `tests/v1/worker/test_gpu_model_runner.py`
  - `tests/v1/worker/test_utils.py`
  - `tools/pre_commit/mypy.py`
  - `vllm/attention/__init__.py`
  - `vllm/attention/utils/__init__.py`
  - `vllm/attention/utils/kv_sharing_utils.py`
  - `vllm/compilation/fusion_attn.py`
  - `vllm/compilation/qk_norm_rope_fusion.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/model_executor/layers/attention/__init__.py`
  - `vllm/model_executor/layers/attention/attention.py`
  - `vllm/model_executor/layers/attention/chunked_local_attention.py`
  - `vllm/model_executor/layers/attention/cross_attention.py`
  - `vllm/model_executor/layers/attention/encoder_only_attention.py`
  - `vllm/model_executor/layers/attention/kv_transfer_utils.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/model_executor/layers/attention/static_sink_attention.py`
  - `vllm/model_executor/layers/mla.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/petit.py`
  - `vllm/model_executor/layers/quantization/ptpc_fp8.py`
  - `vllm/model_executor/layers/quantization/quark/quark.py`
  - `vllm/model_executor/model_loader/utils.py`
  - `vllm/model_executor/models/afmoe.py`
  - `vllm/model_executor/models/aimv2.py`
  - `vllm/model_executor/models/apertus.py`
  - `vllm/model_executor/models/arctic.py`
  - `vllm/model_executor/models/baichuan.py`
  - `vllm/model_executor/models/bailing_moe.py`
  - `vllm/model_executor/models/bamba.py`
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/bert_with_rope.py`
  - `vllm/model_executor/models/blip.py`
  - `vllm/model_executor/models/bloom.py`
  - `vllm/model_executor/models/chameleon.py`
  - `vllm/model_executor/models/chatglm.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/commandr.py`
  - `vllm/model_executor/models/dbrx.py`
  - `vllm/model_executor/models/deepencoder.py`
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/model_executor/models/dots1.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/ernie45_moe.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/ernie45_vl_moe.py`
  - `vllm/model_executor/models/exaone.py`
  - `vllm/model_executor/models/exaone4.py`
  - `vllm/model_executor/models/falcon.py`
  - `vllm/model_executor/models/falcon_h1.py`
  - `vllm/model_executor/models/gemma.py`
  - `vllm/model_executor/models/gemma2.py`
  - `vllm/model_executor/models/gemma3.py`
  - `vllm/model_executor/models/gemma3n.py`
  - `vllm/model_executor/models/glm4.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4_moe.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/gpt2.py`
  - `vllm/model_executor/models/gpt_bigcode.py`
  - `vllm/model_executor/models/gpt_j.py`
  - `vllm/model_executor/models/gpt_neox.py`
  - `vllm/model_executor/models/gpt_oss.py`
  - `vllm/model_executor/models/granite.py`
  - `vllm/model_executor/models/granitemoe.py`
  - `vllm/model_executor/models/granitemoehybrid.py`
  - `vllm/model_executor/models/grok1.py`
  - `vllm/model_executor/models/hunyuan_v1.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/idefics2_vision_model.py`
  - `vllm/model_executor/models/intern_vit.py`
  - `vllm/model_executor/models/internlm2.py`
  - `vllm/model_executor/models/interns1_vit.py`
  - `vllm/model_executor/models/iquest_loopcoder.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/jais.py`
  - `vllm/model_executor/models/jais2.py`
  - `vllm/model_executor/models/jamba.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/lfm2.py`
  - `vllm/model_executor/models/lfm2_moe.py`
  - `vllm/model_executor/models/lfm2_siglip2.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/llama4.py`
  - `vllm/model_executor/models/mimo_v2_flash.py`
  - `vllm/model_executor/models/minicpm.py`
  - `vllm/model_executor/models/minicpm3.py`
  - `vllm/model_executor/models/minimax_m2.py`
  - `vllm/model_executor/models/minimax_text_01.py`
  - `vllm/model_executor/models/mixtral.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/moonvit.py`
  - `vllm/model_executor/models/mpt.py`
  - `vllm/model_executor/models/nemotron.py`
  - `vllm/model_executor/models/nemotron_h.py`
  - `vllm/model_executor/models/olmo.py`
  - `vllm/model_executor/models/olmo2.py`
  - `vllm/model_executor/models/olmoe.py`
  - `vllm/model_executor/models/openpangu.py`
  - `vllm/model_executor/models/opt.py`
  - `vllm/model_executor/models/orion.py`
  - `vllm/model_executor/models/ouro.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/persimmon.py`
  - `vllm/model_executor/models/phi.py`
  - `vllm/model_executor/models/phimoe.py`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/plamo3.py`
  - `vllm/model_executor/models/qwen.py`
  - `vllm/model_executor/models/qwen2.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_moe.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3.py`
  - `vllm/model_executor/models/qwen3_moe.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/models/seed_oss.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/siglip2navit.py`
  - `vllm/model_executor/models/solar.py`
  - `vllm/model_executor/models/stablelm.py`
  - `vllm/model_executor/models/starcoder2.py`
  - `vllm/model_executor/models/step1.py`
  - `vllm/model_executor/models/step3_text.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/transformers/base.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/model_executor/models/whisper_causal.py`
  - `vllm/model_executor/models/zamba2.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/spec_decode/draft_model.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
This PR completes the migration of attention-related code from the `vllm/attention` folder to `vllm/model_executor/layers/attention`. The main changes include splitting the former `layer.py` into separate files for standard attention and MLA attention, moving utility files, updating imports across the codebase, and removing the old directory. This is step 5 of a larger refactoring effort (#31919) to consolidate attention layers under the model executor.

**Technical impact**  
The refactoring centralizes attention layer definitions within the model executor hierarchy, improving code organization and maintainability. The architectural change simplifies import paths and aligns attention modules with other model layers. Functionality remains unchanged, but the consolidation reduces fragmentation and clarifies module boundaries.

**Potential risks**  
Despite extensive import updates, there is a risk of missing edge cases in downstream dependencies or custom integrations that may have relied on the old import paths. The CI configuration updates include a TODO note about removing a test dependency, indicating potential lingering references. Additionally, the large number of modified files increases the chance of merge conflicts in active development branches.

**Key insights**  
Developers must update any local references to the old `vllm.attention` imports to use the new `vllm.model_executor.layers.attention` paths. The PR successfully maintains backward compatibility at the API level while modernizing the code structure. Future work should address the remaining test dependency noted in the CI configuration to fully complete the migration.

---

## 31. [[Bugfix] Disable CG for Whisper+FA2](https://github.com/vllm-project/vllm/pull/33164)


### Base Information

- **PR Number:** #33164
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-27 05:46:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33164/files) (1):**
  - `vllm/v1/attention/backends/flash_attn.py`

### Summary

**What changed and why**  
This PR temporarily disables CUDA graph (CG) support for encoder-decoder models when FlashAttention2 (FA2) is enabled. The change addresses an accuracy issue (#33091) in models like Whisper, where FA2 with CG produces incorrect outputs without causing crashes. This is a stopgap fix to prevent user-facing accuracy degradation until a proper solution is implemented.

**Technical impact**  
The modification adds a conditional check in the `FlashAttentionMetadataBuilder` class that overrides CUDA graph support for the specific combination of encoder-decoder architecture and FA2. This prevents the system from using CUDA graphs in these scenarios, which will likely increase inference latency but ensures correct model outputs. The change is localized to the attention backend configuration.

**Potential risks**  
The primary risk is performance regression for encoder-decoder models using FA2, as they will fall back to eager execution without CUDA graph optimizations. There's also a risk that this temporary fix might remain in place longer than intended if the root cause isn't addressed promptly. The warning message uses `logger.warning_once`, which may not be visible in all deployment scenarios.

**Key insights**  
This is a targeted, defensive fix that prioritizes correctness over performance for a specific problematic configuration. Developers should monitor the linked issue (#33091) for a permanent resolution. The implementation correctly uses class inheritance to override CUDA graph support while maintaining the existing behavior for other model types and attention versions.

---

## 32. [[Metrics] [KVConnector] Add Offloading Connector metrics](https://github.com/vllm-project/vllm/pull/27942)


### Base Information

- **PR Number:** #27942
- **Author:** [omerpaz95](https://github.com/omerpaz95)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-01-27 05:34:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/27942/files) (6):**
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `tests/v1/kv_offload/test_cpu_gpu.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/metrics.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/v1/kv_offload/worker/cpu_gpu.py`
  - `vllm/v1/kv_offload/worker/worker.py`

### Summary

**What changed and why**  
Added comprehensive metrics collection for the KV Offloading Connector, including transfer size, duration, and throughput for CPU↔GPU data movements. This enables performance monitoring via Prometheus and StatLogger to support future optimization efforts.

**Technical impact**  
Introduces new stats classes (`OffloadingConnectorStats`, `OffloadPromMetrics`) that aggregate per-transfer data and expose histograms, counters, and gauges. The worker-side instrumentation now captures CUDA event timings and byte counts, while the connector API is extended to require `kv_cache_config` for computing `bytes_per_block`.

**Potential risks**  
- Increased memory overhead from storing per-operation metrics in lists; unbounded growth could occur if stats are not regularly reduced/cleared.  
- The `OffloadPromMetrics` histogram buckets may not cover all transfer sizes, potentially skewing metrics for very large transfers.  
- Thread safety concerns if stats aggregation occurs concurrently with recording (though current usage appears single-threaded).

**Key insights**  
- Metrics are only collected on the worker side, ensuring minimal scheduler overhead.  
- The `TransferResult` dataclass now enriches observability but requires careful validation of `transfer_type` and size calculations.  
- Developers should monitor the cardinality of Prometheus labels (`transfer_type`) to avoid metric explosion in multi-engine deployments.

---

## 33. [Fix weight mapping test for Transfomers v5](https://github.com/vllm-project/vllm/pull/33162)


### Base Information

- **PR Number:** #33162
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-27 04:30:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33162/files) (3):**
  - `tests/models/multimodal/test_mapping.py`
  - `vllm/model_executor/models/transformers/base.py`
  - `vllm/model_executor/models/transformers/causal.py`

### Summary

**What changed and why**  
This PR fixes a test failure related to weight mapping for Transformers v5 models and improves robustness when accessing the `tie_word_embeddings` attribute. The changes add a workaround for tied weights on meta devices in tests and replace direct attribute access with `getattr` calls to handle models where this attribute may be undefined.

**Technical impact**  
The modifications ensure compatibility with Transformers v5 by safely handling optional configuration attributes. The test fix addresses a known transformers issue with tied weight initialization on meta devices, while the `getattr` changes prevent AttributeErrors in production code when `tie_word_embeddings` is missing from model configurations.

**Potential risks**  
The workaround in the test creates a temporary dependency on transformers issue #43522. If this issue is resolved but the workaround remains, it could cause incorrect test behavior. Additionally, using `getattr` with default values might mask genuine configuration errors where `tie_word_embeddings` should be defined but isn't.

**Key insights**  
Developers should track the transformers issue and remove the test workaround once it's resolved. The `getattr` pattern improves code robustness but should be documented to clarify when attributes are truly optional versus required. Consider adding configuration validation to catch missing required attributes early.

---

## 34. [[Frontend] Frontend will only attach supported tasks corresponding entrypoints.](https://github.com/vllm-project/vllm/pull/33139)


### Base Information

- **PR Number:** #33139
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-27 04:15:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33139/files) (10):**
  - `tests/entrypoints/openai/test_transcription_validation_whisper.py`
  - `tests/entrypoints/openai/test_translation_validation.py`
  - `tests/entrypoints/pooling/classify/test_online.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/generate/__init__.py`
  - `vllm/entrypoints/openai/generate/api_router.py`
  - `vllm/entrypoints/openai/translations/api_router.py`
  - `vllm/entrypoints/pooling/__init__.py`
  - `vllm/entrypoints/sagemaker/api_router.py`
  - `vllm/entrypoints/sagemaker/routes.py`

### Summary

**What changed and why**  
This PR refactors the API server initialization to be task-aware, conditionally attaching routers and initializing state based on the engine's `supported_tasks`. It introduces modular initialization functions for different task groups (generate, transcription, pooling) and consolidates SageMaker routing into a new `api_router.py`.

**Technical impact**  
The architecture shifts from a monolithic initialization to a modular, task-driven approach. Routers and serving objects are now only instantiated if the engine supports the corresponding task, reducing unnecessary overhead. The SageMaker router dynamically constructs its invocation handlers based on supported tasks, improving flexibility and maintainability.

**Potential risks**  
Conditional initialization could lead to missing routers if task detection fails or if new task types are added without corresponding updates. The dynamic SageMaker routing may introduce subtle bugs if the task-to-handler mapping is incomplete. Changes in error handling (e.g., switching from 400 BadRequest to 404 NotFound in tests) may affect client compatibility.

**Key insights**  
Developers should ensure that any new task types are properly integrated into the conditional registration logic. The modular design improves scalability but requires careful testing to verify that all supported tasks are correctly wired. The updated error responses align with RESTful conventions but may require client-side adjustments.

---

## 35. [[AMD][QWEN3-NEXT] FP8 Tunings](https://github.com/vllm-project/vllm/pull/32042)


### Base Information

- **PR Number:** #32042
- **Author:** [draftbk](https://github.com/draftbk)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-27 01:34:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32042/files) (5):**
  - `benchmarks/kernels/benchmark_w8a8_block_fp8.py`
  - `vllm/model_executor/layers/quantization/utils/configs/N=1024,K=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json`
  - `vllm/model_executor/layers/quantization/utils/configs/N=12288,K=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json`
  - `vllm/model_executor/layers/quantization/utils/configs/N=2048,K=4096,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json`
  - `vllm/model_executor/layers/quantization/utils/configs/N=9216,K=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json`

### Summary

**What changed and why**  
This PR adds four pre-tuned FP8 block-scaled matrix multiplication kernel configuration files for the AMD MI300X GPU, targeting specific tensor dimensions (N, K) used by the Qwen3-Next-80B model. Additionally, it updates the benchmark script to support both CUDA and ROCm platforms, enabling tuning and validation on AMD hardware.

**Technical impact**  
The new configuration files eliminate performance warnings and allow the system to use optimized kernel parameters for FP8 operations on MI300X, resulting in measurable latency improvements (3–10% across batch sizes). The benchmark script modification extends tuning capability to ROCm, broadening platform support for future optimizations.

**Potential risks**  
The configurations are specific to MI300X and the given tensor dimensions; they may not be optimal for other GPU models or dimension combinations. Relying on these static JSON files could lead to suboptimal performance if model architectures or kernel implementations evolve. The benchmark script change introduces ROCm support but assumes equivalent tuning behavior between CUDA and ROCm.

**Key insights**  
Developers should note that these tunings are hardware- and dimension-specific; similar optimizations may be needed for other GPUs or model variants. The performance gains validate the importance of kernel tuning for FP8 operations. Future work should consider automating configuration generation or expanding the tuning coverage to more dimension combinations.

---

