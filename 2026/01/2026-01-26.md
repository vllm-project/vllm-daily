# vLLM Merged PR Report

**Report Date:** 2026-01-26 PST

**Total Merged PRs:** 36

---

## 1. [[AMD][Kernel][BugFix] Use correct scale in concat_and_cache_ds_mla_kernel when on gfx942](https://github.com/vllm-project/vllm/pull/32976)


### Base Information

- **PR Number:** #32976
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-26 23:16:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32976/files) (1):**
  - `csrc/cache_kernels.cu`

### Summary

**What changed and why**  
The PR fixes an incorrect scale divisor (448.0) used in FP8 quantization kernels for AMD gfx942 architectures (e.g., MI300, MI325). It introduces a compile-time constant `kFp8ScaleDivisor` set to 224.0 for gfx942 and 448.0 otherwise, ensuring proper scaling for DeepSeek-R1 model accuracy on ROCm.

**Technical impact**  
This change ensures correct FP8 scaling behavior on gfx942 GPUs, directly affecting quantization accuracy in KV cache operations. The consolidation into a shared `constexpr` constant improves maintainability and reduces duplication across two kernels (`concat_and_cache_ds_mla_kernel` and `indexer_k_quant_and_cache_kernel`).

**Potential risks**  
The fix relies on the `__gfx942__` macro, which may not cover future AMD architectures with similar scaling requirements. Hardcoded divisor values lack clear documentation on their derivation, making future adjustments opaque. Additionally, the change could subtly affect numerical precision for non-gfx942 platforms if the constant is inadvertently modified.

**Key insights**  
Always validate quantization scaling factors per hardware architecture, as seen here with gfx942 requiring a different divisor. Consider defining architecture-specific constants in a central header for broader reuse. Future updates should document the rationale for divisor values to aid debugging and maintenance.

---

## 2. [[Models] Kimi-K2.5](https://github.com/vllm-project/vllm/pull/33131)


### Base Information

- **PR Number:** #33131
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-01-26 22:50:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33131/files) (16):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/envs.py`
  - `vllm/model_executor/models/kimi_k25.py`
  - `vllm/model_executor/models/kimi_k25_vit.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/parse.py`
  - `vllm/multimodal/video.py`
  - `vllm/reasoning/__init__.py`
  - `vllm/reasoning/kimi_k2_reasoning_parser.py`
  - `vllm/renderers/hf.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/kimi_k25.py`

### Summary

**What changed and why**  
This PR adds support for the Kimi-K2.5 multimodal model, which extends Kimi-K2 with vision capabilities. The changes introduce a new model class (`KimiK25ForConditionalGeneration`), a vision tower implementation, multimodal processing logic, and a unified `vision_chunk` modality to handle both images and video chunks. The implementation follows the recipe documented at the provided URL.

**Technical impact**  
The PR extends vLLM's multimodal framework by introducing a new `vision_chunk` modality that unifies image and video processing. It adds a new video loader backend (`identity`) to pass raw video bytes to the model processor for custom chunk splitting. The changes also affect chat rendering, UUID rebuilding, and reasoning parser delegation (defaulting to thinking mode). The architecture integrates with existing multimodal pipelines while adding model-specific handling for video chunk prompts and positional embeddings.

**Potential risks**  
- The `identity` video loader is marked as temporary and may need to be reverted before release, which could break video processing if overlooked.  
- The unified `vision_chunk` modality introduces complexity in modality mapping and UUID tracking, which could lead to mismatches if not handled consistently.  
- Custom video chunk splitting and prompt replacement logic may have edge cases with varying numbers of placeholders or video items.  
- The reasoning parser defaults to thinking mode, which may not align with all use cases and could affect output formatting.

**Key insights**  
- The PR demonstrates a pattern for adding new multimodal models with unified vision handling, which can inform future model integrations.  
- Developers should ensure the `identity` video loader is replaced with a stable backend before release.  
- The `use_unified_vision_chunk` flag and associated logic are model-specific; similar additions should be carefully namespaced to avoid conflicts.  
- Testing should cover mixed image/video inputs and edge cases in video chunk splitting and prompt replacement.

---

## 3. [[CI][Pooling] Stabilize ModernBERT test](https://github.com/vllm-project/vllm/pull/32909)


### Base Information

- **PR Number:** #32909
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-26 21:26:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32909/files) (1):**
  - `tests/models/language/pooling/test_token_classification.py`

### Summary

**What changed and why**  
This PR addresses a flaky test for the ModernBERT model by adding a global test fixture for deterministic random seeding, marking the specific test as flaky with retries, and adding an explanatory print statement. The changes aim to stabilize test execution by ensuring reproducibility and allowing automatic retries for intermittent failures caused by numerical precision variance in randomly initialized models.

**Technical impact**  
The addition of the `seed_everything` fixture ensures all random number generators (Python, NumPy, PyTorch CPU/CUDA) are seeded consistently across test runs, improving reproducibility. Marking `test_modernbert_models` with `@pytest.mark.flaky(reruns=3)` allows the test to automatically retry up to three times upon failure, reducing CI noise from intermittent numerical issues.

**Potential risks**  
The global seeding fixture may affect other tests in the same module, potentially masking non-deterministic issues elsewhere. Relying on retries rather than addressing the root cause of numerical instability could allow underlying precision problems to persist. The increased test runtime from retries may impact CI performance if failures are frequent.

**Key insights**  
The test instability stems from randomly initialized model weights causing numerical variance, which is a known characteristic of this specific model. Developers should consider if the tolerance (`atol=1.2e-2`) remains appropriate given the observed differences up to ~0.0176. For long-term stability, investigate whether the model's architecture or the pooling implementation introduces unexpected non-determinism beyond typical floating-point variance.

---

## 4. [[code clean] remove duplicate code](https://github.com/vllm-project/vllm/pull/33135)


### Base Information

- **PR Number:** #33135
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-26 20:57:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33135/files) (1):**
  - `vllm/v1/structured_output/__init__.py`

### Summary

**What changed and why**  
Removed three lines of duplicate code that assigned `reasoning_parser` from the structured outputs configuration. The variable was declared but not used elsewhere in the code, indicating it was leftover from a previous implementation or refactoring.

**Technical impact**  
This change eliminates dead code, reducing cognitive load and potential confusion for developers. It does not affect runtime behavior since the variable was unused, and the removal simplifies the initialization logic slightly.

**Potential risks**  
Minimal risk, as the variable was not referenced. However, if `reasoning_parser` was intended to be used elsewhere (e.g., in future features), its removal might require re-adding it later. Ensure no downstream code or tests implicitly depend on this variable.

**Key insights**  
Always remove unused variables to maintain clean, readable code. Before deleting, verify the variable isn't referenced in other modules or via dynamic access. Consider if the variable name suggests planned functionality that should be documented or implemented.

---

## 5. [[Frontend] Cleanup serving engine](https://github.com/vllm-project/vllm/pull/33103)


### Base Information

- **PR Number:** #33103
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-26 20:47:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33103/files) (5):**
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/utils.py`

### Summary

**What changed and why**  
This PR refactors prompt handling logic across OpenAI API endpoints by replacing class methods with direct imports of `get_prompt_components` and extending `get_max_tokens` to handle all request types. The changes eliminate duplicate logic for calculating prompt length and remove unnecessary initialization checks for `default_sampling_params`.

**Technical impact**  
The refactoring centralizes prompt parsing logic through the imported `get_prompt_components` function and standardizes max token calculation via the enhanced `get_max_tokens` utility. This creates a more modular architecture where prompt handling is decoupled from individual serving classes, improving code reuse and maintainability across chat completion, completion, and responses APIs.

**Potential risks**  
The removal of `self.default_sampling_params` initialization checks could lead to `None` values being passed if not properly initialized elsewhere. The extended `get_max_tokens` function now has more complex logic for extracting max tokens from different request types, which increases the risk of missing edge cases or incorrect fallback behavior.

**Key insights**  
Developers should verify that `default_sampling_params` is properly initialized in all serving classes before these changes. The refactored `get_max_tokens` function now supports three request types, so any future API additions should follow the same pattern. The centralized prompt handling improves consistency but requires thorough testing across all OpenAI endpoints.

---

## 6. [[torch.compile] Stop assuming 32 bit indexing](https://github.com/vllm-project/vllm/pull/33113)


### Base Information

- **PR Number:** #33113
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-26 20:25:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33113/files) (1):**
  - `vllm/config/compilation.py`

### Summary

**What changed and why**  
The change reverts the default value of `assume_32_bit_indexing` from `True` to `False` in the `DynamicShapesConfig` class. This flag was introduced in PyTorch 2.10 to allow vLLM to assume tensor sizes use 32-bit indexing, but internal errors revealed that the assumption doesn't always hold—specifically, it incorrectly assumes `Tensor.numel()` is always 32-bit, which isn't guaranteed. Until the condition can be properly inferred, the safer default is restored.

**Technical impact**  
This change affects how PyTorch compile handles tensor indexing during dynamic shape compilation. With `assume_32_bit_indexing=False`, the compiler will no longer rely on 32-bit indexing assumptions, potentially avoiding runtime errors when tensors exceed 32-bit indexable sizes. This reverts to the PyTorch 2.9 behavior, ensuring broader compatibility but possibly impacting performance optimizations that depended on the 32-bit assumption.

**Potential risks**  
Reverting to `False` may reintroduce performance overheads in scenarios where 32-bit indexing was valid and beneficial. Additionally, if downstream code or configurations explicitly depended on `True` being the default, they might need adjustments. There's also a risk that the underlying issue—accurately inferring when 32-bit indexing is safe—remains unresolved, leaving a latent optimization gap.

**Key insights**  
Developers should be aware that this flag now defaults to `False` and requires PyTorch 2.10+ to set it to `True`. When enabling `assume_32_bit_indexing=True`, ensure tensor sizes are within 32-bit limits to avoid errors. Consider adding validation or inference logic in the future to automatically determine the safe setting, balancing performance and correctness.

---

## 7. [[Frontend] Reduce mixin usage in serving pooling](https://github.com/vllm-project/vllm/pull/33101)


### Base Information

- **PR Number:** #33101
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-26 19:50:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33101/files) (3):**
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/pooling/classify/serving.py`
  - `vllm/entrypoints/pooling/embed/serving.py`

### Summary

**What changed and why**  
This PR simplifies the inheritance structure by removing mixins (`RequestProcessingMixin` and `ResponseGenerationMixin`) from the serving engine code. The mixins were previously used to share functionality between classification and embedding serving classes but complicated the code. The changes consolidate functionality directly into a single `ServeContext` class and refactor the embedding and classification serving implementations to inherit directly from `OpenAIServing` without intermediate mixin classes.

**Technical impact**  
The refactoring reduces code complexity by flattening the inheritance hierarchy. Type annotations are improved with more specific generics (e.g., `ServeContext[ClassificationRequest]`). The `ServeContext` class now directly contains fields like `engine_prompts`, `result_generator`, and `final_res_batch`, eliminating the need for mixin composition. This makes the code easier to follow and maintain, especially for new developers.

**Potential risks**  
There's a risk of breaking changes if any external code depended on the specific mixin class interfaces. The removal of type casts (`cast(ClassificationServeContext, ctx)`) could introduce runtime type errors if the generic type constraints are not properly enforced. Additionally, the consolidation of logic might obscure subtle differences between classification and embedding request handling that were previously separated by mixins.

**Key insights**  
This is a positive refactoring that improves code clarity by reducing abstraction layers. Developers should verify that all request types (chat, completion, embedding) are correctly handled after the changes. Pay close attention to the `_prepare_generators` and `_collect_batch` methods in the embedding serving class, as they contain custom chunked processing logic that differs from the base implementation.

---

## 8. [[Perf] avoid duplicate mem_get_info() call in get_current_memory_usage](https://github.com/vllm-project/vllm/pull/33064)


### Base Information

- **PR Number:** #33064
- **Author:** [pacoxu](https://github.com/pacoxu)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-26 19:45:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33064/files) (1):**
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
The change replaces two separate `torch.cuda.mem_get_info(device)` calls with a single call, storing the returned `(free_mem, total_mem)` tuple in variables. This eliminates a redundant system call, improving performance by reducing overhead, and enhances readability by making the memory calculation more explicit.

**Technical impact**  
This optimization reduces the number of CUDA memory queries from two to one per invocation of `get_current_memory_usage()`. While the performance gain per call is small, it may accumulate in scenarios where this method is called frequently, such as during memory monitoring or profiling loops. The logic remains functionally identical.

**Potential risks**  
The risk is minimal since the calculation (`total_mem - free_mem`) is unchanged. However, if `mem_get_info()` were to return inconsistent values between the two original calls (e.g., due to rapid memory fluctuations), the new version using a single snapshot could theoretically yield a more consistent result, though this is unlikely to affect correctness.

**Key insights**  
This is a straightforward micro-optimization that improves both performance and code clarity. Developers should adopt similar patterns to avoid redundant expensive operations, especially in performance-critical paths. Ensure that any related tests or benchmarks confirm no regression in memory usage reporting.

---

## 9. [[DOC]: Add warning about max_num_batched_tokens and max_model_len when chunked prefill is disabled](https://github.com/vllm-project/vllm/pull/33109)


### Base Information

- **PR Number:** #33109
- **Author:** [VincentG1234](https://github.com/VincentG1234)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-26 19:05:03
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33109/files) (1):**
  - `docs/configuration/optimization.md`

### Summary

**What changed and why**  
This PR adds a documentation warning clarifying that when chunked prefill is disabled, `max_num_batched_tokens` must be greater than `max_model_len`. The change addresses a previously undocumented constraint that can cause vLLM to fail at server startup during argument validation.

**Technical impact**  
No functional changes are introduced; this is purely a documentation update. However, it clarifies a critical configuration requirement that is already enforced in the scheduler code, helping users avoid startup failures when tuning memory or throughput parameters.

**Potential risks**  
Since this is documentation-only, there are no direct code risks. However, if users ignore this warning, they may encounter confusing startup errors. The warning could be missed if users do not review the optimization documentation thoroughly.

**Key insights**  
This update improves user experience by documenting an implicit validation rule. Developers should ensure that any configuration changes respect this constraint, especially when disabling chunked prefill. Consider adding similar validation warnings in other relevant configuration sections or error messages for better visibility.

---

## 10. [Fix IndexError with encoder-decoder models when using Custom Paged Attention](https://github.com/vllm-project/vllm/pull/33112)


### Base Information

- **PR Number:** #33112
- **Author:** [sstamenk](https://github.com/sstamenk)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-26 18:33:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33112/files) (2):**
  - `vllm/v1/attention/backends/rocm_attn.py`
  - `vllm/v1/attention/ops/chunked_prefill_paged_decode.py`

### Summary

**What changed and why**  
Fixed an `IndexError` in encoder-decoder models (e.g., Whisper) when using the ROCm Attention backend. The error occurred during cross-attention decode steps where key/value tensors are `None` (already cached from the encoder). The backend was unconditionally passing these `None` values to cache-writing and attention functions, causing dimension access errors.

**Technical impact**  
The changes add null checks in two locations: `rocm_attn.py` now guards cache writes and slices key/value only when non-`None`, while `chunked_prefill_paged_decode.py` derives `num_kv_heads` from `key_cache` when `key` is `None`. This ensures encoder-decoder cross-attention works correctly with the ROCm Paged Attention backend, maintaining compatibility with existing prefill/decode logic.

**Potential risks**  
If `key_cache` is also `None` in cross-attention scenarios, the fallback in `chunked_prefill_paged_decode.py` could still fail. Additionally, other attention backends might have similar unhandled `None` cases for encoder-decoder models, suggesting a broader review of cross-attention support.

**Key insights**  
Always validate tensor existence before shape access in attention layers, especially for encoder-decoder architectures. Consider adding a shared utility or base class method to handle `None` key/value tensors consistently across all attention backends. The fix is minimal and focused, but testing should include varied encoder-decoder models to ensure robustness.

---

## 11. [fix: preserve native tool call ID in multi-turn tool calling](https://github.com/vllm-project/vllm/pull/32768)


### Base Information

- **PR Number:** #32768
- **Author:** [wangln19](https://github.com/wangln19)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-01-26 18:22:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32768/files) (7):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/tool_parsers/kimi_k2_tool_parser.py`

### Summary

**What changed and why**  
This PR fixes multi-turn tool calling by preserving native tool call IDs generated by models like Kimi K2. Previously, the system was discarding these IDs and generating random ones, which broke model expectations for consistent IDs across turns. The fix adds an optional `id` field to `FunctionCall`, preserves it through parsing, and uses it in response generation.

**Technical impact**  
The changes affect tool call ID handling across multiple components: `FunctionCall` now stores native IDs internally, tool parsers preserve them, and serving layers use them when available. Models relying on specific ID formats (like Kimi K2's `functions.get_weather:0`) will now maintain consistency. The architecture now distinguishes between ID generation strategies (`kimi_k2` vs `random`) based on model type.

**Potential risks**  
The internal `id` field in `FunctionCall` is excluded from serialization, which could cause confusion if other code paths expect it to be serialized. Changes to ID generation logic in multiple places increase complexity and risk of inconsistencies. Edge cases with mixed tool call scenarios (some with IDs, some without) may need additional validation.

**Key insights**  
Developers should ensure tool parsers properly populate the `id` field when available. The fix requires coordinated changes across parsing, serving, and response layers—future modifications should maintain this consistency. Testing should verify multi-turn tool calling works correctly for both Kimi K2 and other models.

---

## 12. [[MoE Refactor] Integrate Naive Prepare Finalize into MK](https://github.com/vllm-project/vllm/pull/32567)


### Base Information

- **PR Number:** #32567
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-26 17:28:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32567/files) (46):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/kernels.yaml`
  - `benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py`
  - `docs/design/moe_kernel_features.md`
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/modular_kernel_tools/mk_objects.py`
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/kernels/moe/test_flashinfer_moe.py`
  - `tests/kernels/moe/test_nvfp4_moe.py`
  - `vllm/distributed/device_communicators/all2all.py`
  - `vllm/distributed/device_communicators/base_device_communicator.py`
  - `vllm/distributed/device_communicators/cpu_communicator.py`
  - `vllm/distributed/device_communicators/cuda_communicator.py`
  - `vllm/distributed/device_communicators/mnnvl_compat.py`
  - `vllm/distributed/device_communicators/xpu_communicator.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/model_executor/layers/fused_moe/all2all_utils.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_a2a_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_method_base.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/mori_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/shared_fused_moe.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`
  - `vllm/model_executor/warmup/deep_gemm_warmup.py`

### Summary

**What changed and why**  
This PR integrates naive All-to-All (A2A) kernels into the modular kernel (MK) framework, unifying how FP8 and NVFP4 quantization methods "own" the modular kernels for both tensor parallelism (TP) and data/expert parallelism (DP/EP). The changes migrate FlashInfer implementations to use the new naive EP prepare/finalize class, update communication managers to support dispatching top-k IDs and weights (not just router logits), and remove legacy dispatch/combine logic from `FusedMoE.forward()`. This sets up the removal of `ModularKernelMethod` and consolidates kernel ownership under quant methods.

**Technical impact**  
The refactor centralizes kernel creation and dispatch logic within quant methods (FP8/NVFP4), making the architecture more consistent and reducing special-case handling. Communication interfaces now have separate `dispatch_router_logits` and `dispatch` methods, enabling optimized data movement for top-k metadata. The new `MoEPrepareAndFinalizeNaiveEP` handles DP/EP cases with naive A2A, while `FlashInferA2APrepareAndFinalize` replaces the old FlashInfer-specific prepare/finalize classes. Several expert kernels (DeepGEMM, Triton, etc.) now guard against unsupported FlashInfer A2A backends.

**Potential risks**  
- The migration introduces a transitional `allow_new_interface` flag; inconsistent use could lead to kernel selection failures.  
- Some expert kernels (DeepGEMM, Marlin, Triton, VLLM CUTLASS) explicitly disable FlashInfer A2A support, which may limit deployment options.  
- The removal of `defer_input_quant` parameters in tests and benchmarks assumes all quant methods now handle input quantization internally—any oversight could break existing configurations.  
- The refactor touches many communication layers; subtle bugs in dispatch/combine logic could affect distributed correctness.

**Key insights**  
- Developers must ensure quant methods set `supports_internal_mk` and `mk_owns_shared_expert` appropriately during migration.  
- The new `dispatch` method for top-k tensors improves performance but requires all communication backends to implement it (naive, AG/RS, etc.).  
- Testing should focus on edge cases: DP>1 with EP=1, sequence parallelism, and mixed quantization backends.  
- The TODO list indicates FlashInfer static per-tensor FP8, DeepGEMM, Triton, and VLLM CUTLASS kernels still have compatibility issues—guards are added but need resolution.

---

## 13. [[Model Runner V2] Remove UvaBufferPool for cpu->gpu copy](https://github.com/vllm-project/vllm/pull/33055)


### Base Information

- **PR Number:** #33055
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-26 16:47:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33055/files) (3):**
  - `vllm/v1/worker/gpu/buffer_utils.py`
  - `vllm/v1/worker/gpu/mm/encoder_runner.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The PR replaces the `UvaBufferPool` mechanism for CPU-to-GPU copies with a simpler approach using temporary pinned CPU tensors. A new `async_copy_to_gpu` function was added to handle these asynchronous copies, and all usages of `UvaBufferPool` were removed from `encoder_runner.py` and `model_runner.py`.

**Technical impact**  
This change simplifies memory management by eliminating the `UvaBufferPool` abstraction layer. The new approach directly uses PyTorch's `pin_memory()` and `copy_()` with `non_blocking=True` for asynchronous transfers, which may reduce overhead and complexity. The `is_mm_embed` tensor in `encoder_runner.py` is now created with `pin_memory=True` to enable efficient transfers.

**Potential risks**  
The removal of the pooling mechanism could lead to increased memory allocation overhead if `async_copy_to_gpu` is called frequently with varying tensor sizes. There's also a risk of memory fragmentation from repeatedly creating and destroying pinned memory regions. The change assumes the new copy mechanism provides equivalent or better performance without the pooling benefits.

**Key insights**  
This refactoring improves code simplicity by leveraging PyTorch's built-in memory pinning capabilities. Developers should monitor performance metrics to ensure the new approach doesn't introduce regressions in high-throughput scenarios. Consider adding metrics to track pinned memory allocation frequency if performance becomes a concern.

---

## 14. [[Bugfix][TPU] Return a Default fp8 MoE Backend](https://github.com/vllm-project/vllm/pull/32908)


### Base Information

- **PR Number:** #32908
- **Author:** [vanbasten23](https://github.com/vanbasten23)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-26 15:46:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32908/files) (1):**
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`

### Summary

**What changed and why**  
The change modifies the `select_fp8_moe_backend` function to return a default `Fp8MoeBackend.NONE` value for non-CUDA/non-ROCm platforms (specifically TPU) instead of raising a `NotImplementedError`. This addresses a TPU runtime failure caused by the previous strict error-raising behavior.

**Technical impact**  
This introduces platform-specific behavior where TPU deployments will now default to no FP8 MoE backend, allowing execution to proceed without FP8 optimizations. The change is a temporary workaround until a proper plugin-based backend registration system is implemented, as noted in the TODO comment.

**Potential risks**  
The default `NONE` backend may lead to performance degradation or unexpected behavior on TPU if FP8 support is expected. There's also a risk of masking configuration issues, as the function no longer raises an error for unsupported backends on TPU. The temporary nature of this fix could lead to technical debt if the plugin registration system is delayed.

**Key insights**  
Developers should treat this as an interim solution and prioritize implementing the plugin-based backend registration system referenced in the TODO. TPU users should verify that their models function correctly without FP8 MoE optimizations. The change highlights the need for a more extensible architecture to handle platform-specific backend variations.

---

## 15. [[Bugfix][MXFP4] Call `trtllm_fp4_block_scale_moe` with kwargs](https://github.com/vllm-project/vllm/pull/33104)


### Base Information

- **PR Number:** #33104
- **Author:** [wpc](https://github.com/wpc)
- **Merged By:** [luccafong](https://github.com/luccafong)
- **Merged time:** 2026-01-26 15:13:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33104/files) (1):**
  - `vllm/model_executor/layers/quantization/mxfp4.py`

### Summary

**What changed and why**  
The PR changes the `trtllm_fp4_block_scale_moe` kernel call from positional arguments to keyword arguments. This addresses a breaking integration issue caused by a removed positional argument (`tile_tokens_dim`) in an internal flashinfer version, ensuring compatibility across different flashinfer versions and improving code readability.

**Technical impact**  
The change decouples the function call from parameter order dependencies, making the code more resilient to upstream API changes in flashinfer. It maintains identical runtime behavior but reduces the risk of silent errors when argument positions shift in future updates.

**Potential risks**  
If the keyword argument names do not match the underlying kernel's expected parameter names (e.g., due to version mismatches), runtime errors could occur. Additionally, any future renaming of these parameters in flashinfer would require corresponding updates here.

**Key insights**  
Using keyword arguments is a best practice for stability in cross-version integrations. Developers should verify that the keyword names align with the target flashinfer version's API. Consider adding a version compatibility check or documentation note to prevent future integration breaks.

---

## 16. [[fix] CPUDNNLGEMMHandler pointer baked into inductor artifact](https://github.com/vllm-project/vllm/pull/32913)


### Base Information

- **PR Number:** #32913
- **Author:** [dolpm](https://github.com/dolpm)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-26 13:59:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32913/files) (4):**
  - `csrc/cpu/dnnl_kernels.cpp`
  - `csrc/cpu/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
The fix addresses an issue where CPU DNNL GEMM handler pointers were being passed as integers through the PyTorch graph, causing them to be baked into inductor artifacts. Instead, handler pointers are now wrapped in tensors, allowing them to be treated as graph constants that can be properly folded. This ensures the loaded artifacts remain functional.

**Technical impact**  
The changes modify the kernel interfaces to accept handler pointers as tensors rather than raw integers. This aligns with PyTorch's graph representation, preventing the pointers from being inlined as constants in compiled graphs. The Python-side wrapper now stores handlers in tensors, and the C++ side extracts the pointer via `item<int64_t>()`.

**Potential risks**  
Extracting pointers via `item<int64_t>()` assumes the tensor contains a valid pointer value; corrupted or mismatched tensors could lead to undefined behavior or crashes. There is also a risk if the tensor is moved or modified unexpectedly. Additionally, re-enabling AOT compilation for CPU (removed guard) may reintroduce issues if underlying problems are not fully resolved.

**Key insights**  
Wrapping pointers in tensors is a standard pattern for passing opaque data through PyTorch graphs. Developers should ensure handler tensors are kept alive and not mutated. The removal of the CPU AOT compilation guard suggests the root cause is fixed, but monitoring for related issues is advised.

---

## 17. [[feat][log]: add `--disable-access-log-for-endpoints` CLI option](https://github.com/vllm-project/vllm/pull/30011)


### Base Information

- **PR Number:** #30011
- **Author:** [JaredforReal](https://github.com/JaredforReal)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-01-26 13:49:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30011/files) (6):**
  - `examples/others/logging_configuration.md`
  - `tests/test_access_log_filter.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/logging_utils/__init__.py`
  - `vllm/logging_utils/access_log_filter.py`

### Summary

**What changed and why**  
A new CLI option `--disable-access-log-for-endpoints` was added to suppress uvicorn access logs for specified endpoints (e.g., `/health`, `/metrics`). This addresses production log noise from frequent health-check requests by load balancers and monitoring systems, while preserving logs for other endpoints.

**Technical impact**  
The changes introduce a configurable logging filter (`UvicornAccessLogFilter`) that integrates with uvicorn's logging configuration. The filter examines log record arguments to match request paths, ignoring query parameters. A new function `create_uvicorn_log_config` dynamically builds a logging dictionary with the filter applied, and the CLI argument is parsed as a comma-separated string into a list of paths.

**Potential risks**  
The filter relies on uvicorn's log format remaining stable; changes to log argument ordering could break filtering. Path matching is exact and case-sensitive, which may miss variations (e.g., trailing slashes). The filter only applies to the `uvicorn.access` logger, so other loggers (e.g., application logs) are unaffected, but misconfiguration could inadvertently disable all access logs if the filter logic fails.

**Key insights**  
This solution effectively reduces log volume without disabling all access logs. Developers should note that paths must match exactly and query strings are stripped. Consider extending the filter to support regex or prefix matching if needed. Ensure the filter is tested after uvicorn updates to verify compatibility with log format changes.

---

## 18. [[Refactor] Remove unused `_moe_permute` function](https://github.com/vllm-project/vllm/pull/33108)


### Base Information

- **PR Number:** #33108
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-26 13:06:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33108/files) (2):**
  - `benchmarks/kernels/benchmark_moe_permute_unpermute.py`
  - `vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py`

### Summary

**What changed and why**  
This PR removes the unused `_moe_permute` and `_moe_unpermute_and_reduce` functions from the codebase. The functions were identified as unused and slower than the current implementation (`moe_permute`/`moe_unpermute`), as evidenced by the benchmark results showing significantly better performance with `use_customized_permute=True` (the new implementation).

**Technical impact**  
The removal eliminates dead code and simplifies the codebase by removing two functions and their dependencies. The benchmark script is updated to only test the active `moe_permute`/`moe_unpermute` functions, removing the comparison logic between old and new implementations. This reduces maintenance overhead and potential confusion.

**Potential risks**  
The main risk is if any external or internal code was importing these functions directly (though the PR states they're unused). The benchmark changes could affect performance tracking if historical comparisons with the old implementation were needed. There's also a risk that the removed functions contained unique logic that might be needed for edge cases not covered by the current implementation.

**Key insights**  
This is a clean-up PR that improves code hygiene by removing unused components. Developers should verify no other modules import these functions and ensure the benchmark accurately reflects production performance. The performance data shows the new implementation is 2-3x faster for permute operations, validating the removal decision.

---

## 19. [[ci] Sync test areas with test-pipeline.yaml and enable new pipeline generator](https://github.com/vllm-project/vllm/pull/33080)


### Base Information

- **PR Number:** #33080
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-26 12:28:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33080/files) (24):**
  - `.buildkite/ci_config.yaml`
  - `.buildkite/hardware_tests/amd.yaml`
  - `.buildkite/hardware_tests/arm.yaml`
  - `.buildkite/hardware_tests/ascend_npu.yaml`
  - `.buildkite/hardware_tests/gh200.yaml`
  - `.buildkite/hardware_tests/intel.yaml`
  - `.buildkite/image_build/image_build.sh`
  - `.buildkite/image_build/image_build.yaml`
  - `.buildkite/test_areas/attention.yaml`
  - `.buildkite/test_areas/compile.yaml`
  - `.buildkite/test_areas/distributed.yaml`
  - `.buildkite/test_areas/e2e_integration.yaml`
  - `.buildkite/test_areas/engine.yaml`
  - `.buildkite/test_areas/expert_parallelism.yaml`
  - `.buildkite/test_areas/kernels.yaml`
  - `.buildkite/test_areas/lm_eval.yaml`
  - `.buildkite/test_areas/lora.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `.buildkite/test_areas/models_basic.yaml`
  - `.buildkite/test_areas/models_distributed.yaml`
  - `.buildkite/test_areas/models_multimodal.yaml`
  - `.buildkite/test_areas/plugins.yaml`
  - `.buildkite/test_areas/quantization.yaml`
  - `.buildkite/test_areas/weight_loading.yaml`

### Summary

**What changed and why**  
This PR introduces a new hardware test pipeline and refactors the CI configuration to support multiple device types beyond GPUs. Key changes include adding hardware test configurations for AMD, Arm, Intel, Ascend NPU, and GH200 devices; refactoring `image-build.sh` to optimize Docker image builds with caching and buildx improvements; and standardizing the terminology from `gpu` to `device` across test area files to accommodate diverse hardware.

**Technical impact**  
The changes expand CI coverage to heterogeneous hardware platforms, enabling cross-architecture testing. The refactored `image-build.sh` introduces advanced caching strategies, buildx optimizations, and conditional logic for main vs. non-main branches, which should reduce build times and improve resource utilization. The shift from `gpu` to `device` as a configuration key makes the pipeline more generic and maintainable.

**Potential risks**  
The new hardware tests rely on external scripts (e.g., `run-cpu-test-arm.sh`) that are not included in this PR, which could lead to failures if those scripts are missing or incompatible. The conditional branch logic in `image_build.yaml` may introduce complexity if not all branches are properly tested. Additionally, the extensive changes to `image-build.sh` increase script complexity, raising the risk of subtle bugs in caching or build logic.

**Key insights**  
Developers should verify that all referenced hardware test scripts exist and are functional before merging. The `device` field should be used consistently in future test additions. The image build optimizations are significant but require monitoring to ensure caching behaves as expected across different branch types (main, PR, feature branches).

---

## 20. [[Bugfix] Fix Dtypes for Pynccl Wrapper](https://github.com/vllm-project/vllm/pull/33030)


### Base Information

- **PR Number:** #33030
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-26 12:09:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33030/files) (2):**
  - `vllm/distributed/device_communicators/pynccl_wrapper.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py`

### Summary

**What changed and why**  
This PR fixes dtype compatibility issues in the PyNCCL wrapper and FlashInfer integration. The primary change adds support for `torch.float8_e4m3fn` dtype in NCCL collectives, addressing a regression introduced when scales were switched from uint8 to fp8. Additionally, a workaround ensures uint8 scale tensors are properly handled in FlashInfer's fused MoE layer.

**Technical impact**  
The NCCL wrapper now correctly maps `torch.float8_e4m3fn` to the `ncclFloat8e4m3` enum, enabling proper collective operations with fp8 scale tensors. The FlashInfer modification adds a view operation to convert 1-byte element scale tensors to uint8 format before processing, maintaining compatibility with existing tensor manipulation functions.

**Potential risks**  
The FlashInfer change assumes scale tensors with element size 1 should be treated as uint8, which may not hold for all future data types. The NCCL dtype mapping could become outdated if new torch dtypes are introduced without corresponding updates. There's also a risk that the error message update might miss other unsupported dtypes.

**Key insights**  
Always verify NCCL compatibility when introducing new tensor dtypes in distributed operations. The FlashInfer workaround suggests underlying tensor manipulation functions have strict dtype requirements. Consider adding systematic dtype validation in both locations to catch similar issues earlier in development.

---

## 21. [[Model] Bump transformers version for test registry](https://github.com/vllm-project/vllm/pull/33100)


### Base Information

- **PR Number:** #33100
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-26 10:53:23
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33100/files) (6):**
  - `tests/models/registry.py`
  - `tests/models/test_transformers.py`
  - `tests/v1/e2e/test_spec_decode.py`
  - `vllm/model_executor/models/transformers/base.py`
  - `vllm/model_executor/models/transformers/moe.py`
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
This PR updates the minimum transformers version requirements from development versions (5.0.0.dev/5.0.0.dev0) to stable releases (5.0.0/5.1.0) across the test registry and codebase. The changes reflect that Transformers 5.0 is now officially released, though some models still require upcoming 5.1.0 features.

**Technical impact**  
The updates ensure compatibility checks now validate against stable transformer releases rather than development versions. This affects model availability testing, version-based feature gating (encoder models, MoE support, Eagle3), and configuration handling for rope parameters. The test infrastructure now properly reflects the actual minimum version requirements for production use.

**Potential risks**  
Models requiring 5.1.0 (ExaoneMoE, GlmOcr) may become unavailable if transformers 5.1.0 isn't released yet. The PR description notes that transformers nightly tests are still failing, suggesting potential instability with the new release. There's also risk of regression if any features were incorrectly version-gated to development releases.

**Key insights**  
This is a preparatory update aligning version checks with the official transformers 5.0 release. Developers should monitor transformers nightly test results before updating production requirements. Pay special attention to models bumped to 5.1.0 as they may have dependencies on unreleased features. Consider adding clearer documentation about which models require which specific transformers versions.

---

## 22. [[Bugfix] Fix Voxtral streaming slot_mapping](https://github.com/vllm-project/vllm/pull/33073)


### Base Information

- **PR Number:** #33073
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-26 10:40:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33073/files) (1):**
  - `vllm/model_executor/models/whisper_causal.py`

### Summary

**What changed and why**  
This PR adds a custom implementation class `WhisperCausalAttentionWithBlockPoolingImpl` to ensure that the transformed `slot_mapping` computed by the existing `WhisperCausalAttentionWithBlockPoolingBuilder` is used during KV cache updates, rather than the default `slot_mapping` from `forward_context`. This fixes a regression introduced by PR #25954 that broke custom attention backends manipulating `slot_mapping`.

**Technical impact**  
The change introduces a new `Impl` class that overrides the `forward` method to conditionally call `do_kv_cache_update` with `attn_metadata.slot_mapping`, aligning the behavior with the pattern used in `CrossAttentionImpl`. This ensures compatibility with custom attention backends that rely on transformed slot mappings, while maintaining the existing block-pooling logic for Whisper models.

**Potential risks**  
If the underlying attention backend already includes KV cache updates in its forward pass (`forward_includes_kv_cache_update=True`), the conditional logic might skip the necessary `do_kv_cache_update` call. Additionally, the fix assumes the `attn_metadata` is non-null when required, which could lead to runtime errors if metadata is unexpectedly missing.

**Key insights**  
Developers should verify that `forward_includes_kv_cache_update` is correctly configured for any custom attention backends integrated with this model. The pattern mirrors the CrossAttention fix, suggesting a reusable strategy for similar slot_mapping issues, but thorough testing is recommended to ensure no side effects in streaming or multi-backend scenarios.

---

## 23. [[FIX] Always support TP > 4 for FP4 Gemm](https://github.com/vllm-project/vllm/pull/31099)


### Base Information

- **PR Number:** #31099
- **Author:** [danielafrimi](https://github.com/danielafrimi)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-26 10:04:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31099/files) (3):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`

### Summary

**What changed and why**  
This PR fixes FP4 (NVFP4) quantization to support tensor parallelism (TP) >= 4 by ensuring weight and activation tensors meet the CUTLASS/FlashInfer kernel alignment requirement that matrix dimensions K and N be divisible by 32. It introduces padding for weights and activations, and slicing for outputs, to maintain compatibility.

**Technical impact**  
The changes modify weight loading and forward pass logic in two quantization backends (`compressed_tensors_w4a4_nvfp4.py` and `modelopt.py`), adding padding utilities in `quant_utils.py`. This ensures FP4 GEMM kernels operate correctly under TP >= 4, enabling previously failing models (e.g., Nemotron Nano v3) to initialize and run.

**Potential risks**  
Padding introduces slight memory overhead and potential performance impact due to extra computations. If `weights_padding_cols` is incorrectly calculated or not propagated, it could lead to shape mismatches. Edge cases with very small tensor sizes or non-standard alignments may need validation.

**Key insights**  
Developers should verify that padding utilities are applied consistently across all FP4 quantization paths. The fix is tightly coupled with kernel alignment constraints; any future changes to kernel requirements must update these padding functions. Testing should include varied TP configurations and model architectures to ensure robustness.

---

## 24. [Remove unused logic in `models/mistral.py`](https://github.com/vllm-project/vllm/pull/33095)


### Base Information

- **PR Number:** #33095
- **Author:** [andylolu2](https://github.com/andylolu2)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-26 09:01:52
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33095/files) (1):**
  - `vllm/model_executor/models/mistral.py`

### Summary

**What changed and why**  
Removed unused quantization scaling fusion logic that was previously added but never utilized. The code cleaned up conditional checks and assignments related to `quant_scaling_from` properties in layer normalization modules.

**Technical impact**  
This cleanup reduces code complexity and eliminates dead code paths, making the model initialization more straightforward. No functional changes occur since the removed logic was never executed under current configurations.

**Potential risks**  
Low risk as the logic was unused, but ensure no downstream dependencies or future configurations rely on these properties. Verify that `quant_config` and related fusion features remain properly handled elsewhere if needed.

**Key insights**  
Regular removal of dead code improves maintainability and readability. Developers should periodically audit for unused code segments introduced during feature development or refactoring. Consider adding linting rules to flag unused variables or conditions.

---

## 25. [[CI] Fix AssertionError: MCP tool call not found in output_messages](https://github.com/vllm-project/vllm/pull/33093)


### Base Information

- **PR Number:** #33093
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-26 07:19:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33093/files) (1):**
  - `tests/entrypoints/openai/responses/test_harmony.py`

### Summary

**What changed and why**  
The change modifies a test input from "Calculate 123 * 456 using python and print the result." to "Calculate 1234 * 4567 using python tool and print the result." This aims to fix an `AssertionError` related to MCP tool calls not being found in `output_messages` during CI test failures.

**Technical impact**  
This update adjusts the test prompt to explicitly mention "python tool," which likely ensures the test triggers the expected MCP tool call path. It may affect test coverage by changing the specific calculation, but the core behavior of testing MCP tool integration remains intact.

**Potential risks**  
If the test failure was due to flakiness or timing issues rather than the prompt wording, this fix might not fully resolve the underlying problem. Additionally, changing the numbers could inadvertently affect how the tool processes the request, though this is unlikely given the simple arithmetic nature.

**Key insights**  
Developers should verify that the fix addresses the root cause of the `AssertionError` and consider whether similar adjustments are needed in other tests. Monitoring CI results post-change is essential to confirm stability, and if issues persist, further investigation into the MCP tool call detection logic may be required.

---

## 26. [[ROCm][Bugfix] Fix ptpc scale load issue for fused shared expert path in deepseek mtp](https://github.com/vllm-project/vllm/pull/33018)


### Base Information

- **PR Number:** #33018
- **Author:** [ganyi1996ppo](https://github.com/ganyi1996ppo)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-26 07:19:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33018/files) (1):**
  - `vllm/model_executor/models/deepseek_mtp.py`

### Summary

**What changed and why**  
The fix addresses a model loading issue when both MTP (Model Tensor Parallelism) and the fused shared experts feature (`VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS`) are enabled. Specifically, it modifies weight splitting logic to handle 1D tensors (like biases) correctly during the loading of shared expert layers in DeepSeek MTP models.

**Technical impact**  
The changes ensure that weight slicing for shared experts works correctly for both 1D and 2D tensors by adding a dimension check (`loaded_weight.ndim`) before deciding the split axis and slicing logic. This prevents runtime errors when loading biases (1D tensors) that previously assumed 2D weight matrices, ensuring compatibility with the fused shared experts optimization path.

**Potential risks**  
If other tensor dimensionalities (e.g., 3D+ tensors) are introduced in future model architectures, the current fix may not handle them. Additionally, the condition `loaded_weight.ndim > 1` assumes all 2D tensors use the same split logic, which might not hold for all possible weight layouts in expert layers.

**Key insights**  
Always validate tensor dimensions when implementing generic slicing logic, especially in parallel loading scenarios. The fix is minimal and focused, but consider adding defensive assertions or logging for unexpected tensor shapes to aid future debugging. Ensure that any similar code paths in other model architectures are reviewed for analogous issues.

---

## 27. [[Bugfix] Fix Can't instantiate abstract class DeepseekV32IndexerBackend](https://github.com/vllm-project/vllm/pull/33052)


### Base Information

- **PR Number:** #33052
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-26 06:44:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33052/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
The change fixes a bug where `backend()` was incorrectly called as a method instead of referencing the `backend` class directly. This caused an "Can't instantiate abstract class" error when checking attention backend names in the `prefer_cross_layer_blocks` property.

**Technical impact**  
This correction ensures the `get_name()` method is called on the backend class (not an instance), aligning with the expected interface of `get_current_attn_backend()`. It resolves runtime instantiation errors and allows proper backend detection for KV transfer optimizations.

**Potential risks**  
If `get_current_attn_backend()` returns an instance rather than a class in some configurations, this change could introduce AttributeErrors. Additionally, the fix assumes `get_name()` is a class method; if it's an instance method elsewhere, inconsistent behavior may arise.

**Key insights**  
Always verify whether backend utilities return classes or instances. Consider adding type hints or assertions to clarify the expected return type of `get_current_attn_backend()`. This fix is minimal and targeted, but reviewing related backend abstraction patterns could prevent similar issues.

---

## 28. [[GLM-OCR] GLM-OCR with MTP Support](https://github.com/vllm-project/vllm/pull/33005)


### Base Information

- **PR Number:** #33005
- **Author:** [zRzRzRzRzRzRzR](https://github.com/zRzRzRzRzRzRzR)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-26 06:24:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33005/files) (14):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `tests/models/multimodal/generation/test_common.py`
  - `tests/models/multimodal/generation/test_vit_backend_functionality.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/registry.py`
  - `vllm/config/speculative.py`
  - `vllm/model_executor/models/glm4.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm_ocr.py`
  - `vllm/model_executor/models/glm_ocr_mtp.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
This PR adds support for the GLM-OCR model, a dense vision-language model based on the GLM-4-0414 architecture with a new VIT structure and MTP (Multi-Token Prediction) implementation. The changes include new model classes, integration into the model registry, and updates to documentation, examples, and tests.

**Technical impact**  
The addition introduces a new model type (`GlmOcrForConditionalGeneration`) with a custom vision transformer (VIT) backbone and MTP support. It extends the existing GLM-4V infrastructure, reusing components like `Glm4vForConditionalGeneration` for the language backbone while adding specialized vision attention (`GlmOcrVisionAttention`) and MTP layers (`GlmOcrMTP`). The model is integrated into the speculative decoding pipeline and multimodal processing framework.

**Potential risks**  
- The model requires a minimum transformers version of `5.0.0.dev`, which may cause compatibility issues with stable releases.  
- The `is_available_online=False` flag for `GlmOcrForConditionalGeneration` suggests the model weights are not publicly accessible, limiting immediate testing.  
- The new VIT structure and MTP implementation may introduce edge cases in multimodal data processing or speculative decoding that are not fully covered by existing tests.

**Key insights**  
- The implementation leverages existing GLM-4V abstractions, promoting code reuse and consistency.  
- Developers should ensure the required transformers version is met and verify multimodal input handling (image/video) works correctly with the new VIT configuration.  
- Testing should focus on the integration of MTP with the vision backbone, as this is a key feature of the model.

---

## 29. [[Chore] Update type annotation of `input_ids` in model forward](https://github.com/vllm-project/vllm/pull/33063)


### Base Information

- **PR Number:** #33063
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-26 06:02:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33063/files) (164):**
  - `docs/contributing/model/basic.md`
  - `tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py`
  - `vllm/model_executor/models/afmoe.py`
  - `vllm/model_executor/models/apertus.py`
  - `vllm/model_executor/models/arcee.py`
  - `vllm/model_executor/models/arctic.py`
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/bagel.py`
  - `vllm/model_executor/models/baichuan.py`
  - `vllm/model_executor/models/bailing_moe.py`
  - `vllm/model_executor/models/bamba.py`
  - `vllm/model_executor/models/bert_with_rope.py`
  - `vllm/model_executor/models/blip2.py`
  - `vllm/model_executor/models/bloom.py`
  - `vllm/model_executor/models/chameleon.py`
  - `vllm/model_executor/models/chatglm.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/commandr.py`
  - `vllm/model_executor/models/dbrx.py`
  - `vllm/model_executor/models/deepseek_mtp.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/model_executor/models/deepseek_vl2.py`
  - `vllm/model_executor/models/dots1.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/eagle2_5_vl.py`
  - `vllm/model_executor/models/ernie45_moe.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/ernie45_vl_moe.py`
  - `vllm/model_executor/models/ernie_mtp.py`
  - `vllm/model_executor/models/exaone.py`
  - `vllm/model_executor/models/exaone4.py`
  - `vllm/model_executor/models/exaone_moe.py`
  - `vllm/model_executor/models/falcon.py`
  - `vllm/model_executor/models/falcon_h1.py`
  - `vllm/model_executor/models/fuyu.py`
  - `vllm/model_executor/models/gemma.py`
  - `vllm/model_executor/models/gemma2.py`
  - `vllm/model_executor/models/gemma3.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/gemma3n.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/glm4.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4_moe.py`
  - `vllm/model_executor/models/glm4_moe_lite.py`
  - `vllm/model_executor/models/glm4_moe_lite_mtp.py`
  - `vllm/model_executor/models/glm4_moe_mtp.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/gpt2.py`
  - `vllm/model_executor/models/gpt_bigcode.py`
  - `vllm/model_executor/models/gpt_j.py`
  - `vllm/model_executor/models/gpt_neox.py`
  - `vllm/model_executor/models/gpt_oss.py`
  - `vllm/model_executor/models/granite.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/granitemoe.py`
  - `vllm/model_executor/models/granitemoehybrid.py`
  - `vllm/model_executor/models/granitemoeshared.py`
  - `vllm/model_executor/models/grok1.py`
  - `vllm/model_executor/models/hunyuan_v1.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/internlm2.py`
  - `vllm/model_executor/models/internlm2_ve.py`
  - `vllm/model_executor/models/interns1.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/iquest_loopcoder.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/jais.py`
  - `vllm/model_executor/models/jais2.py`
  - `vllm/model_executor/models/jamba.py`
  - `vllm/model_executor/models/jina_vl.py`
  - `vllm/model_executor/models/kanana_v.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/kimi_linear.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2.py`
  - `vllm/model_executor/models/lfm2_moe.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/llava_next.py`
  - `vllm/model_executor/models/llava_next_video.py`
  - `vllm/model_executor/models/llava_onevision.py`
  - `vllm/model_executor/models/longcat_flash.py`
  - `vllm/model_executor/models/longcat_flash_mtp.py`
  - `vllm/model_executor/models/mamba.py`
  - `vllm/model_executor/models/mamba2.py`
  - `vllm/model_executor/models/midashenglm.py`
  - `vllm/model_executor/models/mimo.py`
  - `vllm/model_executor/models/mimo_mtp.py`
  - `vllm/model_executor/models/mimo_v2_flash.py`
  - `vllm/model_executor/models/minicpm.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/minimax_m2.py`
  - `vllm/model_executor/models/minimax_text_01.py`
  - `vllm/model_executor/models/minimax_vl_01.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/mixtral.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/mpt.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron.py`
  - `vllm/model_executor/models/nemotron_h.py`
  - `vllm/model_executor/models/nemotron_nas.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/nemotron_vl.py`
  - `vllm/model_executor/models/olmo.py`
  - `vllm/model_executor/models/olmo2.py`
  - `vllm/model_executor/models/olmoe.py`
  - `vllm/model_executor/models/openpangu.py`
  - `vllm/model_executor/models/openpangu_mtp.py`
  - `vllm/model_executor/models/opt.py`
  - `vllm/model_executor/models/orion.py`
  - `vllm/model_executor/models/ouro.py`
  - `vllm/model_executor/models/ovis.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/persimmon.py`
  - `vllm/model_executor/models/phi.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/model_executor/models/phimoe.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/plamo3.py`
  - `vllm/model_executor/models/qwen.py`
  - `vllm/model_executor/models/qwen2.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/model_executor/models/qwen2_moe.py`
  - `vllm/model_executor/models/qwen2_rm.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3.py`
  - `vllm/model_executor/models/qwen3_moe.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/models/qwen3_next_mtp.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`
  - `vllm/model_executor/models/qwen_vl.py`
  - `vllm/model_executor/models/seed_oss.py`
  - `vllm/model_executor/models/skyworkr1v.py`
  - `vllm/model_executor/models/solar.py`
  - `vllm/model_executor/models/stablelm.py`
  - `vllm/model_executor/models/starcoder2.py`
  - `vllm/model_executor/models/step3_text.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/tarsier.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_streaming.py`
  - `vllm/model_executor/models/zamba2.py`

### Summary

**What changed and why**  
The PR updates type annotations for the `input_ids` parameter in model forward methods across 164 files, changing from `torch.Tensor` to `torch.Tensor \| None`. This change is described as necessary to correctly consider pipeline parallelism (PP) cases, where `input_ids` may be `None` when intermediate tensors are provided.

**Technical impact**  
This change aligns type hints with the actual runtime behavior, where `input_ids` can be optional when `inputs_embeds` or `intermediate_tensors` are used. It improves type safety and developer experience by accurately reflecting the contract of the forward method. The update also includes modifications to interface definitions and documentation to maintain consistency.

**Potential risks**  
While the change is largely mechanical, there is a risk that some model implementations may not properly handle `None` values for `input_ids` in all code paths. Additionally, the eagle2_5_vl model removes a line that explicitly sets `input_ids = None`, which could affect logic if other code depends on that assignment.

**Key insights**  
Developers should verify that their model implementations correctly handle `input_ids` being `None` when `intermediate_tensors` is provided. The PR highlights the importance of consistent type annotations across the codebase, especially for distributed training scenarios like pipeline parallelism. Future changes should ensure all forward methods adhere to this updated contract.

---

## 30. [[Performance] Tune Mamba selective scan kernel for B200](https://github.com/vllm-project/vllm/pull/32873)


### Base Information

- **PR Number:** #32873
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-26 05:56:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32873/files) (2):**
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`
  - `vllm/model_executor/layers/mamba/ops/mamba_ssm.py`

### Summary

**What changed and why**  
The PR introduces platform-specific kernel tuning for the Mamba selective scan operation on NVIDIA Blackwell (B200) GPUs. It adds a check for Blackwell architecture (SM100+) in the Mamba mixer initialization and passes this flag to the `selective_state_update` kernel, which adjusts its thread block configuration (`BLOCK_SIZE_M`, `num_warps`) when the state dimension (`dstate`) exceeds 64.

**Technical impact**  
This change optimizes kernel performance for large state dimensions on Blackwell GPUs by using a larger block size (32 vs. 4) and more warps (8 vs. 4), improving throughput at high batch sizes as shown in the test results. The tuning is conditional and does not affect other GPU architectures (e.g., H100 performance remains unchanged).

**Potential risks**  
The heuristic (`dstate > 64`) may not generalize to all model configurations or future Blackwell variants. Performance at smaller batch sizes could degrade slightly, as hinted by the test results with `VLLM_FUSED_MOE_CHUNK_SIZE` adjustments. There is also a risk of regression if the `is_blackwell` detection fails or is inaccurate on mixed-GPU systems.

**Key insights**  
Platform-aware kernel tuning is effective for leveraging architecture-specific optimizations. Developers should monitor performance across a range of batch sizes and model configurations to ensure balanced improvements. Consider making these tuning parameters configurable via environment variables for further experimentation without code changes.

---

## 31. [[Feature] Add LoRA support for Gemma3 vision components](https://github.com/vllm-project/vllm/pull/32764)


### Base Information

- **PR Number:** #32764
- **Author:** [vihaan-that](https://github.com/vihaan-that)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-26 05:56:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32764/files) (1):**
  - `vllm/model_executor/models/gemma3_mm.py`

### Summary

**What changed and why**  
Two methods were added to `Gemma3ForConditionalGeneration` to enable full LoRA support for the model's vision components. Previously, LoRA adapters could only load weights for the language model, but these methods now allow proper token-count calculations for the vision encoder and multimodal connector, enabling LoRA weight loading for the vision tower and connector.

**Technical impact**  
The changes allow LoRA adapters to correctly map and load weights for Gemma3's vision pipeline by providing the necessary token-count transformations between the vision encoder and connector. This aligns Gemma3's LoRA support with other vision-language models in the codebase (like PaliGemma and Pixtral) and ensures the vision components can be fine-tuned with parameter-efficient adapters.

**Potential risks**  
The hard-coded multiplier of 16 in `get_num_mm_encoder_tokens` assumes a fixed token grid size (16×16). If the model configuration changes (e.g., different `tokens_per_side`), this could break token calculations. Additionally, the 1:1 mapping in `get_num_mm_connector_tokens` assumes no spatial reduction in the connector, which may not hold if the architecture is updated.

**Key insights**  
These methods are critical for LoRA compatibility but should derive values from model configuration (e.g., `tokens_per_side`) rather than hard-coded constants to improve robustness. Developers should verify that the token calculations match the actual vision encoder and connector implementations, especially if the Gemma3 architecture evolves. Consider adding unit tests that validate token counts against real model outputs.

---

## 32. [[Misc] HF Hub LoRA Resolver](https://github.com/vllm-project/vllm/pull/20320)


### Base Information

- **PR Number:** #20320
- **Author:** [alex-jw-brooks](https://github.com/alex-jw-brooks)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-26 05:56:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/20320/files) (8):**
  - `.buildkite/test-pipeline.yaml`
  - `docs/design/lora_resolver_plugins.md`
  - `docs/features/lora.md`
  - `pyproject.toml`
  - `tests/plugins/lora_resolvers/test_hf_hub_resolver.py`
  - `vllm/envs.py`
  - `vllm/plugins/lora_resolvers/filesystem_resolver.py`
  - `vllm/plugins/lora_resolvers/hf_hub_resolver.py`

### Summary

**What changed and why**  
This PR introduces a new Hugging Face Hub LoRA resolver (`HfHubResolver`) that dynamically downloads LoRA adapters from specified HF repositories. The resolver extends the existing filesystem resolver pattern to support remote adapter loading, addressing use cases like IBM's Granite RAG library where multiple adapters are hosted in a single repository.

**Technical impact**  
The changes add a new plugin architecture component that integrates with vLLM's LoRA resolution system. The resolver inherits from `FilesystemResolver` and reuses its validation logic via a refactored `_get_lora_req_from_path` method. It introduces environment variable configuration (`VLLM_LORA_RESOLVER_HF_REPO_LIST`) and uses Hugging Face Hub APIs (`snapshot_download`, `HfApi`) to fetch adapters on-demand, caching them locally.

**Potential risks**  
1. Security: The resolver warns against production use due to uncontrolled remote downloads, which could expose systems to malicious repositories or data exfiltration.
2. Network dependency: Adapter loading now depends on HF Hub availability and network latency, which may impact request performance.
3. Cache management: No explicit cache cleanup mechanism is provided, potentially leading to disk space exhaustion over time.
4. Repository scanning: The resolver lists all files in configured repos to find adapter configs, which may be inefficient for large repositories.

**Key insights**  
1. The resolver cleverly reuses filesystem validation logic, maintaining consistency between local and remote adapter handling.
2. Developers should carefully restrict allowed repositories and monitor network calls in production-like environments.
3. Consider implementing cache expiration or size limits if this feature moves toward production readiness.
4. The plugin-based approach keeps the core vLLM codebase clean while enabling flexible extension points for custom storage backends.

---

## 33. [[Model] Use mm_position to compute mrope positions for Qwen3-Omni](https://github.com/vllm-project/vllm/pull/33010)


### Base Information

- **PR Number:** #33010
- **Author:** [Etelis](https://github.com/Etelis)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-26 05:48:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33010/files) (2):**
  - `examples/offline_inference/qwen3_omni/only_thinker.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
The PR optimizes M-RoPE position calculation for Qwen3-Omni by replacing token-by-token search through `input_tokens` with direct use of `mm_position.offset` from `MultiModalFeatureSpec`. This follows the pattern established for Qwen2.5-Omni, improving efficiency and maintainability while preserving Qwen3-Omni-specific logic for audio token formulas and `use_audio_in_video` handling.

**Technical impact**  
The changes streamline position computation by leveraging structured multimodal feature metadata, reducing algorithmic complexity from O(n) token scanning to O(1) offset access. This enhances performance for sequences with multiple multimodal inputs and aligns the codebase with a consistent pattern across Omni models, improving code reuse and reducing duplication.

**Potential risks**  
If `mm_position.offset` values are incorrectly populated or out-of-sync with token positions, position calculations could become misaligned, leading to incorrect model outputs. The interleaving logic for `use_audio_in_video` remains complex and requires careful validation to ensure audio-video pairing is correctly maintained across varied input configurations.

**Key insights**  
Developers should verify that all multimodal feature specs consistently provide accurate `mm_position.offset` values. The refactored `iter_mm_features` method centralizes feature iteration logic, making future modifications easier. Ensure thorough testing of edge cases, especially mixed-modality inputs and the `use_audio_in_video=True` scenario, to prevent regression in position-sensitive computations.

---

## 34. [[lora/moe] Improve fused MoE‑LoRA kernel indexing and memory access](https://github.com/vllm-project/vllm/pull/32770)


### Base Information

- **PR Number:** #32770
- **Author:** [cwazai](https://github.com/cwazai)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-26 04:56:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32770/files) (1):**
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`

### Summary

**What changed and why**  
This PR optimizes the Triton kernel for fused MoE-LoRA operations by improving indexing arithmetic and memory access patterns. Key changes include using int32 instead of int64 for indices, removing modulo operations for column offsets, early-exiting invalid lora_ids, reducing grid launches to only active LoRA adapters, and enabling L2 cache hints for B-matrix loads.

**Technical impact**  
The changes reduce computational overhead in hot paths by simplifying index calculations and minimizing wasted GPU threads. The kernel now launches fewer programs (axis-2) and uses more efficient memory access patterns, particularly through `.ca` cache modifiers for repeated expert-weight reads, which improves L2 cache hit rates and overall throughput.

**Potential risks**  
The early-exit guard (`if lora_id >= max_loras`) assumes correct `max_loras` input; incorrect values could cause premature exits. Switching to int32 indices may cause overflow with very large tensors. The cache modifier (`USE_B_L2_CACHE=True`) might not benefit all hardware or data patterns and should be validated across different GPU architectures.

**Key insights**  
These optimizations yield measurable performance gains, as shown by improved throughput and reduced latency in benchmarks. Developers should ensure `max_loras` is accurately passed to avoid kernel early exits. The int32 index change is generally safe but warrants attention for extreme-scale models. The L2 cache optimization is beneficial for repeated reads but may require tuning for varied workloads.

---

## 35. [[Doc] Further update multi-modal impl doc](https://github.com/vllm-project/vllm/pull/33065)


### Base Information

- **PR Number:** #33065
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-26 02:54:20
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33065/files) (1):**
  - `docs/contributing/model/multimodal.md`

### Summary

**What changed and why**  
This PR updates the multimodal implementation documentation to clarify the separation of responsibilities between the `forward` and `embed_multimodal` methods. It explicitly instructs developers to remove multimodal embedding logic from `forward` and move it to `embed_multimodal`, while noting that text embedding and merging are handled automatically by a default `embed_input_ids` implementation.

**Technical impact**  
The changes reinforce a cleaner architectural pattern where multimodal processing is decoupled from the core forward pass. This simplifies model implementations by centralizing multimodal feature extraction in `embed_multimodal` and leveraging default text embedding handling, reducing boilerplate and potential errors.

**Potential risks**  
Developers might incorrectly assume the default `embed_input_ids` handles all embedding scenarios, potentially overlooking custom text embedding requirements. The diff example removes `pixel_values` from `forward` parameters, which could cause issues if existing models still expect it. Additionally, the tensor shape requirements for `multimodal_embeddings` must be strictly followed to avoid runtime errors.

**Key insights**  
The update promotes consistency across multimodal models by standardizing the interface. Developers should verify that their model’s text embedding logic aligns with the default `embed_input_ids` behavior and ensure multimodal embeddings adhere to the specified shape constraints. Reviewing existing model implementations for compliance with this pattern is recommended.

---

## 36. [[StepVL] add step vl offline example](https://github.com/vllm-project/vllm/pull/33054)


### Base Information

- **PR Number:** #33054
- **Author:** [ltd0924](https://github.com/ltd0924)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-26 01:00:32
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33054/files) (2):**
  - `examples/offline_inference/vision_language.py`
  - `examples/offline_inference/vision_language_multi_image.py`

### Summary

**What changed and why**  
Two new model loading functions (`run_step_vl` and `load_step_vl`) were added to support the **Step-VL-10B** model in both single-image and multi-image inference examples. The changes integrate the model into the existing example pipelines by adding its configuration to the model registry dictionaries.

**Technical impact**  
The Step-VL-10B model is now available for offline inference tasks via the example scripts. The configuration uses `trust_remote_code=True`, enables the `deepseek_r1` reasoning parser, and sets appropriate token/context limits. For multi-image inference, it disables the patch-based vision encoder (`enable_patch: False`) to handle multiple images per prompt.

**Potential risks**  
- The model requires `trust_remote_code=True`, which could pose security risks if the remote code is not from a trusted source.  
- The token limit (`max_num_batched_tokens=4096`) may be insufficient for long multi-turn conversations or complex multi-image reasoning tasks.  
- The multi-image implementation assumes all images are processed in a single batch; memory usage could spike with many high-resolution images.

**Key insights**  
- Ensure the model ID (`stepfun-ai/Step3-VL-10B`) is accessible and correctly registered in Hugging Face.  
- Validate that the `deepseek_r1` parser is compatible with the model’s output format.  
- Monitor GPU memory when processing multiple images, as the patch-based encoder is disabled for multi-image scenarios.

---

