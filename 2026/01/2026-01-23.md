# vLLM Merged PR Report

**Report Date:** 2026-01-23 PST

**Total Merged PRs:** 44

---

## 1. [[Doc] Ignore typo check on governance doc](https://github.com/vllm-project/vllm/pull/32999)


### Base Information

- **PR Number:** #32999
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-23 23:52:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32999/files) (1):**
  - `pyproject.toml`

### Summary

**What changed and why**  
The PR modifies the `pyproject.toml` configuration to exclude `docs/governance/process.md` from the project's typo checking tool (likely `codespell`). This is a documentation-only change to prevent false positives from spell-checking on a specific governance document.

**Technical impact**  
This change has minimal technical impact, as it only affects the linting/validation pipeline. The governance document will no longer be scanned for potential typos, which may allow unintentional spelling errors to remain in that file but prevents unnecessary CI failures for acceptable terminology or project-specific terms.

**Potential risks**  
The main risk is that legitimate typos in the governance document could go unnoticed. There's also a risk of this pattern being overused to exclude other files from quality checks without proper justification. The PR description is incomplete, lacking the required purpose and test plan sections.

**Key insights**  
This is a low-risk configuration change, but the incomplete PR description is a concern for maintainability. Developers should ensure PR descriptions follow the project's checklist, even for simple changes. Consider if the exclusion is truly necessary or if adding custom words to the dictionary would be a better long-term solution.

---

## 2. [[Models] Add `SharedFusedMoE` support to Qwen3MoE](https://github.com/vllm-project/vllm/pull/32082)


### Base Information

- **PR Number:** #32082
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-23 23:36:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32082/files) (1):**
  - `vllm/model_executor/models/qwen3_moe.py`

### Summary

**What changed and why**  
This PR adds support for shared experts in Qwen3MoE models by replacing `FusedMoE` with `SharedFusedMoE`. The change addresses a discrepancy where vLLM's original implementation assumed no shared experts in the sparse MoE block, while Qwen3-omni's architecture includes them. This upstreams the shared-expert support to avoid duplicate implementations in vLLM-omni.

**Technical impact**  
The modifications introduce optional shared expert gating and a shared MLP layer controlled by `shared_expert_intermediate_size`. The forward logic now computes both shared and fused expert outputs, sums them when present, and handles tensor parallelism reductions appropriately. Expert parameter loading is updated to use `SharedFusedMoE.make_expert_params_mapping`.

**Potential risks**  
If `shared_expert_intermediate_size` is not properly configured in model configs, the shared expert components may not initialize correctly. The conditional tensor parallelism reduction logic (`maybe_all_reduce_tensor_model_parallel`) could introduce synchronization overhead or errors in non-sequence-parallel distributed setups. Changes to `Qwen3MoeMLP`'s forward pass with sigmoid gating may affect numerical stability or performance.

**Key insights**  
Developers should ensure model configurations explicitly set `shared_expert_intermediate_size` when using shared experts. The reduction strategy (`reduce_results=False`) defers all-reduce operations, which may impact performance profiling. Testing should validate both shared-expert and non-shared-expert paths, especially in multi-GPU environments.

---

## 3. [[docs] Update governance process links](https://github.com/vllm-project/vllm/pull/32995)


### Base Information

- **PR Number:** #32995
- **Author:** [esmeetu](https://github.com/esmeetu)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-23 23:32:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32995/files) (1):**
  - `docs/governance/process.md`

### Summary

**What changed and why**  
The PR updates the governance documentation to enhance clarity and professionalism by reformatting maintainer lists with proper Markdown formatting, adding GitHub profile links, and restructuring responsibilities into bulleted lists. This improves readability and provides better attribution for project leadership.

**Technical impact**  
These changes are purely documentation improvements with no impact on code functionality, build processes, or system behavior. The update enhances the project's public documentation quality and maintainer transparency without altering any technical components.

**Potential risks**  
The main risk is potential broken links if GitHub usernames change in the future. There's also a minor risk of inconsistency if other governance documents aren't updated similarly, though this is isolated to a single file.

**Key insights**  
This is a well-executed documentation improvement that follows Markdown best practices. Developers should ensure all linked GitHub profiles remain active and consider applying similar formatting consistency to other project documentation. The changes demonstrate good attention to documentation quality and contributor recognition.

---

## 4. [[Tests] Standardize RNG seed utility across test files](https://github.com/vllm-project/vllm/pull/32982)


### Base Information

- **PR Number:** #32982
- **Author:** [sjhddh](https://github.com/sjhddh)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 22:47:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32982/files) (4):**
  - `tests/kernels/test_flex_attention.py`
  - `tests/utils.py`
  - `tests/v1/logits_processors/test_custom_offline.py`
  - `vllm/utils/torch_utils.py`

### Summary

**What changed and why**  
This PR standardizes random number generator (RNG) seeding across test files by replacing scattered custom seed functions with a centralized `set_random_seed` utility from `vllm.utils.torch_utils`. It removes a custom `set_seed()` function, replaces direct `random.seed()` calls, and re-exports the utility in `tests/utils.py` for convenience.

**Technical impact**  
The changes improve test determinism by ensuring consistent seeding across `random`, `numpy`, and `torch` RNGs. This reduces code duplication and centralizes seed management, making future updates to seeding logic easier. The addition of `torch.cuda.manual_seed_all(seed)` in the utility function enhances reproducibility for CUDA-enabled tests.

**Potential risks**  
Low risk, as the changes are confined to test files and use an existing utility. However, if `set_random_seed` had undiscovered issues (e.g., incomplete seeding), it could affect test reliability. The re-export in `tests/utils.py` may cause circular imports if not managed carefully, though the current structure appears safe.

**Key insights**  
Centralizing RNG utilities is a best practice that improves maintainability and consistency. Developers should adopt `set_random_seed` for all new tests and consider updating other test files that may still use custom seeding. Ensure the utility is imported correctly via `tests.utils` to avoid direct dependencies on internal modules.

---

## 5. [[Tests] Clarify pytest skip reasons with actionable context](https://github.com/vllm-project/vllm/pull/32981)


### Base Information

- **PR Number:** #32981
- **Author:** [sjhddh](https://github.com/sjhddh)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 22:38:51
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32981/files) (2):**
  - `tests/samplers/test_beam_search.py`
  - `tests/v1/sample/test_topk_topp_sampler.py`

### Summary

**What changed and why**  
The changes replace vague "FIXME" comments in pytest skip markers with descriptive, actionable reasons. In `test_beam_search.py`, the skip reason clarifies that the V1 engine lacks beam search support. In `test_topk_topp_sampler.py`, the skip reason details a specific FlashInfer renorm comparison failure requiring investigation.

**Technical impact**  
These updates improve test documentation without altering test logic or behavior. The skip reasons now provide clear context for future developers, aiding in triage and prioritization of test fixes. This enhances maintainability by explicitly linking skipped tests to underlying technical limitations or known issues.

**Potential risks**  
No functional risks exist since only comment strings were modified. However, if the underlying issues are resolved without updating these skip markers, tests may remain incorrectly skipped. The detailed reasons could become outdated if the codebase evolves without corresponding documentation updates.

**Key insights**  
Always provide actionable context in test skips to accelerate debugging and planning. Consider establishing a process to review and update skip reasons during related code changes. These improvements exemplify good software hygiene—clear documentation reduces cognitive load for the entire team.

---

## 6. [[Perf] Cache xpu_get_mem_info() result to avoid duplicate calls](https://github.com/vllm-project/vllm/pull/32983)


### Base Information

- **PR Number:** #32983
- **Author:** [sjhddh](https://github.com/sjhddh)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-23 20:56:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32983/files) (1):**
  - `vllm/v1/worker/xpu_worker.py`

### Summary

**What changed and why**  
The change replaces two consecutive calls to `xpu_get_mem_info()` with a single call, caching the result in local variables `free_mem` and `total_mem`. This eliminates redundant system calls in a performance-sensitive memory profiling context.

**Technical impact**  
This reduces overhead by avoiding duplicate XPU memory queries, which likely involve driver or system calls. The computation logic remains identical, preserving functional behavior while improving execution efficiency.

**Potential risks**  
The risk is minimal as the logic is unchanged. However, if `xpu_get_mem_info()` returns inconsistent values between calls (e.g., due to concurrent memory operations), caching ensures consistency, which is actually a benefit.

**Key insights**  
This is a straightforward performance optimization that also enhances readability. Developers should apply similar caching patterns for expensive or repeated calls in performance-critical sections, ensuring thread safety if applicable.

---

## 7. [[Dev UX] Add auto-detection for VLLM_PRECOMPILED_WHEEL_VARIANT during install](https://github.com/vllm-project/vllm/pull/32948)


### Base Information

- **PR Number:** #32948
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-23 19:15:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32948/files) (2):**
  - `docs/getting_started/installation/gpu.cuda.inc.md`
  - `setup.py`

### Summary

**What changed and why**  
Added auto-detection for the `VLLM_PRECOMPILED_WHEEL_VARIANT` environment variable during installation. This change simplifies the developer experience by automatically determining the correct CUDA variant (cu129 or cu130) based on the system's CUDA version, eliminating the need for manual specification.

**Technical impact**  
The installation process now automatically detects CUDA version through three fallback methods: respecting `VLLM_MAIN_CUDA_VERSION` if set, checking `torch.version.cuda`, and finally using `nvidia-smi`. This reduces manual configuration and aligns with the existing `--torch-backend=auto` pattern, making the setup more intuitive for developers.

**Potential risks**  
The detection logic assumes that if PyTorch is installed, its CUDA version is compatible with the system, which may not always be true. Additionally, the regex parsing of `nvidia-smi` output could fail on different system configurations or localized outputs. The fallback to `envs.VLLM_MAIN_CUDA_VERSION` when detection fails might not always map correctly to supported variants.

**Key insights**  
This enhancement significantly improves developer workflow by reducing manual steps. Developers should verify that the auto-detected variant matches their actual CUDA environment, especially in containerized or multi-GPU setups. Consider adding validation or warnings if detected CUDA versions don't match available wheel variants.

---

## 8. [Auth_token added in documentation as it is required](https://github.com/vllm-project/vllm/pull/32988)


### Base Information

- **PR Number:** #32988
- **Author:** [ruizcrp](https://github.com/ruizcrp)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 19:03:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32988/files) (1):**
  - `docs/serving/integrations/claude_code.md`

### Summary

**What changed and why**  
The PR adds documentation for the `ANTHROPIC_AUTH_TOKEN` environment variable in the Claude Code integration guide. The contributor discovered this token is required for the integration to work properly, even though the existing documentation didn't mention it.

**Technical impact**  
This change only affects documentation and has no impact on code functionality. It improves the accuracy and completeness of the integration instructions, helping users successfully configure Claude Code with vLLM servers.

**Potential risks**  
The documentation now states the token "is required" but also says it "can be any value." This could be confusing if there are specific validation requirements. Additionally, the change doesn't explain why this token is needed or whether it's specific to certain vLLM configurations.

**Key insights**  
Documentation updates like this are valuable for improving user experience, but consider adding context about why this token is required. Verify whether this requirement applies to all vLLM deployments or only specific configurations. The "dummy" value suggestion is appropriate for testing but should be clarified if production deployments need actual authentication tokens.

---

## 9. [[ROCm][ViT] Enable Flash Attention Triton backend on RDNA3/RDNA4](https://github.com/vllm-project/vllm/pull/32944)


### Base Information

- **PR Number:** #32944
- **Author:** [monajafi-amd](https://github.com/monajafi-amd)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-23 18:03:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32944/files) (1):**
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
Added support for Flash Attention's Triton backend on RDNA3/RDNA4 GPUs (gfx11xx/gfx12xx) to accelerate vision transformer (ViT) attention, which previously fell back to slower PyTorch SDPA. The change introduces an opt-in mechanism via an environment variable and a detection function to verify backend availability.

**Technical impact**  
The modification extends the existing attention backend selection logic in the ROCm platform to include a new path for RDNA architectures. It prioritizes the Triton backend when available and properly configured, which can significantly improve time-to-first-token (TTFT) performance for vision-language models, as evidenced by the 1.8-6.5x speedup in benchmarks.

**Potential risks**  
The feature requires an explicit opt-in (`FLASH_ATTN_TRITON_AMD_ENABLE=TRUE`), which could lead to missed performance gains if users are unaware. There is a dependency on Flash Attention 2.8.3+ and its Triton AMD module, which may not be installed or could have compatibility issues. The conditional check for `on_gfx1x()` and specific dtypes (float16/bfloat16) might exclude some valid use cases.

**Key insights**  
This is a performance-critical enhancement for vision models on RDNA GPUs. Developers should ensure Flash Attention with Triton support is installed and the environment variable is set. The backend selection logic now has a clear hierarchy: AITER for CDNA, CK for CDNA, then Triton for RDNA, falling back to SDPA. Consider documenting the setup requirements prominently for end-users.

---

## 10. [[Bugfix] Fix FusedMoE LoRA kernel offs_token out of bound value](https://github.com/vllm-project/vllm/pull/32279)


### Base Information

- **PR Number:** #32279
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-23 17:41:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32279/files) (1):**
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`

### Summary

**What changed and why**  
This PR fixes two out-of-bound access issues in the FusedMoE LoRA kernel. The `offs_token` sentinel value is changed from 0 to `num_valid_tokens` to prevent reading token 0 for invalid indices, and the `moe_weight` default is corrected from integer 0 to float 0.0 for type consistency.

**Technical impact**  
These changes ensure memory safety by preventing invalid memory reads and improve numerical correctness by using the proper floating-point sentinel. The accuracy improvement shown in testing indicates the kernel now correctly handles boundary conditions, while the slight performance gain suggests reduced branching overhead.

**Potential risks**  
If `num_valid_tokens` is incorrectly calculated or exceeds expected bounds, it could still lead to out-of-range accesses. The fix assumes the sentinel logic is consistent across all kernel invocations; any mismatch in sentinel handling elsewhere could cause subtle bugs.

**Key insights**  
Always validate sentinel values and data types in GPU kernels to avoid silent memory corruption. The accuracy improvement highlights how boundary condition fixes can directly affect model quality. Consider adding assertions to validate `num_valid_tokens` range during kernel development.

---

## 11. [[Core][Bugfix] allow graceful worker termination](https://github.com/vllm-project/vllm/pull/32965)


### Base Information

- **PR Number:** #32965
- **Author:** [joerunde](https://github.com/joerunde)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-23 17:28:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32965/files) (1):**
  - `vllm/v1/executor/multiproc_executor.py`

### Summary

**What changed and why**  
The PR modifies the multiprocess executor to allow worker processes to gracefully shut down before receiving a SIGTERM. Previously, the parent process immediately sent SIGTERM after closing the death pipe, interrupting worker cleanup. Now, it waits up to 4 seconds for workers to self-terminate before escalating to SIGTERM and then SIGKILL.

**Technical impact**  
This change improves the reliability of worker shutdown by ensuring cleanup tasks (like releasing resources or handling pending operations) can complete. The addition of warning logs for `SystemExit` exceptions helps monitor unexpected terminations, while the refactored `active_procs` lambda avoids redundant code and maintains clarity.

**Potential risks**  
The 4-second wait could delay overall shutdown if workers hang during cleanup, though this is mitigated by the subsequent SIGTERM/SIGKILL. Logging `SystemExit` might generate noise if signals are expected, but the PR confirms no warnings occur in normal shutdowns. There’s a minor risk of zombie processes if cleanup fails silently.

**Key insights**  
Always prioritize graceful shutdowns to prevent resource leaks in distributed systems. The lambda refactor improves readability but should be tested for performance with many workers. Consider making the timeout configurable for different workloads, and ensure monitoring for the new warning logs to catch abnormal terminations early.

---

## 12. [[Performance] Split FlashAttn attention and cache update](https://github.com/vllm-project/vllm/pull/25954)


### Base Information

- **PR Number:** #25954
- **Author:** [ElizaWszola](https://github.com/ElizaWszola)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-23 17:28:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/25954/files) (21):**
  - `tests/v1/attention/test_attention_backends.py`
  - `tests/v1/attention/utils.py`
  - `tests/v1/kv_connector/unit/test_decode_bench_connector.py`
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `tests/v1/spec_decode/test_tree_attention.py`
  - `vllm/attention/layer.py`
  - `vllm/forward_context.py`
  - `vllm/model_executor/layers/attention/cross_attention.py`
  - `vllm/utils/torch_utils.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/spec_decode/medusa.py`
  - `vllm/v1/spec_decode/ngram_proposer.py`
  - `vllm/v1/spec_decode/suffix_decoding.py`
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_ubatch_wrapper.py`

### Summary

**What changed and why**  
This PR separates KV cache updates from attention forward operations, specifically for the FlashAttn backend. It introduces a new `forward_includes_kv_cache_update` flag (default True) and sets it to False for FlashAttn, moving KV cache updates into a separate `do_kv_cache_update` method. This architectural change enables future optimizations like unwrapping and better control over execution order.

**Technical impact**  
The changes affect multiple layers of the system: attention backends now conditionally handle KV updates, a new `unified_kv_cache_update` custom op ensures proper ordering under torch.compile, and the GPU runner propagates slot mappings per layer. Cross-attention and speculative decoding components are updated to maintain compatibility, and tests are adjusted to call the new update method when needed.

**Potential risks**  
- Incorrect slot mapping handling could lead to cache corruption, especially in padded or ubatched scenarios.
- The dependency on `kv_cache_dummy_dep` for ordering might break if torch.compile behavior changes.
- Edge cases like cross-attention (where key/value may be None) require careful validation to avoid silent failures.
- Changes to `weak_ref_tensor` ignoring zero-size tensors could affect memory management in unexpected ways.

**Key insights**  
- Developers must ensure that any new attention backend sets `forward_includes_kv_cache_update` appropriately and implements `do_kv_cache_update` if false.
- Slot mappings must be correctly shaped (padded vs. unpadded) based on whether the backend separates KV updates.
- Testing should cover both split and non-split backends, as shown in the PR’s lm_eval results.

---

## 13. [[fix] add VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME to compile factors](https://github.com/vllm-project/vllm/pull/32912)


### Base Information

- **PR Number:** #32912
- **Author:** [dolpm](https://github.com/dolpm)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 14:53:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32912/files) (2):**
  - `tests/config/test_config_utils.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
The PR adds `VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME` to the `compile_factors()` function's list of environment variables. This environment variable has a dynamic default value (a UUID) that changes on each initialization, which was causing cache misses in the compilation factor hashing system. A test was added to verify that the hash remains stable across fresh process initializations.

**Technical impact**  
This change ensures that the dynamic `VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME` environment variable (with its UUID-based default) is properly excluded from compilation factor calculations. This maintains cache stability for systems that rely on these hashes, preventing unnecessary recompilations or cache invalidations due to the changing UUID.

**Potential risks**  
If other environment variables with dynamic defaults exist but aren't included in `compile_factors()`, they could still cause cache instability. The test uses subprocesses which adds some overhead but is necessary to properly test fresh initializations. There's also a risk that future changes to environment variable handling might reintroduce similar issues.

**Key insights**  
The solution demonstrates good defensive programming by explicitly excluding dynamic environment variables from cache key calculations. Developers should be aware that any environment variable with non-deterministic default values must be added to `compile_factors()` to prevent cache issues. The test methodology using subprocesses is appropriate for this type of isolation testing.

---

## 14. [[CI] fix version comparsion and exclusion patterns in upload-release-wheels.sh](https://github.com/vllm-project/vllm/pull/32971)


### Base Information

- **PR Number:** #32971
- **Author:** [Harry-Chen](https://github.com/Harry-Chen)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-01-23 14:21:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32971/files) (1):**
  - `.buildkite/scripts/upload-release-wheels.sh`

### Summary

**What changed and why**  
The PR fixes two issues in the release wheel upload script: version comparison logic and exclusion patterns for release candidate wheels. The version comparison was incorrectly comparing `v$RELEASE_VERSION` against `$GIT_VERSION` (which already includes the 'v' prefix), and the exclusion pattern for release candidates was too broad, potentially excluding valid architecture names like "aarch64".

**Technical impact**  
These changes ensure proper version validation during release builds and correct filtering of wheel files. The script now compares version strings consistently (both with or without 'v' prefix) and uses a more precise regex pattern (`*rc[0-9]*`) to exclude only actual release candidate wheels while preserving architecture-specific builds.

**Potential risks**  
The new exclusion pattern `*rc[0-9]*` might still miss unconventional release candidate naming patterns. Additionally, the version comparison fix assumes `$GIT_VERSION` always contains the 'v' prefix, which could cause issues if this assumption changes in the future. The script's reliance on string manipulation (`${RELEASE_VERSION#v}`) could fail with unexpected version formats.

**Key insights**  
Always validate version string formats consistently throughout the pipeline. When using glob patterns for file filtering, be specific to avoid unintended exclusions. Consider adding validation for version format assumptions or documenting expected patterns. The changes are minimal but critical for release automation reliability.

---

## 15. [[Bugfix] Fix missing is_layer_skipped check for FusedMoE in AWQConfig](https://github.com/vllm-project/vllm/pull/32935)


### Base Information

- **PR Number:** #32935
- **Author:** [joninco](https://github.com/joninco)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-23 14:19:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32935/files) (1):**
  - `vllm/model_executor/layers/quantization/awq.py`

### Summary

**What changed and why**  
The fix addresses a missing `is_layer_skipped` check for FusedMoE layers in AWQ quantization. When models had unquantized layers (specified in `modules_to_not_convert`), the code incorrectly assigned quantized methods to those layers, causing weight loading failures. The changes ensure skipped layers properly receive `UnquantizedFusedMoEMethod` by delegating to `AWQMarlinConfig.get_quant_method` and passing the skip list to `MoeWNA16Config`.

**Technical impact**  
This correction ensures consistency in how skipped layers are handled across different quantization configurations. The delegation pattern now matches the existing logic in `AWQMarlinConfig`, maintaining architectural symmetry. Models with mixed precision (e.g., bfloat16 MTP layers) will now load correctly without expecting quantized weights for skipped layers.

**Potential risks**  
If `AWQMarlinConfig.get_quant_method` has undiscovered bugs or different behavior, this delegation could propagate issues. The `MoeWNA16Config` now receives `modules_to_not_convert`, but its internal handling of this parameter must be verified to ensure proper layer skipping.

**Key insights**  
Always use existing delegation patterns (`get_quant_method`) rather than directly instantiating quantization methods to maintain consistent skip logic. This fix highlights the importance of propagating all relevant configuration parameters (like `modules_to_not_convert`) when creating sub-configurations to prevent subtle integration bugs.

---

## 16. [[Refactor] Clean up unused variables & func](https://github.com/vllm-project/vllm/pull/32692)


### Base Information

- **PR Number:** #32692
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-23 14:04:25
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32692/files) (5):**
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/model_executor/layers/quantization/utils/petit_utils.py`
  - `vllm/model_executor/models/glmasr_utils.py`
  - `vllm/model_executor/models/phi4mm_audio.py`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
This PR removes unused variables, function aliases, and entire functions across five files to clean up dead code. The changes include eliminating an unused constant `_LONG_INFO`, a redundant function alias `_require_petit`, an unused helper function `_get_num_features_for_item`, an unused token constant `_AUDIO_PLACEHOLDER_TOKEN_ID`, and an empty tuple constant `_ROCM_SWA_REASON`.

**Technical impact**  
These deletions reduce codebase clutter and potential maintenance overhead without affecting runtime behavior, as the removed elements were not referenced elsewhere. The cleanup slightly improves code readability and may marginally reduce import overhead (e.g., removing an unused `torch` import in `protocol.py`).

**Potential risks**  
Low risk, but developers should verify the removed `_get_num_features_for_item` function isn't used via dynamic imports or reflection. The removal of `_ROCM_SWA_REASON` suggests ROCm platform support may have evolved, but the empty tuple was likely a legacy placeholder.

**Key insights**  
This is a safe cleanup that follows good hygiene practices. Future similar efforts should use static analysis tools to systematically identify unused code. The `torch.iinfo(torch.long)` constant removal hints at previous platform-specific integer handling that is no longer needed.

---

## 17. [[Refactor] Rename `gptq_marlin` to `marlin` to match MoE](https://github.com/vllm-project/vllm/pull/32952)


### Base Information

- **PR Number:** #32952
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-23 13:48:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32952/files) (24):**
  - `CMakeLists.txt`
  - `benchmarks/kernels/benchmark_machete.py`
  - `benchmarks/kernels/benchmark_marlin.py`
  - `csrc/moe/marlin_moe_wna16/kernel.h`
  - `csrc/moe/marlin_moe_wna16/marlin_template.h`
  - `csrc/quantization/gptq_allspark/allspark_utils.cuh`
  - `csrc/quantization/marlin/.gitignore`
  - `csrc/quantization/marlin/awq_marlin_repack.cu`
  - `csrc/quantization/marlin/dequant.h`
  - `csrc/quantization/marlin/generate_kernels.py`
  - `csrc/quantization/marlin/gptq_marlin_repack.cu`
  - `csrc/quantization/marlin/kernel.h`
  - `csrc/quantization/marlin/marlin.cu`
  - `csrc/quantization/marlin/marlin.cuh`
  - `csrc/quantization/marlin/marlin_dtypes.cuh`
  - `csrc/quantization/marlin/marlin_int4_fp8_preprocess.cu`
  - `csrc/quantization/marlin/marlin_mma.h`
  - `csrc/quantization/marlin/marlin_template.h`
  - `csrc/torch_bindings.cpp`
  - `tests/kernels/quantization/test_marlin_gemm.py`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py`

### Summary

**What changed and why**  
The PR renames the `gptq_marlin` directory and associated functions to `marlin` to better reflect the broader scope of the Marlin kernel, which now supports multiple quantization schemes (GPTQ, AWQ, FP8, NVFP4, MXFP4) beyond just GPTQ. This aligns the naming with its usage in MoE (Mixture of Experts) contexts.

**Technical impact**  
These changes update file paths, function names, and references across the codebase, including build scripts (CMakeLists.txt), kernel benchmarks, tests, and Python bindings. The refactor ensures consistency but does not alter the underlying functionality or performance of the Marlin kernels.

**Potential risks**  
The main risk is missing updates to any hardcoded references in documentation, scripts, or external dependencies that might still use the old `gptq_marlin` naming. Additionally, any conditional compilation or dynamic imports based on the old paths could break if not updated.

**Key insights**  
This is a straightforward refactoring that improves naming clarity and consistency. Developers should verify that all build and test pipelines pass, and update any external documentation or scripts that reference the old names. The change simplifies the codebase by using a generic name that matches the kernel's expanded capabilities.

---

## 18. [[CI][AMD][BugFix] Update wvSplitK (and other skinny_gemm wrappers) to ensure tensors passed will be made contiguous for the kernel](https://github.com/vllm-project/vllm/pull/32831)


### Base Information

- **PR Number:** #32831
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-23 13:35:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32831/files) (2):**
  - `csrc/rocm/skinny_gemms.cu`
  - `vllm/_custom_ops.py`

### Summary

**What changed and why**  
Added a helper function `rocm_enforce_contiguous_skinny_gemm_inputs` to ensure tensors passed to ROCm skinny GEMM kernels are contiguous, and applied it to all relevant wrapper functions (`LLMM1`, `wvSplitK`, `wvSplitKrc`, `wvSplitKQ`). This fixes correctness issues where non-contiguous tensors caused kernel failures, particularly affecting AMD CI tests. A code comment also highlights underlying kernel limitations (lack of stride support, integer overflow risks).

**Technical impact**  
The change guarantees tensor contiguity before kernel execution, eliminating immediate correctness failures but introducing potential performance overhead due to implicit cloning of non-contiguous tensors. The fix is a temporary workaround; the kernels themselves remain unchanged and still lack proper stride handling and overflow protection.

**Potential risks**  
Performance degradation may occur when tensors are non-contiguous, as `.contiguous()` triggers memory copies. The underlying kernel issues (integer overflow, missing stride support) persist and could cause subtle bugs with large tensors or specific memory layouts. The warning about performance loss is currently not implemented as described in the PR.

**Key insights**  
This is a correctness-first patch that trades performance for reliability. Developers should treat this as an interim solution and prioritize updating the CUDA kernels in `skinny_gemms.cu` to natively support strides and prevent overflow. Consider adding the promised warnings for non-contiguous inputs to alert users of performance impacts.

---

## 19. [[Bug] Fix benchmark script `moe_permute_unpermute`](https://github.com/vllm-project/vllm/pull/32949)


### Base Information

- **PR Number:** #32949
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-23 13:18:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32949/files) (1):**
  - `benchmarks/kernels/benchmark_moe_permute_unpermute.py`

### Summary

**What changed and why**  
Fixed two bugs in the benchmark script: 1) Import error where `fused_topk` was not defined due to incorrect import statement, and 2) Type error in `moe_align_block_size` function call where `block_size` parameter was `None`. The changes replace a wildcard import with a specific import and provide explicit integer values for alignment parameters.

**Technical impact**  
The benchmark script now correctly imports necessary functions and provides valid parameters to underlying MoE kernel functions. This ensures the benchmark can execute without runtime errors and produce accurate performance measurements for the permute/unpermute operations used in mixture-of-experts layers.

**Potential risks**  
The hardcoded `block_m=16` value may not match the actual block size requirements of different hardware configurations or model architectures. If the benchmark needs to test different alignment scenarios, this fixed value limits its flexibility and could produce misleading results for systems with different optimal block sizes.

**Key insights**  
Always use explicit imports instead of wildcards to avoid undefined name errors. When calling functions with optional parameters that have `None` defaults, ensure the calling context provides appropriate values if the function logic doesn't handle `None` correctly. Consider making the block size configurable via command-line arguments for more flexible benchmarking.

---

## 20. [fix: Add glm4_moe_lite to MLA detection](https://github.com/vllm-project/vllm/pull/32614)


### Base Information

- **PR Number:** #32614
- **Author:** [marksverdhei](https://github.com/marksverdhei)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-23 12:38:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32614/files) (4):**
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/platforms/cuda.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`
  - `vllm/v1/attention/backends/mla/flashinfer_mla.py`

### Summary

**What changed and why**  
The PR adds `glm4_moe_lite` and `glm4_moe_lite_mtp` to the MLA detection logic to enable efficient KV caching for these models, preventing a fallback to standard caching that uses 4x more memory. It also introduces compatibility checks for DeepSeek R1 MLA dimensions (qk_nope_head_dim=128, qk_rope_head_dim=64, v_head_dim=128) to disable unsupported kernels on SM100 (Blackwell) GPUs, ensuring CUTLASS MLA is used as a fallback.

**Technical impact**  
These changes ensure that GLM-4.7-Flash models leverage MLA’s memory-efficient KV caching, reducing VRAM usage. On SM100, the system now defaults to CUTLASS MLA and FA2 prefill when DeepSeek R1 dimensions are not met, avoiding kernel incompatibilities while maintaining performance. The MLA detection is now more precise, distinguishing between MLA and non-MLA variants (e.g., `glm4_moe` is excluded).

**Potential risks**  
If the DeepSeek R1 dimension check is too restrictive, it might incorrectly disable optimized kernels for other MLA-compatible models. The fallback to CUTLASS MLA on SM100 could impact performance if FlashInfer MLA is otherwise viable. Additionally, hardcoded dimension values may require updates for future model architectures.

**Key insights**  
Developers should verify that any new MLA-based models are added to `is_deepseek_mla()` and confirm their dimension compatibility. The SM100 workaround highlights the need for robust kernel selection logic; consider making dimension checks configurable. Testing across GPU architectures is essential to ensure optimal backend selection.

---

## 21. [[cudagraphs] Refactor cudagraph capture loop](https://github.com/vllm-project/vllm/pull/32946)


### Base Information

- **PR Number:** #32946
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-23 12:22:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32946/files) (3):**
  - `tests/v1/cudagraph/test_cudagraph_dispatch.py`
  - `vllm/v1/cudagraph_dispatcher.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The changes refactor the CUDA graph capture loop to centralize capture descriptor generation in `CudagraphDispatcher.get_capture_descs()`. This replaces hardcoded capture logic in `GPUModelRunner` with a more flexible system that returns pre-sorted descriptors for different runtime modes (PIECEWISE and FULL). This prepares the codebase for supporting different PIECEWISE/FULL sizes and dynamic spec-decode sizes.

**Technical impact**  
The architecture now separates capture descriptor generation from execution logic. `CudagraphDispatcher` becomes the single source of truth for what needs to be captured, returning properly grouped and sorted descriptors. This simplifies `GPUModelRunner._capture_cudagraphs()` to iterate over descriptors without mode-specific logic, making it easier to extend with new capture strategies.

**Potential risks**  
The refactor changes the order of graph capture - previously FULL mode captures happened after mixed mode, but now descriptors are returned PIECEWISE first then FULL. This could affect memory fragmentation patterns. The uniform decode logic now relies on the first descriptor's `uniform` field, which assumes all descriptors in a group share the same uniform value (validated in tests but could break with future changes).

**Key insights**  
The refactor successfully decouples capture planning from execution, making the system more maintainable. Developers should note that any new capture strategies should be implemented in `CudagraphDispatcher` rather than `GPUModelRunner`. The test coverage is good but should be expanded to verify actual capture behavior matches the descriptor ordering.

---

## 22. [[Model Runner V2] Add KV Connector support](https://github.com/vllm-project/vllm/pull/32742)


### Base Information

- **PR Number:** #32742
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-23 10:49:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32742/files) (4):**
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/kv_connector.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR adds KV Connector support to the Model Runner V2 architecture. The changes introduce a new `KVConnector` class that handles key-value cache transfer operations, replacing the previous mixin-based approach with a dedicated component. The connector manages pre-forward KV loading and post-forward KV saving operations, integrating with the existing KV transfer infrastructure.

**Technical impact**  
The architecture now centralizes KV cache management through a configurable connector component. The `init_kv_cache` function returns KV cache dictionaries for connector registration, enabling dynamic KV cache handling. The model runner delegates KV operations to the connector, simplifying the main execution flow and supporting both active and no-op connector modes based on system configuration.

**Potential risks**  
The forward context handling in `ActiveKVConnector.pre_forward` has a TODO comment indicating unresolved complexity. The `set_disabled` method modifies a global `kv_transfer_state`, which could cause race conditions in multi-threaded scenarios. Edge cases where `kv_connector_output` is `None` in `sample_tokens` may need explicit handling to prevent runtime errors.

**Key insights**  
The connector pattern improves separation of concerns but requires careful lifecycle management—note the new `ensure_kv_transfer_shutdown` call in worker cleanup. Developers should verify that the disabled state correctly isolates KV operations during dummy runs. The removal of `KVConnectorModelRunnerMixin` simplifies inheritance but necessitates thorough testing of all KV transfer scenarios.

---

## 23. [[Bugfix][CI] Fix pre-commit](https://github.com/vllm-project/vllm/pull/32956)


### Base Information

- **PR Number:** #32956
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-23 10:26:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32956/files) (1):**
  - `vllm/v1/worker/mamba_utils.py`

### Summary

**What changed and why**  
The changes fix a pre-commit issue caused by a previous PR (#30877) by moving the imports of `triton` and `triton.language as tl` from direct imports to imports via `vllm.triton_utils`. This ensures that pre-commit checks pass when merging main into PR branches, even though the issue doesn't break CI because `check_triton_import` only checks diffs.

**Technical impact**  
This refactor centralizes Triton-related imports through a utility module (`vllm.triton_utils`), promoting consistency and potentially simplifying future maintenance. The change is minimal and localized, affecting only the import statements in one file without altering any functional logic or dependencies.

**Potential risks**  
If `vllm.triton_utils` does not properly expose `triton` and `tl`, or if there are version mismatches, it could lead to import errors at runtime. Additionally, other files that still import Triton directly might need similar updates to maintain consistency and avoid similar pre-commit issues.

**Key insights**  
Centralizing imports through utility modules can help manage dependencies and pre-commit checks, but it's crucial to ensure all relevant modules use the same approach. Developers should verify that `vllm.triton_utils` is robust and consider applying this pattern to other Triton imports in the codebase to prevent similar issues.

---

## 24. [[CI][torch nightlies] Use main Dockerfile with flags for nightly torch tests](https://github.com/vllm-project/vllm/pull/30443)


### Base Information

- **PR Number:** #30443
- **Author:** [orionr](https://github.com/orionr)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-23 10:22:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30443/files) (4):**
  - `docker/Dockerfile`
  - `docker/Dockerfile.nightly_torch`
  - `docs/assets/contributing/dockerfile-stages-dependency.png`
  - `use_existing_torch.py`

### Summary

**What changed and why**  
The changes consolidate PyTorch nightly testing into the main Dockerfile by adding a `PYTORCH_NIGHTLY=1` flag, replacing the separate `Dockerfile.nightly_torch`. This introduces a unified build process that handles both release and nightly PyTorch versions, ensuring consistent torch library versions across all Docker stages through version tracking in `torch_lib_versions.txt`.

**Technical impact**  
This refactor simplifies CI maintenance by eliminating a dedicated nightly Dockerfile and reduces duplication. The version tracking mechanism prevents torch library drift between build stages, which is critical for reproducibility. The changes also modify dependency installation logic to conditionally use nightly indexes and strip torch dependencies from requirement files via an enhanced `use_existing_torch.py` script.

**Potential risks**  
The conditional logic based on `PYTORCH_NIGHTLY` increases Dockerfile complexity and could lead to subtle bugs if the flag is misused. The prefix-based stripping in `use_existing_torch.py` might inadvertently remove valid dependencies if requirement lines deviate from expected patterns. Additionally, caching strategies for nightly builds require careful management to avoid conflicts with release builds.

**Key insights**  
Developers should use the main Dockerfile with `PYTORCH_NIGHTLY=1` for nightly testing, as the dedicated nightly file is deprecated. The version tracking approach is a robust pattern for ensuring consistency across multi-stage builds. Future work should focus on further simplifying torch dependency management, as outlined in the PR description, to reduce conditional logic and improve maintainability.

---

## 25. [[V1][Hybrid] Mamba Prefix Caching with align mode](https://github.com/vllm-project/vllm/pull/30877)


### Base Information

- **PR Number:** #30877
- **Author:** [peakcrosser7](https://github.com/peakcrosser7)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-23 09:56:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30877/files) (42):**
  - `tests/v1/core/test_single_type_kv_cache_manager.py`
  - `tests/v1/e2e/test_mamba_prefix_cache.py`
  - `vllm/config/cache.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/layers/mamba/abstract.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`
  - `vllm/model_executor/layers/mamba/mamba_utils.py`
  - `vllm/model_executor/models/bamba.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/falcon_h1.py`
  - `vllm/model_executor/models/granitemoehybrid.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/jamba.py`
  - `vllm/model_executor/models/kimi_linear.py`
  - `vllm/model_executor/models/lfm2.py`
  - `vllm/model_executor/models/lfm2_moe.py`
  - `vllm/model_executor/models/mamba.py`
  - `vllm/model_executor/models/mamba2.py`
  - `vllm/model_executor/models/minimax_text_01.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron_h.py`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/models/qwen3_next_mtp.py`
  - `vllm/model_executor/models/zamba2.py`
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/linear_attn.py`
  - `vllm/v1/attention/backends/mamba_attn.py`
  - `vllm/v1/attention/backends/utils.py`
  - `vllm/v1/core/block_pool.py`
  - `vllm/v1/core/kv_cache_coordinator.py`
  - `vllm/v1/core/kv_cache_manager.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/core/single_type_kv_cache_manager.py`
  - `vllm/v1/kv_cache_interface.py`
  - `vllm/v1/worker/block_table.py`
  - `vllm/v1/worker/cp_utils.py`
  - `vllm/v1/worker/gpu_input_batch.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/mamba_utils.py`

### Summary

**What changed and why**  
This PR introduces Mamba prefix caching with "align" mode, enabling efficient caching of Mamba states without modifying kernel code. It adds a new `mamba_cache_mode` configuration with options "none", "all", and "align", where "align" uses block-aligned scheduling to cache Mamba states at block boundaries. The changes extend scheduler logic, KV cache management, and worker-side state copying to support prefix caching for Mamba models like Qwen3-Next and LFM2, while maintaining compatibility with existing attention-based caching.

**Technical impact**  
The implementation modifies scheduler behavior to enforce block-aligned token scheduling during prefill, ensuring Mamba states are cacheable at multiples of `block_size`. It introduces new Mamba-specific copy functions for state management and adjusts KV cache allocation to handle placeholder null-blocks. The changes affect core components like the scheduler, KV cache managers, and model runners, requiring careful coordination between scheduling granularity and state persistence.

**Potential risks**  
Speculative decoding is temporarily disabled due to corner-case bugs with prefix caching in align mode. The block-aligned scheduling may reduce scheduling flexibility, potentially underutilizing GPU resources when prompts don't align perfectly. There is complexity in state copying logic, especially with speculative tokens, which could lead to subtle bugs if copy offsets are miscalculated. The reliance on `block_size` alignment assumes consistent configuration across components.

**Key insights**  
Developers must enable chunked prefill and ensure `long_prefill_token_threshold >= block_size` when using align mode. The solution avoids kernel modifications but adds scheduling constraints. Testing should focus on edge cases around block boundaries and resumed requests. Future work should address speculative decoding compatibility and optimize the copy operations currently implemented with Triton kernels.

---

## 26. [[Model] Enable LoRA support for internvl2](https://github.com/vllm-project/vllm/pull/32397)


### Base Information

- **PR Number:** #32397
- **Author:** [MatteoFari](https://github.com/MatteoFari)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-23 09:39:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32397/files) (1):**
  - `vllm/model_executor/models/internvl.py`

### Summary

**What changed and why**  
The changes enable LoRA support for InternVL2 by adding two helper methods: `get_num_mm_encoder_tokens()` and `get_num_mm_connector_tokens()`. These methods correctly map token counts between the language model and the vision tower/connector, accounting for InternVL2's CLS token and pixel-shuffle downsampling (downsample_ratio). The initialization code was also refactored to store `patch_tokens` separately for reuse.

**Technical impact**  
These additions allow dynamic LoRA adapters to function properly with InternVL2's multi-modal architecture by ensuring accurate token count mappings across different components. The refactoring improves code clarity by separating patch token calculation from the total image token computation, making the logic more maintainable.

**Potential risks**  
If `num_image_tokens` or `num_vision_tokens` are not perfectly divisible by `self.num_image_token` or `(self.patch_tokens + 1)`, the integer division could lead to incorrect token counts due to truncation. Additionally, edge cases with zero or negative token inputs are handled but should be validated in calling code to prevent unexpected behavior.

**Key insights**  
Developers should ensure that token counts passed to these methods align with expected multiples to avoid mapping errors. The separation of `patch_tokens` enhances readability and reusability. Consider adding assertions or logging for divisibility checks to catch potential mismatches early in development.

---

## 27. [[torch.compile][CI] Add back attn fusion on hopper/ada](https://github.com/vllm-project/vllm/pull/32940)


### Base Information

- **PR Number:** #32940
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-23 08:49:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32940/files) (1):**
  - `tests/compile/test_fusion_attn.py`

### Summary

**What changed and why**  
The changes replace large Llama-4-Scout model names with smaller 8B variants in attention quant fusion tests and remove a GPU count check. This ensures tests run on single-GPU systems by using appropriately sized models while maintaining test coverage for quantization patterns.

**Technical impact**  
These modifications enable attention quantization fusion tests to execute on standard single-GPU CI environments without skipping. The test logic now relies on capability checks (Blackwell architecture) rather than model size constraints, making test execution more predictable across different hardware configurations.

**Potential risks**  
The smaller 8B models might not fully exercise memory or performance boundaries that the original 17B models would have tested. There's a slight risk that certain edge cases related to larger model sizes could be missed, though the quantization patterns themselves should remain valid.

**Key insights**  
Developers should ensure that test models are appropriately sized for CI environments to prevent unnecessary test skipping. When modifying test configurations, verify that the new models maintain equivalent coverage for the features being tested, particularly for quantization and fusion patterns.

---

## 28. [[Frontend] add logprob, compression_rate to 'verbose_json' features](https://github.com/vllm-project/vllm/pull/31059)


### Base Information

- **PR Number:** #31059
- **Author:** [sangbumlikeagod](https://github.com/sangbumlikeagod)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-23 08:35:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31059/files) (4):**
  - `docs/serving/openai_compatible_server.md`
  - `tests/entrypoints/openai/test_transcription_validation_whisper.py`
  - `vllm/entrypoints/openai/translations/protocol.py`
  - `vllm/entrypoints/openai/translations/speech_to_text.py`

### Summary

**What changed and why**  
This PR adds `avg_logprob` and `compression_ratio` fields to the `verbose_json` response format for Whisper transcription and translation segments. Previously, these fields were omitted in segment outputs despite being available in the overall response. The changes compute per-segment metrics and update the API schemas to make these fields required.

**Technical impact**  
The implementation automatically enables `sampling_params.logprobs` when `response_format="verbose_json"` to capture token-level probabilities, calculates average logprobs per segment, and computes compression ratios using zlib. The Pydantic models in `protocol.py` now enforce non-nullable fields, ensuring consistent output structure. This enhances the API's diagnostic capabilities by providing segment-level quality metrics.

**Potential risks**  
If logprobs are unavailable in generation results (e.g., due to model or configuration issues), the `assert op.outputs[0].logprobs` statement could raise an unhandled exception. The compression ratio calculation assumes UTF-8 encoding, which may not be appropriate for all text content. Additionally, the segment slicing logic depends on timestamp token ordering, which could produce incorrect segments if tokens are unsorted.

**Key insights**  
Developers should verify that logprobs are consistently available when using `verbose_json`. Consider adding fallback behavior or validation for missing logprobs. The compression ratio implementation is a useful hallucination detector but may need adjustment for non-UTF-8 contexts. Ensure timestamp token ordering is guaranteed in the Whisper tokenizer to maintain segment accuracy.

---

## 29. [[Hardware][AMD][CI][Bugfix] Fix Kernels Attention Cache test](https://github.com/vllm-project/vllm/pull/32904)


### Base Information

- **PR Number:** #32904
- **Author:** [mawong-amd](https://github.com/mawong-amd)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-23 08:24:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32904/files) (1):**
  - `tests/kernels/attention/test_cache.py`

### Summary

**What changed and why**  
The PR fixes a failing unit test for the attention cache kernel on AMD hardware by replacing a hardcoded `torch.float8_e4m3fn` data type with a platform-aware `current_platform.fp8_dtype()` call. This corrects a mismatch where the test's reference dequantization logic assumed a specific FP8 format (`e4m3fn`) that differs from the one used on AMD `gfx942` cards (`e4m3fnuz`).

**Technical impact**  
This change ensures the test's reference implementation correctly aligns with the actual FP8 data type used by the underlying hardware during kernel execution. It makes the test portable across different platforms (e.g., NVIDIA vs. AMD) that may support different FP8 variants, preventing false test failures on AMD CI.

**Potential risks**  
The risk is minimal as this is a test-only change. However, it assumes the `current_platform.fp8_dtype()` method reliably returns the correct FP8 type for all supported platforms and contexts. If this utility is incomplete or incorrect for other hardware, similar test issues could arise elsewhere.

**Key insights**  
The fix highlights the importance of avoiding hardcoded data type assumptions in cross-platform tests. Developers should leverage existing platform abstraction utilities (`current_platform`) for hardware-specific properties. This pattern should be reviewed and applied elsewhere in the codebase to prevent similar platform-dependent test failures.

---

## 30. [[ROCm][PD] Remove unused moriio connector proxy code](https://github.com/vllm-project/vllm/pull/32939)


### Base Information

- **PR Number:** #32939
- **Author:** [markmc](https://github.com/markmc)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-23 07:59:04
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32939/files) (1):**
  - `examples/online_serving/disaggregated_serving/moriio_toy_proxy_server.py`

### Summary

**What changed and why**  
Removed the unused `send_request_to_decode()` function and eliminated a redundant call to `extract_ip_port_fast()` for the prefill instance endpoint. The changes clean up dead code and remove unnecessary operations, as indicated by the PR description stating the function is uncalled and the duplicate extraction.

**Technical impact**  
This is a straightforward code cleanup that reduces the codebase size and eliminates unused logic. The removal of the redundant function call slightly improves runtime efficiency for the affected code path by avoiding duplicate URL parsing operations.

**Potential risks**  
Low risk since the removed code was confirmed unused. However, developers should verify no other parts of the codebase or external systems indirectly depended on the `send_request_to_decode()` function's behavior or side effects. The change to `extract_ip_port_fast()` should be checked to ensure the `ip` and `port` variables weren't used later in the function (they appear to be unused based on the diff).

**Key insights**  
This cleanup improves code maintainability by removing dead weight. When removing unused code, always verify dependencies and usage across the entire project. The pattern of removing duplicate function calls is a good practice for performance and clarity, but ensure the removed variables don't have downstream uses.

---

## 31. [[Bugfix] Fix FP8 MoE EP Weight Loading for ModelOpt Llama4](https://github.com/vllm-project/vllm/pull/32886)


### Base Information

- **PR Number:** #32886
- **Author:** [baonudesifeizhai](https://github.com/baonudesifeizhai)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-23 07:31:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32886/files) (1):**
  - `vllm/model_executor/models/llama4.py`

### Summary

**What changed and why**  
Added a version-guarded workaround in the MoE expert weight loading logic for FP8 models on older PyTorch versions (<2.11). When FP8 weights reside on CPU, indexing operations fail due to lack of support, so the fix temporarily casts weights to FP16 for indexing before casting back, preserving behavior on newer PyTorch releases.

**Technical impact**  
This change ensures backward compatibility with PyTorch <2.11 when loading FP8 MoE models with expert parallelism, preventing `index_cpu` errors. The logic is isolated to CPU-based FP8 indexing scenarios, leaving GPU paths and newer PyTorch versions unaffected. It introduces minimal overhead (two dtype conversions) only under specific conditions.

**Potential risks**  
The FP8 dtype detection heuristic (`element_size() == 1`) may incorrectly match other 1-byte dtypes (e.g., `torch.uint8`), though context suggests FP8 is the target. The workaround adds small performance cost for affected configurations. Additionally, the noted FlashInfer kernel crashes on SM100 hardware remain unresolved and could affect production deployment.

**Key insights**  
Always validate dtype detection logic to avoid false positives with non-FP8 1-byte types. Consider adding a more explicit FP8 check (e.g., `is_fp8_dtype = new_loaded_weight.dtype in [torch.float8_e4m3fn, torch.float8_e5m2]` if available). Monitor the upstream FlashInfer stability issue separately, as it may require disabling FP8 kernels or alternative backends on affected hardware.

---

## 32. [[Misc] Postpone torch_profiler deprecation](https://github.com/vllm-project/vllm/pull/32867)


### Base Information

- **PR Number:** #32867
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-01-23 06:39:49
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32867/files) (1):**
  - `vllm/config/profiler.py`

### Summary

**What changed and why**  
The change postpones the deprecation warning for the `torch_profiler` environment variable from v0.14.0 to v0.15.0 (or v1.0.0, whichever is soonest). This provides users with additional time to migrate from environment variable usage to the preferred configuration methods.

**Technical impact**  
This is a minor version bump in the deprecation schedule that delays the removal of legacy environment variable support. It does not affect system functionality or architecture, only the timing of when the deprecated feature will be removed.

**Potential risks**  
If users rely on the original v0.14.0 timeline for planning migrations, they might be caught off guard by the extension. There's also a risk of deprecation schedule creep if this pattern repeats frequently.

**Key insights**  
Maintain clear communication about deprecation timelines across documentation to prevent user confusion. Consider establishing a formal deprecation policy to ensure consistent version planning for future deprecated features.

---

## 33. [[Bugfix] Disable tma_aligned_scales in test_fusions_e2e](https://github.com/vllm-project/vllm/pull/32916)


### Base Information

- **PR Number:** #32916
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-23 06:34:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32916/files) (3):**
  - `tests/compile/distributed/test_fusions_e2e.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`

### Summary

**What changed and why**  
This PR introduces a new environment variable `VLLM_USE_DEEP_GEMM_TMA_ALIGNED_SCALES` to control the `tma_aligned_scales` parameter in FP8 quantization utilities. The variable is disabled in the `test_rms_group_quant` test to unblock CI failures, with a TODO comment indicating this is a temporary workaround for an underlying fusion issue.

**Technical impact**  
The change adds a configuration point for DeepGEMM's tensor memory access (TMA) alignment behavior for scale tensors. This allows runtime control over a low-level optimization that appears to be causing test failures, without modifying the core logic. The test modification is a targeted workaround that isolates the problematic configuration.

**Potential risks**  
The temporary test workaround could mask the root cause if not properly tracked and resolved. There's a risk that other tests or production code paths might encounter similar issues with TMA-aligned scales enabled. The boolean environment variable parsing (`int(os.getenv(...))`) could be fragile if non-numeric values are provided.

**Key insights**  
This is clearly a stopgap solution—developers should prioritize fixing the underlying fusion issue referenced in the TODO. Consider adding test coverage for both enabled and disabled states of this flag once the root cause is resolved. The pattern of adding environment variables for granular control of DeepGEMM features is becoming established in the codebase.

---

## 34. [[Bugfix] Fix getting vision features in Transformer Multimodal backend](https://github.com/vllm-project/vllm/pull/32933)


### Base Information

- **PR Number:** #32933
- **Author:** [zucchini-nlp](https://github.com/zucchini-nlp)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 05:34:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32933/files) (1):**
  - `vllm/model_executor/models/transformers/multimodal.py`

### Summary

**What changed and why**  
The change adds handling for new output formats from `self.model.get_image_features()` in the Transformers multimodal backend. This addresses a breaking change introduced in Transformers v5 where the method now returns either a tuple (containing features plus optional attentions/hidden_states) or a dict, instead of always returning a single tensor or list of tensors.

**Technical impact**  
This update ensures compatibility with Transformers v5 while maintaining backward compatibility. The code now extracts the primary vision features from either tuple or dict outputs, allowing the multimodal backend to continue functioning across different Transformers versions without requiring model-specific adjustments.

**Potential risks**  
The extraction logic assumes the first element of a tuple contains the needed features, which may not hold true for all future model configurations. Similarly, relying on `pooler_output` key for dict outputs could fail if models use different output key names. The comment about enabling Qwen3-VL suggests this is a temporary fix that may need refinement.

**Key insights**  
This is a necessary compatibility patch, but consider implementing a more robust feature extraction method that checks output structure more thoroughly. The temporary nature of this solution suggests monitoring Transformers v5 stabilization and updating the logic accordingly. Adding logging or warnings when extracting from tuple/dict could help debug future compatibility issues.

---

## 35. [[Feature]: Remove DtoH Copy for lfm2_vl On Default Stream](https://github.com/vllm-project/vllm/pull/32815)


### Base Information

- **PR Number:** #32815
- **Author:** [tianshu-Michael-yu](https://github.com/tianshu-Michael-yu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 05:20:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32815/files) (5):**
  - `vllm/model_executor/models/lfm2_siglip2.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/mamba_attn.py`
  - `vllm/v1/attention/backends/utils.py`

### Summary

**What changed and why**  
This PR eliminates Device-to-Host (DtoH) memory copies during LFM2-VL preprocessing by implementing packed (unpadded) tensor processing throughout the vision pipeline. Key changes include modifying Siglip2 vision embedding to handle packed inputs directly, updating the multi-modal projector to operate on packed tensors, and ensuring metadata computations stay on CPU to avoid GPU syncs.

**Technical impact**  
The changes fundamentally alter how variable-length image sequences are processed by eliminating padding and associated synchronization points. This reduces GPU-CPU synchronization overhead and improves preprocessing throughput. The architecture now maintains spatial metadata on CPU and uses packed tensor representations end-to-end, which is more efficient for variable-length sequences.

**Potential risks**  
The packed tensor approach assumes spatial dimensions are divisible by the downsample factor, which could cause runtime errors with malformed inputs. Removing padding logic eliminates safety checks for irregular shapes. CPU-side metadata calculations could become bottlenecks for very large batch sizes, and the complex index manipulation in the projector increases code complexity.

**Key insights**  
This optimization demonstrates the importance of minimizing GPU-CPU synchronization for performance-critical paths. Developers should maintain CPU-side metadata for variable-length sequences and consider packed tensor representations early in the design phase. The changes require careful validation of input dimensions and thorough testing across diverse image sizes to ensure robustness.

---

## 36. [[CPU][Feat] Update PyTorch to v2.10 for CPU Backend](https://github.com/vllm-project/vllm/pull/32869)


### Base Information

- **PR Number:** #32869
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-23 05:13:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32869/files) (3):**
  - `docker/Dockerfile.cpu`
  - `requirements/cpu-build.txt`
  - `requirements/cpu.txt`

### Summary

**What changed and why**  
Updated PyTorch from v2.9.1 to v2.10.0 across CPU backend configuration files (Dockerfile, build, and runtime requirements). This change leverages performance improvements in PyTorch v2.10 for Arm CPUs, including mimalloc allocation optimizations, LLM throughput gains, better thread scaling, and updated Arm Compute Library (ACL) support.

**Technical impact**  
The upgrade introduces performance enhancements for AArch64 systems, such as faster inference for models like DenseNet121 and GPT2-Large, improved multi-threaded scaling, and optimized attention kernels. It also removes the explicit `scons` dependency for ACL builds on AArch64, implying PyTorch v2.10 handles this internally or no longer requires it.

**Potential risks**  
Compatibility issues may arise if PyTorch v2.10 introduces breaking API changes or deprecations not yet reflected in vLLM’s codebase. The removal of `scons` on AArch64 could affect builds if ACL compilation still requires it. Additionally, platform-specific conditional logic (e.g., `platform_machine == "aarch64"`) may need verification to ensure correct package resolution across all supported architectures.

**Key insights**  
Validate that all CPU backend tests pass, especially on AArch64, to confirm no regressions in performance or functionality. Monitor for any build failures related to ACL compilation due to the removed `scons` dependency. Consider updating documentation or release notes to highlight the performance benefits for Arm-based deployments.

---

## 37. [[Benchmark][Bugfix] Fix race condtion when starting server for sweep benchmark](https://github.com/vllm-project/vllm/pull/32927)


### Base Information

- **PR Number:** #32927
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 04:11:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32927/files) (2):**
  - `vllm/benchmarks/sweep/serve.py`
  - `vllm/benchmarks/sweep/server.py`

### Summary

**What changed and why**  
Added a timeout mechanism to wait for the server to become ready before starting benchmark commands in the sweep benchmark. This fixes a race condition where benchmarks were executing before the server was fully initialized, causing connection failures.

**Technical impact**  
The changes introduce a new `wait_until_ready()` method in the `ServerProcess` class that polls the `/health` endpoint, and a configurable `--server-ready-timeout` CLI parameter (default 300 seconds) that propagates through the benchmark execution flow. This ensures proper synchronization between server startup and benchmark execution.

**Potential risks**  
If the server's health endpoint becomes unresponsive or returns false positives, the timeout mechanism could either wait indefinitely (until timeout) or proceed with benchmarks prematurely. The 1-second polling interval may cause unnecessary delays in fast-starting servers, though this is minor.

**Key insights**  
The fix properly addresses synchronization issues in distributed benchmark execution. Developers should verify that the `/health` endpoint reliably indicates server readiness across different vLLM configurations. Consider making the polling interval configurable if fine-tuning startup performance becomes important.

---

## 38. [[CPU Backend][BugFix] Fix failing CPU MoE test](https://github.com/vllm-project/vllm/pull/32876)


### Base Information

- **PR Number:** #32876
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-23 04:06:52
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32876/files) (1):**
  - `tests/kernels/moe/test_moe.py`

### Summary

**What changed and why**  
Added the `@pytest.mark.usefixtures("default_vllm_config")` decorator to the `test_moe_sum` test function. This ensures the test uses a default configuration fixture, likely to provide necessary runtime settings or state that was missing, which caused the test to fail.

**Technical impact**  
The change is minimal and localized, affecting only the test execution environment for this specific test. It ensures the test has access to required configuration (e.g., model settings, backend flags) before running, aligning it with other tests that may already use this fixture.

**Potential risks**  
If the `default_vllm_config` fixture is not properly defined or has side effects (e.g., modifies global state), it could introduce flakiness or dependencies between tests. Additionally, other similar tests might still lack this fixture, potentially causing future failures.

**Key insights**  
Always verify that test fixtures are consistently applied across related test cases to avoid environment-specific failures. Consider auditing other MoE tests for missing fixtures and ensure fixture definitions are lightweight and isolated to prevent unintended interactions.

---

## 39. [[Frontend][3/n] Make pooling entrypoints request schema consensus \| EmbedRequest & ClassifyRequest](https://github.com/vllm-project/vllm/pull/32905)


### Base Information

- **PR Number:** #32905
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-23 04:03:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32905/files) (11):**
  - `docs/serving/openai_compatible_server.md`
  - `examples/pooling/score/convert_model_to_seq_cls.py`
  - `tests/entrypoints/pooling/classify/test_online_vision.py`
  - `tests/entrypoints/test_utils.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/pooling_params.py`

### Summary

**What changed and why**  
This PR refactors pooling entrypoints by splitting shared request mixins (`EncodingRequestMixin`, `EmbedRequestMixin`, `ClassifyRequestMixin`) into a common base module. The changes centralize parameter definitions for embedding, classification, and encoding tasks, reducing code duplication and aligning schema across different pooling endpoints (embed, classify, score, rerank). Documentation updates reflect the new structure and parameter groupings.

**Technical impact**  
The refactoring improves code maintainability by consolidating duplicate field definitions and validation logic into reusable mixins. It ensures consistent parameter handling across pooling-related APIs (e.g., `normalize`, `encoding_format`, `use_activation`) and simplifies future extensions. The `PoolingParams` generation is now delegated to mixin methods, promoting a cleaner separation between request parsing and pooling configuration.

**Potential risks**  
There is a risk of breaking changes if existing clients rely on the previous parameter names or locations (e.g., `embedding-pooling-params` renamed to `embed-pooling-params`). The deprecation warnings for `softmax` and `activation` in favor of `use_activation` must be clearly communicated to users. Additionally, the consolidation of mixins could introduce subtle bugs if any endpoint-specific overrides are missed during the refactor.

**Key insights**  
Developers should verify that all pooling endpoints (embed, classify, score, rerank) correctly inherit the new mixins and that the `to_pooling_params()` methods produce the expected `PoolingParams`. The documentation changes are extensive; ensure the updated code snippets accurately reflect the new parameter groupings. Testing should focus on edge cases, especially for multimodal inputs and deprecated parameter fallbacks.

---

## 40. [[Voxtral] Add new streaming arch](https://github.com/vllm-project/vllm/pull/32861)


### Base Information

- **PR Number:** #32861
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged time:** 2026-01-23 03:41:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32861/files) (9):**
  - `tests/models/multimodal/generation/test_voxtral_streaming.py`
  - `vllm/model_executor/models/llama.py`
  - `vllm/model_executor/models/mistral.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_streaming.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/model_executor/models/whisper_causal.py`
  - `vllm/model_executor/models/whisper_utils.py`
  - `vllm/transformers_utils/configs/mistral.py`

### Summary

**What changed and why**  
This PR introduces a new streaming architecture for Voxtral models by refactoring causal Whisper logic into a separate file (`whisper_causal.py`) and adapting the Voxtral streaming architecture to use this improved structure. Key changes include adding support for causal Whisper encoders with block pooling, extending the Mistral model to handle time conditioning (`t_cond`), and updating weight loading and processing logic accordingly. A new test file is added but currently skipped.

**Technical impact**  
The changes enable Voxtral to support streaming audio processing by leveraging a causal Whisper encoder with block-pooled attention, which reduces KV cache size and improves efficiency. The architecture now properly propagates time conditioning through the model and handles position embedding expansion for audio tokens. This refactoring separates causal and non-causal Whisper implementations, improving code modularity and maintainability.

**Potential risks**  
The new `WhisperCausalAttentionWithBlockPooling` class currently only supports FlashAttention backends, limiting compatibility with other attention implementations. The weight loading logic has become more complex with additional mappings, increasing the risk of loading errors. The skipped tests mean the streaming functionality is not yet validated in CI, which could hide integration issues.

**Key insights**  
Developers should note that the causal Whisper encoder now requires specific configuration flags (`is_causal`, `block_pool_size`) and uses RoPE position embeddings. The `t_cond` parameter must be passed through the model forward pass for time conditioning. Ensure attention backend compatibility when extending support beyond FlashAttention, and prioritize enabling the skipped tests to validate the streaming pipeline.

---

## 41. [[CI/Build][CPU] Fix failed pooling tests and macos smoke test](https://github.com/vllm-project/vllm/pull/32907)


### Base Information

- **PR Number:** #32907
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 02:48:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32907/files) (2):**
  - `csrc/cpu/torch_bindings.cpp`
  - `vllm/model_executor/layers/utils.py`

### Summary

**What changed and why**  
Two changes were made: 1) In `torch_bindings.cpp`, the condition for enabling SHM CCL operations was modified to exclude Apple platforms (macOS) on aarch64, preventing unsupported operations from being registered. 2) In `utils.py`, a check was added to skip post-weight processing for layers with meta tensors, avoiding errors when weights are missing.

**Technical impact**  
The SHM CCL change restricts shared memory manager operations to non-macOS aarch64 systems, aligning with platform-specific capabilities. The meta tensor check ensures CPU unquantized GEMM dispatch gracefully handles layers without initialized weights, improving robustness during model loading or initialization phases.

**Potential risks**  
Excluding macOS from SHM CCL may affect performance on Apple Silicon if shared memory features are later supported. Skipping processing for meta tensors could mask underlying issues if weights are unintentionally in meta state, potentially leading to silent failures in downstream operations.

**Key insights**  
These are defensive fixes for test failures—platform compatibility and missing weight handling. Developers should verify that meta tensor cases are intentional (e.g., during model tracing) and consider logging or assertions for debugging. Future work may involve revisiting macOS support for SHM CCL if hardware capabilities evolve.

---

## 42. [[Misc] Add `get_name` to missing AttentionBackends](https://github.com/vllm-project/vllm/pull/32698)


### Base Information

- **PR Number:** #32698
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-23 02:35:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32698/files) (6):**
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/linear_attn.py`
  - `vllm/v1/attention/backends/mamba1_attn.py`
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/attention/backends/mla/indexer.py`
  - `vllm/v1/attention/backends/short_conv_attn.py`

### Summary

**What changed and why**  
Added the `get_name()` static method to six attention backend classes (GDN, Linear, Mamba1, Mamba2, DeepseekV32 Indexer, ShortConv) that were missing this identifier. The change ensures all backends provide a unique string tag for identification, aligning with the expected structure of the `AttentionBackend` interface.

**Technical impact**  
This update standardizes the backend interface, making it easier to identify and manage different attention mechanisms programmatically. It does not affect functional behavior but improves debuggability, logging, and dynamic backend selection by providing consistent metadata.

**Potential risks**  
Low risk since `get_name()` is non-functional metadata. However, ensuring name uniqueness across all backends is critical to avoid conflicts. The change also introduces a minor maintenance burden to keep names consistent if new backends are added.

**Key insights**  
The PR successfully fills a consistency gap in the attention backend design. Consider adding a base class default implementation or validation to enforce uniqueness and reduce boilerplate in future backend implementations.

---

## 43. [[CI][Models] Add VLM Support for Sequence Classification Conversion](https://github.com/vllm-project/vllm/pull/32885)


### Base Information

- **PR Number:** #32885
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-23 00:22:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32885/files) (3):**
  - `vllm/model_executor/layers/layernorm.py`
  - `vllm/model_executor/models/adapters.py`
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
This PR adds support for converting Vision-Language Models (VLMs) like Gemma 3 to sequence classifiers using existing `no_post_processing` and `from_2_way_softmax` methods. It also fixes two PyTorch compiler warnings: a torch.dynamo recompilation warning in RMSNorm and a nested tensor prototype warning.

**Technical impact**  
The changes introduce VLM-aware logic in sequence classification weight loading by adding a helper to correctly locate the inner language model component. This prevents attribute access failures and recursive loading issues specific to VLM architectures. The RMSNorm optimization splits compilation paths to eliminate type-based recompilation, improving performance. The nested tensor fix ensures future compatibility with PyTorch API changes.

**Potential risks**  
The VLM detection logic relies on attribute names and class name patterns, which could break with future model architectures. Temporarily clearing configs on inner models introduces state mutation that must be carefully reverted. The split RMSNorm functions increase code duplication, though they maintain identical numerical behavior.

**Key insights**  
Developers should note that VLMs now correctly support sequence classification conversion without affecting standard LLMs. The RMSNorm change is a performance optimization that doesn't alter functionality. Always test both classification and generation modes when modifying VLM support, as demonstrated in the PR's comprehensive testing.

---

## 44. [[Bugfix] Fix _CPU_MOE_ACT AssertionError when vLLM config not set](https://github.com/vllm-project/vllm/pull/32777)


### Base Information

- **PR Number:** #32777
- **Author:** [karanb192](https://github.com/karanb192)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-23 00:22:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32777/files) (2):**
  - `tests/kernels/moe/test_cpu_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`

### Summary

**What changed and why**  
The fix replaces a lazy activation dictionary (`_LazyActivationDict`) with direct function references to resolve an `AssertionError` when vLLM config isn't set. The lazy dictionary triggered `CustomOp` class instantiation during forward passes, which called `get_current_vllm_config()` prematurely. Now, static methods (`SiluAndMul.forward_native`) and a standalone function (`_swigluoai_forward_native`) are used directly, avoiding any `CustomOp` instantiation.

**Technical impact**  
This change eliminates a runtime dependency on the vLLM configuration system during kernel execution, making the CPU fused MoE implementation more robust, especially under `torch.compile`. The architecture shifts from object-oriented lazy loading to functional programming, reducing initialization overhead and potential side effects from class constructors.

**Potential risks**  
The standalone `_swigluoai_forward_native` function duplicates logic from `SwigluOAIAndMul.forward_native`, creating a maintenance risk if the original implementation changes. Additionally, the fix assumes all required activations are covered by the static method or standalone function; any future activation additions must follow this pattern to avoid regressions.

**Key insights**  
Always prefer static methods or pure functions over class instantiation when configuration state is involved. For kernel code, minimize dependencies on global state or configuration systems that may not be initialized during execution. Consider adding a unit test that validates the fix under `torch.compile` to prevent similar issues.

---

