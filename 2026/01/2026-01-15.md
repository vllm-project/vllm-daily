# vLLM Merged PR Report

**Report Date:** 2026-01-15 PST

**Total Merged PRs:** 38

---

## 1. [fix(rocm): Enable non-gated MoE (is_act_and_mul=False) support on ROCm](https://github.com/vllm-project/vllm/pull/32244)


### Base Information

- **PR Number:** #32244
- **Author:** [rabi](https://github.com/rabi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 23:31:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32244/files) (2):**
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`

### Summary

**What changed and why**  
The changes enable non-gated MoE (is_act_and_mul=False) support on ROCm platforms. Previously, the platform check only allowed CUDA, blocking models like NemotronH that use activations like relu2_no_mul. The fix updates platform checks and disables AITER kernels for non-gated activations since they only support gated activations (silu/gelu), falling back to the Triton implementation.

**Technical impact**  
These modifications extend platform compatibility for non-gated MoE from CUDA-only to include ROCm, allowing models with non-gated activations to run on AMD GPUs. The AITER kernel is conditionally disabled for non-gated MoE configurations, ensuring the system uses the Triton implementation via apply_moe_activation() which properly handles these activations.

**Potential risks**  
There's a risk if the Triton implementation has performance regressions compared to AITER kernels for ROCm platforms. The conditional logic introduces complexity that could lead to maintenance overhead. Edge cases may arise if future non-gated activations have different requirements not covered by the current fallback mechanism.

**Key insights**  
Developers should verify that the Triton implementation provides adequate performance for non-gated MoE on ROCm. The changes demonstrate good platform abstraction by using is_cuda_alike() instead of is_cuda(). Testing should include various non-gated activation types to ensure comprehensive coverage beyond the tested relu2_no_mul.

---

## 2. [[Bugfix] Refactor to support DP parallel in R3](https://github.com/vllm-project/vllm/pull/32306)


### Base Information

- **PR Number:** #32306
- **Author:** [xhx1022](https://github.com/xhx1022)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-01-15 23:13:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32306/files) (3):**
  - `vllm/model_executor/layers/fused_moe/routed_experts_capturer.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The changes fix a bug introduced during data parallel (DP) support for R3 by modifying the routed experts capturer to handle DP parallelism. The key changes include using `VllmConfig` instead of `ModelConfig`, incorporating DP rank into shared memory naming, and adjusting token indexing logic to account for token distribution across DP ranks.

**Technical impact**  
This refactoring enables proper shared memory isolation between DP ranks by appending DP rank to shared memory buffer names. The capture logic now correctly handles token slicing based on DP metadata, ensuring each DP rank only accesses its assigned tokens. The changes also simplify API calls by passing `VllmConfig` instead of separate parameters.

**Potential risks**  
The DP rank-based shared memory naming could cause issues if DP rank assignments change dynamically. The token slicing logic assumes contiguous token allocation across DP ranks, which may not hold in all scheduling scenarios. There's also a risk of memory leaks if shared memory cleanup doesn't properly handle the DP rank suffix.

**Key insights**  
Developers should ensure DP rank stability throughout execution and verify token distribution assumptions. The changes demonstrate a pattern for DP-aware shared memory management that could be applied elsewhere. Consider adding validation for the contiguous token allocation assumption and implementing proper cleanup for DP-specific shared memory resources.

---

## 3. [[CI] Breakup h200 tests](https://github.com/vllm-project/vllm/pull/30499)


### Base Information

- **PR Number:** #30499
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-15 22:23:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30499/files) (5):**
  - `.buildkite/test-pipeline.yaml`
  - `tests/compile/distributed/test_fusions_e2e.py`
  - `tests/compile/fusion_test_utils.py`
  - `tests/compile/test_fusion_attn.py`
  - `vllm/env_override.py`

### Summary

**What changed and why**  
The PR addresses CI flakiness and long runtime of the "Distributed Tests (H200)" suite by splitting it into smaller, more targeted test groups. It moves some tests to cheaper H100 GPUs, adds new optional CI steps for fusion and sequence‑parallel tests on H100, and refactors test code to extract shared utilities into a common module (`fusion_test_utils.py`). This reduces the cost of retries and isolates failures.

**Technical impact**  
CI pipeline now has three new optional steps: two for Hopper fusion tests (single‑GPU and 2xH100) and one for sequence‑parallel tests on H100. The H200 suite is trimmed to a smaller set, lowering its runtime and memory pressure. Test logic is centralized in `fusion_test_utils.py`, simplifying maintenance and enabling reuse across test files. The refactor also expands E2E coverage for attention‑quant and RMSNorm‑group‑FP8 fusions in dedicated unit‑test files.

**Potential risks**  
Splitting tests across different GPU types (H100 vs. H200) could introduce environment‑specific discrepancies if tests depend on hardware capabilities. The new optional steps may be skipped in CI if resources are limited, reducing test coverage. There is a risk of missing inter‑test dependencies that were previously covered in the monolithic suite. The refactored `test_fusions_e2e.py` now focuses only on TP2 cases, which might leave some multi‑GPU scenarios untested.

**Key insights**  
This change significantly improves CI efficiency and reliability by isolating expensive tests and enabling cheaper retries. Developers should ensure that new test groups are regularly monitored for flakiness and that hardware‑specific assumptions (e.g., Blackwell‑only features) are properly guarded. Future work should optimize the remaining long‑running tests (`test_sequence_parallel.py` and `test_fusions_e2e.py`) as noted in the PR description.

---

## 4. [[Frontend][1/n] Make pooling entrypoints request schema consensus \| CompletionRequest](https://github.com/vllm-project/vllm/pull/32395)


### Base Information

- **PR Number:** #32395
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-15 22:17:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32395/files) (22):**
  - `docs/serving/openai_compatible_server.md`
  - `examples/pooling/classify/classification_online.py`
  - `examples/pooling/classify/openai_classification_client.py`
  - `examples/pooling/pooling/vision_language_pooling.py`
  - `examples/pooling/score/vision_reranker_offline.py`
  - `tests/entrypoints/pooling/classify/test_online.py`
  - `tests/entrypoints/pooling/embed/test_online.py`
  - `tests/entrypoints/pooling/pooling/test_online.py`
  - `tests/entrypoints/pooling/score/test_online_rerank.py`
  - `tests/entrypoints/pooling/score/test_utils.py`
  - `tests/models/multimodal/pooling/test_jinavl_reranker.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/pooling/__init__.py`
  - `vllm/entrypoints/pooling/base/__init__.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/pooling/score/utils.py`

### Summary

**What changed and why**  
This PR refactors pooling entrypoints to establish consistent request schemas by extracting common fields into two new mixins: `PoolingBasicRequestMixin` (containing `model`, `user`, `truncate_prompt_tokens`, `priority`, and `request_id`) and `CompletionRequestMixin` (containing `input` and `add_special_tokens`). These mixins are now used across classification, embedding, pooling, and score protocols, reducing code duplication. Additionally, the PR moves `score_utils.py` to a new `pooling/score/utils` directory, updates example files and tests, and centralizes pooling state initialization in `vllm/entrypoints/pooling/__init__.py`.

**Technical impact**  
The changes improve code maintainability and consistency by standardizing request structures across all pooling-related endpoints. The refactoring simplifies future modifications to shared fields and reduces the risk of inconsistencies. The relocation of `score_utils.py` better aligns with the project’s modular architecture, and the updated tests ensure robustness for both string and token-based inputs.

**Potential risks**  
There is a risk of breaking changes if any downstream code relies on the previous structure of request objects or the location of `score_utils.py`. The PR notes a known issue with the `add_special_tokens` parameter not functioning correctly in tests, which should be investigated. Additionally, the large number of file changes increases the chance of merge conflicts in active development branches.

**Key insights**  
Developers should adopt the new mixins for any new pooling-related endpoints to maintain consistency. The centralized `init_pooling_state` function improves initialization logic separation. All tests pass, but the `add_special_tokens` bug requires attention. Ensure that all references to the old `score_utils` path are updated to `vllm.entrypoints.pooling.score.utils`.

---

## 5. [[Bug] Add TPU backend option](https://github.com/vllm-project/vllm/pull/32438)


### Base Information

- **PR Number:** #32438
- **Author:** [vanbasten23](https://github.com/vanbasten23)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-15 21:17:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32438/files) (1):**
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`

### Summary

**What changed and why**  
Added `TPU` as a new enum value in `UnquantizedMoeBackend` and included it in the `UNSUPPORTED_BACKEND` list. The change also updates the backend detection logic in `_make_log_backend` to recognize TPU platforms. This fixes a regression where TPU backend support was broken after a previous PR.

**Technical impact**  
The TPU backend is now explicitly recognized in the fused MoE (Mixture of Experts) layer's backend enumeration and logging. However, since it's added to the unsupported backend list, TPU will not be used for actual computation—maintaining the existing fallback behavior while preventing runtime errors.

**Potential risks**  
If TPU hardware is present but the backend is unsupported, the system may fall back to a different backend (e.g., CPU or GPU), which could lead to performance degradation or unexpected behavior. There is also a risk that the logging change might incorrectly identify TPU in non-TPU environments if `current_platform.is_tpu()` has edge cases.

**Key insights**  
This is a minimal fix to restore TPU compatibility without enabling TPU execution. Developers should ensure that TPU support is fully implemented in the future if needed, and verify that the fallback backend provides acceptable performance. The change highlights the importance of maintaining platform-specific enums when adding new hardware support.

---

## 6. [[bugfix] Fix online serving crash when text type response_format is received](https://github.com/vllm-project/vllm/pull/26822)


### Base Information

- **PR Number:** #26822
- **Author:** [cjackal](https://github.com/cjackal)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-15 20:23:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/26822/files) (6):**
  - `tests/entrypoints/openai/test_chat.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/completion/protocol.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/sampling_params.py`
  - `vllm/tool_parsers/abstract_tool_parser.py`

### Summary

**What changed and why**  
This PR fixes a crash when receiving `response_format: {"type": "text"}` by adding input validation to `StructuredOutputsParams` and adjusting sampling parameter generation logic. Previously, the system incorrectly attempted to enable structured outputs for text responses, which should remain unstructured. The changes ensure that structured outputs are only enabled for JSON-related response formats.

**Technical impact**  
The modifications prevent the creation of invalid `StructuredOutputsParams` objects by adding validation in `__post_init__()` to require at least one constraint. The sampling parameter generation logic now conditionally creates structured outputs only when needed (for JSON object, JSON schema, or structural tag types), leaving text responses as regular unstructured outputs. This maintains backward compatibility while fixing the crash.

**Potential risks**  
The validation change could break existing code that creates empty `StructuredOutputsParams` objects, though this appears intentional. There's a risk of regression if other response format types are added without updating the conditional logic. The changes in `create_responses()` introduce more complex conditional logic that could have edge cases with reasoning parsers and tool servers.

**Key insights**  
Developers should note that `StructuredOutputsParams` now requires at least one constraint, which is a breaking change for any code creating empty instances. The fix properly separates structured and unstructured output handling, making the system more robust. When adding new response format types, ensure they're properly handled in both protocol files' `to_sampling_params()` methods.

---

## 7. [[Bugfix] [DeepSeek-V3.2] fix sparse_attn_indexer padding](https://github.com/vllm-project/vllm/pull/32175)


### Base Information

- **PR Number:** #32175
- **Author:** [kebe7jun](https://github.com/kebe7jun)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-15 19:21:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32175/files) (1):**
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
The fix addresses an indexing issue in the sparse attention mechanism for DeepSeek-V3.2 during decode phase. Specifically, it corrects the padding logic for `weights` and ensures `topk_indices` are correctly sliced using `num_padded_tokens` instead of `num_decode_tokens`, aligning tensor dimensions with the padded decode sequence.

**Technical impact**  
These changes ensure that the sparse attention indexer correctly handles padded decode sequences, particularly when speculative decoding is enabled. The adjustment prevents dimension mismatches and potential out-of-bounds errors in the top-k selection kernel, maintaining model accuracy and stability during inference.

**Potential risks**  
If the padding logic is inconsistent across different configurations (e.g., varying speculative token counts), it could lead to subtle indexing errors. Additionally, the reliance on `num_padded_tokens` assumes correct computation elsewhere; any miscalculation there could propagate failures.

**Key insights**  
Developers should verify that `num_padded_tokens` is always correctly derived from `decode_lens` and aligns with the padded tensor shapes. The fix highlights the importance of consistent padding handling between weights and query tensors in attention mechanisms, especially with speculative decoding.

---

## 8. [[Model] Add Step3vl 10b](https://github.com/vllm-project/vllm/pull/32329)


### Base Information

- **PR Number:** #32329
- **Author:** [ltd0924](https://github.com/ltd0924)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-15 19:04:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32329/files) (4):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/step_vl.py`

### Summary

**What changed and why**  
This PR adds support for the Step3-VL-10B vision-language model to the vLLM codebase. The changes include registering the new model class, updating documentation, and adding a new model implementation file that largely reuses existing Step3-VL components with minor adaptations.

**Technical impact**  
The addition extends vLLM's multimodal model support by integrating a new 10B parameter vision-language variant. The implementation leverages the existing `Step3VLForConditionalGeneration` base class and shares core vision processing infrastructure, minimizing architectural changes. The model is registered for inference with proper tensor parallelism and quantization support.

**Potential risks**  
The model file appears to be a substantial copy (549 lines) with potential code duplication from `step3_vl.py`, which could lead to maintenance overhead. There's a risk of inconsistent updates if shared logic diverges. The test plan only covers basic serving without comprehensive validation of vision capabilities or edge cases like varied image inputs.

**Key insights**  
Developers should verify that the new model properly inherits all necessary multimodal functionalities from its parent class. Consider refactoring to reduce code duplication between Step3-VL variants. Ensure thorough testing of vision-language interactions and tool-calling features before deployment.

---

## 9. [[Bugfix] Fix ROCm dockerfiles](https://github.com/vllm-project/vllm/pull/32447)


### Base Information

- **PR Number:** #32447
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 18:50:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32447/files) (2):**
  - `docker/Dockerfile.rocm`
  - `docker/Dockerfile.rocm_base`

### Summary

**What changed and why**  
The changes remove the RIXL (ROCm InfiniBand X-Library) build stage and associated dependencies from the base Dockerfile, while adding a simple installation step for a pre-built RIXL wheel in the main Dockerfile. This simplifies the Docker build process by eliminating complex multi-stage compilation of RIXL components.

**Technical impact**  
This reduces Docker build complexity and time by removing approximately 100 lines of build instructions for RIXL, UCX, and etcd-cpp-apiv3. The architecture now assumes RIXL wheels are built separately and mounted during the final image assembly, shifting from source compilation to binary distribution for this component.

**Potential risks**  
If the pre-built RIXL wheel is incompatible with the target system or missing required dependencies, runtime failures may occur. The removal of UCX and etcd build steps could break functionality if those libraries are still required by RIXL but not included in the wheel or base image.

**Key insights**  
This change represents a shift toward modular Docker builds where complex components are pre-built. Developers should ensure the RIXL wheel build process is properly maintained elsewhere and that the wheel includes all necessary runtime dependencies. Consider adding validation steps to verify wheel compatibility during Docker build.

---

## 10. [[ROCm][CI] Enable AITER Unified Attention On ROCm For gpt-oss Test](https://github.com/vllm-project/vllm/pull/32431)


### Base Information

- **PR Number:** #32431
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-15 16:55:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32431/files) (1):**
  - `tests/entrypoints/openai/test_serving_chat.py`

### Summary

**What changed and why**  
The PR modifies the GPT-OSS speculative test to conditionally enable the ROCm AITER unified attention backend when running on ROCm systems. This addresses a test failure where the default TRITON_ATTN backend does not currently work for GPT-OSS on ROCm, as documented in issue #32434. The changes ensure the test passes by dynamically selecting the appropriate attention backend and setting the required environment variable.

**Technical impact**  
The test now adapts to the underlying hardware: it uses TRITON_ATTN on non-ROCm systems but switches to ROCM_AITER_UNIFIED_ATTN when AITER is supported (i.e., on ROCm). This introduces a hardware-specific conditional path in the test setup, which may slightly increase test complexity but ensures compatibility across platforms.

**Potential risks**  
The conditional logic could mask underlying issues with the TRITON_ATTN backend on ROCm if not properly resolved in the future. Additionally, if `is_aiter_found_and_supported()` returns inconsistent results across different ROCm environments, it might lead to flaky test behavior. The temporary workaround requires follow-up to address the root cause.

**Key insights**  
This is a targeted fix to unblock ROCm testing for GPT-OSS, but it should be viewed as a temporary solution. Developers should prioritize resolving issue #32434 to restore TRITON_ATTN support on ROCm, after which this conditional logic can be removed. Ensure that the environment variable `VLLM_ROCM_USE_AITER` is only set when necessary to avoid unintended side effects in other tests.

---

## 11. [[CI] Fix LM Eval Large Models (H100)](https://github.com/vllm-project/vllm/pull/32423)


### Base Information

- **PR Number:** #32423
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-15 16:52:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32423/files) (1):**
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`

### Summary

**What changed and why**  
The change modifies how the `group_shape` attribute is passed to the CUDA kernel in FP8 quantization. Previously, the entire `self.group_shape` object was passed when `self.static` is True; now it explicitly unpacks the object into a tuple of `(row, col)` dimensions. This ensures compatibility with the kernel's expected data format for static quantization configurations.

**Technical impact**  
This adjustment affects only the data packaging for the CUDA kernel call in static quantization mode. The kernel likely expects a tuple of integers rather than a structured object, so this change aligns the Python interface with the underlying C++/CUDA implementation. It should resolve runtime errors or incorrect behavior when using static FP8 quantization with large models on H100 GPUs.

**Potential risks**  
If the `group_shape` object does not have `row` and `col` attributes, this will raise an `AttributeError`. Additionally, any other code paths that rely on the original `group_shape` object format may need similar updates. The change assumes the kernel handles `None` correctly for dynamic quantization (`self.static` is False).

**Key insights**  
Always verify that data structures match the expected types in kernel interfaces. Consider adding type hints or validation for `group_shape` to prevent attribute errors. Review other quantization layers for similar issues to ensure consistency across the codebase.

---

## 12. [Add thread_n=64 support to Marlin MoE](https://github.com/vllm-project/vllm/pull/32360)


### Base Information

- **PR Number:** #32360
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-15 16:45:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32360/files) (3):**
  - `csrc/moe/marlin_moe_wna16/generate_kernels.py`
  - `csrc/moe/marlin_moe_wna16/ops.cu`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py`

### Summary

**What changed and why**  
This PR adds support for MoE models with intermediate dimensions not divisible by 128 (e.g., NemotronH with N=1856) by introducing a new thread configuration `(thread_k=128, thread_n=64, num_threads=128)` to the Marlin kernels. The change resolves a runtime error that occurred when the kernel failed to find a valid thread configuration for shapes where N % 64 == 0 but N % 128 != 0.

**Technical impact**  
The update extends the kernel configuration tables in both the code generation script and the CUDA implementation to include the new `(128, 64, 128)` thread layout. Additionally, it fixes a dimension calculation in the FP8 Marlin utility to correctly derive `w13_n` from the weight tensor's shape rather than assuming `n * 2`, ensuring proper tensor repacking for non-standard intermediate sizes.

**Potential risks**  
Introducing a new thread configuration could affect performance for existing model shapes, as the kernel selection logic may now choose a suboptimal configuration. There is also a risk of regression if the `w13_n` calculation change inadvertently impacts other model architectures or quantization schemes. The dependency on another PR (#32257) for plumbing adds integration risk.

**Key insights**  
Developers should verify that the new thread configuration does not degrade performance for common model shapes. The fix to `w13_n` is critical for correctness when intermediate dimensions are not evenly divisible by 128. Ensure thorough testing across different MoE model variants to confirm both functional correctness and performance parity.

---

## 13. [[Feat] Support non-gated MoE with Marlin, NVFP4 CUTLASS, FP8, INT8, compressed-tensors](https://github.com/vllm-project/vllm/pull/32257)


### Base Information

- **PR Number:** #32257
- **Author:** [TomerBN-Nvidia](https://github.com/TomerBN-Nvidia)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-15 16:15:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32257/files) (17):**
  - `tests/kernels/moe/test_cutlass_moe.py`
  - `tests/kernels/moe/test_moe.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/fused_moe/utils.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py`

### Summary

**What changed and why**  
This PR extends support for non-gated MoE (Mixture of Experts) activations—like `relu2_no_mul`—across multiple quantization backends (Marlin, NVFP4 CUTLASS, FP8, INT8, compressed-tensors). The changes standardize shard‑count handling (`w13_num_shards = 2 if self.moe.is_act_and_mul else 1`), propagate the `is_act_and_mul` flag to relevant methods, and update activation‑dispatch logic to accommodate non‑gated variants.

**Technical impact**  
The modifications enable Nemotron‑style non‑gated MoE models to run on more backends, increasing flexibility and performance options. Key adjustments include dynamic intermediate‑cache sizing based on shard count, activation‑function routing via `apply_moe_activation`, and backend‑selection logic that now considers `is_act_and_mul`. This broadens the supported model/backend matrix but may affect kernel selection and memory usage for non‑gated configurations.

**Potential risks**  
- Some backends still lack full non‑gated support (e.g., CUTLASS FP8/FP4 for Nemotron models, as noted in the PR description).  
- The `activation` string handling now includes `_no_mul` suffixes; incorrect propagation could lead to runtime errors or silent mis‑selection of kernels.  
- Changes to workspace/cache sizing could introduce out‑of‑memory issues for specific tensor shapes if not thoroughly tested.

**Key insights**  
- Developers must ensure `is_act_and_mul` is correctly passed through the quantization‑method chain, especially when adding new backends.  
- The `apply_moe_activation` utility centralizes activation dispatch—use it consistently instead of inline callables.  
- Verify that any backend‑specific constraints (e.g., FlashInfer TRTLLM FP4 requiring `silu`) are still enforced for non‑gated activations.

---

## 14. [[Refactor] Remove unused file](https://github.com/vllm-project/vllm/pull/32422)


### Base Information

- **PR Number:** #32422
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-15 14:59:38
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32422/files) (1):**
  - `vllm/entrypoints/pooling/embed/conftest.py`

### Summary

**What changed and why**  
A single unused test configuration file (`vllm/entrypoints/pooling/embed/conftest.py`) was removed from the codebase. The change is a straightforward cleanup to eliminate dead code that is no longer needed for testing.

**Technical impact**  
This removal has no functional impact on the system's runtime behavior, as it only deletes a pytest configuration hook. It simplifies the codebase by reducing maintenance overhead and potential confusion from unused files.

**Potential risks**  
The primary risk is if this file was actually required by other tests in the `pooling/embed` directory. If those tests depend on the ROCm-specific SDP configuration it provided, they may fail or behave incorrectly on ROCm platforms after this removal.

**Key insights**  
Before merging, verify that no active tests in the associated module rely on this configuration. The removal of the ROCm workaround suggests the underlying issue (#30167) may be resolved, but this should be confirmed to ensure no regression in ROCm testing accuracy.

---

## 15. [[MoE Refactor][17/N] Apply Refactor to Bf16](https://github.com/vllm-project/vllm/pull/31827)


### Base Information

- **PR Number:** #31827
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-15 12:53:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31827/files) (12):**
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-BF16-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-BF16-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-BF16-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/config-b200.txt`
  - `tests/evals/gsm8k/configs/moe-refactor/config-h100.txt`
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`

### Summary

**What changed and why**  
This PR refactors unquantized MoE (Mixture of Experts) execution by centralizing backend selection logic into a new `unquantized.py` module. It introduces an `UnquantizedMoeBackend` enum and helper functions to select backends (Aiter/FlashInfer/Triton), convert weight layouts, and construct kernels. Additionally, it adds BF16 GSM8K evaluation configurations for three MoE models to expand test coverage.

**Technical impact**  
The refactor simplifies the `unquantized_fused_moe_method.py` by removing duplicated backend‑selection and weight‑shuffling code, promoting consistency and maintainability. The new centralized logic handles DP/EP checks, weight padding, and kernel initialization uniformly across backends. The added test configs enable performance and accuracy validation for BF16 MoE models across different hardware and parallelization strategies.

**Potential risks**  
The assertion `assert self.kernel` in `forward_cuda` assumes the kernel is always initialized for CUDA‑like platforms, which could fail if `_setup_kernel` is not called or returns `None`. The refactor changes the condition for DP/EP checks from `dp_size == 1` to `dp_size > 1`, which may affect backend selection logic. Additionally, the weight‑shuffling and swapping logic is now centralized, so any errors in `convert_to_unquantized_kernel_format` could propagate to all backends.

**Key insights**  
Developers should verify that the new backend‑selection logic correctly handles all platform and configuration combinations, especially edge cases like DP+EP. The added test configs provide a baseline for BF16 MoE evaluation, but ensure they align with existing testing frameworks. The kernel assertion should be accompanied by proper error handling or logging to aid debugging.

---

## 16. [[ROCM] DSfp4 mla projection gemms weight dynamic quantization](https://github.com/vllm-project/vllm/pull/32238)


### Base Information

- **PR Number:** #32238
- **Author:** [maleksan85](https://github.com/maleksan85)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-15 12:13:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32238/files) (4):**
  - `vllm/_aiter_ops.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/model_executor/layers/quantization/quark/utils.py`

### Summary

**What changed and why**  
This PR adds support for FP4 weight dynamic quantization in MLA projection GEMMs for ROCm. It introduces a new environment variable `VLLM_ROCM_USE_AITER_FP4BMM` to enable FP4 BMM kernels, adds corresponding checks in `rocm_aiter_ops`, and implements FP4 quantization for MLA attention weights when supported. The changes allow models like DeepSeek-R1 to use FP4-quantized weights for improved performance and memory efficiency.

**Technical impact**  
The modifications extend the existing FP8 quantization pathway to support FP4, adding a new quantization method via `quark_quantize_weight_to_mxfp4`. This introduces an additional weight representation (FP4) for MLA attention layers, potentially reducing memory bandwidth and improving inference speed on compatible ROCm hardware. The changes are backward-compatible, as FP8 remains the default when FP4 is not enabled.

**Potential risks**  
The FP4 quantization is only supported for BF16 weights, which could lead to runtime errors if applied to other dtypes. The pre-compilation step for FP8 kernels is removed in the FP4 path, which may cause first-time kernel compilation overhead during inference. Additionally, the transpose operations on weights (`self.W_K.transpose(0, 1)`) could introduce subtle bugs if the tensor dimensions are misinterpreted.

**Key insights**  
Developers should ensure that the `aiter` library includes the required `batched_gemm_a16wfp4` kernel. The FP4 optimization is conditional on both the environment flag and weight dtype, so verify that models use BF16 weights. Consider adding pre-compilation for FP4 kernels to avoid runtime latency, similar to the FP8 implementation. The removal of duplicate `process_weights_after_loading` code is a positive cleanup but requires careful testing to avoid regressions.

---

## 17. [[BugFix] Python file source reading can fail on UnicodeDecodeError](https://github.com/vllm-project/vllm/pull/32416)


### Base Information

- **PR Number:** #32416
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [luccafong](https://github.com/luccafong)
- **Merged time:** 2026-01-15 12:01:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32416/files) (1):**
  - `vllm/compilation/backends.py`

### Summary

**What changed and why**  
The change replaces a single exception handler for `OSError` with a tuple handler for both `OSError` and `UnicodeDecodeError`. This addresses a bug where reading Python source files for hashing could fail with a `UnicodeDecodeError` in addition to the previously handled `OSError`, as identified in the linked GitHub issue.

**Technical impact**  
This modification makes the file reading operation more robust by gracefully handling encoding-related failures alongside filesystem errors. The system will now log a warning and continue processing other files when encountering either type of error, preventing crashes during the code hashing process.

**Potential risks**  
The change could potentially mask legitimate encoding issues that should be addressed rather than ignored. If `UnicodeDecodeError` becomes frequent, it might indicate systematic encoding problems in source files that should be corrected. The warning log provides some visibility but doesn't differentiate between the two exception types.

**Key insights**  
This is a targeted hotfix that improves error resilience while a more comprehensive solution is developed separately. Developers should monitor the warning logs for frequency of `UnicodeDecodeError` occurrences to identify potential encoding issues in their codebase. Consider enhancing the warning message to distinguish between the two exception types for better debugging.

---

## 18. [[ROCm] [CI] [Release] Rocm wheel pipeline with sccache](https://github.com/vllm-project/vllm/pull/32264)


### Base Information

- **PR Number:** #32264
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 10:56:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32264/files) (10):**
  - `.buildkite/release-pipeline.yaml`
  - `.buildkite/scripts/annotate-rocm-release.sh`
  - `.buildkite/scripts/cache-rocm-base-wheels.sh`
  - `.buildkite/scripts/generate-nightly-index.py`
  - `.buildkite/scripts/upload-rocm-wheels.sh`
  - `docker/Dockerfile.rocm`
  - `docker/Dockerfile.rocm_base`
  - `requirements/rocm-test.txt`
  - `requirements/rocm.txt`
  - `tools/vllm-rocm/pin_rocm_dependencies.py`

### Summary

**What changed and why**  
This PR introduces a ROCm wheel release pipeline with sccache integration to drastically reduce build times. It adds a multi‑stage Buildkite pipeline that caches pre‑built ROCm base wheels in S3, uses sccache to cache compiled artifacts, and generates installable Python wheels for vLLM with ROCm support. The changes also include Dockerfile modifications to conditionally enable sccache and a new script to pin dependencies to custom‑built ROCm wheels.

**Technical impact**  
The pipeline decouples the expensive ROCm base‑wheel builds (PyTorch, Triton, etc.) from the vLLM wheel build, caching them via S3. This reduces base‑image build time from ~6 hours to ~1.5 hours when cached. The sccache integration (via wrapper scripts) accelerates compilation across multiple projects. The release process now produces versioned wheels with proper PEP 503 indices, enabling installation via `--extra‑index‑url`.

**Potential risks**  
S3 caching relies on a hash of the Dockerfile and build args; changes to untracked dependencies (e.g., ROCm version in the base image) could cause cache misses or stale wheels. The sccache wrapper approach may interfere with build systems that expect the original compiler paths. The pipeline assumes AWS credentials and permissions are correctly configured; failures in upload/download could break the build. The dependency‑pinning script must accurately map wheel filenames to package names to avoid version mismatches.

**Key insights**  
The pipeline is designed for efficiency but requires careful maintenance of cache‑key invariants. Developers should ensure any changes to ROCm version or build arguments are reflected in the cache‑key generation. The sccache integration is opt‑in (`USE_SCCACHE=1`) and should be used only in CI to avoid credential leakage. The new wheel‑release stage in the Dockerfile enables reproducible builds by pinning exact wheel versions, which is critical for release quality.

---

## 19. [[UX] Use kv_offloading_backend=native by default](https://github.com/vllm-project/vllm/pull/32421)


### Base Information

- **PR Number:** #32421
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-15 10:55:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32421/files) (4):**
  - `tests/v1/kv_connector/unit/test_config.py`
  - `vllm/config/cache.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
The changes make `kv_offloading_backend="native"` the default when KV offloading is enabled via `kv_offloading_size`. Previously, both parameters needed explicit setting; now, setting only `kv_offloading_size` automatically uses the native backend. The PR simplifies configuration by reducing required parameters and clarifying that offloading is activated solely by `kv_offloading_size`.

**Technical impact**  
The default backend is now "native" instead of `None`, and the validation logic shifts: offloading activates only when `kv_offloading_size` is set, removing the previous requirement for both parameters. This reduces configuration complexity and aligns behavior with the principle that `kv_offloading_size` is the primary toggle for offloading.

**Potential risks**  
Existing code that relied on `kv_offloading_backend=None` as a default may inadvertently enable native offloading if `kv_offloading_size` is set. The removal of validation for missing `kv_offloading_size` when a backend is specified could lead to silent misconfigurations if the backend is explicitly set without a size.

**Key insights**  
Developers should update configurations to remove explicit `kv_offloading_backend` settings unless needed, as "native" is now default. Ensure `kv_offloading_size` is intentionally set to avoid unintended offloading. The changes improve usability but require careful review of existing configurations to prevent unexpected behavior.

---

## 20. [[BugFix] Fix `assert x_s.shape[-1] == x_q.shape[-1] // group_shape[1]` in Blackwell Quantized MoE Test](https://github.com/vllm-project/vllm/pull/32362)


### Base Information

- **PR Number:** #32362
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-15 10:19:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32362/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`

### Summary

**What changed and why**  
The change replaces `x_s.ndim == 0` with `x_s.numel() == 1` to detect scalar-like tensors, and uses `reshape(1, 1)` instead of `unsqueeze(-1).unsqueeze(-1)` to normalize them to shape `(1, 1)`. This fixes a test failure in the Blackwell Quantized MoE test by handling scalar weights more robustly.

**Technical impact**  
This improves the function's ability to handle tensors with a single element but varying dimensions (e.g., shape `(1,)` or `()`), ensuring consistent shape normalization. The change maintains backward compatibility while addressing edge cases where scalar-like tensors caused assertion errors in quantization utilities.

**Potential risks**  
If `x_s` is a non-scalar tensor with `numel() == 1` (e.g., shape `(1, 1, 1)`), it will be reshaped to `(1, 1)`, which may alter broadcasting behavior in some contexts. However, this is likely intentional for consistency. The fix assumes that any single-element tensor should be treated as a scalar for scaling purposes.

**Key insights**  
Use `numel()` over `ndim` for scalar detection when the goal is to identify single-element tensors regardless of shape. The `reshape` method is more direct and efficient than chained `unsqueeze` calls. Developers should verify that downstream operations correctly handle the normalized `(1, 1)` shape for all quantization group configurations.

---

## 21. [[Attention][AMD] Make flash-attn optional](https://github.com/vllm-project/vllm/pull/30361)


### Base Information

- **PR Number:** #30361
- **Author:** [mgehre-amd](https://github.com/mgehre-amd)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 09:18:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30361/files) (1):**
  - `vllm/v1/attention/backends/fa_utils.py`

### Summary

**What changed and why**  
The change modifies the ROCm platform's handling of missing `flash-attn` by replacing an immediate import-time `ImportError` with a stub function. This defers the failure until `flash_attn_varlen_func` is actually called, allowing vLLM to run with alternative attention backends (e.g., Triton attention) when `flash-attn` is not installed.

**Technical impact**  
This enables vLLM on ROCm to start successfully without `flash-attn`, improving flexibility for users who rely on other attention backends. The CUDA/XPU code paths remain unchanged, and `get_flash_attn_version` still returns `None` for ROCm, preserving existing behavior for version checks.

**Potential risks**  
If a workflow inadvertently tries to use flash attention on ROCm without the library installed, it will fail at runtime rather than startup, which could delay error detection. The stub function may also mask other import-related issues if the error handling is too broad.

**Key insights**  
This is a pragmatic fix that aligns with the principle of failing only when necessary, enhancing user experience on ROCm. Developers should ensure that any code relying on flash attention explicitly checks for its availability or handles the deferred `ImportError` appropriately.

---

## 22. [fixing podman build issue](https://github.com/vllm-project/vllm/pull/32131)


### Base Information

- **PR Number:** #32131
- **Author:** [smitkadvani](https://github.com/smitkadvani)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-15 09:07:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32131/files) (1):**
  - `docker/Dockerfile.rocm`

### Summary

**What changed and why**  
The fix adds `ENV` declarations for `VLLM_REPO` and `VLLM_BRANCH` in the `fetch_vllm_1` stage of `Dockerfile.rocm`. This ensures these variables are available in the `ONBUILD` execution context when building with `REMOTE_VLLM=1`, resolving a Docker/Podman build failure where `git clone` could not access the repository URL.

**Technical impact**  
The change preserves the build arguments as environment variables, making them accessible to subsequent `ONBUILD` instructions. This aligns with Docker/Podman's behavior where `ARG` values do not persist into `ONBUILD` contexts, ensuring remote vLLM cloning works correctly across different build environments and formats.

**Potential risks**  
If the environment variables are inadvertently overridden in later stages, it could affect the intended vLLM repository or branch. Additionally, the fix assumes that the default values for `VLLM_REPO` and `VLLM_BRANCH` are always appropriate when `REMOTE_VLLM=1` is set without explicit overrides.

**Key insights**  
Always use `ENV` to propagate build-time arguments to `ONBUILD` instructions, as `ARG` values are not inherited. Developers should verify that any custom repository or branch arguments are correctly passed during builds and consider adding validation for required variables in multi-stage Dockerfiles.

---

## 23. [[Feature] Support async scheduling + PP](https://github.com/vllm-project/vllm/pull/32359)


### Base Information

- **PR Number:** #32359
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-15 09:06:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32359/files) (5):**
  - `tests/v1/core/utils.py`
  - `vllm/config/vllm.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/executor/ray_executor.py`

### Summary

**What changed and why**  
The changes enable async scheduling to work with pipeline parallelism (PP) by removing previous restrictions. Previously, async scheduling was incompatible with PP > 1; now it's allowed, with a temporary limitation that prevents scheduling multiple tokens from the same request while PP output placeholders exist.

**Technical impact**  
Async scheduling can now be used with PP configurations, improving throughput and TTFT as shown in benchmarks. The scheduler logic prevents interleaving tokens from the same request during PP execution, while executor concurrency limits adapt based on PP size (PP size batches when PP > 1, else 2 for async scheduling).

**Potential risks**  
The restriction on scheduling tokens from the same request while `num_output_placeholders > 0` may reduce concurrency benefits for PP workloads. Edge cases could arise if output placeholders aren't managed correctly across pipeline stages, potentially causing deadlocks or underutilization.

**Key insights**  
Async scheduling with PP shows measurable performance gains (e.g., ~3% higher throughput in benchmarks). Developers should note the pending TODO to lift the single-request token scheduling limit. Ensure thorough testing for PP > 1 with async scheduling, especially for variable request lengths and mixed workloads.

---

## 24. [[Model Runner V2] Support FlashInfer backend & Fix CUDA Graph bug [1/2]](https://github.com/vllm-project/vllm/pull/32348)


### Base Information

- **PR Number:** #32348
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-15 08:59:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32348/files) (2):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The changes add FlashInfer as a supported attention backend and fix a bug in CUDA graph handling for mixed prefill/decode workloads. Specifically, the code now allows FlashInfer alongside Flash Attention and corrects the logic that previously prevented CUDA graph usage in `FULL_DECODE_ONLY` mode when prefill requests are present.

**Technical impact**  
These modifications expand the system's compatibility with additional high-performance attention backends, enabling potential performance improvements. The CUDA graph fix ensures correct behavior for mixed prefill/decode scenarios, aligning graph usage with the intended mode (e.g., `FULL` for mixed workloads). This improves inference efficiency in diverse request patterns.

**Potential risks**  
The FlashInfer integration may introduce race conditions, as noted in the PR description regarding the attention metadata builder. Additionally, the updated CUDA graph logic assumes `mixed_mode()` correctly identifies supported modes; if this method is inaccurate or undefined, it could lead to incorrect graph selection or runtime errors.

**Key insights**  
Developers should prioritize addressing the noted race condition in FlashInfer's metadata builder to ensure stability. The CUDA graph fix is a logical improvement but requires validation that `mixed_mode()` behaves as expected across all `CUDAGraphMode` variants. Future work should extend backend support systematically, possibly via a registry pattern.

---

## 25. [[ROCm][Bugfix] Disable hip sampler to fix deepseek's accuracy issue on ROCm](https://github.com/vllm-project/vllm/pull/32413)


### Base Information

- **PR Number:** #32413
- **Author:** [ganyi1996ppo](https://github.com/ganyi1996ppo)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 08:35:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32413/files) (1):**
  - `vllm/v1/sample/ops/topk_topp_sampler.py`

### Summary

**What changed and why**  
A temporary flag `DISABLE_AITER_SAMPLER = True` was added to the HIP sampler’s `forward_hip` method to bypass the `aiter_sample` path and fall back to the native sampler. This addresses an accuracy issue on ROCm where DeepSeek models generated repetitive responses due to a bug in the HIP sampler implementation.

**Technical impact**  
The change forces all sampling on ROCm to use the native CPU-based sampler (`forward_native`) instead of the GPU-accelerated `aiter_sample` path. This ensures correct sampling behavior at the cost of potential performance degradation, as the optimized HIP sampler is disabled system-wide.

**Potential risks**  
Performance may degrade for all ROCm users due to the fallback to CPU-based sampling. The fix is a temporary workaround—the underlying HIP sampler bug remains unresolved and could affect other models or scenarios. The flag is hardcoded, making it difficult to toggle without code changes.

**Key insights**  
This is a critical stopgap fix for a severe accuracy bug, but the HIP sampler must be debugged and re-enabled promptly to restore performance. Consider making the flag configurable (e.g., via environment variable) to allow selective testing. Validate that the native sampler does not introduce regressions in other areas, such as throughput or memory usage.

---

## 26. [[ROCm][Perf] Enable shuffle kv cache layout and assembly paged attention kernel for `AiterFlashAttentionBackend`](https://github.com/vllm-project/vllm/pull/29887)


### Base Information

- **PR Number:** #29887
- **Author:** [ganyi1996ppo](https://github.com/ganyi1996ppo)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 07:29:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29887/files) (3):**
  - `vllm/_aiter_ops.py`
  - `vllm/envs.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
This PR introduces a shuffle KV cache layout and assembly paged attention kernel for the ROCm AIter backend to improve performance. It adds an environment flag `VLLM_ROCM_SHUFFLE_KV_CACHE_LAYOUT` (default false) to control the feature, enabling a ~20% throughput gain and latency reduction in benchmarks while maintaining accuracy.

**Technical impact**  
The changes extend the gather kernel to support per-head processing and the new `"SHUFFLE"` layout, alongside adding Triton kernels for writing KV cache in shuffle format with FP8 dequantization support. The assembly path is integrated into the decode flow via `aiter.pa_fwd_asm`, while the existing NHD layout remains as a fallback. This optimizes memory access patterns for ROCm hardware.

**Potential risks**  
The shuffle layout is incompatible with sliding window attention, which is now explicitly disallowed. The feature is gated behind a flag to mitigate rollout risks, but incorrect handling of FP8 dequantization scales or tensor shape mismatches could lead to silent data corruption. The per-slot scale logic in shuffle layout may have edge cases with dynamic scaling.

**Key insights**  
Developers should enable the flag only after thorough validation, as the assembly kernel is performance-critical but not yet default. Ensure sliding window attention is disabled when using shuffle layout. Monitor accuracy and performance regressions, especially for FP8 models, and verify tensor shape assumptions align with the new cache layout.

---

## 27. [[Quant] Support MXFP4 W4A16 for compressed-tensors MoE models](https://github.com/vllm-project/vllm/pull/32285)


### Base Information

- **PR Number:** #32285
- **Author:** [dsikka](https://github.com/dsikka)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-15 07:25:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32285/files) (4):**
  - `tests/evals/gsm8k/configs/Qwen3-30B-A3B-MXFP4A16.yaml`
  - `tests/evals/gsm8k/configs/models-small.txt`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`

### Summary

**What changed and why**  
Added support for MXFP4 W4A16 quantization in compressed-tensors MoE models, enabling weight-only quantization via Marlin integration. This extends existing NVFP4 support to include MXFP4 format for MoE layers, with a new method class and helper functions.

**Technical impact**  
Introduces a new `CompressedTensorsW4A4Mxfp4MoEMethod` class that parallels the NVFP4 implementation, reusing the same Marlin backend and kernel infrastructure. The changes integrate MXFP4 into the MoE quantization dispatch logic and add configuration handling for scales and packed weights.

**Potential risks**  
The implementation shares significant boilerplate with NVFP4 code, which could lead to maintenance overhead if not refactored. There is also a risk of subtle differences in scale handling or packing between MXFP4 and NVFP4 causing correctness issues. The hardcoded `group_size=32` and backend selection may limit flexibility.

**Key insights**  
Consider consolidating shared logic between MXFP4 and NVFP4 implementations to reduce duplication. Ensure thorough testing across different MoE model architectures and hardware configurations. The addition of a test configuration and successful GSM8K evaluation provides confidence in functional correctness.

---

## 28. [[Attention][MLA] Make `FLASHINFER_MLA` the default MLA backend on Blackwell, and TRTLLM the default prefill](https://github.com/vllm-project/vllm/pull/32339)


### Base Information

- **PR Number:** #32339
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-15 06:49:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32339/files) (3):**
  - `vllm/config/attention.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/platforms/cuda.py`

### Summary

**What changed and why**  
This PR changes the default MLA (Multi-Head Latent Attention) backends for Blackwell GPUs. It makes `FLASHINFER_MLA` the default decode backend (replacing `CUTLASS_MLA`) and enables `TRTLLM` as the default prefill backend. A log line is also added to indicate which prefill backend is active.

**Technical impact**  
The changes affect backend selection logic in the attention system. For Blackwell devices, the priority order for MLA backends now lists `FLASHINFER_MLA` before `CUTLASS_MLA`. The prefill backend selection order is reordered to prioritize `TRTLLM` over `FlashInfer`, and logging is upgraded from `debug_once` to `info_once` for better visibility.

**Potential risks**  
Switching default backends could introduce performance regressions or compatibility issues if the new backends are less stable or have different numerical behaviors. The removal of the `use_trtllm_ragged_deepseek_prefill` check in `use_flashinfer_prefill()` might cause unintended interactions if both backends are enabled simultaneously in future configurations.

**Key insights**  
Developers should verify that `FLASHINFER_MLA` and `TRTLLM` backends are thoroughly tested on Blackwell hardware, as they are now the defaults. The enhanced logging will help with debugging, but performance monitoring is essential to ensure the new defaults meet expectations. Consider adding fallback mechanisms if the new backends fail unexpectedly.

---

## 29. [[ROCm] Improve error handling while loading quantized model on gfx120…](https://github.com/vllm-project/vllm/pull/31715)


### Base Information

- **PR Number:** #31715
- **Author:** [brian033](https://github.com/brian033)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-15 04:16:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31715/files) (1):**
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py`

### Summary

**What changed and why**  
The PR modifies the import error handling in `quark_ocp_mx.py` to catch `RuntimeError` in addition to `ImportError` and `AttributeError`. This prevents a crash when `aiter` fails to load on unsupported GPU architectures (like gfx1201) during the import of `QuarkOCP_MX` modules, which occurs even when the user is not using Quark OCP MX quantization.

**Technical impact**  
The change ensures that quantized models (e.g., AWQ, GPTQ) can load successfully on architectures unsupported by `aiter`. By gracefully handling the `RuntimeError` and setting the quantization modules to `None`, the system avoids a hard crash and allows other quantization methods to proceed normally.

**Potential risks**  
If the `RuntimeError` is caught too broadly, it might mask other legitimate runtime issues unrelated to GPU architecture. Additionally, the warning log may be emitted frequently in unsupported environments, potentially causing noise in logs.

**Key insights**  
This fix is critical for compatibility with newer AMD GPUs and demonstrates the importance of lazy or guarded imports for hardware-dependent modules. Developers should ensure that similar import patterns elsewhere in the codebase are reviewed to prevent analogous startup crashes.

---

## 30. [[3/N] Group together media-related code](https://github.com/vllm-project/vllm/pull/32406)


### Base Information

- **PR Number:** #32406
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-15 03:52:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32406/files) (25):**
  - `tests/conftest.py`
  - `tests/entrypoints/openai/test_sparse_tensor_validation.py`
  - `tests/entrypoints/openai/test_vision.py`
  - `tests/entrypoints/pooling/embed/test_online_vision.py`
  - `tests/multimodal/media/__init__.py`
  - `tests/multimodal/media/test_audio.py`
  - `tests/multimodal/media/test_base.py`
  - `tests/multimodal/media/test_image.py`
  - `tests/multimodal/media/test_video.py`
  - `tests/multimodal/test_audio.py`
  - `tests/multimodal/test_image.py`
  - `tests/multimodal/test_video.py`
  - `tools/pre_commit/check_pickle_imports.py`
  - `vllm/multimodal/audio.py`
  - `vllm/multimodal/hasher.py`
  - `vllm/multimodal/image.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/media/__init__.py`
  - `vllm/multimodal/media/audio.py`
  - `vllm/multimodal/media/base.py`
  - `vllm/multimodal/media/image.py`
  - `vllm/multimodal/media/video.py`
  - `vllm/multimodal/parse.py`
  - `vllm/multimodal/utils.py`
  - `vllm/multimodal/video.py`

### Summary

**What changed and why**  
This PR reorganizes media-related code by moving audio, image, and video MediaIO classes from separate modules into a consolidated `vllm/multimodal/media/` directory. The changes include moving `AudioMediaIO`, `ImageMediaIO`, `VideoMediaIO`, and their embedding variants, along with corresponding test files. The purpose is to improve code organization by grouping related media handling functionality together.

**Technical impact**  
The refactoring centralizes media I/O implementations, making the codebase more maintainable and discoverable. Import paths have been updated throughout the codebase to point to the new location. The `MediaWithBytes` class received a fix to prevent recursion errors during unpickling by implementing explicit `__getstate__` and `__setstate__` methods instead of relying on `__getattr__` fallback.

**Potential risks**  
The main risk is import breakage if any external code directly imports from the old module paths (`vllm.multimodal.audio`, `vllm.multimodal.image`, `vllm.multimodal.video`). However, the PR appears to update all internal imports. There's also a risk that the pickle serialization changes to `MediaWithBytes` could affect existing serialized objects, though the fix addresses a known recursion issue.

**Key insights**  
This is part 3 of a larger refactoring effort (indicated by "[3/N]" in the title). Developers should update any custom code that imports media classes from the old locations. The pickle fix in `MediaWithBytes` resolves issue #30818 and ensures proper serialization behavior. The reorganization creates a cleaner separation between media I/O implementations and other multimodal functionality.

---

## 31. [[CI][BugFix][AMD][FP8] Fix test_rms_norm so it runs correctly on ROCm](https://github.com/vllm-project/vllm/pull/32372)


### Base Information

- **PR Number:** #32372
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 03:05:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32372/files) (1):**
  - `tests/kernels/core/test_fused_quant_layernorm.py`

### Summary

**What changed and why**  
This PR fixes test failures on AMD ROCm platforms by addressing two issues: replacing hardcoded `torch.float8_e4m3fn` with `current_platform.fp8_dtype()` to ensure correct FP8 dtype selection across platforms, and adding `torch.cuda.set_device(device)` alongside `torch.set_default_device(device)` to prevent illegal memory access by ensuring tensors and kernels execute on the same device.

**Technical impact**  
The changes improve platform compatibility by abstracting FP8 dtype selection through the `current_platform` utility, making the test suite more portable. The device synchronization fix ensures proper CUDA/ROCm execution context alignment, which is critical for multi-GPU environments and prevents runtime memory access violations.

**Potential risks**  
If `current_platform.fp8_dtype()` returns a different dtype than expected on some platforms, quantization behavior could diverge. The device setting logic assumes CUDA/ROCm availability; non-CUDA platforms might require conditional execution. There's also a risk that similar hardcoded dtype references exist elsewhere in the codebase.

**Key insights**  
Always use platform-aware utilities for hardware-specific features like FP8 dtypes. When setting default devices in CUDA/ROCm contexts, explicitly synchronize both tensor and kernel execution contexts. Consider auditing other tests for similar hardcoded dtype references to ensure consistent platform compatibility.

---

## 32. [[ROCM] Add ROCm image build to release pipeline](https://github.com/vllm-project/vllm/pull/31995)


### Base Information

- **PR Number:** #31995
- **Author:** [dllehr-amd](https://github.com/dllehr-amd)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-15 03:01:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31995/files) (3):**
  - `.buildkite/release-pipeline.yaml`
  - `.buildkite/scripts/annotate-release.sh`
  - `docker/Dockerfile.rocm`

### Summary

**What changed and why**  
Added a new Buildkite pipeline step to build and publish a ROCm-based Docker release image, alongside updates to the release annotation script and Dockerfile to support this new image variant. This enables official releases to include a version compatible with AMD ROCm hardware.

**Technical impact**  
The release pipeline now produces a third image variant (`-rocm`) alongside existing x86_64 and aarch64 images. The build uses a two-stage process: first a base ROCm image, then the final `vllm-openai` target. The annotation script has been extended to include pull, tag, and push instructions for the ROCm image, ensuring it is properly published to Docker Hub during releases.

**Potential risks**  
The new build step depends on a manual approval block (`block-rocm-release-image-build`), which could delay or halt the release if not approved. There is no explicit dependency on the existing multi-arch manifest creation step, potentially causing ordering issues. The ROCm image is not included in the final multi-arch manifest (`latest` or versioned), which may confuse users expecting a unified manifest.

**Key insights**  
Ensure the approval block is monitored to prevent release bottlenecks. Consider adding the ROCm image to the multi-arch manifest if cross-platform compatibility is desired, or clearly document that ROCm requires a separate image tag. Verify that the `cpu_queue_postmerge` agent has the necessary ROCm drivers and tooling for successful builds.

---

## 33. [[Refactor] [11/N] to simplify the mcp architecture](https://github.com/vllm-project/vllm/pull/32396)


### Base Information

- **PR Number:** #32396
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-15 02:49:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32396/files) (16):**
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`
  - `tests/entrypoints/openai/test_gptoss_structural_tags_integration.py`
  - `tests/entrypoints/openai/test_serving_responses.py`
  - `tests/entrypoints/test_context.py`
  - `tests/entrypoints/test_responses_utils.py`
  - `tests/v1/structured_output/test_gptoss_structural_tags.py`
  - `vllm/entrypoints/mcp/__init__.py`
  - `vllm/entrypoints/mcp/tool.py`
  - `vllm/entrypoints/mcp/tool_server.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/context.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/openai/responses/utils.py`
  - `vllm/reasoning/abs_reasoning_parsers.py`
  - `vllm/reasoning/gptoss_reasoning_parser.py`

### Summary

**What changed and why**  
This PR refactors the MCP (Model Context Protocol) architecture by reorganizing related modules into a dedicated `mcp` package. The changes primarily involve moving and renaming files to create a clearer separation between MCP components and other entrypoint logic, specifically relocating tool-related classes and consolidating response utilities under appropriate subdirectories.

**Technical impact**  
The refactoring improves code organization by grouping MCP-related functionality (`tool.py`, `tool_server.py`) under `vllm/entrypoints/mcp/`, while response-specific utilities and context classes are moved to `vllm/entrypoints/openai/responses/`. This enhances modularity, making the architecture more maintainable and reducing circular dependencies. Import paths across the codebase are updated accordingly.

**Potential risks**  
There is a risk of import errors in environments where the updated module paths are not fully propagated, especially in external integrations or dynamic imports. The renaming of modules could break existing code that relies on direct imports from the old locations, though the changes appear comprehensive. Additionally, any missed updates in test files or indirect dependencies could lead to test failures.

**Key insights**  
The refactoring successfully decouples MCP tools from general entrypoint logic, promoting better separation of concerns. Developers should verify that all import statements in their local environments are updated to reflect the new paths. It is also recommended to run a full test suite to ensure no regressions in tool-serving or response-handling functionality.

---

## 34. [[Benchmark] [Feature] add vllm bench sweep startup command](https://github.com/vllm-project/vllm/pull/32337)


### Base Information

- **PR Number:** #32337
- **Author:** [lengrongfu](https://github.com/lengrongfu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-15 01:25:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32337/files) (3):**
  - `docs/benchmarking/sweeps.md`
  - `vllm/benchmarks/sweep/cli.py`
  - `vllm/benchmarks/sweep/startup.py`

### Summary

**What changed and why**  
A new `vllm bench sweep startup` command has been added to benchmark vLLM startup times across different parameter combinations. This addresses issue #32214 by enabling systematic comparison of cold/warm startup performance under various engine configurations (e.g., tensor parallelism) and startup-specific settings (e.g., iteration counts).

**Technical impact**  
The change extends the existing sweep benchmarking framework with a new subcommand that integrates with `vllm bench startup`. It introduces parameter filtering logic to safely apply only supported CLI arguments, maintains result caching to avoid redundant runs, and outputs structured JSON/CSV summaries for analysis. The architecture follows patterns established by other sweep commands (serve, serve_sla).

**Potential risks**  
Parameter filtering may silently ignore unsupported keys unless `--strict-params` is used, potentially leading to misconfigured benchmarks. The dependency on external `pandas` for CSV output could cause runtime failures if not installed. Path sanitization and file I/O operations may encounter permission or filesystem issues in certain environments.

**Key insights**  
Developers should use `--strict-params` during initial testing to validate parameter compatibility. Ensure pandas is available if CSV output is required. The implementation effectively reuses existing sweep infrastructure, but reviewers should verify the parameter application logic correctly handles nested or conflicting CLI arguments between serve and startup parameters.

---

## 35. [[2/N] Move cache factories to MM registry](https://github.com/vllm-project/vllm/pull/32382)


### Base Information

- **PR Number:** #32382
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-15 01:02:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32382/files) (10):**
  - `tests/multimodal/test_cache.py`
  - `tests/v1/engine/test_process_multi_modal_uuids.py`
  - `vllm/lora/model_manager.py`
  - `vllm/multimodal/cache.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/utils.py`
  - `vllm/v1/worker/worker_base.py`

### Summary

**What changed and why**  
This PR moves cache factory functions from `vllm/multimodal/cache.py` to the `MultiModalRegistry` class in `vllm/multimodal/registry.py`. The primary purpose is to eliminate a circular dependency between `cache.py` and `registry.py`, as the cache factories previously required the registry as a parameter. This refactor centralizes cache creation logic within the registry.

**Technical impact**  
The changes consolidate cache creation logic into the `MultiModalRegistry` class, making it the single source of truth for cache instantiation. All cache factory calls are now replaced with registry method calls (`processor_cache_from_config`, `engine_receiver_cache_from_config`, etc.). This improves architectural clarity and simplifies dependency management across the multimodal subsystem.

**Potential risks**  
The refactor changes the function signatures of cache factory methods, which could break external integrations or custom extensions that directly imported these functions. Additionally, the consolidation of cache type determination logic into `_get_cache_type()` introduces a single point of failure for cache configuration validation.

**Key insights**  
Developers should update any custom code that imports cache factory functions to instead use the registry methods. The PR successfully decouples modules and improves code organization, but thorough testing of all cache configurations (processor_only, lru, shm) is essential to ensure the refactor doesn't introduce behavioral regressions in cache initialization paths.

---

## 36. [[Model] Avoid token selection in SigLIP pooling head](https://github.com/vllm-project/vllm/pull/32389)


### Base Information

- **PR Number:** #32389
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-15 01:01:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32389/files) (1):**
  - `vllm/model_executor/models/siglip.py`

### Summary

**What changed and why**  
The changes modify the SigLIP pooling head's forward method to return the full hidden state tensor instead of selecting only the first token (`hidden_state[:, 0]`). This adjustment ensures compatibility with `resolve_visual_encoder_outputs(feature_select_strategy="all")`, which expects the complete feature set rather than a single pooled token.

**Technical impact**  
This change shifts responsibility for token selection from the model's pooling head to the downstream `resolve_visual_encoder_outputs` function. It aligns the SigLIP implementation with a more flexible feature extraction strategy, allowing external components to decide how to aggregate visual features based on configuration.

**Potential risks**  
If `resolve_visual_encoder_outputs` is not correctly configured or called, the system may pass unpooled features to subsequent layers, potentially causing shape mismatches or performance degradation. Additionally, other parts of the codebase that implicitly depend on the previous single-token output may need updates.

**Key insights**  
Developers should verify that all calls to the SigLIP vision encoder use `resolve_visual_encoder_outputs` with appropriate strategies. This change promotes modularity but requires careful integration testing to ensure no downstream components break due to the altered output shape.

---

## 37. [fix: avoid crash on zero-arg tool calls in glm4 parser](https://github.com/vllm-project/vllm/pull/32321)


### Base Information

- **PR Number:** #32321
- **Author:** [seekskyworld](https://github.com/seekskyworld)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-15 00:45:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32321/files) (1):**
  - `vllm/tool_parsers/glm4_moe_tool_parser.py`

### Summary

**What changed and why**  
The change adds a null check for `tc_detail` after regex matching and handles empty argument strings in the GLM4 tool parser. This prevents crashes when parsing zero-argument tool calls where the argument group can be `None`, which previously caused a `TypeError`.

**Technical impact**  
The parser now gracefully handles malformed tool call patterns by logging warnings instead of crashing, and treats missing arguments as empty dictionaries. This improves robustness for edge cases like `<tool_call>get_current_time</tool_call>` where no arguments are provided.

**Potential risks**  
The warning logging could produce noise if malformed patterns are frequent, though this is preferable to crashes. There's a slight behavior change where previously invalid patterns would crash, but now they're silently skipped, which could mask deeper data quality issues.

**Key insights**  
Always validate regex match objects before accessing groups to avoid `AttributeError`. The pattern `if tc_args else []` elegantly handles `None` values from unmatched groups. Consider whether skipped tool calls should be tracked for monitoring purposes beyond just warnings.

---

## 38. [[Bugfix] Strengthen the check of X-data-parallel-rank in Hybrid LB mode](https://github.com/vllm-project/vllm/pull/32314)


### Base Information

- **PR Number:** #32314
- **Author:** [dtcccc](https://github.com/dtcccc)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-15 00:31:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32314/files) (7):**
  - `tests/v1/engine/test_engine_core_client.py`
  - `vllm/config/parallel.py`
  - `vllm/entrypoints/cli/serve.py`
  - `vllm/v1/engine/coordinator.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/utils.py`

### Summary

**What changed and why**  
This PR fixes a bug in hybrid load balancing mode where requests with `X-data-parallel-rank` headers could cause index-out-of-range errors. The issue occurred because the rank validation logic didn't account for hybrid/external LB modes where clients only manage local engines. The fix centralizes the `local_engines_only` property in the parallel config and updates validation to use appropriate rank bounds.

**Technical impact**  
The changes introduce a new `local_engines_only` property that cleanly encapsulates the logic for determining whether clients manage only local engines (hybrid/external LB) or all engines (internal LB). This property is then consistently used across the codebase—in input validation, engine client initialization, coordinator setup, and server launching—ensuring rank checks align with the actual managed engine set.

**Potential risks**  
If the `local_engines_only` property is incorrectly defined (e.g., missing a relevant configuration flag), it could break rank validation in other LB modes. Additionally, the changes assume `data_parallel_size_local` is always defined when `local_engines_only` is true; if not, `num_ranks` could be incorrectly computed.

**Key insights**  
Centralizing the `local_engines_only` logic improves maintainability and reduces duplication. Developers should ensure that any future LB-related configurations are reflected in this property. The fix also clarifies the architectural distinction between internal and hybrid/external LB modes, which is critical for distributed setup correctness.

---

