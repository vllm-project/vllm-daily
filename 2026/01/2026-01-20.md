# vLLM Merged PR Report

**Report Date:** 2026-01-20 PST

**Total Merged PRs:** 33

---

## 1. [[Documentation] Fix typo in `docs/design/torch_compile_multimodal.md`](https://github.com/vllm-project/vllm/pull/32741)


### Base Information

- **PR Number:** #32741
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-20 23:54:20
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32741/files) (1):**
  - `docs/design/torch_compile_multimodal.md`

### Summary

**What changed and why**  
This PR fixes a typo in the documentation file `torch_compile_multimodal.md`, changing `supports_torch_compile` to `support_torch_compile` (singular) in five instances. The change ensures consistency with the actual decorator name used in the codebase.

**Technical impact**  
The update has no functional impact on the code or system behavior—it is purely a documentation correction. It improves accuracy and clarity for developers reading the documentation, ensuring they reference the correct decorator name when implementing or debugging torch.compile support for multimodal components.

**Potential risks**  
There are minimal risks since this is a documentation-only change. The primary concern is if any external references or automated documentation tools rely on the previous phrasing, but given the context, such dependencies are unlikely.

**Key insights**  
Always verify that documentation matches the actual API names used in the code. While minor, such typos can cause confusion during development. Consider adding a linting rule or automated check to catch inconsistencies between code and documentation in the future.

---

## 2. [[Bugfix] Support HF sharded weights for Mistral3/Pixtral models](https://github.com/vllm-project/vllm/pull/32673)


### Base Information

- **PR Number:** #32673
- **Author:** [ricky-chaoju](https://github.com/ricky-chaoju)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-20 23:27:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32673/files) (1):**
  - `vllm/model_executor/models/pixtral.py`

### Summary

**What changed and why**  
The changes fix weight loading for Mistral3/Pixtral models when using HuggingFace sharded weights (without consolidated.safetensors). Specifically, they adapt weight name handling to support HuggingFace's naming conventions by stripping the `language_model.` prefix and adding support for alternative prefixes like `vision_tower` and `multi_modal_projector`.

**Technical impact**  
These modifications make the weight loading process more robust and compatible with HuggingFace's sharded weight format. The changes ensure that vision encoder, adapter, and language model weights are correctly identified and loaded, preventing failures when the consolidated weight file is absent. The use of `.get()` for dictionary lookups also improves error tolerance.

**Potential risks**  
If weight names in sharded files deviate unexpectedly from the expected prefixes, some weights might not be loaded, potentially causing model performance issues. Additionally, the prefix-stripping logic assumes a consistent naming structure; malformed weight names could lead to silent failures or incorrect parameter assignments.

**Key insights**  
Developers should verify that all expected model components are correctly initialized after loading sharded weights. The changes enhance compatibility but rely on HuggingFace's naming conventions, so any future updates to those conventions may require adjustments. Testing with various sharded weight configurations is recommended to ensure robustness.

---

## 3. [[Docs] Fix GitHub handle in governance process](https://github.com/vllm-project/vllm/pull/32582)


### Base Information

- **PR Number:** #32582
- **Author:** [pacoxu](https://github.com/pacoxu)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-20 23:07:51
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32582/files) (1):**
  - `docs/governance/process.md`

### Summary

**What changed and why**  
Fixed a typo in the GitHub handle `@robertshaw2-redhat` to `@robertgshaw2-redhat` in two sections of the governance documentation. This ensures accurate attribution and proper tagging of the contributor in project communications.

**Technical impact**  
This change has no impact on code functionality or system behavior, as it only updates documentation. It improves the accuracy of maintainer references, which aids in clear project governance and communication.

**Potential risks**  
Minimal risk exists since this is purely a documentation fix. The only concern is if the incorrect handle was referenced elsewhere in the codebase or documentation, which could lead to inconsistent tagging.

**Key insights**  
Always verify GitHub handles in documentation to ensure proper attribution and tagging. Consider running a quick search for the old handle to catch any other instances that may need correction.

---

## 4. [[Bugfix] Fix Nemotron-Nano-v2-vlm static resolution](https://github.com/vllm-project/vllm/pull/32682)


### Base Information

- **PR Number:** #32682
- **Author:** [netanel-haber](https://github.com/netanel-haber)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-20 22:28:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32682/files) (1):**
  - `vllm/model_executor/models/nano_nemotron_vl.py`

### Summary

**What changed and why**  
The fix corrects a parameter name mismatch when constructing `NanoNemotronVLImagePixelInputs`. The constructor expects `num_patches`, but the calling code was passing `image_num_patches` from the kwargs dictionary. This regression was introduced in a previous PR (#32121).

**Technical impact**  
This change ensures the image pixel inputs are correctly initialized with the proper patch count parameter. Without this fix, the model would likely fail during initialization or runtime when processing image inputs, as a required parameter would be missing or incorrectly named.

**Potential risks**  
The use of `kwargs.pop()` modifies the original kwargs dictionary, which could affect downstream code if other parts of the function rely on `image_num_patches` after this call. However, since this appears to be the final use of kwargs in this return path, the risk is minimal.

**Key insights**  
Always verify parameter names when refactoring or integrating components. The fix is surgical and correct, but developers should be cautious when using `pop()` on shared dictionaries to avoid unintended side effects in the control flow.

---

## 5. [[FlashMLA] Update FlashMLA](https://github.com/vllm-project/vllm/pull/32491)


### Base Information

- **PR Number:** #32491
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-01-20 21:03:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32491/files) (4):**
  - `cmake/external_projects/flashmla.cmake`
  - `tests/kernels/attention/test_flashmla_sparse.py`
  - `tests/v1/attention/test_sparse_mla_backends.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`

### Summary

**What changed and why**  
Updated FlashMLA dependency to a newer commit and restructured kernel source file inclusion to support expanded functionality. Modified test configurations to align with kernel requirements (e.g., adjusting head counts) and added SM100-specific scale simulation for FP8 quantization. Enhanced the FlashMLA sparse backend to handle kernel padding requirements and updated metadata calculations.

**Technical impact**  
The changes enable support for new FlashMLA kernels (including SM100 sparse decode/prefill and expanded SM90 kernels), improve compatibility with different GPU architectures, and ensure correct quantization behavior for Blackwell GPUs. The backend now dynamically pads head counts to meet kernel constraints (64/128 for FP8 decode, 64/128 for BF16 prefill), which may affect memory usage and performance.

**Potential risks**  
Padding heads increases memory consumption and may introduce overhead. The SM100 scale simulation (e8m0 truncation) could affect numerical accuracy if not consistently applied. Kernel compatibility relies on specific head-count multiples, which may break for unconventional model configurations. Changes to test tolerances might mask regression issues.

**Key insights**  
Developers must ensure model head counts align with kernel constraints (64 or 128 for FP8 decode). The SM100 e8m0 scale conversion is now simulated in tests; verify this matches hardware behavior. Monitor memory usage due to padding, and validate that all new kernel files are correctly compiled for target architectures.

---

## 6. [Added qwen3 vision language moe support for speculative decoding](https://github.com/vllm-project/vllm/pull/32048)


### Base Information

- **PR Number:** #32048
- **Author:** [shanjiaz](https://github.com/shanjiaz)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-01-20 19:24:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32048/files) (2):**
  - `vllm/model_executor/models/qwen3_vl_moe.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
Added support for Qwen3 vision language MoE (Mixture of Experts) models in speculative decoding. The changes enable the draft model to properly handle auxiliary hidden states from MoE layers and correctly identify Qwen3VLMoeForConditionalGeneration model type.

**Technical impact**  
The modifications allow speculative decoding to work with Qwen3 vision language MoE models by: 1) collecting auxiliary hidden states from specific layers in the MoE transformer, and 2) ensuring proper position handling when draft and target models have different M-RoPE configurations. This maintains compatibility between text-only draft models and multimodal target models.

**Potential risks**  
The auxiliary hidden state collection (`aux_hidden_states.append(hidden_states + residual)`) occurs before layer processing, which may not capture the correct intermediate states needed for all MoE architectures. There's also a risk of position dimension mismatch when converting M-RoPE positions for models with different configurations.

**Key insights**  
The implementation correctly separates draft and target model configurations for M-RoPE usage, which is crucial when draft models are text-only. Developers should verify that the auxiliary hidden state collection points align with the specific MoE layer architecture requirements. Consider adding validation for the `aux_hidden_state_layers` configuration to ensure proper state extraction.

---

## 7. [Enable Eagle3 speculative decoding for Pixtral (LlavaForConditionalGeneration)](https://github.com/vllm-project/vllm/pull/32542)


### Base Information

- **PR Number:** #32542
- **Author:** [gopalsarda](https://github.com/gopalsarda)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-20 19:18:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32542/files) (1):**
  - `vllm/model_executor/models/llava.py`

### Summary

**What changed and why**  
The PR adds Eagle3 speculative decoding support to the Pixtral model (LlavaForConditionalGeneration) by implementing the `SupportsEagle3` interface. This enables the model to work with speculative decoding using a locally trained speculator model, which was tested with reasonable acceptance rates.

**Technical impact**  
The LlavaForConditionalGeneration class now implements the SupportsEagle3 interface, adding two required methods: `set_aux_hidden_state_layers()` and `get_eagle3_aux_hidden_state_layers()`. This integrates Pixtral into vLLM's speculative decoding framework without modifying the core model architecture.

**Potential risks**  
The hardcoded layer selection `(2, num_layers // 2, num_layers - 3)` may not be optimal for all Pixtral variants or configurations. There's also a risk that the auxiliary layers might interfere with the multimodal aspects of Llava if not properly tested with image inputs during speculative decoding.

**Key insights**  
The implementation follows vLLM's established pattern for Eagle3 support, making Pixtral compatible with speculative decoding. Developers should verify that the chosen auxiliary layers work well across different Pixtral model sizes and multimodal scenarios. Consider making the layer selection configurable if performance varies significantly.

---

## 8. [[Cleanup] Remove unused `KVConnectorModelRunnerMixin` methods](https://github.com/vllm-project/vllm/pull/32077)


### Base Information

- **PR Number:** #32077
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-20 19:16:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32077/files) (2):**
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/v1/worker/kv_connector_model_runner_mixin.py`

### Summary

**What changed and why**  
This PR removes four unused methods from `KVConnectorModelRunnerMixin` (`maybe_setup_kv_connector`, `maybe_wait_for_kv_save`, `get_finished_kv_transfers`, and `ensure_kv_transfer_shutdown`) and updates a test file to reflect the removal. The changes eliminate dead code resulting from a prior refactoring, simplifying the codebase.

**Technical impact**  
The removal reduces code complexity and maintenance burden by eliminating unused APIs. The test updates ensure continued correctness by referencing the new implementation details (`_get_kv_connector_output`) instead of the removed mixin methods. The system's functional behavior remains unchanged as these methods were redundant.

**Potential risks**  
If any external code or downstream dependencies still reference the removed methods, it could cause runtime errors. The test changes rely on internal implementation details (`_get_kv_connector_output`), which could break if that method's logic changes in the future.

**Key insights**  
This cleanup improves code hygiene by removing technical debt. Developers should verify no other modules import or call the deleted methods. The test updates, while necessary, introduce a dependency on a non-public method; consider whether the test should be refactored to avoid this coupling.

---

## 9. [[Bugfix] Fix Granite Vision / Don't use Siglip Pooling Head Nested Models by Default](https://github.com/vllm-project/vllm/pull/32299)


### Base Information

- **PR Number:** #32299
- **Author:** [alex-jw-brooks](https://github.com/alex-jw-brooks)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-20 19:11:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32299/files) (5):**
  - `tests/models/multimodal/generation/test_common.py`
  - `tests/models/multimodal/generation/vlm_utils/model_utils.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/vision.py`

### Summary

**What changed and why**  
This PR fixes a bug in Granite Vision models where the Siglip visual encoder's pooling head receives a list instead of a tensor when multiple feature layers are selected, causing an AttributeError. The solution adds a `last_hs_proc` callback to apply post-layer normalization and pooling head only to the last hidden state before feature selection, and disables the pooling head by default for composite models while keeping it available for standalone Siglip usage.

**Technical impact**  
The changes modify the visual encoder output resolution logic to support preprocessing the last hidden state before feature selection. This ensures Siglip's pooling head only processes tensors, not lists, when multiple layers are used. The pooling head is now conditionally loaded based on a new `use_head` parameter, reducing memory usage for models that don't need it. The architecture now better aligns with Hugging Face's Siglip implementation where composite models typically use hidden states, not pooled outputs.

**Potential risks**  
If any existing composite models unexpectedly rely on Siglip's pooled outputs (despite the PR's investigation suggesting none do), they could break or produce different embeddings. The conditional weight loading could cause issues if model checkpoints have inconsistent naming for head weights. The `last_hs_proc` callback adds complexity to the visual encoder output flow, which might affect other vision models if not carefully handled.

**Key insights**  
The fix correctly addresses the root cause by processing the last hidden state before feature selection. Developers should verify that no downstream models depend on Siglip pooled outputs. The new `use_head` parameter provides explicit control over pooling head usage, improving modularity. The added Granite Vision test helps prevent regression, and similar patterns should be considered for other vision encoders with optional pooling heads.

---

## 10. [OffloadingConnector: Prevent redundant loads](https://github.com/vllm-project/vllm/pull/29087)


### Base Information

- **PR Number:** #29087
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-20 17:15:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29087/files) (5):**
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/v1/kv_offload/abstract.py`
  - `vllm/v1/kv_offload/arc_manager.py`
  - `vllm/v1/kv_offload/lru_manager.py`

### Summary

**What changed and why**  
The changes prevent redundant CPU→GPU transfers when multiple concurrent requests share the same prefix and GPU prefix caching is enabled. The `OffloadingConnector` now tracks blocks being loaded (`_blocks_being_loaded`) and defers scheduling for requests whose required blocks are already in transit. The `OffloadingManager.lookup` API is extended to return `int \| None`, allowing delayed lookups.

**Technical impact**  
This modifies the scheduler's behavior by introducing a non-blocking delay mechanism: `get_num_new_matched_tokens` can now return `None` to indicate deferred processing. The change reduces GPU memory waste from duplicate KV cache entries and improves efficiency under concurrent prefix-sharing workloads. All offloading manager implementations (ARC/LRU) are updated to match the new signature, though their core logic remains unchanged.

**Potential risks**  
If the tracking set `_blocks_being_loaded` is not properly cleared (e.g., due to edge cases in load completion), it could cause indefinite request delays or resource leaks. The deferred scheduling logic adds complexity to request lifecycle management, which may affect latency predictability. The test harness modification (extra step after EOS) could mask timing issues in real-world scenarios.

**Key insights**  
Developers should ensure that `_blocks_being_loaded` is consistently updated in both `update_state_after_alloc` and `update_connector_output`. The new `None` return path requires careful integration with the scheduler's retry mechanism. When extending offloading managers, implementers must now handle the optional return type, though existing managers continue returning integer counts.

---

## 11. [Revert "[PluggableLayer][1/N] Define PluggableLayer"](https://github.com/vllm-project/vllm/pull/32725)


### Base Information

- **PR Number:** #32725
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-20 16:21:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32725/files) (4):**
  - `docs/design/custom_op.md`
  - `tests/model_executor/test_enabled_custom_ops.py`
  - `vllm/model_executor/custom_op.py`
  - `vllm/model_executor/layers/mla.py`

### Summary

**What changed and why**  
This PR reverts the PluggableLayer abstraction introduced in PR #32331, removing the separate `PluggableLayer` base class and folding its functionality back into `CustomOp`. The changes also update the MLA layer to inherit from `CustomOp` instead of `PluggableLayer` and adjust registry access to use class-level dictionaries.

**Technical impact**  
The codebase simplifies by eliminating a parallel abstraction layer, reducing maintenance overhead and potential confusion between `PluggableLayer` and `CustomOp`. The MLA layer now uses `CustomOp`’s registration and dispatch mechanism, though it currently bypasses the enable/disable logic by implementing only `forward_native` and `forward_cuda` (which delegates to native). Registry dictionaries are moved into `CustomOp` as class attributes, centralizing management.

**Potential risks**  
If out‑of‑tree (OOT) plugins were relying on the distinct `PluggableLayer` interface or its separate registry, they may break. The MLA layer’s temporary workaround (ignoring enable/disable) could lead to inconsistent behavior if future platforms add optimized implementations. There’s also a risk of regression if any code depended on the removed `op_registry` global variable.

**Key insights**  
Developers should update any OOT plugins that used `PluggableLayer` to now use `CustomOp`. The MLA layer requires a follow‑up to properly integrate with CustomOp’s enable/disable mechanism. Ensure all registry lookups use `CustomOp.op_registry` instead of the previous global `op_registry`. This consolidation improves clarity but demands careful testing of OOT replacements.

---

## 12. [fp8 online quant: split out Fp8OnlineLinearMethod](https://github.com/vllm-project/vllm/pull/32189)


### Base Information

- **PR Number:** #32189
- **Author:** [vkuzo](https://github.com/vkuzo)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-20 15:13:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32189/files) (2):**
  - `tests/quantization/test_fp8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`

### Summary

**What changed and why**  
This PR splits `Fp8LinearMethod` into separate classes for online and offline quantization. `Fp8OnlineLinearMethod` is introduced to handle on-the-fly quantization of fp16/bf16 weights during loading, while `Fp8LinearMethod` now focuses solely on pre-serialized FP8 checkpoints. The change aligns with a similar split previously done for MoE layers and clarifies the separation of concerns between online and offline quantization paths.

**Technical impact**  
The quantization logic is now cleaner and more maintainable, with distinct classes for each quantization mode. The `Fp8Config.get_quant_method` method now selects the appropriate class based on `is_checkpoint_fp8_serialized`. This improves code readability and reduces conditional complexity within the classes. The test suite is also expanded to include online quantization for both dense and MoE models, with added inference validation.

**Potential risks**  
The refactoring moves significant logic between classes, which could introduce subtle bugs if the weight-loading or scale-initialization paths are not perfectly aligned. The patched weight loader in `Fp8OnlineLinearMethod` relies on tracking loaded elements, which may be error-prone if the loading process changes. Additionally, the test expansion only covers smoke tests; edge cases like large models or varied block sizes may need further validation.

**Key insights**  
Developers should note that `Fp8LinearMethod` now exclusively handles serialized FP8 checkpoints, while `Fp8OnlineLinearMethod` manages online quantization. Ensure that any future modifications to weight loading or quantization respect this separation. The enhanced tests provide a good baseline, but consider adding more comprehensive tests for different configurations and failure modes to ensure robustness.

---

## 13. [[ROCm][CI] Remove DS async eplb accuracy test from AMD CI](https://github.com/vllm-project/vllm/pull/32717)


### Base Information

- **PR Number:** #32717
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-20 13:40:49
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32717/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The PR removes the "DeepSeek V2-Lite Async EPLB Accuracy" test step from the AMD CI pipeline configuration. This change is necessary because the underlying test script was already deleted in a previous PR (#30431), making the CI step a no-op that fails with a "No such file or directory" error.

**Technical impact**  
This is a cleanup change that removes a broken CI step, eliminating the error and streamlining the AMD CI pipeline. The removal does not affect test coverage since the test was already non-functional and its logic was previously removed from the codebase.

**Potential risks**  
The primary risk is minimal—if the test script was accidentally removed or needs to be restored, the CI configuration would need to be updated again. There is also a slight risk that other dependent configurations or references to this test might exist elsewhere, but the description indicates the script was already removed.

**Key insights**  
This is a straightforward maintenance update. Developers should verify that no other CI configurations or documentation references this test. When removing CI steps, it's good practice to ensure all associated artifacts (scripts, configurations) are cleaned up to prevent future confusion.

---

## 14. [[Bugfix] Suppress log on non-ROCm platform](https://github.com/vllm-project/vllm/pull/32703)


### Base Information

- **PR Number:** #32703
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-20 13:38:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32703/files) (1):**
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py`

### Summary

**What changed and why**  
The change adds a conditional check (`if current_platform.is_rocm():`) around a warning log statement in a try-except block. This suppresses the log message on non-ROCm platforms (e.g., CUDA) while preserving the existing error handling behavior. The purpose is to avoid unnecessary log noise when the `QuarkOCP_MX` quantization module is unavailable on non-ROCm systems.

**Technical impact**  
This is a minimal, localized change that only affects logging behavior. The underlying fallback logic (setting `dynamic_mxfp4_quant` and `gemm_afp4wfp4` to `None`) remains unchanged, ensuring functional consistency across platforms. The import remains a no-op on unsupported platforms without raising errors.

**Potential risks**  
If the `is_rocm()` check incorrectly identifies the platform, warnings could be suppressed on ROCm systems where they are needed, potentially hiding legitimate compatibility issues. Additionally, developers on non-ROCm platforms may miss the warning if they later switch to ROCm without re-evaluating dependencies.

**Key insights**  
The change is safe and focused, but ensure `is_rocm()` is robust across all deployment environments. Consider adding a debug-level log or documentation note to clarify platform-specific requirements for `QuarkOCP_MX`. This pattern of platform-aware logging can be useful for other cross-platform features.

---

## 15. [[Misc] Remove pad_for_cudagraphs from config](https://github.com/vllm-project/vllm/pull/30143)


### Base Information

- **PR Number:** #30143
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-20 12:05:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30143/files) (9):**
  - `tests/compile/test_config.py`
  - `tests/models/language/generation/test_hybrid.py`
  - `tests/v1/cudagraph/test_cudagraph_dispatch.py`
  - `vllm/compilation/piecewise_backend.py`
  - `vllm/compilation/sequence_parallelism.py`
  - `vllm/config/compilation.py`
  - `vllm/v1/cudagraph_dispatcher.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The PR removes `pad_for_cudagraphs` from the config and centralizes cudagraph padding logic in `CudagraphDispatcher`. This change ensures padding is handled consistently through the dispatcher, making debugging easier and enabling support for separate sizes for FULL and PIECEWISE graphs.

**Technical impact**  
The `bs_to_padded_graph_size` mapping and padding validation are moved from `CompilationConfig` to `CudagraphDispatcher`. This decouples padding logic from configuration, allowing the dispatcher to manage padding dynamically based on the active cudagraph mode. Tests are updated to use the dispatcher, and eagle spec decode now integrates with the dispatcher for PIECEWISE-only graphs.

**Potential risks**  
If `initialize_cudagraph_keys` is not called before dispatching, it may lead to uninitialized state or incorrect padding. The validation of `compile_sizes` now depends on the dispatcher’s initialization, which could cause silent failures if validation is skipped. Changes to eagle’s padding logic may affect performance or correctness in speculative decoding scenarios.

**Key insights**  
Developers must ensure `initialize_cudagraph_keys` is called with the appropriate cudagraph mode before using the dispatcher. The dispatcher now serves as the single source of truth for padding, simplifying maintenance. Pay close attention to eagle’s integration, as it only supports PIECEWISE mode and requires proper initialization after `adjust_cudagraph_sizes_for_spec_decode`.

---

## 16. [[Bugfix] Fix byte fallback handling when using outlines](https://github.com/vllm-project/vllm/pull/31391)


### Base Information

- **PR Number:** #31391
- **Author:** [Alnusjaponica](https://github.com/Alnusjaponica)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-20 11:48:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31391/files) (2):**
  - `vllm/v1/structured_output/backend_outlines.py`
  - `vllm/v1/structured_output/utils.py`

### Summary

**What changed and why**  
This PR fixes a bug where byte fallback tokens (used for unknown characters) were incorrectly handled when constructing a reduced vocabulary for outlines-guided decoding. The issue occurred when processing strings with non-vocabulary characters, causing the server to crash. The fix adjusts conditional logic to properly distinguish legitimate byte tokens from invalid UTF-8 replacement characters.

**Technical impact**  
The changes ensure that byte tokens (e.g., Llama's `<0xXX>` format or GPT-2 byte mappings) are correctly mapped to their token IDs during vocabulary reduction. This prevents failures in outline state transitions during guided decoding, allowing the system to process inputs with out-of-vocabulary characters without crashing.

**Potential risks**  
The updated condition may still have edge cases with complex tokenizer byte-fallback patterns, especially for custom or less common tokenizers. Additionally, the comment added in `backend_outlines.py` highlights that dead states in the FSM could still cause `advance()` to fail, though this is not addressed by the current changes.

**Key insights**  
Developers should verify that the fix works with various tokenizers and byte-fallback mechanisms. The clarification about dead states in `accept_tokens` is important for future debugging but does not change existing behavior. Consider adding more comprehensive tests for byte-fallback scenarios across different tokenizer implementations.

---

## 17. [[AOT compilation] support torch.compile inductor artifacts in VllmCompiledFunction](https://github.com/vllm-project/vllm/pull/25205)


### Base Information

- **PR Number:** #25205
- **Author:** [dolpm](https://github.com/dolpm)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-20 11:45:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/25205/files) (8):**
  - `tests/compile/test_aot_compile.py`
  - `tools/pre_commit/check_pickle_imports.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/caching.py`
  - `vllm/compilation/compiler_interface.py`
  - `vllm/compilation/decorators.py`
  - `vllm/compilation/piecewise_backend.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
This PR adds support for inductor-compiled artifacts in `VllmCompiledFunction` to unify pre-compiled artifacts under `VllmSerializableFunction`. The changes enable serialization and deserialization of inductor artifacts, allowing cached compiled subgraphs to be reused without recompilation or re-splitting, improving cold-start performance.

**Technical impact**  
The implementation introduces a deduplicated cache (`StandaloneCompiledArtifacts`) for compiled bytecode, bulk-loads artifacts during deserialization, and reconstructs the split graph module from cached runnables. It extends the piecewise backend to support both compilation and precompiled modes, and adds environment flags (`VLLM_USE_MEGA_AOT_ARTIFACT`) to control the behavior. The changes affect serialization paths, backend initialization, and CUDA graph wrapping logic.

**Potential risks**  
- Version dependency: AOT compilation requires PyTorch 2.10+, with fallbacks that may cause silent performance degradation.  
- Cache consistency: Ensuring `returns_tuple` logic is correctly applied during deserialization is critical to avoid output type mismatches (e.g., tensor vs. tuple).  
- Memory overhead: The deduplication cache may increase memory usage if many unique artifacts are stored, though deduplication mitigates this.  
- Edge cases: Symbolic shape handling and CUDA graph wrapping could introduce regressions in dynamic shape scenarios or multi-GPU environments.

**Key insights**  
- The deduplication mechanism efficiently reduces storage by sharing identical compiled artifacts across submodules.  
- Developers must ensure `VLLM_USE_MEGA_AOT_ARTIFACT` is paired with `VLLM_USE_STANDALONE_COMPILE` to avoid runtime errors.  
- Testing should focus on output type consistency (single tensor vs. tuple) and shape environment preservation across serialization cycles.  
- The changes are backward-compatible but require careful validation of inductor artifact loading in production workflows.

---

## 18. [[5/N] Initialize MM components in context managers (Q-Z)](https://github.com/vllm-project/vllm/pull/32695)


### Base Information

- **PR Number:** #32695
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-20 11:10:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32695/files) (9):**
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/radio.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/skyworkr1v.py`
  - `vllm/model_executor/models/tarsier.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/voxtral.py`

### Summary

**What changed and why**  
This PR refactors multimodal model initialization by wrapping component creation in context managers (`_mark_tower_model` and `_mark_language_model`). The changes are part of a larger effort (#32631) to enable selective loading of multimodal components (e.g., vision/audio towers) separate from language models, improving flexibility and resource usage.

**Technical impact**  
The refactoring centralizes component initialization within context managers, which likely manage registration or tagging for later selective loading. It also removes redundant `get_language_model()` methods and adds defensive checks (e.g., `getattr(self, "deepstack_input_embeds", None)`) to handle cases where towers may be skipped. This promotes a consistent pattern across all multimodal models.

**Potential risks**  
If the context managers are not properly implemented or tested, component initialization could fail silently. The defensive checks might hide initialization errors if attributes are missing due to misconfiguration. Additionally, the removal of `assert` statements (e.g., in Radio and Tarsier) could reduce error clarity during development.

**Key insights**  
Developers should verify that the context managers correctly handle all multimodal modalities and edge cases. The defensive pattern should be extended to other optional components to ensure robustness. Future work should include testing the selective loading feature to confirm that skipped components do not impact functionality.

---

## 19. [Test: added acceptance length tests](https://github.com/vllm-project/vllm/pull/32030)


### Base Information

- **PR Number:** #32030
- **Author:** [rahul-tuli](https://github.com/rahul-tuli)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-20 10:55:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32030/files) (2):**
  - `tests/v1/spec_decode/__init__.py`
  - `tests/v1/spec_decode/test_acceptance_length.py`

### Summary

**What changed and why**  
Adds a new parameterized pytest file to test EAGLE3 speculative decoding acceptance lengths. The tests run inference on the MT-Bench dataset with three model pairs, extracting acceptance metrics to ensure they stay within 2% tolerance of established baselines. This guards against performance regressions in speculative decoding.

**Technical impact**  
Introduces a regression testing framework for speculative decoding performance. The tests are parameterized across model configurations, tensor-parallelism sizes, and attention backends, making it easy to extend. They rely on the existing `llm.get_metrics()` interface and the MT-Bench dataset, integrating with the existing vLLM testing infrastructure.

**Potential risks**  
The tests depend on specific model versions and dataset samples, which could change over time and cause flaky failures. The 2% tolerance may be too strict for some hardware or backend combinations, leading to false positives. Additionally, the GPU memory requirement (40 GB) limits which machines can run these tests.

**Key insights**  
These tests are critical for maintaining speculative decoding performance but should be monitored for stability. Consider adding tolerance adjustments for different backends or hardware. The parameterized design is excellent for scalability—new model pairs can be added by extending the `EAGLE3_MODEL_CONFIGS` list.

---

## 20. [[Doc] Update docs for MM model development with context usage](https://github.com/vllm-project/vllm/pull/32691)


### Base Information

- **PR Number:** #32691
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-20 10:37:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32691/files) (2):**
  - `docs/contributing/model/multimodal.md`
  - `examples/online_serving/disaggregated_encoder/README.md`

### Summary

**What changed and why**  
Updated documentation for multimodal model development to clarify proper initialization patterns and simplify the interface requirements. The changes remove the requirement to implement `get_language_model()` and update guidance on using `_mark_language_model` and `_mark_tower_model` contexts during initialization.

**Technical impact**  
These documentation changes reflect architectural simplifications in the multimodal interface. The removal of `get_language_model()` requirement reduces boilerplate code for model implementers, while the clarified initialization pattern ensures proper component registration within vLLM's multimodal framework.

**Potential risks**  
The documentation simplification might obscure edge cases where custom merging logic is needed in `embed_input_ids`. Developers might misinterpret the optional nature of `--mm-encoder-only` flag, potentially leading to incorrect initialization in encoder-only deployments.

**Key insights**  
The updated patterns promote cleaner separation between language and multimodal components. Developers should note that `embed_input_ids` merging logic remains customizable when needed. The encoder-only flag clarification emphasizes it's a performance optimization rather than a requirement.

---

## 21. [[Model Runner V2] Support FLASHINFER_MLA backend](https://github.com/vllm-project/vllm/pull/32709)


### Base Information

- **PR Number:** #32709
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-20 10:26:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32709/files) (2):**
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The changes add support for the new `FLASHINFER_MLA` attention backend by updating two conditionals. The first change replaces a substring check (`"FLASHINFER" in attn_backend.get_name()`) with an exact match (`attn_backend.get_name() == "FLASHINFER"`), while the second adds `"FLASHINFER_MLA"` to the list of supported backends for KV cache initialization.

**Technical impact**  
These modifications enable the system to recognize and properly handle the `FLASHINFER_MLA` backend, allowing it to be used alongside existing backends like `FLASH_ATTN` and `FLASHINFER`. The stricter equality check ensures that only the exact `FLASHINFER` backend triggers the flashinfer workspace logic, preventing potential false positives with the new `FLASHINFER_MLA` variant.

**Potential risks**  
If `FLASHINFER_MLA` requires similar workspace buffer handling as `FLASHINFER`, it may be incorrectly excluded from the flashinfer workspace initialization due to the exact match condition. Additionally, the TODO comment about supporting other backends remains, which could cause confusion about whether `FLASHINFER_MLA` is fully supported or still experimental.

**Key insights**  
Developers should verify whether `FLASHINFER_MLA` needs the same workspace buffer setup as `FLASHINFER` and update the condition accordingly if required. Consider refactoring the backend name checks to use a set or enum for better maintainability as more backends are added. The TODO comment should be updated to reflect current support status.

---

## 22. [[Bugfix] fix the ima issue of qwen-vit](https://github.com/vllm-project/vllm/pull/32687)


### Base Information

- **PR Number:** #32687
- **Author:** [JJJYmmm](https://github.com/JJJYmmm)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-20 09:21:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32687/files) (2):**
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`

### Summary

**What changed and why**  
This PR fixes an illegal memory access (IMA) error in Qwen-ViT models when batch size equals 1. The issue occurs because `qk_reshaped` becomes non-contiguous after `einops.rearrange` for `bs=1`, causing the downstream Triton rotary embedding kernel to fail. The fix adds `.contiguous()` to ensure memory layout compatibility. Additionally, the default `_MAX_FRAMES_PER_VIDEO` is corrected from 24576 to 2048 per official recommendations.

**Technical impact**  
The `.contiguous()` call ensures the tensor memory layout meets the assumptions of the Triton kernel, preventing CUDA memory access violations. This change is minimal and localized, affecting only the Qwen-ViT forward pass during rotary embedding computation. The frame limit adjustment aligns the code with documented model constraints, potentially affecting video input handling.

**Potential risks**  
While the fix resolves the immediate issue, similar non-contiguous tensor problems could exist elsewhere in the codebase, especially after shape manipulations. The frame limit reduction may break existing workflows relying on the higher limit, though it aligns with official specs. Performance impact from `.contiguous()` is negligible but adds a small overhead.

**Key insights**  
Always verify tensor contiguity after complex reshape operations before passing to low-level kernels. Consider adding a general utility or assertion for contiguity in similar code paths. The frame limit change should be documented as a breaking change for video processing users. Review other vision models for analogous reshape patterns that might need `.contiguous()`.

---

## 23. [[Doc] [ROCm] Update ROCm getting started doc](https://github.com/vllm-project/vllm/pull/32580)


### Base Information

- **PR Number:** #32580
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-20 09:20:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32580/files) (2):**
  - `docs/getting_started/installation/gpu.rocm.inc.md`
  - `docs/getting_started/quickstart.md`

### Summary

**What changed and why**  
This PR updates ROCm installation documentation to reflect new official release channels. It adds instructions for using vLLM's official Docker image (`vllm/vllm-openai-rocm`) and pre-built ROCm wheels via a custom index, while deprecating references to AMD's Docker images (`rocm/vllm-dev`). The changes aim to provide clearer, more streamlined installation paths for AMD GPU users.

**Technical impact**  
The documentation now directs users to vLLM's official release infrastructure instead of AMD's, centralizing distribution. It introduces a new wheel installation method using `uv` with a custom index (`wheels.vllm.ai/rocm/`), which bundles compatible PyTorch and dependencies. This reduces complexity but may limit flexibility for users needing specific ROCm/PyTorch versions.

**Potential risks**  
- The pre-built wheels only support Python 3.12, ROCm 7.0, and `glibc >= 2.35`, potentially excluding users on older systems or different Python versions.  
- The deprecation of AMD's Docker images might disrupt users relying on those images for development or validation.  
- The `uv` tool is recommended over `pip`, which could require additional onboarding for users unfamiliar with `uv`.

**Key insights**  
- vLLM now provides official, versioned ROCm wheels and Docker images, improving maintainability and user experience.  
- Developers should note the strict compatibility matrix for pre-built wheels and consider building from source if their environment differs.  
- The removal of detailed attention backend configuration in the quickstart may simplify onboarding but could obscure advanced tuning options for ROCm users.

---

## 24. [[Perf] Only clone when needed for `moe_permute`](https://github.com/vllm-project/vllm/pull/32273)


### Base Information

- **PR Number:** #32273
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-20 08:34:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32273/files) (3):**
  - `csrc/moe/moe_permute_unpermute_op.cu`
  - `csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu`
  - `csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.h`

### Summary

**What changed and why**  
The changes optimize memory usage in the MoE permute operation by avoiding unnecessary tensor cloning. Previously, `topk_ids` was always cloned into `copy_topk_ids` for preprocessing, but now cloning only occurs when expert mapping is provided (i.e., when `expert_map` is not null). The `sortAndScanExpert` function signature is updated to accept a `const int*` for `expert_for_source_row` to reflect this conditional cloning.

**Technical impact**  
This reduces memory allocation and copy overhead in cases where `expert_map` is not used, improving performance for standard MoE inference scenarios. The conditional cloning ensures correctness when expert mapping is required, while the const-correctness update in the kernel function aligns with the modified data flow.

**Potential risks**  
If `expert_map` is provided but the cloning logic fails or is skipped incorrectly, it could lead to data corruption or incorrect sorting. Additionally, the const qualifier in `sortAndScanExpert` assumes the kernel does not modify `expert_for_source_row`, which must be verified to avoid undefined behavior.

**Key insights**  
This is a targeted optimization that reduces unnecessary memory operations, particularly beneficial for high-throughput inference. Developers should ensure that all code paths using `expert_map` are thoroughly tested, and the const correctness of the kernel should be validated to prevent accidental modifications to read-only data.

---

## 25. [[PluggableLayer][1/N] Define PluggableLayer](https://github.com/vllm-project/vllm/pull/32331)


### Base Information

- **PR Number:** #32331
- **Author:** [whx-sjtu](https://github.com/whx-sjtu)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-20 08:19:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32331/files) (4):**
  - `docs/design/custom_op.md`
  - `tests/model_executor/test_enabled_custom_ops.py`
  - `vllm/model_executor/custom_op.py`
  - `vllm/model_executor/layers/mla.py`

### Summary

**What changed and why**  
This PR introduces a new `PluggableLayer` base class alongside the existing `CustomOp` system. The primary purpose is to provide a module-composing abstraction that allows out-of-tree (OOT) replacement of entire layer classes at instantiation time, rather than per-platform forward dispatch. The MLA layer is updated to use `PluggableLayer` as an example implementation.

**Technical impact**  
The changes separate layer composition from operation dispatch by moving registry dictionaries (`op_registry` and `op_registry_oot`) to module-level variables shared between `CustomOp` and `PluggableLayer`. This enables both systems to coexist while allowing OOT replacements for entire layers, affecting how layers like MLA are instantiated and composed. The MLA layer no longer uses `forward_native`/`forward_cuda` but a single `forward` method.

**Potential risks**  
There is a risk of registry conflicts since both `CustomOp` and `PluggableLayer` share the same global registries, which could lead to name collisions or unintended overrides. The `__new__` method in `PluggableLayer` may raise `AttributeError` if a class lacks a `name` attribute, potentially causing instantiation failures. Additionally, the MLA layer’s transition might break existing OOT implementations that rely on the previous `CustomOp` dispatch mechanism.

**Key insights**  
Developers should note that `PluggableLayer` is designed for stateful, module-composing layers, while `CustomOp` remains for per-platform forward dispatch. When registering new layers, ensure unique names across both systems to avoid conflicts. The MLA example demonstrates how to migrate from `CustomOp` to `PluggableLayer`, but thorough testing is needed to validate OOT compatibility and edge cases.

---

## 26. [[Bugfix] Fix Off-by-one error in _num_tokens_to_min_blocks calculation](https://github.com/vllm-project/vllm/pull/32603)


### Base Information

- **PR Number:** #32603
- **Author:** [lingebeng](https://github.com/lingebeng)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-20 08:13:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32603/files) (1):**
  - `tests/kernels/utils.py`

### Summary

**What changed and why**  
The PR fixes an off-by-one error in the `_num_tokens_to_min_blocks` function and its corresponding docstring. The original formula `(num_tokens + block_size) // block_size` incorrectly calculates the minimum number of blocks needed, while the corrected formula `(num_tokens + block_size - 1) // block_size` provides the standard ceiling division for integer arithmetic.

**Technical impact**  
This change ensures accurate calculation of the minimum KV cache blocks required for a given number of tokens. It affects memory allocation for sequences, potentially reducing over-allocation by one block in cases where `num_tokens` is exactly divisible by `block_size`.

**Potential risks**  
The main risk is if other parts of the codebase depend on the previous over-allocation behavior. The PR description's "Maybe on purpose, need review" suggests uncertainty about whether the original formula was intentional, so careful verification of dependent logic is required.

**Key insights**  
This is a standard fix for ceiling integer division. Developers should verify no other components rely on the previous allocation pattern, particularly for edge cases where `num_tokens % block_size == 0`. The change improves memory efficiency but requires full regression testing.

---

## 27. [[XPU]Support AgRsAll2AllManager on XPU device](https://github.com/vllm-project/vllm/pull/32654)


### Base Information

- **PR Number:** #32654
- **Author:** [ys950902](https://github.com/ys950902)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-01-20 06:27:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32654/files) (1):**
  - `vllm/distributed/device_communicators/xpu_communicator.py`

### Summary

**What changed and why**  
This PR adds support for the AgRsAll2AllManager (allgather+reduce_scatter) backend on XPU devices. Previously, only the naive all2all backend was supported on XPU, with other backends falling back to naive. Now, the `allgather_reducescatter` backend is enabled, and it becomes the default fallback for unsupported backends.

**Technical impact**  
The changes introduce three new collective communication methods (`reduce_scatter`, `reduce_scatterv`, and `all_gatherv`) to the XPU communicator, enabling efficient all-to-all operations via composed primitives. This improves performance for distributed operations on XPU by replacing the naive implementation with a more optimized approach.

**Potential risks**  
The `all_gatherv` implementation currently only supports `dim=0`, which may limit flexibility. The `reduce_scatter` and `reduce_scatterv` methods rely on tensor contiguity assumptions, and the code comments hint at a potential bug in `reduce_scatter_tensor` if inputs aren't contiguous. Edge cases with non-uniform tensor sizes in `reduce_scatterv` could also introduce subtle bugs.

**Key insights**  
Developers should note that the default fallback behavior for unsupported all2all backends on XPU has changed from "naive" to "AgRs". The new collective methods are performance-critical but have constraints (e.g., `all_gatherv` dimension support). Ensure thorough testing with varied tensor shapes and sizes to validate correctness, especially for non-uniform splits.

---

## 28. [[4/N] Initialize MM components in context managers (M-P)](https://github.com/vllm-project/vllm/pull/32663)


### Base Information

- **PR Number:** #32663
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-20 06:06:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32663/files) (24):**
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/interns1.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/midashenglm.py`
  - `vllm/model_executor/models/minicpmo.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/minimax_vl_01.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/nemotron_vl.py`
  - `vllm/model_executor/models/ovis.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`
  - `vllm/model_executor/models/skyworkr1v.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/tarsier.py`

### Summary

**What changed and why**  
This PR is part of a series to initialize multimodal components within context managers, addressing issue #32631. The changes wrap vision/audio tower and language model initialization in `_mark_tower_model()` and `_mark_language_model()` context managers across 20+ model implementations. Additionally, it removes redundant `get_language_model()` methods and cleans up forward method inconsistencies by eliminating fallback embedding logic when `intermediate_tensors` is provided.

**Technical impact**  
The refactoring centralizes multimodal component initialization through context managers, improving consistency and enabling better control over model component registration. Removing the `get_language_model()` methods suggests a shift toward a more standardized interface for accessing model components. The forward method changes simplify logic by removing conditional embedding generation paths that were likely duplicated or unnecessary.

**Potential risks**  
Removing the fallback embedding logic in forward methods could break backward compatibility if any code paths relied on those conditional branches. The context manager approach introduces new dependencies on the `_mark_tower_model` and `_mark_language_model` methods, which must be properly implemented in base classes. There's also risk of initialization order issues if context managers have side effects affecting component registration.

**Key insights**  
This is a systematic architectural change that standardizes multimodal model initialization across the codebase. Developers should verify that all context manager methods are properly defined in base classes and test that models still function correctly with the simplified forward methods. The removal of `get_language_model()` methods indicates a deprecation of that access pattern in favor of more structured component management.

---

## 29. [[Metrics] Complete removal of deprecated vllm:time_per_output_token_seconds metric](https://github.com/vllm-project/vllm/pull/32661)


### Base Information

- **PR Number:** #32661
- **Author:** [carlory](https://github.com/carlory)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-01-20 04:28:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32661/files) (5):**
  - `docs/design/metrics.md`
  - `examples/online_serving/dashboards/grafana/performance_statistics.json`
  - `examples/online_serving/dashboards/perses/performance_statistics.yaml`
  - `tests/entrypoints/instrumentator/test_metrics.py`
  - `vllm/v1/metrics/loggers.py`

### Summary

**What changed and why**  
This PR completes the removal of the deprecated `vllm:time_per_output_token_seconds` metric, which was renamed to `vllm:inter_token_latency_seconds` in v0.11. The changes include deleting the metric definition and its conditional observation from the codebase, updating tests and documentation, and replacing all references in Grafana and Perses dashboards with the new metric name.

**Technical impact**  
The removal eliminates deprecated code and ensures consistency across the monitoring stack. The replacement metric (`vllm:inter_token_latency_seconds`) provides identical functionality and bucket definitions, so no functional regression is expected. This aligns with the project's deprecation policy and reduces technical debt.

**Potential risks**  
If any external monitoring scripts or alerts still reference the old metric name, they will break after this change. The risk is mitigated by the deprecation timeline (v0.11 to v0.13) and the validation showing no remaining references. Additionally, the similar-named `vllm:request_time_per_output_token_seconds` metric remains, which could cause confusion if not carefully distinguished.

**Key insights**  
Developers should verify that all internal and external tooling now uses `vllm:inter_token_latency_seconds`. The cleanup follows a well-documented deprecation process, serving as a good example for future metric migrations. Ensure that any custom dashboards or alerts are updated to prevent gaps in monitoring.

---

## 30. [[Bugfix] Fix the  fp8_mqa_logits dim mismatch](https://github.com/vllm-project/vllm/pull/32652)


### Base Information

- **PR Number:** #32652
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-01-20 02:48:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32652/files) (2):**
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/utils/deep_gemm.py`

### Summary

**What changed and why**  
The fix addresses a dimension mismatch in the `fp8_mqa_logits` function by flattening the `k_scale` tensor from shape `[N, 1]` to `[N]` before passing it to the underlying DeepGEMM kernel. This aligns with the expected input shape in the reference implementation, resolving issue #32647.

**Technical impact**  
This change ensures compatibility between vLLM's tensor shape handling and the DeepGEMM kernel's interface. The flatten operation is minimal and maintains data integrity while correcting the shape mismatch that was causing runtime errors during FP8 multi-query attention computations.

**Potential risks**  
The flatten operation assumes `k_scale` is either 1D or 2D with a singleton second dimension. If `k_scale` could have other shapes (e.g., `[N, D]` where D>1), this fix might incorrectly alter the data. Additionally, the documentation update suggests the kernel now strictly expects 1D scales, which could break any existing code relying on 2D scales.

**Key insights**  
Always verify tensor shape requirements when interfacing with external kernels. The fix is correct for the current use case, but consider adding a shape assertion or reshape (not just flatten) to make the intent clearer and prevent future issues if tensor shapes evolve. The documentation update is crucial for maintaining consistency.

---

## 31. [[3/N] Initialize MM components in context managers (I-L)](https://github.com/vllm-project/vllm/pull/32650)


### Base Information

- **PR Number:** #32650
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-20 02:21:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32650/files) (10):**
  - `vllm/model_executor/models/interns1.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/kanana_v.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/llava_next.py`
  - `vllm/model_executor/models/llava_next_video.py`
  - `vllm/model_executor/models/llava_onevision.py`

### Summary

**What changed and why**  
This PR refactors multimodal model initialization to use context managers (`_mark_tower_model` and `_mark_language_model`) for component grouping. It also replaces custom weight loading in Kimi-VL with the standardized `AutoWeightsLoader`. These changes are part of a larger effort to unify initialization patterns and improve weight loading consistency across multimodal models.

**Technical impact**  
The changes centralize component initialization logic, making the codebase more maintainable and consistent. By using context managers, the system can better track which components belong to vision towers versus language models. The removal of custom weight loading logic in Kimi-VL reduces code duplication and aligns it with other models using `AutoWeightsLoader`.

**Potential risks**  
The removal of `assert` statements for vision tower existence could mask initialization issues if context managers fail silently. The error message change from `ValueError` to `NotImplementedError` in Keye's `_build_projector` method might affect error handling expectations. Additionally, the significant reduction in Kimi-VL's weight loading logic requires thorough testing to ensure no regression in model loading behavior.

**Key insights**  
Developers should verify that all multimodal models properly implement the `SupportsMultiModal` interface to use the new context managers. The transition to `AutoWeightsLoader` for Kimi-VL is a positive step toward standardization, but requires careful validation of weight mapping. Future work should ensure the removed `get_language_model()` methods don't break any downstream dependencies.

---

## 32. [[Core] Cleanup shm based object store on engine shutdown](https://github.com/vllm-project/vllm/pull/32429)


### Base Information

- **PR Number:** #32429
- **Author:** [walterbm](https://github.com/walterbm)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-20 00:53:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32429/files) (7):**
  - `tests/distributed/test_shm_buffer.py`
  - `tests/distributed/test_shm_storage.py`
  - `vllm/distributed/device_communicators/shm_object_storage.py`
  - `vllm/envs.py`
  - `vllm/multimodal/cache.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This PR adds automatic cleanup of shared memory (SHM) object stores used by the multi-modal cache when the engine shuts down. It appends a unique UUID to SHM buffer names to prevent conflicts between multiple vLLM processes on the same host and ensures orphaned SHM objects are properly cleaned up.

**Technical impact**  
The changes introduce a UUID-based naming scheme for SHM buffers via a new `get_env_or_set_default` utility, add explicit `close()` methods throughout the SHM object storage hierarchy, and ensure these methods are called during engine shutdown. This prevents naming collisions and resource leaks in multi-process deployments.

**Potential risks**  
The UUID-based naming could make debugging more difficult if SHM buffers need manual inspection. There's a slight risk of race conditions during cleanup if multiple processes attempt to unlink the same SHM simultaneously, though `suppress(FileNotFoundError)` mitigates this. The changes assume proper shutdown sequences are always followed.

**Key insights**  
The PR effectively solves two critical deployment issues: SHM naming conflicts and orphaned memory. Developers should ensure all SHM-using components implement proper `close()` methods. The test updates from `del` to explicit `close()` calls demonstrate better resource management practices that should be adopted elsewhere in the codebase.

---

## 33. [[2/N] Initialize MM components in context managers (E-H)](https://github.com/vllm-project/vllm/pull/32641)


### Base Information

- **PR Number:** #32641
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-20 00:12:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32641/files) (12):**
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/fuyu.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`

### Summary

**What changed and why**  
This PR refactors multimodal model initialization by wrapping vision/audio tower components and language models in context managers (`_mark_tower_model` and `_mark_language_model`). It removes redundant `assert` statements and `get_language_model()` methods, streamlining the initialization process across 12 models (E-H in alphabetical order). The changes are part of a larger effort to improve modularity and initialization patterns for multimodal components.

**Technical impact**  
The refactoring centralizes component initialization logic, likely enabling better resource management, configuration tracking, or parallelism control via the context managers. By removing direct assertions and explicit getter methods, the code becomes more declarative and relies on the context managers to ensure proper setup. This also reduces boilerplate and enforces a consistent pattern across different multimodal architectures.

**Potential risks**  
If the context managers (`_mark_tower_model`, `_mark_language_model`) are not properly implemented or have side effects (e.g., affecting device placement or quantization), initialization could break silently. The removal of `assert` statements might hide initialization errors if the context managers fail to validate required components. Additionally, changes to `compute_logits` in `fuyu.py` delegate to the language model’s method, which must exist and behave identically.

**Key insights**  
Developers should verify that the base class providing the context managers is robust and well-tested. The pattern simplifies future maintenance but requires understanding the context managers’ responsibilities. When extending or debugging these models, ensure that the context managers correctly handle the intended modalities (e.g., `{"image", "video"}`) and configuration. The cleanup of obsolete methods reduces technical debt but necessitates updates to any external code relying on `get_language_model()`.

---

