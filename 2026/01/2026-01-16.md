# vLLM Merged PR Report

**Report Date:** 2026-01-16 PST

**Total Merged PRs:** 15

---

## 1. [[Models] Lfm2Moe: minor name changes for resolving lora conflicts](https://github.com/vllm-project/vllm/pull/29063)


### Base Information

- **PR Number:** #29063
- **Author:** [paulpak58](https://github.com/paulpak58)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-16 22:12:55
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29063/files) (2):**
  - `vllm/model_executor/models/lfm2.py`
  - `vllm/model_executor/models/lfm2_moe.py`

### Summary

**What changed and why**  
The PR renames the `conv` attribute to `short_conv` in two model classes (`Lfm2DecoderLayer` and `Lfm2MoeDecoderLayer`) and updates weight loading logic to map checkpoint keys containing `.conv.` to `.short_conv.`. This resolves naming conflicts with LoRA (Low-Rank Adaptation) modules, as the generic name `conv` could cause ambiguous parameter matching during LoRA fine-tuning or weight loading.

**Technical impact**  
These changes ensure that LoRA adapters and checkpoint loading correctly distinguish between the model's convolutional layers and other potential `conv`-named parameters. The `stacked_params_mapping` is also updated to include `"in_proj"` in both models, aligning parameter grouping for efficient tensor stacking (e.g., for QKV projections). This maintains compatibility with existing checkpoints while preventing misalignment during LoRA operations.

**Potential risks**  
If external checkpoints or LoRA adapters rely on the old `.conv.` naming convention without the updated mapping, weight loading could fail or silently ignore those parameters. The string replacement (`replace(".conv.", ".short_conv.", 1)`) assumes a specific pattern; malformed weight names (e.g., containing `.conv.` in unexpected contexts) might be incorrectly rewritten. Additionally, any serialized model states (e.g., saved optimizer states) referencing `conv` would become incompatible.

**Key insights**  
This is a targeted fix for LoRA integration, highlighting the importance of unique parameter naming in modular architectures. Developers should verify that all related checkpoints and adapters are updated or mapped accordingly. Consider adding validation in `load_weights` to log unrecognized parameters, and ensure downstream tools (e.g., model converters) are aware of the naming change to avoid silent errors.

---

## 2. [[CI] Implement uploading to PyPI and GitHub in the release pipeline, enable release image building for CUDA 13.0](https://github.com/vllm-project/vllm/pull/31032)


### Base Information

- **PR Number:** #31032
- **Author:** [Harry-Chen](https://github.com/Harry-Chen)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-01-16 20:52:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31032/files) (3):**
  - `.buildkite/release-pipeline.yaml`
  - `.buildkite/scripts/upload-nightly-wheels.sh`
  - `.buildkite/scripts/upload-release-wheels.sh`

### Summary

**What changed and why**  
This PR automates the release pipeline by implementing automated uploads to PyPI and GitHub Releases, and adds support for building release images for CUDA 13.0. The changes rename wheel build steps for clarity, introduce a new script for release uploads, and restructure the pipeline to include CUDA 13.0 release images with multi-arch manifests.

**Technical impact**  
The pipeline now distinguishes between nightly and release uploads, with release uploads requiring version validation and secure token handling. The addition of CUDA 13.0 release images expands platform support, while the multi-arch manifest creation ensures compatibility across architectures. The changes centralize release logic in a new script, improving maintainability.

**Potential risks**  
The release script lacks robust error handling for partial upload failures (e.g., PyPI success but GitHub failure). Version mismatch checks may fail if tags are not properly set, and the dependency on external services (PyPI, GitHub, AWS S3) introduces points of failure. The use of `--exclude "*dev*"` in wheel filtering could inadvertently exclude valid release wheels if naming conventions change.

**Key insights**  
Developers should ensure `PYPI_TOKEN` and `GITHUB_TOKEN` are securely configured in the CI environment. The pipeline now requires exact tag matching for releases, so tagging practices must be consistent. Consider adding rollback mechanisms or idempotent retry logic for upload steps to handle transient failures gracefully.

---

## 3. [Revert "[Attention][MLA] Make `FLASHINFER_MLA` the default MLA backen…](https://github.com/vllm-project/vllm/pull/32484)


### Base Information

- **PR Number:** #32484
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-16 20:42:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32484/files) (3):**
  - `vllm/config/attention.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/platforms/cuda.py`

### Summary

**What changed and why**  
This PR reverts a previous change that made FlashInfer MLA the default attention backend on Blackwell GPUs and TRTLLM the default prefill method. The revert is necessary because the FI prefill backend has correctness issues that caused CI failures, as documented in issue #27684.

**Technical impact**  
The changes restore the previous default configuration: TRTLLM ragged DeepSeek prefill is disabled (`use_trtllm_ragged_deepseek_prefill: bool = False`), and the default MLA backend on Blackwell GPUs is switched back from FlashInfer MLA to CUTLASS MLA. The prefill selection logic in `mla_attention.py` is reordered to prioritize FlashInfer prefill over TRTLLM when both are available.

**Potential risks**  
While reverting addresses the immediate correctness issues, it may reintroduce performance trade-offs that the original PR aimed to improve. The conditional check added to `use_flashinfer_prefill()` could create subtle dependencies between backend flags. There's also a risk that the CI coverage gap mentioned will remain unaddressed if not followed up separately.

**Key insights**  
This is a tactical revert to stabilize the codebase, highlighting the importance of comprehensive CI testing for attention backend changes. Developers should ensure the separate PR for improved CI coverage is prioritized to prevent similar regressions. The reordering of prefill backend priority in the initialization logic should be validated to ensure it doesn't introduce unexpected behavior in multi-backend scenarios.

---

## 4. [apply _validate_input to MistralTokenizer token-id chat prompts](https://github.com/vllm-project/vllm/pull/32448)


### Base Information

- **PR Number:** #32448
- **Author:** [vanshilshah97](https://github.com/vanshilshah97)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-16 19:23:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32448/files) (2):**
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `vllm/entrypoints/openai/engine/serving.py`

### Summary

**What changed and why**  
The PR fixes inconsistent request validation in the OpenAI-compatible server when using `MistralTokenizer`. Previously, when the Mistral chat template returned token IDs directly as a `list[int]`, the code bypassed `_validate_input`, skipping critical checks like max context window validation and overflow checks for `max_tokens`. Now, token-ID prompts are routed through `_validate_input` to ensure consistent behavior with other tokenizers and fail early with clear 400 errors for invalid requests.

**Technical impact**  
This change ensures that MistralTokenizer token-ID prompts undergo the same validation as HuggingFace tokenizer prompts, including max context length (`max_model_len`) and `max_tokens` overflow checks. The fix aligns validation paths across different tokenizer types, preventing silent failures and ensuring requests are rejected early when they exceed model limits, improving system reliability and user experience.

**Potential risks**  
The main risk is that the validation may now reject previously accepted requests that were erroneously allowed due to the bug, potentially breaking existing client workflows that relied on the incorrect behavior. Additionally, if `_validate_input` has side effects or performance overhead, it could impact latency for Mistral token-ID prompts, though this is likely negligible.

**Key insights**  
Developers should ensure that any custom tokenizers or chat template implementations return token IDs only when they can pass validation checks. The fix highlights the importance of consistent validation paths across different code branches to prevent security and reliability issues. Consider adding similar validation for other tokenizer types if they have analogous code paths.

---

## 5. [[Docs][Governance] Add @robertshaw2-redhat to lead maintainers group](https://github.com/vllm-project/vllm/pull/32498)


### Base Information

- **PR Number:** #32498
- **Author:** [simon-mo](https://github.com/simon-mo)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-01-16 18:35:49
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32498/files) (1):**
  - `docs/governance/process.md`

### Summary

**What changed and why**  
A single documentation update adds @robertshaw2-redhat to the list of lead maintainers in the project governance document. This change reflects an organizational decision to expand the leadership committee responsible for the project's overall direction and strategy.

**Technical impact**  
This change has no direct technical impact on the codebase, build processes, or system functionality. It is purely an administrative update to the project's governance structure, affecting only the documented list of individuals in a leadership role.

**Potential risks**  
There are minimal risks, as this is a documentation-only change. However, if the governance document is used programmatically (e.g., for automated permissions or notifications), the update might need to be synchronized with other systems. Additionally, any inaccuracies in the listed responsibilities or divided duties could lead to confusion.

**Key insights**  
Developers should note the updated leadership structure, as it may affect decision-making and project direction. Ensure that any automated systems referencing lead maintainers are updated accordingly. This change underscores the importance of keeping governance documentation aligned with actual project roles.

---

## 6. [[TPU][Core] Enable Pipeline Parallelism on TPU backend](https://github.com/vllm-project/vllm/pull/28506)


### Base Information

- **PR Number:** #28506
- **Author:** [Chenyaaang](https://github.com/Chenyaaang)
- **Merged By:** [yaochengji](https://github.com/yaochengji)
- **Merged time:** 2026-01-16 15:29:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28506/files) (2):**
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/executor/ray_utils.py`

### Summary

**What changed and why**  
The changes enable pipeline parallelism support on TPU backend by extracting key methods to allow TPU-specific overrides. In `multiproc_executor.py`, `_get_parallel_sizes` and `_post_init_executor` methods were extracted, and `_is_driver_worker` logic was moved to a separate method. In `ray_utils.py`, `_is_intermediate_tensors` was extracted to handle JAX's IntermediateTensor type differences.

**Technical impact**  
These changes create extension points in the executor framework, allowing TPU backend to override parallel size calculations and intermediate tensor detection without modifying core logic. The architecture now supports pipeline parallelism across different hardware backends while maintaining the same user interface (command remains unchanged with `--pipeline-parallel-size`).

**Potential risks**  
If TPU backend implementations don't properly override the extracted methods, pipeline parallelism may fail silently or behave incorrectly. The `_is_driver_worker` logic assumes tensor parallelism grouping, which might not align with all pipeline parallelism configurations. There's also a risk of inconsistent behavior between PyTorch and JAX model implementations since PP isn't supported on all Jax models.

**Key insights**  
The refactoring follows good OOP principles by using template method pattern for hardware-specific customizations. Developers implementing new backends should carefully override all extracted methods to ensure compatibility. Note that users must set `MODEL_IMPL_TYPE=vllm` for PyTorch implementations when using PP on TPU, as JAX model support is limited. The changes maintain backward compatibility while enabling new functionality.

---

## 7. [[CI] Fix OOM in Hopper Fusion E2E Tests (H100)](https://github.com/vllm-project/vllm/pull/32489)


### Base Information

- **PR Number:** #32489
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-16 13:27:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32489/files) (1):**
  - `.buildkite/test-pipeline.yaml`

### Summary

**What changed and why**  
The change modifies the Hopper Fusion E2E Tests (H100) step in the CI pipeline to skip the Llama-4 model during test execution. This is a targeted fix to resolve an Out-Of-Memory (OOM) error, which was introduced by a prior code change (PR #30499) that presumably increased memory usage for this model.

**Technical impact**  
This update only affects the specific CI test step by adding a pytest filter (`-k 'not Llama-4'`). It prevents the failing test from blocking the pipeline but does not address the root cause of the memory increase for the Llama-4 model. The rest of the test suite for fusion attention continues to run.

**Potential risks**  
The primary risk is that the underlying issue causing the OOM for Llama-4 remains unaddressed and could surface in other test configurations or production scenarios. There is also a risk that similar memory issues for other models might be missed if this is treated as an isolated fix.

**Key insights**  
This is a temporary workaround to unblock CI. The team should investigate PR #30499 to understand why Llama-4 now exceeds H100 memory limits and implement a proper fix, such as model optimization or adjusted resource allocation. The comment added to the code clearly documents the reason for the skip, which is good practice for temporary exclusions.

---

## 8. [[responsesAPI] allow tuning include_stop_str_in_output](https://github.com/vllm-project/vllm/pull/32383)


### Base Information

- **PR Number:** #32383
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-16 13:14:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32383/files) (1):**
  - `vllm/entrypoints/openai/responses/protocol.py`

### Summary

**What changed and why**  
A new optional parameter `include_stop_str_in_output` (default `False`) was added to the `ResponsesRequest` protocol and passed through to `SamplingParams`. This allows developers debugging the responsesAPI with `enable_response_messages` to include the final stop sequence token in the output, addressing a token-message discrepancy where the last stop token was previously omitted.

**Technical impact**  
The change extends the OpenAI-compatible responsesAPI interface without altering default behavior. It propagates the flag to the underlying sampling logic, enabling more accurate token-level debugging when `enable_response_messages` is used, while existing parsing for reasoning and tool calls remains unaffected.

**Potential risks**  
If downstream systems assume the stop token is always excluded, enabling this flag could introduce unexpected tokens in parsed outputs. There is also a risk that the flag might be misinterpreted as affecting stop sequence handling rather than purely output inclusion, though the default `False` preserves backward compatibility.

**Key insights**  
This is a low-risk additive change that improves debuggability. Developers should note that the flag is intended for debugging purposes and does not change parsing logic. When enabling it, verify that any post-processing steps are tolerant of the additional stop token in the raw output.

---

## 9. [[LoRA] Update LoRA expand kernel heuristic](https://github.com/vllm-project/vllm/pull/32425)


### Base Information

- **PR Number:** #32425
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-16 10:38:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32425/files) (1):**
  - `vllm/lora/ops/triton_ops/utils.py`

### Summary

**What changed and why**  
The PR modifies the heuristic for the LoRA expand kernel's `block_n` configuration. It reduces `block_n` from a fixed 128 to `max(64, next_power_of_2(128 // num_slices))` when `num_slices > 1`, which occurs in layers like `QKVParallelLinearWithLoRA`. This adjustment aims to increase SM concurrency by launching more GPU blocks, improving kernel performance.

**Technical impact**  
This change optimizes GPU utilization for LoRA operations with multiple slices by tuning the block size to better match the reduced output dimension per slice. It directly accelerates the `_lora_expand_kernel`, which showed a ~30% improvement in profiling, contributing to a modest 1% end-to-end token throughput gain in benchmarks.

**Potential risks**  
The heuristic assumes `num_slices` divides 128 evenly; if `num_slices` is not a power of two or doesn't divide evenly, the `next_power_of_2` operation may produce suboptimal block sizes. Additionally, the fixed lower bound of 64 might not be optimal for all hardware or slice configurations, potentially limiting gains on architectures with different warp sizes or memory constraints.

**Key insights**  
The update demonstrates how tailoring kernel configurations to specific parallelization patterns (like sliced LoRA layers) can yield meaningful performance improvements. Developers should verify that `num_slices` is always a power of two in relevant layers to maximize benefits. Consider extending this heuristic to other LoRA kernels if similar slicing patterns exist.

---

## 10. [Atomics Reduce Counting Optimization for SplitK Skinny GEMMs.](https://github.com/vllm-project/vllm/pull/29843)


### Base Information

- **PR Number:** #29843
- **Author:** [amd-hhashemi](https://github.com/amd-hhashemi)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-16 09:45:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29843/files) (6):**
  - `csrc/rocm/ops.h`
  - `csrc/rocm/skinny_gemms.cu`
  - `csrc/rocm/torch_bindings.cpp`
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/utils.py`

### Summary

**What changed and why**  
This PR introduces a new ROCm kernel `wvSplitKrc` optimized for skinny GEMMs where N=16-128, K=2880, and M=128/640, targeting decode GEMMs in low-concurrency GPT workloads. The optimization uses atomic reduce counting to improve performance when `(M/16)*(K/512)` is less than the SIMD count, reducing synchronization overhead in SplitK reductions.

**Technical impact**  
The changes add a new kernel path in the ROCm backend for specific GEMM shapes, introducing a conditional dispatch in `rocm_unquantized_gemm_impl`. The kernel leverages shared memory and atomic operations to optimize reduction steps, potentially improving throughput for targeted matrix dimensions. This adds a specialized execution path without affecting existing kernels.

**Potential risks**  
The kernel currently targets only gfx950 (MI300) via a compile-time guard, limiting portability. The optimization relies on specific shape constraints and may not generalize well to other architectures or dimensions. There is a risk of silent LLVM upcasting due to the `min` function, addressed with a custom `min__`, but similar issues could exist elsewhere. The PR description lacks test results and plans, making validation unclear.

**Key insights**  
Developers should note that this optimization is highly specific to MI300 and certain GEMM shapes; expansion to other architectures requires updating the `#if defined(__gfx950__)` guard. The kernel introduces a new `wvSplitKrc` op with Python bindings and tests, ensuring integration. Future work should validate performance gains across a broader range of dimensions and consider dynamic dispatch based on hardware detection rather than compile-time guards.

---

## 11. [[CI] Update deepgemm to newer version](https://github.com/vllm-project/vllm/pull/32479)


### Base Information

- **PR Number:** #32479
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-01-16 09:18:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32479/files) (1):**
  - `tools/install_deepgemm.sh`

### Summary

**What changed and why**  
Updated the DeepGEMM Git reference from commit `594953acce41793ae00a1233eb516044d604bcb6` to `0f5f2662027f0db05d4e3f6a94e56e2d8fc45c51` to incorporate changes from a recent upstream pull request (deepseek-ai/DeepGEMM#280) that introduced significant updates.

**Technical impact**  
This change will cause the installation script to fetch and build a newer version of the DeepGEMM library, potentially including performance improvements, bug fixes, or new features from the upstream repository. The system's dependency on DeepGEMM will now be pinned to this newer commit hash.

**Potential risks**  
The new commit may introduce breaking changes, compatibility issues with existing code, or new bugs that haven't been fully tested in this specific environment. Since the PR description mentions "updated a lot of things," there's increased risk of unexpected behavior changes.

**Key insights**  
Always verify that the new DeepGEMM version maintains compatibility with the rest of the codebase. Consider running integration tests before deploying this change to production. The commit hash update is minimal but carries significant impact since it changes the entire DeepGEMM dependency version.

---

## 12. [[EPLB][BugFix]Possible deadlock fix](https://github.com/vllm-project/vllm/pull/32418)


### Base Information

- **PR Number:** #32418
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-16 06:11:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32418/files) (1):**
  - `vllm/distributed/eplb/eplb_state.py`

### Summary

**What changed and why**  
The fix changes `move_to_workspace` from a non-blocking lock acquisition to a blocking acquisition with retries and a timeout. Previously, if the `buffer_lock` wasn't immediately available, the function would return early, potentially causing a deadlock when some ranks proceeded without completing weight transfers. Now, ranks wait (up to 1 minute) to acquire the lock, ensuring all ranks in the expert-parallel group synchronize properly before continuing.

**Technical impact**  
This change enforces stronger synchronization within the EPLB (Expert-Parallel Load Balancing) system by guaranteeing that all ranks acquire the buffer lock before proceeding. It prevents the scenario where ranks could diverge—some starting new weight transfers while others advance to the next model iteration—which could corrupt collective operations across the expert-parallel group.

**Potential risks**  
Introducing a blocking wait with a fixed timeout could lead to hangs if the lock is held longer than expected (e.g., due to slow I/O or system issues). The 1-minute timeout is arbitrary and may not suit all environments; if exceeded, it raises a `RuntimeError` that could crash the application. Additionally, frequent retries with warnings might clutter logs if contention is high.

**Key insights**  
The fix addresses a critical synchronization bug but trades off potential liveness for safety. Developers should monitor for timeout errors in production and consider making the timeout configurable. Ensure that the lock-holding duration (e.g., during async weight transfers) remains short to avoid performance degradation. This change highlights the importance of explicit synchronization in async distributed systems.

---

## 13. [[CI][AMD] Skip test_permute_cols since the kernel is not used and not built for ROCm](https://github.com/vllm-project/vllm/pull/32444)


### Base Information

- **PR Number:** #32444
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-16 00:22:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32444/files) (1):**
  - `tests/kernels/core/test_permute_cols.py`

### Summary

**What changed and why**  
Added a conditional skip at the module level in `test_permute_cols.py` to check if the `permute_cols` kernel attribute exists in `torch.ops._C`. If the attribute is missing (e.g., on ROCm builds where the kernel is not built or used), the entire test module is skipped with a clear reason.

**Technical impact**  
This change prevents test failures on ROCm platforms by gracefully skipping tests that depend on unsupported kernels. It ensures test suites remain green across different hardware backends without affecting functionality, as the kernel itself is not utilized in ROCm deployments.

**Potential risks**  
The skip condition relies on the presence of a specific attribute in `torch.ops._C`, which could change in future PyTorch versions. Additionally, if the kernel becomes supported on ROCm later, the test may be unintentionally skipped unless the condition is updated. There is also a minor risk of masking other issues if the attribute is missing for reasons other than ROCm (e.g., build misconfiguration).

**Key insights**  
This is a pragmatic solution for cross-platform compatibility, but consider adding a more robust backend detection mechanism (e.g., checking for ROCm explicitly) to future-proof the skip logic. Ensure the skip reason is documented in test reports to avoid confusion. Review similar tests for other ROCm-unsupported kernels to apply consistent patterns.

---

## 14. [[Chore] Replace swish with silu](https://github.com/vllm-project/vllm/pull/32459)


### Base Information

- **PR Number:** #32459
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-16 00:22:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32459/files) (1):**
  - `vllm/model_executor/models/phi4mm_utils.py`

### Summary

**What changed and why**  
The PR replaces the custom `Swish` activation implementation with PyTorch's built-in `nn.SiLU` module. It also centralizes activation function selection by refactoring the `GLU` and `Conv1dGLU` classes to use a shared `get_activation` helper function, which now includes proper error handling for unsupported activation names.

**Technical impact**  
This change simplifies the codebase by removing redundant custom code and leveraging PyTorch's optimized `SiLU` implementation. The refactoring improves maintainability by consolidating activation function logic into a single `get_activation` function, ensuring consistent behavior across different components that use activation functions.

**Potential risks**  
The `get_activation` function now raises `NotImplementedError` for unsupported names instead of silently returning `nn.Identity()`, which could break existing code that relies on the fallback behavior. Additionally, the `GLU` class no longer uses `inplace=True` for ReLU activations, which may affect memory usage but should not change functional behavior.

**Key insights**  
The refactoring is a positive step toward code simplification and standardization. Developers should verify that all activation names used in the codebase are supported by the updated `get_activation` function. Consider adding unit tests for the activation function helper to ensure all expected activation types work correctly with the new implementation.

---

## 15. [[ROCm][CI] Skip Qwen3-30B-A3B-MXFP4A16 Eval Test On Non-CUDA Platforms](https://github.com/vllm-project/vllm/pull/32460)


### Base Information

- **PR Number:** #32460
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-16 00:17:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32460/files) (1):**
  - `tests/evals/gsm8k/test_gsm8k_correctness.py`

### Summary

**What changed and why**  
Added a conditional skip in the GSM8K correctness test for the Qwen3-30B-A3B-MXFP4A16 model on non-CUDA platforms. This is necessary because the model's MXFP4A16 implementation relies on CUDA-specific Marlin kernels, causing test failures on AMD CI environments.

**Technical impact**  
The change prevents the test from executing on platforms without CUDA support, specifically AMD hardware, by checking the platform type and model name. This ensures CI stability without altering the test logic for CUDA platforms, maintaining existing coverage where the kernels are available.

**Potential risks**  
If the model name or platform detection logic changes, the skip condition may become inaccurate. Additionally, other non-CUDA platforms (e.g., CPU-only or future accelerators) running this test could encounter similar issues if not covered by the current check.

**Key insights**  
This is a targeted fix for CI failures, but consider extending the skip mechanism to a more generic helper if similar CUDA-specific tests emerge. Ensure platform detection (`current_platform.is_cuda()`) is robust across all environments. Documenting kernel dependencies in test configurations could help prevent future issues.

---

