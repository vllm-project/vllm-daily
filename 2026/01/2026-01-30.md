# vLLM Merged PR Report

**Report Date:** 2026-01-30 PST

**Total Merged PRs:** 43

---

## 1. [[Kernel] [Helion] [3/N] Helion kernel registry](https://github.com/vllm-project/vllm/pull/33203)


### Base Information

- **PR Number:** #33203
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 23:38:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33203/files) (3):**
  - `tests/kernels/helion/test_register.py`
  - `vllm/kernels/helion/__init__.py`
  - `vllm/kernels/helion/register.py`

### Summary

**What changed and why**  
This PR introduces a `register_kernel` decorator and a global registry for Helion kernels. It allows raw Python functions containing Helion language to be registered, automatically wrapped in `HelionKernelWrapper`, and stored in a central dictionary for later retrieval. The changes include overloaded type hints for better mypy support, automatic fake implementation generation, and duplicate registration prevention.

**Technical impact**  
The addition creates a centralized mechanism for kernel management, enabling dynamic lookup and reuse of Helion kernels across the codebase. It standardizes kernel registration, ensures proper configuration validation (e.g., rejecting custom autotuners), and provides utilities (`get_kernel_by_name`, `get_registered_kernels`) for introspection and access. The registry is thread-safe due to the use of a plain dictionary with copying on retrieval.

**Potential risks**  
- The global registry (`_REGISTERED_KERNELS`) is mutable and could be inadvertently modified if references are retained, though `get_registered_kernels` returns a copy.  
- Auto-generated fake implementations rely on `infer_fake_impl`, which may have performance overhead or fail for complex kernels.  
- The overloaded decorator signature, while improving type safety, adds complexity and may confuse developers unfamiliar with the pattern.

**Key insights**  
- Use `@register_kernel` to seamlessly integrate Helion kernels; it handles naming, fake implementation generation, and settings validation.  
- Avoid setting `static_shapes=True` in Helion settings, as it triggers a warning—this likely indicates unsupported or discouraged usage.  
- Always check for existing kernel names before registration to prevent `ValueError` due to duplicates. The registry enforces uniqueness.

---

## 2. [[CPU][Feat] Enable KleidiAI accelerated int4 dynamic quant with BF16 activations on Arm CPUs](https://github.com/vllm-project/vllm/pull/33122)


### Base Information

- **PR Number:** #33122
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-30 23:16:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33122/files) (1):**
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit.py`

### Summary

**What changed and why**  
The PR enables BF16 activations for KleidiAI-accelerated int4 dynamic quantization on Arm CPUs. Previously, the dynamic 4-bit quantization path only supported FP32 activations, which increased memory usage and limited performance. This change allows models to use BF16 activations, reducing memory footprint and accelerating operations like paged attention and KV cache management.

**Technical impact**  
This update modifies the `Dynamic4bitLinearKernel` to accept both `torch.float32` and `torch.bfloat16` activation types, aligning with PyTorch's support for KleidiAI kernels (introduced in torch==2.10.0). It ensures that int4 quantized models can leverage BF16 activations, improving efficiency in memory-bound and compute-intensive layers without altering the core quantization logic.

**Potential risks**  
The primary risk involves compatibility with older PyTorch versions (<2.10.0) that lack KleidiAI kernel support for BF16 activations, potentially causing runtime errors. Additionally, the change assumes that all Arm CPU environments with KleidiAI acceleration will handle BF16 correctly; edge cases in heterogeneous hardware setups or unsupported configurations may lead to silent performance degradation or failures.

**Key insights**  
Developers should verify that their PyTorch version is >=2.10.0 and that KleidiAI kernels are properly enabled on Arm CPUs. This enhancement significantly optimizes memory usage and inference speed for int4 quantized models, but thorough validation on target hardware is recommended to ensure expected performance gains and numerical stability.

---

## 3. [Add EAGLE3 support for AFMoE](https://github.com/vllm-project/vllm/pull/33111)


### Base Information

- **PR Number:** #33111
- **Author:** [AutumnAurelium](https://github.com/AutumnAurelium)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 22:53:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33111/files) (2):**
  - `vllm/config/speculative.py`
  - `vllm/model_executor/models/afmoe.py`

### Summary

**What changed and why**  
This PR adds EAGLE3 speculative decoding support to the AFMoE architecture by implementing the `SupportsEagle3` interface. The changes enable AFMoE models to work with EAGLE3-based draft models for accelerated inference through speculative decoding.

**Technical impact**  
The AFMoE model now implements the `SupportsEagle3` interface with methods to specify auxiliary hidden state layers (`set_aux_hidden_state_layers`) and retrieve default layer positions (`get_eagle3_aux_hidden_state_layers`). The forward methods now optionally return auxiliary hidden states needed for EAGLE3's speculative decoding mechanism. AFMoE has also been added to the allowed model list in the speculative configuration validation.

**Potential risks**  
The implementation assumes specific layer indices (2, middle layer, and n-3) for auxiliary hidden states without configuration flexibility. The changes modify return types of forward methods, which could affect existing code that expects consistent return types. There's no test coverage with public EAGLE3 models, relying solely on internal testing.

**Key insights**  
The PR successfully integrates EAGLE3 support while maintaining backward compatibility through conditional returns. Developers should verify that downstream code handles the new return type `tuple[torch.Tensor, list[torch.Tensor]]` in addition to the existing return types. Consider making the auxiliary layer positions configurable rather than hardcoded for better flexibility across different AFMoE model sizes.

---

## 4. [Add support for Mistral Large 3 inference with Flashinfer MoE](https://github.com/vllm-project/vllm/pull/33174)


### Base Information

- **PR Number:** #33174
- **Author:** [dbari](https://github.com/dbari)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 22:48:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33174/files) (16):**
  - `benchmarks/kernels/benchmark_moe.py`
  - `docker/Dockerfile`
  - `docker/Dockerfile.nightly_torch`
  - `docker/versions.json`
  - `requirements/cuda.txt`
  - `tests/kernels/moe/test_flashinfer.py`
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8.json`
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_B200.json`
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_GB200,dtype=fp8_w8a8.json`
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json`
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_H200.json`
  - `vllm/model_executor/layers/fused_moe/configs/E=16,N=4096,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json`
  - `vllm/model_executor/layers/fused_moe/configs/E=16,N=4096,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
This PR adds support for Mistral Large 3 inference using Flashinfer's MoE kernels, primarily by upgrading Flashinfer to version 0.6.2 (which includes fixes for blockwise quantized FP8 MoE) and extending vLLM's code to call these kernels. It also adds optimized Triton configurations for Mistral Large 3 FP8 on Blackwell (B200/GB200) and H200 GPUs, and improves the MoE benchmarking script to handle more model architectures.

**Technical impact**  
The changes enable significantly better performance for Mistral Large 3 on Blackwell GPUs, as shown by the benchmark results: Flashinfer MoE backend reduces TTFT by ~15-18% and increases throughput by ~15-17% compared to Triton. The update extends Flashinfer's FP8 per-tensor quantization support beyond Llama4 routing, allowing other routing methods like Renormalize. The benchmarking script now supports additional MoE models (including MistralLarge3ForCausalLM and Pixtral) and better handles quantization configs.

**Potential risks**  
The Flashinfer version bump could introduce compatibility issues with existing models or kernels if the new version has regressions. The extended routing method support for FP8 per-tensor quantization may need validation across different model types. The added Triton configurations are hardware-specific (B200/GB200/H200) and may not generalize well to other GPU architectures.

**Key insights**  
Developers should verify that the Flashinfer 0.6.2 update doesn't break existing MoE workloads. The performance gains for Mistral Large 3 are substantial, making Flashinfer the preferred MoE backend for this model on supported hardware. The benchmarking script improvements enhance maintainability by centralizing model parameter extraction and supporting serialization with Ray.

---

## 5. [[Bugfix] Handle Asym W4A16 (ConchLinearKernel) for CT](https://github.com/vllm-project/vllm/pull/33200)


### Base Information

- **PR Number:** #33200
- **Author:** [mgehre-amd](https://github.com/mgehre-amd)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-30 22:21:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33200/files) (1):**
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/conch.py`

### Summary

**What changed and why**  
The fix adds a new `transform_w_zp` function to unpack and reorder zero-point tensors for asymmetric quantization in the ConchLinearKernel. Previously, the kernel expected unpacked zero points with shape `(K//G, N)`, but vLLM was passing packed zeros with shape `(N//pack, K//G)`, causing illegal memory access on ROCm. The new logic correctly unpacks int32-packed zero points based on the weight bit width (4-bit or 8-bit) and reorders them to the expected layout.

**Technical impact**  
This change enables correct execution of asymmetric W4A16 and W8A16 quantized models on ROCm by ensuring zero-point tensors are properly formatted for the Conch kernel. The kernel can now handle both symmetric and asymmetric quantization schemes, expanding model compatibility. The fix also updates internal parameter metadata to reflect the unpacked state, maintaining consistency within the parameter transformation pipeline.

**Potential risks**  
The unpacking logic assumes a specific packing format (int32 with sequential bit packing) that may not be validated for all model types, especially since no asymmetric W8A16 models were available for testing. The lack of unit tests due to unavailable small asymmetric models means edge cases in packing or device compatibility (e.g., different GPU architectures) might not be covered. Additionally, the metadata updates rely on the presence of specific attributes (`_input_dim`, `_output_dim`, `_packed_factor`), which could cause silent failures if those attributes are missing.

**Key insights**  
Developers should note that asymmetric quantization support now depends on correct zero-point unpacking, and future changes to weight packing formats must align with this logic. Consider adding a validation step or assertion for the expected packed tensor shape to catch format mismatches early. For robustness, explore creating a synthetic test model to enable unit testing of asymmetric quantization paths, even if real models are scarce.

---

## 6. [[Bugfix] Fix `Qwen3ASR` language asr tag in output](https://github.com/vllm-project/vllm/pull/33410)


### Base Information

- **PR Number:** #33410
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-30 21:24:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33410/files) (3):**
  - `vllm/entrypoints/openai/translations/speech_to_text.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/qwen3_asr.py`

### Summary

**What changed and why**  
A post-processing hook was added to the `SupportsTranscription` interface to clean up structured language tags (e.g., `language English<asr_text>`) that Qwen3-ASR models append to transcriptions. The Qwen3-ASR implementation now strips these tags via a new `post_process_output` class method, ensuring clean text output for non-streaming requests.

**Technical impact**  
This change extends the transcription interface with a post-processing capability, allowing model-specific output cleanup without modifying core serving logic. It improves user experience by removing extraneous metadata from transcriptions, but streaming responses remain unaffected due to chunking complexities.

**Potential risks**  
Streaming responses still contain the raw tags, as noted in the TODO, which could confuse clients expecting clean text. The `rsplit` logic assumes the tag appears only once; malformed outputs with multiple tags may produce incorrect results. Other ASR models implementing this interface must ensure their post-processing is idempotent and efficient.

**Key insights**  
Implement model-specific output sanitization via dedicated hooks to maintain separation of concerns. Address streaming inconsistencies by developing a buffering mechanism that can detect and remove structured prefixes across chunks. Consider adding validation to handle edge cases where tags are missing or duplicated.

---

## 7. [[Kernel] [Helion] [2/N] Helion kernel wrapper](https://github.com/vllm-project/vllm/pull/32964)


### Base Information

- **PR Number:** #32964
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 20:53:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32964/files) (5):**
  - `tests/kernels/helion/test_register.py`
  - `tests/kernels/helion/test_utils.py`
  - `vllm/kernels/helion/__init__.py`
  - `vllm/kernels/helion/register.py`
  - `vllm/kernels/helion/utils.py`

### Summary

**What changed and why**  
This PR introduces a Helion kernel wrapper system that enables vLLM to use pre-tuned kernel configurations instead of runtime autotuning. It adds `HelionKernelWrapper` and `ConfiguredHelionKernel` classes to register Helion kernels as PyTorch custom ops, overriding Helion's default autotuning with a config picker that selects from pre‑tuned configs based on input shapes.

**Technical impact**  
The changes integrate Helion's kernel compilation cache with vLLM's config management, allowing shape‑based dispatching via custom cache keys and a preset autotuner. This shifts kernel optimization from runtime autotuning to using pre‑tuned configurations, which can reduce compilation overhead and improve consistency. The system is designed to support dynamic shapes (default `static_shapes=False`) and is platform‑aware via GPU name canonicalization.

**Potential risks**  
If the config picker returns an invalid key or `None` when no default config exists, kernel registration or invocation will fail. There is also a risk of configuration mismatch if the picker logic does not align with available pre‑tuned configs. The override of Helion's autotuning assumes the provided configs are optimal for all runtime shapes, which may not hold for unseen input patterns.

**Key insights**  
Developers must register a config picker for each kernel via `@op_name.register_config_picker`; otherwise, registration will raise an error. The system warns if `static_shapes=True` is set, as most vLLM ops require dynamic shapes. Ensure that the config picker always returns a valid key from the platform's config set, with a fallback to "default" when appropriate.

---

## 8. [[Attention] Clarify comment explaining attn_logits +1 dimension](https://github.com/vllm-project/vllm/pull/33427)


### Base Information

- **PR Number:** #33427
- **Author:** [fuscof-ibm](https://github.com/fuscof-ibm)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-30 20:50:30
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33427/files) (1):**
  - `vllm/v1/attention/backends/mla/triton_mla.py`

### Summary

**What changed and why**  
The change replaces an uncertain comment about a `+1` dimension in tensor allocation with a precise explanation. The extra dimension stores LogSumExp (LSE) values computed by the stage1 kernel, which the stage2 kernel uses to merge partial attention outputs across KV splits.

**Technical impact**  
This clarifies the tensor shape's purpose without altering functionality, improving code maintainability. Developers working on the multi-head latent attention (MLA) backend will now understand the tensor's structure and how LSE values facilitate cross-split attention merging.

**Potential risks**  
No functional risks exist since only comments were updated. However, if the explanation is inaccurate or incomplete, future modifications could misinterpret the tensor's usage, potentially leading to bugs in attention computation or split-merging logic.

**Key insights**  
Always document unclear code constructs to aid collaboration and reduce technical debt. When referencing external code (like sglang), verify the rationale to ensure comments are accurate and actionable for the current codebase.

---

## 9. [[Voxtral Streaming -> Voxtral Realtime] Rename all voxtral related classes, fn, files](https://github.com/vllm-project/vllm/pull/33415)


### Base Information

- **PR Number:** #33415
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 20:49:01
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33415/files) (6):**
  - `docs/serving/openai_compatible_server.md`
  - `tests/models/multimodal/generation/test_voxtral_realtime.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/voxtral_realtime.py`
  - `vllm/transformers_utils/configs/mistral.py`

### Summary

**What changed and why**  
This PR renames all Voxtral-related classes, functions, and files from using "streaming" terminology to "realtime" terminology. The change aims to reduce confusion since "streaming" is an overloaded term in the codebase. Additionally, it removes inline code samples from the OpenAI-compatible server documentation and replaces them with links to example client files.

**Technical impact**  
The changes affect model class names (`VoxtralStreamingGeneration` → `VoxtralRealtimeGeneration`), processor classes (`VoxtralStreamingMultiModalProcessor` → `VoxtralRealtimeMultiModalProcessor`), configuration mappings, and test files. This ensures consistent terminology across the codebase for real-time audio processing features. The documentation update centralizes example code in dedicated files, improving maintainability.

**Potential risks**  
Renaming could break any external integrations or scripts that directly reference the old class names. The model registry changes must align with actual model implementations to avoid runtime errors. There is also a risk if any references were missed in the rename, though the changes appear comprehensive across key files.

**Key insights**  
Developers should update any code or configurations that reference the old "streaming" class names. The documentation change promotes cleaner, more maintainable examples by linking to centralized client implementations. Ensure all team members are aware of the terminology shift to maintain consistency in future development.

---

## 10. [[ROCm][CI] Force max_num_seqs=1 on ROCm In test_sharded_state_loader to reduce flakiness](https://github.com/vllm-project/vllm/pull/33277)


### Base Information

- **PR Number:** #33277
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 20:28:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33277/files) (1):**
  - `tests/model_executor/model_loader/test_sharded_state_loader.py`

### Summary

**What changed and why**  
The change adds platform-specific arguments to force `max_num_seqs=1` when running on ROCm (AMD) platforms in the `test_sharded_state_loader` test. This addresses flaky test failures in AMD CI caused by batch variance, with local validation showing the test passes consistently (20/20 runs) with this modification.

**Technical impact**  
This change only affects test execution on ROCm platforms by limiting batch processing to single sequences. It introduces a conditional platform check that adds the `max_num_seqs` parameter to the LLM initialization arguments, while maintaining default behavior on other platforms (CUDA). The test architecture remains unchanged.

**Potential risks**  
The fix addresses symptoms (flakiness) rather than root causes of batch variance on ROCm. There's a risk this could mask underlying platform-specific issues in the sharded state loader implementation. The conditional logic creates platform-dependent test behavior, which could complicate future debugging if similar issues arise on other platforms.

**Key insights**  
This is a targeted workaround for CI stability, not a general solution. Developers should monitor whether similar batch variance issues appear in production ROCm deployments. Consider investigating the root cause of ROCm-specific batch variance in the sharded state loader implementation for a more robust long-term fix. The pattern of platform-specific test arguments could be useful for other platform-dependent workarounds if documented appropriately.

---

## 11. [[Misc] offest -> offset in comments and variable names](https://github.com/vllm-project/vllm/pull/33444)


### Base Information

- **PR Number:** #33444
- **Author:** [russellb](https://github.com/russellb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 20:19:22
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33444/files) (3):**
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/engine/detokenizer.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
This PR fixes a common spelling error where "offset" was misspelled as "offest" across three files. The changes correct a comment typo in `mamba2_attn.py`, rename a variable from `read_offest` to `read_offset` in `detokenizer.py`, and rename `token_offests` to `token_offsets` in `eagle.py`. The primary goal is to improve code clarity and consistency.

**Technical impact**  
The correction in `detokenizer.py` is the only change with potential behavioral impact, as it ensures the variable `self.read_offset` is properly initialized and referenced. The other changes are purely cosmetic, affecting only comments and variable names, with no functional alterations to the code logic or runtime behavior.

**Potential risks**  
There is minimal risk since the changes are straightforward spelling corrections. However, if any external code or dependencies rely on the misspelled variable name `read_offest` (e.g., via reflection or dynamic access), it could cause runtime errors. This is unlikely given the scope of the changes.

**Key insights**  
Maintaining consistent naming conventions enhances code readability and reduces confusion. Developers should ensure that all references to the renamed variable are updated, though the PR appears comprehensive. This type of cleanup is low-risk but contributes to long-term codebase maintainability.

---

## 12. [[BugFix] Fix whisper FA2 + full cudagraphs](https://github.com/vllm-project/vllm/pull/33360)


### Base Information

- **PR Number:** #33360
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 20:15:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33360/files) (2):**
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The fix addresses a CUDA graph capture issue in encoder-decoder models using FlashAttention2. The problem occurred because `CrossAttentionBuilder.build()` was incorrectly setting `max_seq_len` to zero during graph capture when encoder sequence lengths were zero-padded, leading to invalid graphs. The solution removes the blanket CUDA graph disablement for FA2 encoder-decoder models and instead ensures realistic encoder lengths are used during capture.

**Technical impact**  
This change enables CUDA graph support for encoder-decoder models with FlashAttention2 by fixing the underlying capture mechanism. The system now properly initializes encoder sequence lengths with a realistic maximum (from `max_source_positions` or `max_encoder_len`) during graph capture, allowing correct kernel specialization and graph construction while maintaining zero-padding for actual execution.

**Potential risks**  
If `max_source_positions` or `max_encoder_len` doesn't accurately represent typical encoder lengths, it could lead to suboptimal graph specialization or memory usage. There's also a risk that the original accuracy issues mentioned in #33091 might resurface since the workaround (disabling graphs) was removed rather than fixing the root cause in attention kernels.

**Key insights**  
The fix demonstrates that the issue was in graph capture initialization, not fundamental incompatibility. Developers should verify that `max_encoder_len` is properly configured for their models. Consider adding validation to ensure captured graphs match execution patterns, and monitor for any regression in accuracy or performance with encoder-decoder FA2 graphs.

---

## 13. [[UX] Use gguf `repo_id:quant_type` syntax for examples and docs](https://github.com/vllm-project/vllm/pull/33371)


### Base Information

- **PR Number:** #33371
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 20:14:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33371/files) (4):**
  - `docs/features/quantization/gguf.md`
  - `examples/offline_inference/basic/README.md`
  - `tests/transformers_utils/test_utils.py`
  - `vllm/transformers_utils/gguf_utils.py`

### Summary

**What changed and why**  
This PR enhances GGUF model loading by extending support for the `repo_id:quant_type` syntax to include extended quantization naming conventions (e.g., `Q4_K_M`, `Q3_K_S`). The changes update documentation, examples, and test files to promote this simplified loading method directly from HuggingFace, replacing previous instructions that required manual file downloads.

**Technical impact**  
The core change is in `gguf_utils.py` where `is_valid_gguf_quant_type()` now recognizes extended quantization types by checking if the base type (e.g., `Q4_K`) is a valid `GGMLQuantizationType` and the suffix matches predefined patterns (`_M`, `_S`, `_L`, `_XL`, `_XS`, `_XXS`). This improves user experience by supporting common GGUF file naming conventions without requiring code changes for each variant.

**Potential risks**  
The suffix matching logic could potentially match unintended strings if future quantization types include these suffixes as part of their base names. There's also a risk that users might incorrectly assume all extended suffixes are supported for all base types, though the validation correctly checks base type existence. The error message update helps clarify valid formats.

**Key insights**  
This is a user-facing improvement that simplifies GGUF model loading. Developers should note that the system now supports both exact GGML quant types and extended conventions. The updated documentation consistently uses the new `unsloth/Qwen3-0.6B-GGUF:Q4_K_M` format as the recommended approach, moving away from manual file downloads. Test coverage has been appropriately expanded to validate the new functionality.

---

## 14. [[Misc] Algin Qwen3-VL-embedding image example outputs with HF repo example](https://github.com/vllm-project/vllm/pull/33419)


### Base Information

- **PR Number:** #33419
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 19:36:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33419/files) (1):**
  - `examples/pooling/embed/vision_embedding_offline.py`

### Summary

**What changed and why**  
The PR aligns vLLM's Qwen3-VL-Embedding example with Hugging Face's implementation by conditionally using `qwen_vl_utils.smart_resize` for image preprocessing. This ensures consistent image resizing behavior between the two frameworks, eliminating output discrepancies that could confuse users.

**Technical impact**  
The changes modify the example script to optionally apply the same resizing logic as HF when the `qwen-vl-utils` package is installed. When unavailable, a warning is shown, and images are processed without automatic resizing. The `mm_processor_kwargs={"do_resize": False}` is passed to the engine when smart_resize is used to prevent double resizing.

**Potential risks**  
If users install `qwen-vl-utils` but the package version changes its resizing behavior, outputs may still diverge from HF. The warning about missing dependencies might be overlooked in automated environments. There's also a risk of incorrect image dimensions if the resizing logic doesn't match the model's expected input format exactly.

**Key insights**  
This change prioritizes consistency with HF's reference implementation, which is crucial for reproducibility. Developers should ensure `qwen-vl-utils` is installed in production if matching HF outputs is required. The conditional approach maintains backward compatibility but adds complexity—consider making this dependency explicit in documentation.

---

## 15. [[ModelRunner V2] Fix spec decoding + logprobs](https://github.com/vllm-project/vllm/pull/33391)


### Base Information

- **PR Number:** #33391
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-30 19:33:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33391/files) (7):**
  - `tests/v1/engine/test_output_processor.py`
  - `vllm/v1/engine/logprobs.py`
  - `vllm/v1/outputs.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/logprob.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The changes fix logprobs selection for sampled/accepted tokens when using expanded tensors in ModelRunner V2. The core issue is ensuring correct logprobs mapping when the batch contains requests with different sequence lengths, requiring proper cumulative token counting (`cu_num_logits`) to select the right logprobs for each request.

**Technical impact**  
These modifications propagate cumulative token counts through the sampling pipeline—from the model runner to the logprobs computation. The `LogprobsTensors` class now stores `cu_num_generated_tokens` to handle non-uniform batch sizes, and the sampler uses this data to correctly index into expanded logits tensors. This ensures logprobs accuracy in batched inference with variable-length sequences.

**Potential risks**  
The `filter` method now asserts `cu_num_generated_tokens` is `None`, which could cause failures if filtering is attempted on batches with cumulative counts. Additionally, the new `cu_num_logits` parameter defaults to `None` in `compute_topk_logprobs`, so callers must ensure it’s provided when needed to avoid silent errors in expanded tensor scenarios.

**Key insights**  
Developers should verify that all code paths handling expanded tensors properly pass `cu_num_logits` through the sampling stack. The `filter` restriction means alternative approaches may be needed for post-processing filtered logprobs with cumulative counts. Testing should focus on edge cases with mixed-length requests to confirm logprobs alignment.

---

## 16. [[Attention] Move MLA `forward` from backend to layer](https://github.com/vllm-project/vllm/pull/33284)


### Base Information

- **PR Number:** #33284
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 19:30:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33284/files) (13):**
  - `tests/v1/attention/test_mla_backends.py`
  - `tests/v1/attention/test_sparse_mla_backends.py`
  - `vllm/model_executor/layers/attention/attention.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/mla/cutlass_mla.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/attention/backends/mla/flashinfer_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/backends/mla/triton_mla.py`

### Summary

**What changed and why**  
This PR refactors MLA (Multi-Head Latent Attention) by moving the `forward` method from backend implementations to the layer (`MLAAttention`). The primary goal is to facilitate prefill-decode splitting by centralizing the forward logic in the layer, which now calls separate `forward_mha` (prefill) and `forward_mqa` (decode) methods on the backend implementations. This change also introduces a new `SparseMLAAttentionImpl` class for sparse MLA backends that only support decode-style attention.

**Technical impact**  
The architecture shifts from a backend-driven forward method to a layer-driven approach, where `MLAAttention.forward_impl` orchestrates prefill and decode paths. Backend implementations now expose `forward_mha` and `forward_mqa` (or just `forward_mqa` for sparse variants) instead of a monolithic `forward`. This improves separation of concerns and enables more flexible handling of prefill/decode splitting. The changes also affect testing, as mock layers now replicate the layer’s forward logic.

**Potential risks**  
The refactor touches multiple backend implementations and test files, increasing the risk of integration errors or behavioral discrepancies. Sparse MLA implementations now rely solely on `forward_mqa`, which may limit future support for prefill in sparse contexts. The introduction of new base classes (`AttentionImplBase`, `SparseMLAAttentionImpl`) adds complexity to the inheritance hierarchy, and any misalignment in method signatures could break downstream backends.

**Key insights**  
Developers should note that MLA backends must now implement `forward_mha` and `forward_mqa` (or inherit from `SparseMLAAttentionImpl`). The layer’s `forward_impl` handles the prefill/decode split and weight transformations (e.g., `W_UK_T`, `W_UV`), reducing duplication. Testing has been updated to use mock layers that mirror this logic, ensuring backend correctness. When extending MLA, ensure new backends adhere to the updated interface and consider sparse vs. dense implementation requirements.

---

## 17. [[Deprecation] Deprecate `seed_everything` and `scatter_mm_placeholders` in v0.15](https://github.com/vllm-project/vllm/pull/33362)


### Base Information

- **PR Number:** #33362
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-30 18:54:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33362/files) (3):**
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/platforms/interface.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
This PR removes deprecated code scheduled for deletion in v0.15. Specifically, it deletes the deprecated `seed_everything` method from the platform interface and the `scatter_mm_placeholders`/`gather_mm_placeholders` functions from worker utilities. Additionally, it removes two commented-out properties related to transcript tokens in the Voxtral model.

**Technical impact**  
The removal reduces codebase complexity and maintenance overhead by eliminating deprecated APIs. Users who were still calling `seed_everything` must now migrate to `vllm.utils.torch_utils.set_random_seed`, while any remaining usage of the placeholder scattering/gathering functions will break. The commented-out code removal is a cleanup with no functional impact.

**Potential risks**  
If any downstream code still references the removed deprecated functions, it will result in `AttributeError` or `ImportError` after upgrading to v0.15. The risk is mitigated by the deprecation warnings that were previously in place, but developers should verify that no internal or external dependencies rely on these APIs.

**Key insights**  
This is a routine cleanup following a deprecation cycle. Developers should ensure all uses of `seed_everything` and the multimodal placeholder utilities are updated before upgrading. The removal of commented-out code is a positive step toward maintaining a clean codebase, though it may obscure historical context if needed for future reference.

---

## 18. [[Bugfix] Fix typo in read_offset variable name](https://github.com/vllm-project/vllm/pull/33426)


### Base Information

- **PR Number:** #33426
- **Author:** [bet0x](https://github.com/bet0x)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-30 17:26:15
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33426/files) (1):**
  - `vllm/v1/engine/detokenizer.py`

### Summary

**What changed and why**  
Fixed a typo in a variable name from `read_offest` to `read_offset` in the `Detokenizer` class initialization. This corrects a misspelling that likely caused issues with prompt embedding functionality, as referenced in issue #33294.

**Technical impact**  
The change ensures consistency in variable naming and resolves potential runtime errors or incorrect behavior when `read_offset` is accessed elsewhere in the codebase. It aligns the variable with its intended purpose in tracking read positions during detokenization.

**Potential risks**  
If other parts of the codebase still reference the old variable name `read_offest`, they may raise `AttributeError` or cause silent failures. The fix is isolated, but dependencies on the old variable should be verified to ensure full compatibility.

**Key insights**  
Always use consistent naming conventions to avoid subtle bugs. Developers should search for any remaining references to `read_offest` and update them accordingly. This fix highlights the importance of thorough code reviews to catch typos early.

---

## 19. [[Bugfix][ROCm] Fixing the skinny gemm dispatch logic from #32831](https://github.com/vllm-project/vllm/pull/33366)


### Base Information

- **PR Number:** #33366
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-01-30 17:05:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33366/files) (4):**
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/rocm.py`
  - `vllm/model_executor/layers/utils.py`

### Summary

**What changed and why**  
This PR fixes two issues from a previous change (#32831) related to ROCm skinny GEMM dispatch logic. First, it ensures that when the wvsplitk kernel isn't applicable, the fallback uses hipblaslt via `torch._scaled_mm` instead of cloning and modifying input tensors, addressing a +100% performance regression in FP8 models. Second, it corrects the condition for tensor contiguity to only apply to activation tensors, since weight tensors can be non-contiguous (especially in FP8 cases where they're explicitly padded).

**Technical impact**  
The changes remove the blanket tensor contiguity enforcement that was causing unnecessary tensor cloning, improving performance for smaller batch sizes. The dispatch logic now properly distinguishes between activation and weight tensor requirements, allowing non-contiguous weight tensors while ensuring activation tensors remain contiguous for kernel compatibility. Test coverage is expanded to validate both padded and unpadded weight scenarios.

**Potential risks**  
The selective application of contiguity checks could lead to inconsistent behavior if other kernels have different requirements. There's a risk that future changes might incorrectly assume all skinny GEMM kernels handle non-contiguous tensors. The performance improvement might mask underlying issues with the wvsplitk kernel's tensor handling limitations.

**Key insights**  
Developers should note that only activation tensors require contiguity checks for these kernels. The performance regression fix demonstrates the importance of proper fallback mechanisms in kernel dispatch logic. When working with FP8 weights, expect padding that creates non-contiguous tensors, and ensure kernel selection logic accounts for this. The test expansion provides valuable coverage for both padded and unpadded weight scenarios.

---

## 20. [Refactor NVFP4 Linear utils for ModelOpt and CT](https://github.com/vllm-project/vllm/pull/33201)


### Base Information

- **PR Number:** #33201
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 16:37:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33201/files) (12):**
  - `tests/kernels/moe/modular_kernel_tools/mk_objects.py`
  - `tests/quantization/test_compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_mxfp4.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py`
  - `vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py`
  - `vllm/model_executor/layers/quantization/utils/nvfp4_utils.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`

### Summary

**What changed and why**  
This PR consolidates NVFP4 linear kernel backends and preparation logic into shared utilities (`nvfp4_utils.py`), eliminating duplicate code between ModelOpt and Compressed-Tensors quantization methods. The refactor centralizes backend selection, weight preparation, and kernel application logic while standardizing parameter naming across both quantization schemes.

**Technical impact**  
The changes create a unified NVFP4 linear layer implementation that both ModelOpt and Compressed-Tensors can reuse. This reduces code duplication by ~300 lines and establishes consistent parameter naming (e.g., `weight_global_scale` instead of `weight_scale_2`). The architecture now uses a standardized backend selection mechanism with proper validation and supports multiple backends (CUTLASS, FlashInfer variants, FBGEMM, Marlin, and emulation).

**Potential risks**  
The refactor changes parameter names and processing logic, which could break existing serialized models or custom integrations. The removal of platform capability checks in Compressed-Tensors (`cutlass_fp4_supported()`) might cause runtime errors on unsupported hardware. Additionally, the flashinfer-cudnn backend appears broken in main (as noted in test results), and the changes don't address this issue.

**Key insights**  
Developers should verify that their NVFP4 models load correctly after this change due to parameter renaming. The unified `nvfp4_utils` module provides a cleaner abstraction but requires thorough testing across all supported backends. Note that the minimum capability for Compressed-Tensors NVFP4 was lowered from 100 to 75, potentially enabling broader hardware support but requiring careful validation.

---

## 21. [[CI][HPU]accelerate hpu test by skip python re-install and clean container name](https://github.com/vllm-project/vllm/pull/33286)


### Base Information

- **PR Number:** #33286
- **Author:** [xuechendi](https://github.com/xuechendi)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-01-30 13:36:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33286/files) (1):**
  - `.buildkite/scripts/hardware_ci/run-hpu-test.sh`

### Summary

**What changed and why**  
This PR accelerates HPU (Habana Processing Unit) testing by modifying the Docker build process to skip PyTorch reinstallation and adding unique identifiers to Docker image and container names. The changes use the existing PyTorch from the base container and append the build commit ID to prevent naming conflicts.

**Technical impact**  
The Docker image now installs vLLM dependencies without rebuilding PyTorch, significantly reducing build time. Unique image and container names based on `BUILDKITE_COMMIT` enable parallel test execution without conflicts. The test command is simplified to run a specific inference example with a timeout instead of executing a comprehensive test script.

**Potential risks**  
The simplified test (`examples/offline_inference/basic/generate.py`) may not provide sufficient coverage compared to the previous comprehensive test script. Removing `--no-build-isolation` from the vLLM installation could cause dependency conflicts. The 120-second timeout may be insufficient for some test scenarios, potentially causing false failures.

**Key insights**  
The optimization effectively reduces CI build times by leveraging pre-installed PyTorch. Developers should verify that the simplified test provides adequate coverage for HPU functionality. Consider adding back some critical tests or creating a more targeted test suite that balances speed and coverage. The unique naming convention is a good practice for parallel CI execution.

---

## 22. [Indicate compile mode in the benchmark results](https://github.com/vllm-project/vllm/pull/32990)


### Base Information

- **PR Number:** #32990
- **Author:** [huydhn](https://github.com/huydhn)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-30 12:34:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32990/files) (2):**
  - `.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh`
  - `vllm/benchmarks/lib/utils.py`

### Summary

**What changed and why**  
This PR adds compile mode tracking to vLLM benchmark results by extracting `compilation_config.mode` and `optimization_level` from server parameters and passing them as metadata. It also implements logic to detect whether the benchmark runs in eager mode (compile disabled) or compile mode, storing this as a boolean `use_compile` field for dashboard compatibility.

**Technical impact**  
The changes enhance benchmark result metadata by including compilation configuration details, enabling performance comparisons between eager and compile modes. The `use_compile` boolean aligns with PyTorch's HUD dashboard field, ensuring consistent reporting. The modifications are backward-compatible, as missing fields default to empty strings or false.

**Potential risks**  
String-based detection (e.g., checking for "eager" in filenames) could be fragile if naming conventions change. The `extract_field` function assumes nested attribute access via dot notation, which may fail if intermediate attributes are missing or non-object types. Default empty string returns might obscure configuration errors.

**Key insights**  
Centralizing compilation mode detection in `use_compile()` improves maintainability. Consider validating server parameter extraction to catch JSON parsing issues early. Future updates should document the `use_compile` logic to prevent accidental breakage of dashboard integration.

---

## 23. [[Hardware][SM100] Add TRTLLM Kernel for INT4 W4A16 Kernel.](https://github.com/vllm-project/vllm/pull/32437)


### Base Information

- **PR Number:** #32437
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 10:30:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32437/files) (6):**
  - `tests/kernels/moe/test_marlin_vs_trtllm_mxint4.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_mxint4_moe.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR adds INT4 W4A16 MoE kernels from TensorRT-LLM (TRT-LLM) via FlashInfer, specifically for SM100 (Hopper) GPUs. The changes introduce a new environment variable `VLLM_USE_FLASHINFER_MOE_INT4=1` to enable the kernel, along with corresponding weight preparation, quantization utilities, and integration into the compressed‑tensors MoE layer. A comprehensive test compares the new TRT‑LLM kernel against the existing Marlin INT4 kernel.

**Technical impact**  
The addition provides an alternative high‑performance INT4 MoE kernel for Hopper GPUs, leveraging FlashInfer’s fused‑MoE implementation. The kernel is integrated as a backend option in `CompressedTensorsWNA16MarlinMoEMethod`, where it is selected automatically when conditions are met (group_size=32, num_bits=4, SM100, and the environment variable is set). This changes weight layout handling (transposition logic) and adds a monolithic execution path that bypasses separate routing kernels.

**Potential risks**  
- The kernel is restricted to SM100 (Hopper) GPUs and requires FlashInfer with TRT‑LLM support, limiting compatibility.  
- Weight‑layout differences between Marlin and TRT‑LLM backends could cause subtle bugs if not handled consistently across all code paths (e.g., the `is_transposed` logic in `layer.py`).  
- The test uses specific model parameters (DeepSeekV3 routing) and may not cover all MoE configurations or quantization variants.

**Key insights**  
- Developers must ensure the environment variable `VLLM_USE_FLASHINFER_MOE_INT4` is set to use the new kernel; otherwise, the system falls back to Marlin.  
- The PR introduces a clear separation between “Flashinfer” and “Marlin” backends via `kernel_backend`, which affects weight shapes and transposition—care is needed when extending or modifying these paths.  
- The added test provides a valuable reference for validating INT4 MoE kernel correctness across different implementations, but should be expanded to cover more diverse shapes and routing methods.

---

## 24. [[Quantization][ROCm] Fix MoE weight loading to be robust (Qwen3_MoE/Qwen3_next as example models)](https://github.com/vllm-project/vllm/pull/33173)


### Base Information

- **PR Number:** #33173
- **Author:** [xuebwang-amd](https://github.com/xuebwang-amd)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-30 09:50:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33173/files) (1):**
  - `vllm/model_executor/layers/fused_moe/layer.py`

### Summary

**What changed and why**  
The fix addresses a RuntimeError when loading quantized MoE (Mixture of Experts) weights for Qwen3_MoE/Qwen3_next models. The error occurred because `narrow()` was called on a 0-dimensional scalar tensor during weight sharding. The changes add a check to ensure `narrow()` is only applied to non-scalar tensors when loading partial weights.

**Technical impact**  
This modification makes weight loading robust for quantized MoE models, particularly under tensor parallelism (TP) scenarios. It ensures scalar tensors (e.g., scaling factors) are handled correctly without attempting dimension-based operations, maintaining compatibility with FP8 and MXFP4 quantization schemes.

**Potential risks**  
If the condition `loaded_weight.ndim > 0` is incorrectly applied, it might skip necessary narrowing for valid multi-dimensional tensors, leading to weight misalignment. Additionally, the fix assumes scalar tensors should never be sharded, which may not hold for all future quantization formats or model architectures.

**Key insights**  
Always validate tensor dimensions before applying dimension-specific operations like `narrow()`. This fix highlights the importance of defensive programming in weight loading logic, especially for quantized and distributed models. Consider adding unit tests for edge cases involving scalar tensors in MoE layers.

---

## 25. [fix QERL attention import path](https://github.com/vllm-project/vllm/pull/33432)


### Base Information

- **PR Number:** #33432
- **Author:** [vkuzo](https://github.com/vkuzo)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 09:29:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33432/files) (1):**
  - `vllm/model_executor/model_loader/reload/layerwise.py`

### Summary

**What changed and why**  
The PR fixes an incorrect import path for `Attention` and `MLAAttention` classes in `layerwise.py`. The import was updated from `vllm.attention.layer` to `vllm.model_executor.layers.attention` to align with a previous refactoring in PR #32064, which PR #32133 missed during rebase.

**Technical impact**  
This change ensures the module correctly imports attention classes from their new location, preventing import failures that would break quantization tests (specifically `test_fp8.py`). It maintains consistency with the updated codebase structure introduced by the earlier refactoring.

**Potential risks**  
If other files still reference the old import path, they may encounter similar import errors. Additionally, any downstream code relying on the old module structure could be affected, though this is unlikely given the refactoring was already applied elsewhere.

**Key insights**  
Developers should verify that all attention-related imports across the codebase use the new path to avoid hidden import issues. This fix highlights the importance of thorough rebasing when multiple PRs modify overlapping code areas, especially during structural refactors.

---

## 26. [[Kernel] [Helion] [1/N] Add Helion ConfigManager](https://github.com/vllm-project/vllm/pull/32740)


### Base Information

- **PR Number:** #32740
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-30 09:19:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32740/files) (4):**
  - `tests/kernels/helion/test_config_manager.py`
  - `vllm/kernels/__init__.py`
  - `vllm/kernels/helion/__init__.py`
  - `vllm/kernels/helion/config_manager.py`

### Summary

**What changed and why**  
Added a new configuration management system for Helion kernels with ConfigManager (singleton) and ConfigSet classes. This provides centralized JSON file management for GPU platform-specific kernel configurations, enabling structured storage and retrieval of Helion config parameters like block sizes and warp counts.

**Technical impact**  
Introduces a singleton-based file I/O layer that abstracts configuration persistence for Helion operations. The architecture uses a two-level hierarchy (platform → config_key) and integrates with the existing `helion.Config` class. This modularizes configuration handling and establishes patterns for future kernel implementations.

**Potential risks**  
Singleton pattern may cause issues in multi-process or dynamic reloading scenarios. File I/O errors (permissions, corruption) are caught but only logged, potentially leading to silent failures. The `get_platform_configs` method loads the entire file but only returns one platform’s configs, which could be inefficient.

**Key insights**  
The design effectively separates in-memory (ConfigSet) and file-level (ConfigManager) concerns. Ensure thread safety if ConfigManager is accessed concurrently. Consider adding validation for config_key naming conventions. The extensive test suite (+361 lines) provides strong coverage for core functionality and error cases.

---

## 27. [Fix encoder-decoder model disabling mm processor cache](https://github.com/vllm-project/vllm/pull/33236)


### Base Information

- **PR Number:** #33236
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 08:30:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33236/files) (2):**
  - `tests/models/multimodal/processing/test_common.py`
  - `vllm/config/model.py`

### Summary

**What changed and why**  
The fix addresses an issue where `mm_processor_cache_gb` was incorrectly being set as an instance attribute on `ModelConfig` when it should be treated as an initialization-only variable (`InitVar`). For encoder-decoder models, the intention was to disable the multimodal processor cache by setting it to 0, but the assignment `self.mm_processor_cache_gb = 0` had no lasting effect. The correction ensures the value is passed correctly to `MultiModalConfig`.

**Technical impact**  
This change ensures that encoder-decoder models properly disable the multimodal processor cache by passing `mm_processor_cache_gb = 0` during initialization rather than storing it. The test update reflects this by setting the cache size on the `multimodal_config` object after `ModelConfig` initialization, confirming the cache can be configured as intended.

**Potential risks**  
If `mm_processor_cache_gb` is accessed elsewhere in `ModelConfig` after initialization, it may now be undefined or have an unexpected value, potentially causing `AttributeError`. The test modification assumes direct access to `multimodal_config` is safe, but this could break if the internal structure changes.

**Key insights**  
Always verify the role of `InitVar` attributes—they are not stored as instance variables. Developers should review other uses of `InitVar` in the codebase to ensure similar patterns are handled correctly. The test update provides a clear pattern for post-initialization configuration of dependent objects.

---

## 28. [[CI] Qwen3-ASR transcriptios tests](https://github.com/vllm-project/vllm/pull/33414)


### Base Information

- **PR Number:** #33414
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 08:17:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33414/files) (1):**
  - `tests/entrypoints/openai/test_transcription_validation.py`

### Summary

**What changed and why**  
The PR adds CI tests for the Qwen3-ASR model by including it in two existing test parameterizations. It modifies the test assertions to use more flexible text matching (removing a trailing comma) and updates the expected text for the `foscolo` audio sample test.

**Technical impact**  
This expands test coverage to include the Qwen3-ASR-0.6B model alongside existing models (Voxtral-Mini and Gemma). The test logic remains unchanged; only the model list and assertion strings are updated, maintaining the existing test structure and validation patterns.

**Potential risks**  
The assertion change from `"Mary had a little lamb,"` to `"Mary had a little lamb"` (removing the comma) could make the test less precise, potentially allowing incorrect transcriptions to pass. The updated assertion for the `foscolo` test uses a completely different phrase, which requires validation that the new phrase is indeed the correct transcription for that audio sample.

**Key insights**  
Ensure the updated assertion strings accurately reflect the expected model outputs for both audio samples. Consider if the comma removal in the first assertion is intentional for model variance or if it should remain for stricter validation. Verify the new `foscolo` phrase is the correct expected transcription.

---

## 29. [Support FP8 block quant for CompressedTensorsW8A16Fp8](https://github.com/vllm-project/vllm/pull/33280)


### Base Information

- **PR Number:** #33280
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-30 08:15:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33280/files) (4):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`

### Summary

**What changed and why**  
The changes add support for block FP8 quantization in the `CompressedTensorsW8A16Fp8` scheme, which was previously missing. This enables models using block FP8 quantization (e.g., `RedHatAI/Qwen3-0.6B-FP8-BLOCK`) to run on GPUs without hardware FP8 support by leveraging Marlin kernels. The fix updates the scheme to handle block quantization similarly to the existing FP8 implementation.

**Technical impact**  
The `CompressedTensorsW8A16Fp8` class now accepts a `weight_quant` object instead of just a strategy string, allowing access to block size metadata. It uses shared FP8 utility functions (`create_fp8_scale_parameter`, `process_fp8_weight_block_strategy`) for consistency and adds block-specific weight processing. The changes also refactor scale parameter creation to unify handling across tensor, channel, and block strategies.

**Potential risks**  
If `weight_quant.block_structure` is `None` for block strategy, it could cause assertion failures. The block quantization path assumes dynamic input scaling (`is_static_input_scheme=False`), which may not hold for all models. Additionally, the refactored weight transposition logic in `process_weights_after_loading` must correctly handle all strategy types to avoid kernel compatibility issues.

**Key insights**  
The PR successfully unifies FP8 quantization support across schemes by reusing utility functions. Developers should ensure that any new quantization strategies are added to the `strategy_to_parameter_type` mapping. The removal of `set_weight_attrs` in `fp8.py` is compensated by its addition in `fp8_utils.py`, maintaining metadata for scale parameters.

---

## 30. [[QeRL] Layerwise Reloading](https://github.com/vllm-project/vllm/pull/32133)


### Base Information

- **PR Number:** #32133
- **Author:** [kylesayrs](https://github.com/kylesayrs)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-30 07:50:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32133/files) (17):**
  - `tests/conftest.py`
  - `tests/model_executor/model_loader/test_reload.py`
  - `tests/quantization/test_torchao.py`
  - `tests/v1/worker/test_gpu_model_runner.py`
  - `vllm/model_executor/model_loader/online_quantization.py`
  - `vllm/model_executor/model_loader/reload/__init__.py`
  - `vllm/model_executor/model_loader/reload/layerwise.py`
  - `vllm/model_executor/model_loader/reload/meta.py`
  - `vllm/model_executor/model_loader/reload/sanitize.py`
  - `vllm/model_executor/model_loader/reload/torchao_decorator.py`
  - `vllm/model_executor/model_loader/reload/types.py`
  - `vllm/model_executor/model_loader/reload/utils.py`
  - `vllm/model_executor/model_loader/utils.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/model_executor/parameter.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR introduces layerwise weight reloading for quantized models, replacing the previous torchao-specific online quantization implementation. It enables reloading weights from checkpoints into already-initialized quantized models (including INT4, FP8_BLOCK, FP8_DYNAMIC, NVFP4_A16) while reducing memory overhead and improving maintainability. The changes include a new reload module with metadata capture, deferred processing, and support for arbitrary kernel processing.

**Technical impact**  
The architecture shifts from model-wide weight reloading to a layerwise approach, tracking metadata via a global weak-key dictionary (`LAYERWISE_INFO`). This allows quantized models to restore to a load-ready state, cache weights, and process them per-layer after all weights are loaded. The reload logic now integrates with `process_weights_after_loading` and supports multiple quantization schemes, not just torchao. The `reload_weights` API is extended to accept either a weights iterator or a path, with optional checkpoint format control.

**Potential risks**  
1. Untested edge cases: CPU offloading, Attention/MLA quantization, and tied parameter handling are not yet supported (per module docstring).  
2. Memory leaks could occur if layer references are not properly sanitized in `LAYERWISE_INFO` (though weak references mitigate this).  
3. Weight padding or qkv consolidation in future quantization methods may break reloading due to mismatched weight counts (Limitation 4).  
4. The `is_checkpoint_format=False` path assumes weights are already sharded and in kernel format, which may cause errors if misused.

**Key insights**  
1. The layerwise approach significantly reduces memory usage (3x for FP8) and improves flexibility for kernel processing.  
2. Developers should use the new public functions (`record_metadata_for_reloading`, `initialize_layerwise_reload`, etc.) for reloading integrations.  
3. Ensure all weight loaders are wrapped correctly; attention layer processing is deferred until `finalize_layerwise_reload`.  
4. The removal of `online_quantization.py` simplifies the codebase but requires updating any dependent custom quantization logic.

---

## 31. [[BugFix][LoRA] TritonExperts is ModularMoEPath for FP8 models](https://github.com/vllm-project/vllm/pull/33393)


### Base Information

- **PR Number:** #33393
- **Author:** [dcmaddix](https://github.com/dcmaddix)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-30 07:27:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33393/files) (1):**
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
The change modifies an assertion in LoRA injection logic for fused MoE layers to allow `TritonExperts` as a valid expert type for FP8 models. Previously, the assertion required FP8 models to use either `MarlinExperts` or `TritonExperts`, but now it only requires `TritonExperts`.

**Technical impact**  
This fixes a bug where FP8 models using `TritonExperts` would fail the assertion during LoRA injection. The change aligns the validation logic with the stated purpose that FP8 models should use `TritonExperts` as the `ModularMoE` implementation, while MXFP4 models continue to support `Marlin` and `UnfusedOAITritonExperts`.

**Potential risks**  
If there are FP8 model configurations that legitimately use `MarlinExperts`, this change could break them by no longer allowing that expert type. The assertion change assumes `TritonExperts` is the only valid expert for FP8, which might not cover all edge cases or future implementations.

**Key insights**  
The fix is minimal and targeted, but developers should verify that all FP8 model paths indeed use `TritonExperts`. Consider adding a comment explaining why `MarlinExperts` is excluded for FP8, or if support is needed later, the condition may need to be revisited. Ensure comprehensive testing of FP8 models with LoRA to confirm the fix resolves the assertion issue without regressions.

---

## 32. [Disable Cascade Attention for Batch Invariance](https://github.com/vllm-project/vllm/pull/32561)


### Base Information

- **PR Number:** #32561
- **Author:** [frankwang28](https://github.com/frankwang28)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-30 07:00:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32561/files) (6):**
  - `tests/v1/determinism/test_batch_invariance.py`
  - `tests/v1/determinism/utils.py`
  - `vllm/config/vllm.py`
  - `vllm/model_executor/layers/batch_invariant.py`
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/layers/utils.py`

### Summary

**What changed and why**  
This PR disables cascade attention when batch invariance is enabled, as its conditional activation on certain input batches caused numerical differences in output logprobs. It also updates the batch invariance test suite with more varied requests and adjusts padding logic for longer prompts to improve test coverage.

**Technical impact**  
The changes enforce deterministic behavior by preventing cascade attention from introducing variability when batch invariance is active. Modifications to the test suite increase scenario diversity and better exercise chunked prefill logic, while configuration updates ensure consistent attention backend selection across runs.

**Potential risks**  
Disabling cascade attention may impact performance optimizations that rely on it. The temporary removal of FlashInfer support reduces backend options and could affect users depending on that implementation. Changes to padding logic and test prompts might inadvertently affect test reliability if not thoroughly validated.

**Key insights**  
Developers should verify that performance regressions from disabling cascade attention are acceptable for their use cases. The test updates should be monitored to ensure they effectively catch invariance issues without introducing flakiness. Consider revisiting FlashInfer support once the underlying CTA size invariance issue is resolved.

---

## 33. [Improve Mistral format checks.](https://github.com/vllm-project/vllm/pull/33253)


### Base Information

- **PR Number:** #33253
- **Author:** [juliendenize](https://github.com/juliendenize)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 06:23:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33253/files) (8):**
  - `tests/config/test_model_arch_config.py`
  - `tests/conftest.py`
  - `tests/transformers_utils/test_repo_utils.py`
  - `vllm/config/model.py`
  - `vllm/tokenizers/registry.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`
  - `vllm/transformers_utils/repo_utils.py`

### Summary

**What changed and why**  
This PR improves edge case handling for Mistral model format detection. It addresses two issues: 1) When tokenizer/parameter files exist but the model file is missing, the auto format now defaults to HF to avoid loading errors. 2) It patches HF hub constants to use `consolidated.safetensors` for Mistral format, preventing misleading error messages when loading Mistral models without explicit dtype.

**Technical impact**  
The changes modify format detection logic in `repo_utils.py`, `config.py`, and `registry.py` to better distinguish Mistral repositories. A context manager temporarily patches Hugging Face hub constants for Mistral format, ensuring correct safetensors file discovery. The tokenizer resolution logic is simplified using new utility functions, making the code more maintainable.

**Potential risks**  
Patching global HF hub constants could affect concurrent operations if multiple threads load different model formats simultaneously. The format detection relies on file pattern matching which might have false positives if other models use similar naming conventions. The changes add new dependencies between modules that could increase coupling.

**Key insights**  
The PR introduces a cleaner abstraction with `any_pattern_in_repo_files` and `is_mistral_model_repo` utility functions. Developers should be aware that the HF hub constant patching is scoped to Mistral format detection only. The simplified tokenizer resolution logic improves readability but removes some explicit error checking for file lists.

---

## 34. [Fix `test_moe.py` for Transformers v5](https://github.com/vllm-project/vllm/pull/33413)


### Base Information

- **PR Number:** #33413
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 06:03:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33413/files) (1):**
  - `tests/kernels/moe/test_moe.py`

### Summary

**What changed and why**  
The changes update the MoE test to support both Transformers v4 and v5. In v5, expert parameters are pre-fused (gate_up_proj and down_proj) and the forward method returns hidden states directly instead of a tuple. The code now detects the version and handles weight loading and forward pass results accordingly.

**Technical impact**  
This maintains test compatibility across Transformers library versions without breaking existing functionality. The test now conditionally handles different weight tensor structures and return types, ensuring accurate comparison between Hugging Face and vLLM MoE implementations regardless of the Transformers version.

**Potential risks**  
The temporary workaround for `_experts_implementation` could become obsolete if the referenced PR is merged, potentially causing future test failures. There's also a risk that the version detection logic (checking `isinstance(hf_moe.experts, torch.nn.ModuleList)`) might not cover all edge cases or future Transformers versions.

**Key insights**  
The changes demonstrate good backward compatibility practices. Developers should monitor the referenced Transformers PR (#43622) and remove the workaround once it's merged. Consider adding explicit version checks rather than relying solely on type inspection for more robust version detection.

---

## 35. [[Doc] Enhance documentation around CPU container images](https://github.com/vllm-project/vllm/pull/32286)


### Base Information

- **PR Number:** #32286
- **Author:** [nathan-weinberg](https://github.com/nathan-weinberg)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-30 05:36:20
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32286/files) (3):**
  - `docs/deployment/k8s.md`
  - `docs/getting_started/installation/cpu.arm.inc.md`
  - `docs/getting_started/installation/cpu.x86.inc.md`

### Summary

**What changed and why**  
This PR enhances documentation for CPU container images by adding explicit instructions for pulling and running Docker images across different processor architectures (x86_64 and arm64). It provides concrete examples for Kubernetes deployments and standalone Docker usage, replacing vague references with actionable commands.

**Technical impact**  
The changes improve user experience by reducing ambiguity in deployment steps. For Kubernetes, users now see explicit image variables for different architectures. For Docker, the documentation now includes complete `docker pull` and `docker run` commands with proper volume, port, and environment variable mappings.

**Potential risks**  
The documentation assumes users will replace `<tag>` and `<secret>` placeholders correctly, which could lead to runtime errors if overlooked. The Kubernetes example uses a hardcoded model (`meta-llama/Llama-3.2-1B-Instruct`), which might not match all users' needs. No validation is provided for CPU instruction set compatibility beyond a warning.

**Key insights**  
Always test the provided commands in a staging environment before production use. Consider adding a note about verifying CPU architecture compatibility (e.g., using `lscpu` or `uname -m`) before pulling images. For Kubernetes deployments, users should customize the model and resource requests based on their workload.

---

## 36. [[Misc] Clean up HIDDEN_DEPRECATED_METRICS after metric removal](https://github.com/vllm-project/vllm/pull/33323)


### Base Information

- **PR Number:** #33323
- **Author:** [carlory](https://github.com/carlory)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-01-30 05:31:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33323/files) (1):**
  - `tests/entrypoints/instrumentator/test_metrics.py`

### Summary

**What changed and why**  
Removed three deprecated metrics (`vllm:gpu_cache_usage_perc`, `vllm:gpu_prefix_cache_queries`, `vllm:gpu_prefix_cache_hits`) from the `HIDDEN_DEPRECATED_METRICS` list in the test file, setting it to an empty list. This cleanup follows their full removal from the codebase in a previous PR (#29330), ensuring the test configuration stays synchronized with actual metric availability.

**Technical impact**  
The change reduces technical debt by eliminating references to nonexistent metrics, which prevents confusion during test maintenance and ensures the test suite accurately reflects the current metric landscape. It has no functional impact on runtime behavior, as it only affects test configuration.

**Potential risks**  
If any downstream tests or tooling still depend on these metric names being present in the `HIDDEN_DEPRECATED_METRICS` list, they could fail or behave unexpectedly. However, since the metrics were already removed from the codebase, this risk is minimal.

**Key insights**  
This is a straightforward cleanup that aligns test code with production changes, demonstrating good hygiene in removing obsolete configuration. Developers should ensure similar cleanups are performed whenever deprecated features are fully removed to keep test files accurate and maintainable.

---

## 37. [Remove deprecated `reasoning_content` message field](https://github.com/vllm-project/vllm/pull/33402)


### Base Information

- **PR Number:** #33402
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-01-30 03:48:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33402/files) (8):**
  - `tests/entrypoints/openai/tool_parsers/test_openai_tool_parser.py`
  - `tests/reasoning/utils.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/engine/protocol.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`
  - `vllm/tokenizers/deepseek_v32_encoding.py`

### Summary

**What changed and why**  
This PR removes the deprecated `reasoning_content` field from vLLM's codebase, consolidating all reasoning-related functionality under the `reasoning` field. The changes span protocol definitions, chat utilities, tokenizer handling, and test files to eliminate backward compatibility code that was previously copying `reasoning` to `reasoning_content`.

**Technical impact**  
The removal simplifies the codebase by eliminating duplicate fields and reducing maintenance overhead. All internal and external APIs now consistently use the `reasoning` field, which may break any downstream code still relying on the deprecated `reasoning_content` field. The changes affect message parsing, serialization, and streaming generation across multiple subsystems.

**Potential risks**  
Breaking changes for clients or integrations that have not migrated from `reasoning_content` to `reasoning`. Edge cases in message handling where `reasoning_content` might have been used in custom tooling or external systems could lead to runtime errors. The removal of backward compatibility assertions in tests reduces safety nets for regression.

**Key insights**  
Developers must ensure all external integrations use the `reasoning` field exclusively. The PR successfully cleans up technical debt but requires careful deployment to avoid disrupting existing workflows. Consider adding a release note highlighting this breaking change to alert users.

---

## 38. [[Doc] [ROCm] Update Documentation to reflect v0.15.0 release](https://github.com/vllm-project/vllm/pull/33388)


### Base Information

- **PR Number:** #33388
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-30 03:06:08
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33388/files) (1):**
  - `docs/getting_started/installation/gpu.rocm.inc.md`

### Summary

**What changed and why**  
Updated ROCm installation documentation from v0.14.1 to v0.15.0 in two code examples—one for `uv pip` and one for standard `pip`. This aligns the documentation with the latest vLLM release.

**Technical impact**  
Ensures users following the documentation will install the correct, up-to-date vLLM version for ROCm. No functional changes to the codebase—only documentation updates.

**Potential risks**  
If the v0.15.0 ROCm wheels are not yet published or have compatibility issues, users may encounter installation failures. The change assumes the new version is stable and available.

**Key insights**  
Always verify that referenced package versions are publicly accessible before merging documentation updates. Consider adding a note if backward compatibility or upgrade paths are affected.

---

## 39. [[BUGFIX] Pixtral cannot be loaded with --limit-mm-per-prompt 0](https://github.com/vllm-project/vllm/pull/33406)


### Base Information

- **PR Number:** #33406
- **Author:** [juliendenize](https://github.com/juliendenize)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 02:52:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33406/files) (1):**
  - `vllm/model_executor/models/pixtral.py`

### Summary

**What changed and why**  
The PR fixes a bug where Pixtral models fail to load when `--limit-mm-per-prompt` is set to 0. Previously, the vision-related modules were set to `StageMissingLayer` instead of `None` in this configuration, but the weight-loading logic only checked for `None`, causing attempts to load weights into non-existent modules. The fix introduces a helper function `_is_layer_none_or_staged` that checks for both `None` and `StageMissingLayer` instances, ensuring weight loading is skipped appropriately.

**Technical impact**  
This change ensures consistent handling of disabled vision components across the model initialization and weight-loading pipeline. By abstracting the check into a reusable function, it improves code maintainability and prevents similar issues in other model architectures that may use `StageMissingLayer`. The fix is minimal and targeted, affecting only the weight-loading logic for Pixtral's vision encoder, patch merger, projector norm, and vision-language adapter.

**Potential risks**  
If `StageMissingLayer` is used elsewhere in the codebase without similar checks, analogous loading failures could occur. There is also a risk that the helper function might be misapplied to layers that should not be skipped (e.g., layers that are intentionally `None` but should have weights loaded later). Additionally, the change does not address whether `StageMissingLayer` should be used for other configurations beyond `--limit-mm-per-prompt 0`.

**Key insights**  
Developers should ensure that `_is_layer_none_or_staged` is used consistently wherever vision-related modules are conditionally loaded. Consider reviewing other models in the codebase that may have similar patterns to prevent regressions. The fix highlights the importance of centralizing conditional checks for module existence, especially when dealing with staged or optional components.

---

## 40. [[Realtime API] Adds minimal realtime API based on websockets](https://github.com/vllm-project/vllm/pull/33187)


### Base Information

- **PR Number:** #33187
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 02:41:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33187/files) (21):**
  - `docs/serving/openai_compatible_server.md`
  - `examples/online_serving/openai_realtime_client.py`
  - `examples/online_serving/openai_realtime_microphone_client.py`
  - `tests/entrypoints/openai/test_realtime_validation.py`
  - `tests/v1/e2e/test_streaming_input.py`
  - `tests/v1/streaming_input/test_async_llm_streaming.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/launcher.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/realtime/__init__.py`
  - `vllm/entrypoints/openai/realtime/api_router.py`
  - `vllm/entrypoints/openai/realtime/connection.py`
  - `vllm/entrypoints/openai/realtime/protocol.py`
  - `vllm/entrypoints/openai/realtime/serving.py`
  - `vllm/inputs/data.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_streaming.py`
  - `vllm/tasks.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces a WebSocket-based realtime API for streaming speech-to-text transcription, inspired by OpenAI's realtime API. It adds a new `/v1/realtime` endpoint that accepts PCM16 audio chunks via WebSocket and streams back incremental transcription results. The implementation includes full protocol handling, audio buffering, model validation, and integration with Voxtral streaming models.

**Technical impact**  
The changes extend the OpenAI-compatible server with a new realtime task type, adding WebSocket routing, connection management, and streaming input support. The engine protocol now accepts `AsyncGenerator[StreamingInput, None]` as a prompt type, enabling continuous audio input processing. The Voxtral streaming model is enhanced with realtime audio buffering and token‑by‑token streaming, and the task registry now includes "realtime" as a supported generation task.

**Potential risks**  
- The realtime API currently only works with Voxtral streaming models; using other models may cause errors.  
- WebSocket connections require careful resource cleanup to avoid memory leaks from orphaned tasks or buffers.  
- The audio chunk queue could grow unbounded if the client sends data faster than the model processes it, potentially leading to high memory usage.  
- The `StreamingInput` class was moved from `vllm.v1.engine.async_llm` to `vllm.inputs.data`, which may break existing V1 API usage if not updated.

**Key insights**  
- The implementation is well‑structured with clear separation between protocol handling, audio streaming, and model interaction.  
- Developers should ensure the server is started with `--enforce-eager` (or piece‑wise compilation) for Voxtral models, as full CUDA graphs are not yet supported.  
- The provided client examples (file‑based and microphone‑based) are essential for testing and demonstrate proper usage of the WebSocket protocol.  
- Future work should expand realtime support to other ASR models and consider adding flow control to prevent audio queue overflows.

---

## 41. [[Misc] Replace Optional[X] with X \| None syntax](https://github.com/vllm-project/vllm/pull/33332)


### Base Information

- **PR Number:** #33332
- **Author:** [carlory](https://github.com/carlory)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-30 01:56:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33332/files) (56):**
  - `vllm/beam_search.py`
  - `vllm/distributed/kv_transfer/kv_connector/factory.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/base.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_common.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_engine.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py`
  - `vllm/distributed/kv_transfer/kv_transfer_state.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/entrypoints/anthropic/protocol.py`
  - `vllm/lora/lora_weights.py`
  - `vllm/lora/utils.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/layers/quantization/cpu_wna16.py`
  - `vllm/model_executor/layers/quantization/experts_int8.py`
  - `vllm/model_executor/layers/quantization/fbgemm_fp8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gguf.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/inc.py`
  - `vllm/model_executor/layers/quantization/ipex_quant.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/moe_wna16.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/petit.py`
  - `vllm/model_executor/layers/quantization/ptpc_fp8.py`
  - `vllm/model_executor/layers/quantization/quark/quark.py`
  - `vllm/model_executor/layers/quantization/torchao.py`
  - `vllm/model_executor/layers/quantization/utils/petit_utils.py`
  - `vllm/model_executor/model_loader/tensorizer.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/interface.py`
  - `vllm/platforms/rocm.py`
  - `vllm/platforms/xpu.py`
  - `vllm/pooling_params.py`
  - `vllm/profiler/layerwise_profile.py`
  - `vllm/transformers_utils/s3_utils.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/backends/tree_attn.py`
  - `vllm/v1/core/sched/interface.py`
  - `vllm/v1/engine/parallel_sampling.py`
  - `vllm/v1/request.py`
  - `vllm/v1/sample/logits_processor/interface.py`
  - `vllm/v1/utils.py`
  - `vllm/v1/worker/ubatching.py`
  - `vllm/v1/worker/workspace.py`

### Summary

**What changed and why**  
This PR modernizes type annotations across 56 files by replacing `Optional[X]` with the union type syntax `X \| None` (PEP 604). The changes also remove unused `Optional` imports from the `typing` module. This aligns the codebase with Python 3.10+ syntax, improving readability and consistency.

**Technical impact**  
The changes are purely syntactic and do not affect runtime behavior. Type checking tools like mypy already support the new syntax, and the PR includes verification that mypy passes. The refactoring reduces import clutter and makes type hints more concise, but requires Python 3.10 or later.

**Potential risks**  
There is a minimal risk if any tooling or environments still rely on Python versions earlier than 3.10, as the union type syntax is not backward compatible. However, the test plan confirms mypy passes, and the project likely already enforces a minimum Python version that supports this syntax. Another minor risk is if any dynamic type introspection code expects `Optional` objects specifically, though this is unlikely.

**Key insights**  
This is a well-executed, mechanical refactoring that improves code maintainability. Developers should ensure their local environments use Python 3.10+ and update any custom tooling that might parse type annotations. The changes are safe and recommended for modern Python codebases.

---

## 42. [[Refactor] Move MM item count validation outside of processor](https://github.com/vllm-project/vllm/pull/33396)


### Base Information

- **PR Number:** #33396
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 01:27:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33396/files) (7):**
  - `tests/multimodal/test_processing.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/lora/model_manager.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
This refactor moves multi-modal item count validation logic from the processor class to the processing info class. The changes centralize validation logic and prepare for further refactoring by moving `_to_mm_items` calls outside the processor in a subsequent PR.

**Technical impact**  
The validation logic is now encapsulated within the `MultiModalProcessingInfo` class, making the processor class leaner. The `supported_mm_limits` and `allowed_mm_limits` properties are converted to cached properties in the info class, improving performance by avoiding recomputation. Several callers are updated to access these properties through `info` instead of directly from the processor.

**Potential risks**  
The deprecated properties on the processor (`supported_mm_limits` and `allowed_mm_limits`) could cause issues if external code still references them after v0.17. The test mock update (`get_supported_mm_limits` lambda) might not fully replicate the original behavior since it bypasses the new cached property mechanism.

**Key insights**  
This is a well-structured refactor that improves separation of concerns. Developers should immediately migrate from the deprecated processor properties to `info.supported_mm_limits` and `info.allowed_mm_limits`. The cached properties optimize performance by computing limits once. Ensure all tests properly mock the new interface structure.

---

## 43. [fix: allow LFM2 MoE prefix caching (align)](https://github.com/vllm-project/vllm/pull/33376)


### Base Information

- **PR Number:** #33376
- **Author:** [tianshu-Michael-yu](https://github.com/tianshu-Michael-yu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-30 00:23:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33376/files) (2):**
  - `vllm/model_executor/models/lfm2_moe.py`
  - `vllm/model_executor/models/lfm2_vl.py`

### Summary

**What changed and why**  
The changes enable prefix caching for LFM2 MoE models in "align" mode by replacing a blanket assertion with a more specific check. Additionally, a Mamba state copy function is added to LFM2-VL to support prefix caching operations.

**Technical impact**  
Prefix caching is now permitted for LFM2 MoE models when using `--mamba-cache-mode=align`, improving inference performance by reusing computed prefixes. The restriction remains for `mamba-cache-mode=all`, which is still unsupported. The new copy function ensures proper state management during caching.

**Potential risks**  
If users attempt to use `mamba-cache-mode=all`, they will encounter a clear error, but this may cause confusion if they are unaware of the "align" alternative. The copy function's correctness is critical; any bugs could lead to incorrect state propagation and model output errors.

**Key insights**  
Developers should note that prefix caching is now selectively enabled, which can reduce latency for repetitive prompts. Always use `--mamba-cache-mode=align` with LFM2 MoE models. Verify that the state copy function behaves correctly across different input sequences to avoid subtle caching issues.

---

