# vLLM Merged PR Report

**Report Date:** 2026-01-18 PST

**Total Merged PRs:** 17

---

## 1. [[Frontend] Add render endpoints for prompt preprocessing](https://github.com/vllm-project/vllm/pull/32473)


### Base Information

- **PR Number:** #32473
- **Author:** [hyeongyun0916](https://github.com/hyeongyun0916)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-18 20:21:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32473/files) (5):**
  - `tests/entrypoints/openai/test_render.py`
  - `vllm/entrypoints/openai/chat_completion/api_router.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/api_router.py`
  - `vllm/entrypoints/openai/completion/serving.py`

### Summary

**What changed and why**  
This PR adds new `/v1/completions/render` and `/v1/chat/completions/render` endpoints that expose prompt preprocessing (chat template application, OpenAI Harmony transformations, and tokenization) without executing text generation. The core logic is extracted from existing `create_completion()` and `create_chat_completion()` methods into separate `render_*_request()` methods, enabling testing, debugging, and reuse of rendering logic in disaggregated architectures as proposed in RFC #22817.

**Technical impact**  
The changes introduce a clean separation between rendering and generation phases, allowing the rendering logic to be invoked independently. This supports the vLLM disaggregated architecture vision where rendering can be a separate service. The API endpoints return tokenized prompts and conversation data, maintaining consistency with what would be sent to the inference engine. The existing completion endpoints remain unchanged, ensuring backward compatibility.

**Potential risks**  
The render endpoints apply both chat templates and OpenAI Harmony transformations, which may not align with the original RFC's intent if Harmony processing was meant to be excluded. There is a risk of duplicated error-handling logic between the new render methods and the original completion methods, though the extraction appears careful. Additionally, the new endpoints expose internal preprocessing details, which could become a maintenance burden if the rendering pipeline evolves significantly.

**Key insights**  
The PR successfully decouples rendering from generation, a key step toward a disaggregated architecture. Developers should verify with the RFC authors whether Harmony transformations should be included. The comprehensive test suite (7 tests) provides good coverage for basic functionality, error handling, and performance. Future work should ensure the render endpoints remain synchronized with any changes to the preprocessing pipeline.

---

## 2. [[CI/Build] Use Common Event Map Fixture in Harmony / MCP Server Tests](https://github.com/vllm-project/vllm/pull/32531)


### Base Information

- **PR Number:** #32531
- **Author:** [alex-jw-brooks](https://github.com/alex-jw-brooks)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-18 20:05:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32531/files) (3):**
  - `tests/entrypoints/openai/responses/conftest.py`
  - `tests/entrypoints/openai/responses/test_harmony.py`
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`

### Summary

**What changed and why**  
This PR introduces a shared pytest fixture `pairs_of_event_types` that centralizes the mapping between 'done' and 'start' event types used in Harmony/MCP server tests. It replaces hardcoded, duplicate mappings in individual test files to eliminate copy-paste errors, fix incorrect event names (e.g., web search events), and ensure all relevant event pairs are included, thereby preventing `KeyError` exceptions.

**Technical impact**  
The changes improve test maintainability and reliability by establishing a single source of truth for event mappings. Tests now depend on a centralized fixture, reducing duplication and making it easier to update event pairs across all tests. This also enables future enhancements, such as filtering the fixture to explicitly allow or disallow specific events for certain test scenarios.

**Potential risks**  
If the shared fixture contains an incorrect mapping, it could affect multiple tests simultaneously. There is also a risk that tests relying on the fixture may inadvertently include event pairs not relevant to their specific context, potentially masking issues that the original hardcoded mappings were designed to catch.

**Key insights**  
Centralizing test configuration is a best practice that enhances consistency and reduces errors. Developers should use this fixture for all new tests involving event streaming and consider filtering it when tests require a restricted subset of events. The fix also corrects a previously incorrect event name (`web_search_call`), demonstrating the value of eliminating duplication.

---

## 3. [[BugFix] Fix embed_input_ids argument error of QwenVLForConditionalGeneration](https://github.com/vllm-project/vllm/pull/32462)


### Base Information

- **PR Number:** #32462
- **Author:** [honglyua-il](https://github.com/honglyua-il)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-18 19:06:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32462/files) (1):**
  - `vllm/model_executor/models/qwen_vl.py`

### Summary

**What changed and why**  
Added a missing `embed_input_ids` attribute to the `QwenVLForConditionalGeneration` class by inheriting it from the `SupportsMultiModal` mixin. This fixes an argument error referenced in issue #32461, likely resolving a runtime error when the model attempts to process multimodal inputs.

**Technical impact**  
This change ensures the class properly implements the multimodal interface by exposing the `embed_input_ids` method, which is presumably required for converting input IDs to embeddings in a multimodal context. It maintains consistency with the inheritance pattern established in related PR #30674.

**Potential risks**  
If the `SupportsMultiModal.embed_input_ids` method has not been properly implemented or tested, this could introduce new runtime errors. There's also a risk that other missing multimodal attributes or methods might still exist in the class, leading to incomplete fixes.

**Key insights**  
Always verify that classes fully implement their intended interfaces, especially when using mixins. Consider adding a test to ensure all required multimodal methods are present and functional. Review related classes for similar omissions to prevent future issues.

---

## 4. [[Model Runner V2] Refactor `update_states`](https://github.com/vllm-project/vllm/pull/32562)


### Base Information

- **PR Number:** #32562
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-18 17:32:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32562/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The `update_states` method has been refactored into four distinct methods: `finish_requests`, `free_states`, `add_requests`, and `update_requests`. This change modularizes the state update logic, separating concerns for handling preempted/finished requests, freeing encoder cache, adding new requests, and updating existing requests. The `execute_model` method now calls these methods sequentially.

**Technical impact**  
This refactoring improves code organization and maintainability by breaking down a large, monolithic function. The state update process is now more explicit, with each step clearly separated. The `block_tables.apply_staged_writes()` call has been moved from within the new methods to the `execute_model` method, centralizing the final write operation.

**Potential risks**  
The refactoring changes the order of operations slightly—state writes are now applied in a different sequence. There's a risk of missing edge cases where the original monolithic method's implicit ordering was important. The conditional logic in `execute_model` now always runs the four new methods when `dummy_run=False`, even when `total_num_scheduled_tokens == 0`, which is a behavioral change from the original code.

**Key insights**  
This is a clean refactoring that enhances code readability and separation of concerns. Developers should verify that the new execution order (`finish_requests` → `free_states` → `add_requests` → `update_requests`) matches the original logic's semantics. The movement of `block_tables.apply_staged_writes()` to the caller requires ensuring all staged writes are properly prepared before this centralized call.

---

## 5. [[Model Runner V2] Support VLM](https://github.com/vllm-project/vllm/pull/32546)


### Base Information

- **PR Number:** #32546
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-18 16:58:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32546/files) (6):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/mm/encoder_runner.py`
  - `vllm/v1/worker/gpu/mm/mrope_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`

### Summary

**What changed and why**  
Added support for Vision-Language Models (VLMs) like Qwen3-VL by introducing multimodal encoder execution and embedding integration. The changes include a new `EncoderRunner` class to handle multimodal feature processing, updates to CUDA graph capture to accept `inputs_embeds`, and modifications to `ModelRunner` to orchestrate VLM workflows.

**Technical impact**  
The system now supports multimodal inputs by extending the model execution pipeline to process visual features through an encoder, cache the outputs, and integrate them into the transformer via `inputs_embeds`. This affects CUDA graph capture, input batching, and attention metadata preparation, ensuring VLMs work within the existing inference framework without breaking existing text-only models.

**Potential risks**  
- Increased memory usage due to caching encoder outputs and additional tensors like `inputs_embeds`.  
- CUDA graph compatibility may be impacted if `inputs_embeds` varies dynamically across captures.  
- Edge cases where multimodal embeddings overlap incorrectly with token positions could lead to assertion failures or incorrect outputs.

**Key insights**  
- The `EncoderRunner` centralizes multimodal logic, promoting separation of concerns.  
- Developers must ensure `inputs_embeds` are correctly aligned with token positions and that encoder caching is managed to avoid memory leaks.  
- Text-only models remain unaffected due to conditional checks (`supports_mm_inputs`), but testing should cover mixed workloads.

---

## 6. [[BUGFIX]  Fix degenerate strides in TRTLLM query tensors for FlashInfer backend. Fixes issue #32353](https://github.com/vllm-project/vllm/pull/32417)


### Base Information

- **PR Number:** #32417
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [pavanimajety](https://github.com/pavanimajety)
- **Merged time:** 2026-01-18 16:57:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32417/files) (1):**
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
The fix addresses degenerate stride patterns in query tensors when using TRTLLM kernels with the FlashInfer backend. Specifically, `.contiguous()` alone fails to correct strides when a tensor dimension has size 1, which can cause kernel execution issues. The solution applies `.contiguous()` followed by `.reshape()` to ensure both memory contiguity and proper stride layout.

**Technical impact**  
This change ensures that query tensors passed to TRTLLM kernels have correct stride patterns, preventing potential runtime errors or incorrect computations in attention operations. It maintains backward compatibility while fixing a specific edge case in tensor memory representation that affects kernel compatibility.

**Potential risks**  
The additional `.reshape()` call introduces minimal overhead but could theoretically affect performance in high-frequency paths. There is also a risk that the reshape operation might inadvertently alter tensor metadata in unexpected ways, though the identical shape parameter should prevent this. Edge cases with extremely unusual stride patterns might still require further validation.

**Key insights**  
Developers should note that `.contiguous()` does not guarantee normalized strides for size-1 dimensions—a subtle PyTorch behavior. This pattern of `.contiguous().reshape(original_shape)` is a useful technique for fixing degenerate strides and could be applied elsewhere in the codebase where similar kernel constraints exist. Always validate tensor stride properties when interfacing with low-level kernels.

---

## 7. [[Bugfix] Add OOT backend option](https://github.com/vllm-project/vllm/pull/32471)


### Base Information

- **PR Number:** #32471
- **Author:** [iboiko-habana](https://github.com/iboiko-habana)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-18 14:20:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32471/files) (1):**
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`

### Summary

**What changed and why**  
Added `OOT` (Out-Of-Tree) as a new enum value in `UnquantizedMoeBackend` and included it in the `UNSUPPORTED_BACKENDS` list. This fixes a regression introduced by PR #31827 that broke OOT backend support, following a similar pattern to a previous fix in PR #32438.

**Technical impact**  
The change ensures the OOT backend is properly recognized in the fused MoE (Mixture of Experts) layer's backend selection logic. It aligns OOT with other unsupported backends (CPU, XPU, TPU), preventing runtime errors when OOT is detected and maintaining consistent behavior across platform-specific backends.

**Potential risks**  
If the OOT backend should actually support fused MoE operations, marking it as unsupported could incorrectly disable optimizations. There's also a risk of missing edge cases where OOT interacts with other platform detection logic, though the fix mirrors existing patterns.

**Key insights**  
This is a straightforward regression fix that restores parity with other unsupported backends. Developers should verify that OOT is intentionally unsupported for fused MoE and consider adding a test to prevent similar breaks. The pattern of adding platform checks alongside existing ones is maintainable but requires consistency across all platform-specific code paths.

---

## 8. [[Refactor] Remove unused cutlass moe problem size function](https://github.com/vllm-project/vllm/pull/32047)


### Base Information

- **PR Number:** #32047
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-18 12:46:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32047/files) (5):**
  - `csrc/ops.h`
  - `csrc/quantization/w8a8/cutlass/moe/moe_data.cu`
  - `csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu`
  - `csrc/torch_bindings.cpp`
  - `vllm/_custom_ops.py`

### Summary

**What changed and why**  
Removed the unused `get_cutlass_moe_mm_problem_sizes` function across the codebase. This cleanup eliminates dead code that was no longer being used, likely replaced by the `get_cutlass_moe_mm_problem_sizes_from_expert_offsets` function which uses a more efficient expert-first-token-offset approach.

**Technical impact**  
The removal reduces code complexity and maintenance burden without affecting functionality. The remaining `get_cutlass_moe_mm_data` and `get_cutlass_moe_mm_problem_sizes_from_expert_offsets` functions continue to support the CUTLASS-based fused MoE operations, indicating the system now uses a more optimized path for computing problem sizes.

**Potential risks**  
Minimal risk since the function was unused. However, developers should verify no downstream code or external integrations were calling this function through the Python wrapper. The removal of the `force_swap_ab` parameter might affect any custom configurations that relied on manual swap control, though this was likely handled automatically.

**Key insights**  
This is a straightforward cleanup that improves code hygiene. Developers should ensure build systems properly recompile all dependencies after this change. The retention of `get_cutlass_moe_mm_problem_sizes_from_expert_offsets` confirms the preferred API for computing problem sizes uses expert offsets rather than topk_ids directly.

---

## 9. [[Refactor] Remove unused file `pallas_kv_cache_update.py`](https://github.com/vllm-project/vllm/pull/32433)


### Base Information

- **PR Number:** #32433
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-18 12:46:39
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32433/files) (1):**
  - `vllm/v1/attention/ops/pallas_kv_cache_update.py`

### Summary

**What changed and why**  
The PR removes the unused file `pallas_kv_cache_update.py`, which contains a JAX/Pallas-based kernel for updating key-value (KV) caches on TPU hardware. The file was introduced in a previous PR (#19928) but is no longer referenced anywhere in the codebase, prompting its deletion to reduce dead code.

**Technical impact**  
Removing this file eliminates unused code, simplifying the codebase and reducing maintenance overhead. Since the kernel is not integrated into any active workflows, its removal does not affect system functionality or performance. However, if future TPU-specific KV cache updates are needed, equivalent logic would need to be reimplemented or restored.

**Potential risks**  
The main risk is accidental dependency: if any external or internal code indirectly relies on this module (e.g., via dynamic imports or undocumented use), removal could cause runtime errors. Additionally, if the kernel was intended for future feature development, deleting it may delay reintroduction efforts, though the git history preserves the implementation.

**Key insights**  
This cleanup aligns with good software hygiene by removing orphaned code. Before merging, verify no references exist via `grep` or static analysis tools. Consider adding a brief comment in the commit linking to the original PR (#19928) for historical context, and ensure the kernel’s functionality is documented elsewhere if it represents a meaningful pattern for TPU operations.

---

## 10. [[Doc] Correct comment for _jobs dict in OffloadingConnectorWorker](https://github.com/vllm-project/vllm/pull/32556)


### Base Information

- **PR Number:** #32556
- **Author:** [DemingCheng](https://github.com/DemingCheng)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-18 12:46:01
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32556/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`

### Summary

**What changed and why**  
The change corrects a misleading comment for the `_jobs` dictionary in `OffloadingConnectorWorker`. The comment previously stated `# req_id -> (job_id, store)`, but it has been updated to `# job_id -> (req_id, store)` to accurately reflect the actual key-value structure, where the key is `job_id` and the value is a tuple of `req_id` and `store`.

**Technical impact**  
This is a non-functional change that only updates documentation. It does not affect runtime behavior, system performance, or architecture. However, it improves code clarity by ensuring the comment aligns with the implementation, reducing cognitive load for developers working with the kv offloading connector module.

**Potential risks**  
There are minimal risks since no functional code was modified. The primary risk is if developers had previously relied on the incorrect comment for understanding the data structure, which could have led to misunderstandings. Ensuring the comment is accurate now mitigates this risk moving forward.

**Key insights**  
Accurate documentation is crucial for maintainability and developer onboarding. This change highlights the importance of keeping comments synchronized with code. Developers should regularly review comments during code reviews to prevent such mismatches, especially for complex data structures that are critical to module functionality.

---

## 11. [Use the same memory for workspace13 and fused_output.](https://github.com/vllm-project/vllm/pull/31531)


### Base Information

- **PR Number:** #31531
- **Author:** [halyavin](https://github.com/halyavin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-18 11:14:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31531/files) (1):**
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`

### Summary

**What changed and why**  
The change modifies memory allocation logic to share a single memory buffer between `workspace13` and `fused_output` when processing without chunking (`num_chunks == 1`). Previously, it reused `workspace13` only if its size was sufficient for `fused_output`; now it allocates a common buffer sized to the maximum of both shapes, ensuring shared memory usage.

**Technical impact**  
This reduces memory fragmentation and potential overallocation by unifying buffer management for two temporary tensors. The `_resize_cache` function reshapes the common buffer to fit each tensor's shape, leveraging the same underlying memory region for both workspaces during non-chunked execution.

**Potential risks**  
If `workspace13` and `fused_output` have different data types or require concurrent access (though the code suggests sequential usage), this could lead to data corruption. Additionally, improper buffer resizing via `_resize_cache` might cause shape mismatches or alignment issues, especially with varying tensor strides or memory layouts.

**Key insights**  
Developers should verify that `workspace13` and `fused_output` are never used simultaneously and that `_resize_cache` robustly handles shape transitions. This optimization is memory-efficient but hinges on correct lifetime management; consider adding assertions to validate buffer reuse safety and dtype consistency.

---

## 12. [[CI] Move Distributed Tests from H200 -> H100](https://github.com/vllm-project/vllm/pull/32555)


### Base Information

- **PR Number:** #32555
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-01-18 10:25:23
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32555/files) (1):**
  - `.buildkite/test-pipeline.yaml`

### Summary

**What changed and why**  
The PR moves distributed tests from H200 to H100 GPU runners. This change addresses two issues: H200 runners have 8 GPUs, but these tests only use 2, leaving 6 GPUs idle. Additionally, there's a memory cleanup issue on the H200 runner that couldn't be reproduced locally but appears in nightly builds.

**Technical impact**  
This change optimizes GPU resource utilization by running distributed tests on H100 runners (which presumably have fewer GPUs) instead of wasting capacity on H200s. It also removes the `CUDA_VISIBLE_DEVICES=1,2` restriction from the data parallel test, allowing it to use default GPU selection.

**Potential risks**  
The memory cleanup issue might persist if it's related to the test code rather than the H200 hardware specifically. Removing `CUDA_VISIBLE_DEVICES` could cause different GPU selection behavior, potentially affecting test reproducibility if the system has multiple GPUs with different configurations.

**Key insights**  
This is primarily an infrastructure optimization that better matches test requirements to available hardware. Developers should monitor test stability after the GPU change and verify the memory cleanup issue is resolved. The H200 runner section remains in the pipeline for other tests, indicating selective migration rather than complete removal.

---

## 13. [[MoE Refactor] Separate Router into OO Classes](https://github.com/vllm-project/vllm/pull/30623)


### Base Information

- **PR Number:** #30623
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-18 08:40:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30623/files) (45):**
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/test_batched_moe.py`
  - `tests/kernels/moe/test_block_fp8.py`
  - `tests/kernels/moe/test_cutlass_moe.py`
  - `tests/kernels/moe/test_flashinfer_moe.py`
  - `tests/kernels/moe/test_grouped_topk.py`
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/moe/test_moe_permute_unpermute.py`
  - `tests/kernels/moe/test_nvfp4_moe.py`
  - `tests/kernels/moe/test_pplx_cutlass_moe.py`
  - `tests/kernels/moe/test_routing.py`
  - `tests/kernels/moe/test_routing_simulator.py`
  - `tests/model_executor/test_enabled_custom_ops.py`
  - `vllm/distributed/eplb/eplb_state.py`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_method_base.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/router/__init__.py`
  - `vllm/model_executor/layers/fused_moe/router/base_router.py`
  - `vllm/model_executor/layers/fused_moe/router/custom_routing_router.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_moe_router.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_router.py`
  - `vllm/model_executor/layers/fused_moe/router/grouped_topk_router.py`
  - `vllm/model_executor/layers/fused_moe/router/router_factory.py`
  - `vllm/model_executor/layers/fused_moe/router/routing_simulator_router.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/bitsandbytes.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/experts_int8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gguf.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/ipex_quant.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/moe_wna16.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`
  - `vllm/model_executor/layers/quantization/rtn.py`
  - `vllm/model_executor/models/ernie45_moe.py`
  - `vllm/model_executor/models/ernie45_vl_moe.py`

### Summary

**What changed and why**  
This PR refactors MoE routing logic by extracting it from `FusedMoE` into dedicated router classes. A new `BaseRouter` abstract class defines a common `select_experts` template, with concrete implementations for different routing strategies (fused top‑k, bias‑corrected, grouped, custom, and simulator). A factory function `create_fused_moe_router` selects the appropriate router based on configuration. The `FusedMoE` layer now holds a router instance instead of embedding routing logic, and EPLB state is centralized in a new `EplbLayerState` dataclass.

**Technical impact**  
The changes decouple routing from the MoE kernel execution, improving modularity and testability. The router factory enables clean extension for new routing methods. EPLB handling is unified across routers via `BaseRouter.select_experts`, reducing duplication. Existing quantization paths (e.g., Quark, MXFP4) are updated to call `router.select_experts`, preserving backward compatibility. The refactor simplifies `FusedMoE` by removing ~375 lines of inline routing code.

**Potential risks**  
The router factory’s priority order (simulator → grouped → custom → bias → default) must be carefully maintained to avoid misrouting. Changes to enum values (`RoutingMethodType` adds `Custom` and `Simulated`, shifting `Unspecified`) could break serialized configurations. The `indices_type_getter` callback introduces a runtime dependency that may cause issues if the modular kernel isn’t initialized. EPLB validation now occurs in the router, which could lead to silent failures if state is incorrectly passed.

**Key insights**  
This refactor is a foundational step toward a more extensible MoE architecture. Developers should ensure new routing strategies are added via the factory pattern and follow the `BaseRouter` template. The `EplbLayerState` centralization simplifies future EPLB enhancements. Test coverage is comprehensive, but integration tests should verify that all quantization backends correctly delegate to the router. The `capture` callback is a temporary workaround; a cleaner separation of routing and expert application is planned for a follow‑up PR.

---

## 14. ["refactor: refactor_repeated_interfaces"](https://github.com/vllm-project/vllm/pull/32486)


### Base Information

- **PR Number:** #32486
- **Author:** [tom-zju](https://github.com/tom-zju)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-18 06:07:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32486/files) (11):**
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py`
  - `vllm/model_executor/layers/quantization/fp_quant.py`
  - `vllm/model_executor/layers/quantization/qutlass_utils.py`
  - `vllm/model_executor/model_loader/bitsandbytes_loader.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/tarsier.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/model_executor/warmup/deep_gemm_warmup.py`

### Summary

**What changed and why**  
This PR refactors repeated utility functions (`ceil_div` and `_get_layer_index`) into centralized implementations. The `ceil_div` function is moved to `vllm.utils.math_utils.cdiv`, and `_get_layer_index` is moved to `vllm.model_executor.models.utils.get_layer_index`. This eliminates code duplication across multiple files.

**Technical impact**  
The changes improve code maintainability by centralizing common mathematical and layer index calculation utilities. This reduces the risk of inconsistencies and makes future updates easier. The refactoring is purely structural—no functional changes to the algorithms themselves.

**Potential risks**  
There's a risk of missing imports in some files if the centralized functions are not properly imported. The rename from `ceil_div` to `cdiv` could cause confusion if developers are accustomed to the old name, though all call sites have been updated. Edge cases in layer index calculations (e.g., negative indices exceeding total layers) should be validated.

**Key insights**  
This is a clean refactoring that follows DRY principles. Developers should use the centralized `cdiv` and `get_layer_index` functions for new code. Ensure all future contributions avoid reimplementing these utilities. The PR description is minimal; consider adding more context about the motivation in future refactors.

---

## 15. [[Bugfix] Fix GLM-ASR audio encoder RoPE dim](https://github.com/vllm-project/vllm/pull/32540)


### Base Information

- **PR Number:** #32540
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-18 03:18:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32540/files) (2):**
  - `examples/offline_inference/audio_language.py`
  - `vllm/model_executor/models/glmasr.py`

### Summary

**What changed and why**  
The PR fixes a bug where GLM-ASR's audio encoder incorrectly applied rotary position embeddings (RoPE) to the full head dimension instead of only a partial (half) rotary dimension. This issue occurred in the native code path, while the Triton-based vLLM flash attention already handled partial RoPE correctly. The fix adjusts the rotary dimension calculation and applies RoPE only to the first `rotary_dim` elements of query and key tensors.

**Technical impact**  
Changes ensure consistent RoPE behavior between native and Triton code paths for GLM-ASR models. The model now correctly uses partial rotary embeddings as intended, aligning with the configuration's `partial_rotary_factor` (default 0.5). This affects attention computation in the audio encoder, potentially improving accuracy for audio-language tasks.

**Potential risks**  
If `rotary_dim` is miscalculated (e.g., due to missing config attributes), it could lead to dimension mismatches or ineffective RoPE application. The fallback logic assumes a default `partial_rotary_factor` of 0.5, which may not match all model variants. Additionally, slicing tensors with `q[..., :self.rotary_dim]` assumes contiguous memory layout, which could cause performance issues if tensors are non-contiguous.

**Key insights**  
Always validate rotary dimension calculations against the model's config to ensure compatibility across variants. Consider adding assertions to verify `self.rotary_dim <= self.head_dim`. The fix highlights the importance of testing both native and Triton code paths for consistency. The example file change is purely organizational (reordering functions) and does not affect functionality.

---

## 16. [[Model] Support Step1 Model](https://github.com/vllm-project/vllm/pull/32511)


### Base Information

- **PR Number:** #32511
- **Author:** [randzero](https://github.com/randzero)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-18 02:20:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32511/files) (9):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `tests/models/test_initialization.py`
  - `vllm/attention/layer.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/step1.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/triton_attn.py`
  - `vllm/v1/attention/ops/triton_unified_attention.py`

### Summary

**What changed and why**  
This PR adds native support for the Step1 model (specifically Step-Audio) to vLLM. The changes include a new model implementation file (`step1.py`) with decoder layers, attention mechanisms, and ALiBi slope generation. Support is also added for a new `use_alibi_sqrt` parameter in the attention layer to handle Step1's modified ALiBi formulation, which uses a square-root scaling for negative relative positions.

**Technical impact**  
The integration extends vLLM's model registry and attention backends. The Triton attention backend now supports `use_alibi_sqrt`, and the attention layer validates backend compatibility. The model follows vLLM's standard patterns (e.g., tensor parallelism, quantization support) and includes necessary updates to documentation, tests, and initialization logic. The changes are backward-compatible, as `use_alibi_sqrt` defaults to `False` for existing models.

**Potential risks**  
The `use_alibi_sqrt` feature is tightly coupled to the Triton attention backend; other backends (e.g., FlashAttention) do not support it, which could limit portability or performance on certain hardware. The ALiBi slope calculation assumes specific head count patterns, which may not generalize to all model variants. Additionally, the test plan in the PR description is incomplete, leaving verification gaps.

**Key insights**  
Developers should ensure that Step1 models are used with the Triton attention backend to avoid runtime errors. The `use_alibi_sqrt` implementation modifies the ALiBi offset calculation, which is critical for correct model behavior. Future model additions requiring similar attention modifications should abstract the ALiBi logic further to reduce backend dependency.

---

## 17. [[Model] Remove the unnecessary dtype conversion in MiniCPM](https://github.com/vllm-project/vllm/pull/32523)


### Base Information

- **PR Number:** #32523
- **Author:** [gcanlin](https://github.com/gcanlin)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-18 00:07:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32523/files) (1):**
  - `vllm/model_executor/models/minicpm.py`

### Summary

**What changed and why**  
Removed explicit float conversion and restoration of query (q) and key (k) tensors before and after applying rotary positional embeddings in MiniCPM's attention forward pass. This change eliminates a workaround previously needed for platform compatibility (vllm-ascend) as the underlying kernels now handle internal precision management.

**Technical impact**  
This simplifies the model execution path by removing redundant dtype operations, potentially improving performance slightly by avoiding unnecessary tensor copies and conversions. The change assumes rotary embedding and attention kernels are now dtype-agnostic or properly handle mixed precision internally across all supported hardware platforms.

**Potential risks**  
If any platform's kernel implementation still requires specific input dtypes (e.g., float) for rotary embeddings or attention computation, this removal could introduce numerical errors or runtime failures. The change depends on correct behavior of downstream operations across diverse hardware (GPUs, Ascend chips).

**Key insights**  
This is a cleanup PR that centralizes a platform-specific fix into the main codebase, reducing technical debt. Developers should verify that all target platforms (especially Ascend) have been thoroughly tested with the modified code. Consider adding a comment explaining why the conversion is no longer needed to prevent regression.

---

