# vLLM Merged PR Report

**Report Date:** 2026-01-11 PST

**Total Merged PRs:** 12

---

## 1. [[Model] Improve multimodal pooling examples](https://github.com/vllm-project/vllm/pull/32085)


### Base Information

- **PR Number:** #32085
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-11 23:54:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32085/files) (10):**
  - `docs/serving/openai_compatible_server.md`
  - `examples/pooling/embed/vision_embedding_offline.py`
  - `examples/pooling/embed/vision_embedding_online.py`
  - `examples/pooling/score/cohere_rerank_online.py`
  - `examples/pooling/score/openai_cross_encoder_score_for_multimodal.py`
  - `examples/pooling/score/rerank_api_online.py`
  - `examples/pooling/score/score_api_online.py`
  - `examples/pooling/score/vision_rerank_api_online.py`
  - `examples/pooling/score/vision_reranker_offline.py`
  - `examples/pooling/score/vision_score_api_online.py`

### Summary

**What changed and why**  
This PR enhances multimodal pooling support by adding new examples and updating documentation. It introduces offline and online embedding examples for Qwen3-VL models, adds multimodal scoring and reranking client examples, and updates documentation links to reflect these new examples while removing obsolete files.

**Technical impact**  
The changes improve the developer experience by providing comprehensive, ready-to-use examples for multimodal embedding, scoring, and reranking tasks. The updates ensure documentation accurately references the latest example implementations, reducing confusion and promoting consistent usage patterns across text and multimodal workflows.

**Potential risks**  
The removal of `openai_cross_encoder_score_for_multimodal.py` could break existing integrations that directly reference this file. Additionally, the new examples assume specific model configurations (e.g., `--runner pooling`) and may not cover all edge cases or model variations, potentially leading to runtime errors if used incorrectly.

**Key insights**  
Developers should adopt the new example patterns for multimodal tasks, as they demonstrate best practices for embedding, scoring, and reranking. Ensure any existing scripts referencing removed files are updated to use the new examples. Always verify model-specific parameters (like `--runner pooling` and `--max-model-len`) when adapting these examples to other models.

---

## 2. [[Model] Avoid hardcoding pooling type](https://github.com/vllm-project/vllm/pull/32119)


### Base Information

- **PR Number:** #32119
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-11 21:28:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32119/files) (6):**
  - `vllm/model_executor/models/bert.py`
  - `vllm/model_executor/models/bert_with_rope.py`
  - `vllm/model_executor/models/gritlm.py`
  - `vllm/model_executor/models/modernbert.py`
  - `vllm/model_executor/models/roberta.py`
  - `vllm/model_executor/models/transformers/pooling.py`

### Summary

**What changed and why**  
This PR replaces hardcoded sequence pooling types with configurable pooling across multiple model architectures. The changes allow users to specify pooling methods via `pooler_config.seq_pooling_type` instead of silently ignoring user preferences. This aligns with a broader effort to standardize pooler construction and prepare for proper integration of `EmbeddingPoolerHead`.

**Technical impact**  
The modifications affect BERT, ModernBert, RoBERTa, GritLM, and transformer mixins by injecting `PoolerConfig` into pooler initialization. Each model now uses `get_seq_pooling_method()` to resolve the pooling type, and `DispatchPooler` methods handle token extraction without explicit `CLSPool`. This centralizes pooling logic and ensures consistency across models, though SPLADE embeddings still override built-in pooling.

**Potential risks**  
- Assertions on `pooler_config` being non-null could fail if configs are improperly initialized.  
- In ModernBert, the HF `classifier_pooling` always overrides user settings, which may cause confusion.  
- GritLM’s custom `GritLMMeanPool` is only used for `"MEAN"`; other types rely on generic pooling, which might not preserve model-specific behaviors.

**Key insights**  
- Developers should ensure `pooler_config` is always provided when initializing these models.  
- The PR sets the stage for passing `EmbeddingPoolerHead` to the `head` argument in a follow-up change.  
- Reviewers should verify that all supported pooling types are compatible with each model’s architecture, especially for edge cases like SPLADE.

---

## 3. [[Model Runner V2] Remove async barrier](https://github.com/vllm-project/vllm/pull/32083)


### Base Information

- **PR Number:** #32083
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-11 20:24:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32083/files) (13):**
  - `vllm/v1/worker/gpu/block_table.py`
  - `vllm/v1/worker/gpu/buffer_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/gumbel.py`
  - `vllm/v1/worker/gpu/sample/metadata.py`
  - `vllm/v1/worker/gpu/sample/min_p.py`
  - `vllm/v1/worker/gpu/sample/penalties.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/states.py`
  - `vllm/v1/worker/gpu/structured_outputs.py`

### Summary

**What changed and why**  
This PR removes the `async_barrier` mechanism from the Model Runner V2 by redesigning CPU↔GPU data movement. Instead of relying on synchronization barriers to prevent race conditions, the new approach uses double‑buffering and UVA‑backed staged writes. Key additions include a new `buffer_utils.py` with classes like `UvaBuffer`, `StagedWriteTensor`, and `DoubleBufferTensor` to manage zero‑copy or async copies, double buffering, and batched writes.

**Technical impact**  
The changes fundamentally alter how state updates and input preparation are coordinated. Previously, `async_barrier` ensured CPU‑side updates completed before GPU kernels used the data. Now, updates are staged in CPU‑side buffers (e.g., via `stage_write`) and later flushed to GPU with `apply_staged_writes`, eliminating the need for explicit barriers. This simplifies the data‑flow logic in `GPUModelRunner`, reduces GPU memory pressure by using UVA for large tensors, and batches writes for efficiency. Sampling kernels and speculative‑decoding paths are updated to use `idx_mapping` for per‑request parameter lookups.

**Potential risks**  
- The new staged‑write approach relies on correct ordering of `stage_write` and `apply_staged_writes` calls; missing a flush could lead to stale data on GPU.  
- UVA availability is now a hard requirement; systems without UVA support will fail.  
- The `StagedWriteTensor` reallocation logic includes a `torch.cuda.synchronize()` that may introduce latency if reallocations occur frequently.  
- Changes to sampling kernels (`gumbel`, `min_p`, penalties) assume `idx_mapping` is correctly populated; incorrect mappings could cause silent data corruption.

**Key insights**  
- The removal of `async_barrier` makes the data‑flow easier to reason about and reduces synchronization overhead.  
- Developers must ensure that all staged writes are applied via `apply_staged_writes()` before dependent GPU operations.  
- The new buffer utilities (`UvaBufferPool`, `StagedWriteTensor`) provide a reusable pattern for managing CPU‑GPU data transfers; consider using them for other large‑tensor scenarios.  
- Verify UVA support in deployment environments, as the code now assumes it is available.

---

## 4. [[Model Runner V2] Skip building deprecated fields in attn metadata](https://github.com/vllm-project/vllm/pull/32132)


### Base Information

- **PR Number:** #32132
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-11 14:31:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32132/files) (5):**
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`

### Summary

**What changed and why**  
The changes remove deprecated CPU-side fields (`seq_lens_np` and `num_computed_tokens_cpu`) from attention metadata construction. The `build_attn_metadata` function now accepts `max_seq_len` directly instead of computing it from `seq_lens_np`, eliminating NumPy dependencies and simplifying the interface.

**Technical impact**  
This streamlines attention metadata construction by removing unused CPU-side data structures, reducing memory overhead and computational steps. The interface simplification affects multiple callers including CUDA graph utilities, model runner paths, and EAGLE spec decode, making the codebase more maintainable and reducing potential synchronization issues between CPU and GPU data.

**Potential risks**  
Removing the NumPy dependency could break any downstream code that relied on `seq_lens_np` being available in `InputBatch`. The comment about "discrepancy between seq_lens (on GPU) and seq_lens_np (on CPU)" being removed might obscure a historical issue that developers should still be aware of when modifying attention backends.

**Key insights**  
The changes represent a cleanup of technical debt by removing redundant data structures. Developers should verify that all attention backends work correctly with the simplified metadata, particularly for edge cases involving sequence length discrepancies. The removal of NumPy operations may improve performance slightly by reducing CPU-GPU synchronization points.

---

## 5. [[CI] fix `test_concat_and_cache_mla_rope_fused`](https://github.com/vllm-project/vllm/pull/32117)


### Base Information

- **PR Number:** #32117
- **Author:** [ZJY0516](https://github.com/ZJY0516)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-11 07:11:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32117/files) (1):**
  - `tests/kernels/core/test_rotary_embedding_mla_cache_fused.py`

### Summary

**What changed and why**  
The PR fixes a failing test by replacing `current_platform.seed_everything(seed)` with `set_random_seed(seed)` from `vllm.utils.torch_utils` and adding a `default_vllm_config` fixture parameter to the test function. This ensures consistent random seed initialization across platforms and aligns with the project's utility functions.

**Technical impact**  
These changes standardize random seed management in tests, removing platform-specific seeding logic. The addition of `default_vllm_config` as a fixture parameter follows test infrastructure conventions but doesn't alter the test's functional behavior.

**Potential risks**  
If `set_random_seed` behaves differently than `current_platform.seed_everything` on certain platforms, it could affect test reproducibility. The unused `default_vllm_config` parameter could confuse developers about its purpose since it doesn't impact test logic.

**Key insights**  
Always use project-specific utility functions (`set_random_seed`) over platform-specific abstractions for consistency. When adding fixture parameters, ensure they're actually utilized or clearly documented as no-op additions to maintain code clarity.

---

## 6. [fix offline inference chat response prompt](https://github.com/vllm-project/vllm/pull/32088)


### Base Information

- **PR Number:** #32088
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-11 06:01:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32088/files) (2):**
  - `examples/offline_inference/context_extension.py`
  - `examples/offline_inference/spec_decode.py`

### Summary

**What changed and why**  
This PR fixes offline inference examples to correctly print the original prompt alongside generated responses. The issue was that `output.prompt` contained tokenized input, not the human-readable prompt. Changes modify `context_extension.py` to return and use the original conversation list, and update `spec_decode.py` to reference the original prompt list instead of `output.prompt`.

**Technical impact**  
The changes ensure offline examples display meaningful prompts during debugging or output verification. This improves developer experience by aligning printed prompts with user inputs, especially for chat-based and multimodal prompts. The architecture remains unchanged; only the display logic in example scripts is affected.

**Potential risks**  
If `conversations` list length mismatches `outputs` in `context_extension.py`, it could cause index errors. In `spec_decode.py`, the conditional logic assumes `prompts[i]` is always accessible and correctly formatted, which may fail if prompts are preprocessed unexpectedly. No validation ensures list consistency.

**Key insights**  
Always pass original user inputs alongside model outputs for clear debugging. Consider adding length checks or using `zip()` for iteration safety. These changes are confined to examples, but similar patterns in production code should ensure prompt traceability without relying on tokenized internal fields.

---

## 7. [[FixBug] Improve exception string in `tensorizer.py`](https://github.com/vllm-project/vllm/pull/31680)


### Base Information

- **PR Number:** #31680
- **Author:** [maang-h](https://github.com/maang-h)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-11 05:01:53
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31680/files) (1):**
  - `vllm/model_executor/model_loader/tensorizer.py`

### Summary

**What changed and why**  
The change fixes a bug where `ValueError` was incorrectly called with multiple arguments, causing error messages to display as raw tuples instead of formatted strings. Specifically, it converts `ValueError("Unsupported file: %s", tensor_path)` to use an f-string for proper string formatting.

**Technical impact**  
This improves error handling and debugging by ensuring clear, readable error messages when unsupported adapter model files are encountered. The change maintains the same functional behavior but enhances the user experience with better error reporting.

**Potential risks**  
The risk is minimal as this only affects error message formatting. However, the new error message now specifies exact file requirements (".safetensors or .bin"), which could confuse users if other valid file types are added in the future without updating this message.

**Key insights**  
Always use proper string formatting when raising exceptions to ensure readable error messages. Consider making error messages forward-compatible by avoiding overly specific constraints unless they're guaranteed to remain constant. This fix demonstrates good attention to detail in error handling code.

---

## 8. [[Misc] fix this log format not space](https://github.com/vllm-project/vllm/pull/32112)


### Base Information

- **PR Number:** #32112
- **Author:** [lengrongfu](https://github.com/lengrongfu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-11 05:01:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32112/files) (1):**
  - `vllm/v1/engine/core_client.py`

### Summary

**What changed and why**  
The change fixes a missing space in an error message string. The original log output incorrectly concatenated "send" and "initial" without a space, while the corrected version adds a space between these words for proper readability.

**Technical impact**  
This is a minor cosmetic change that only affects the formatting of a specific error log. It does not impact any functional behavior, performance, or system logic—purely improving log clarity for debugging and monitoring.

**Potential risks**  
The risk is negligible since it only modifies a string literal. However, if any automated tests or monitoring systems rely on exact string matching for this error message, they may need updating to reflect the new spacing.

**Key insights**  
Always ensure log messages are grammatically correct and readable, as they aid in debugging. While such changes are low-risk, verify that no downstream processes depend on the exact original string format. Consider adding a test to catch similar formatting issues in the future.

---

## 9. [[CI/Build] Separate out flaky responses API tests](https://github.com/vllm-project/vllm/pull/32110)


### Base Information

- **PR Number:** #32110
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-11 05:01:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32110/files) (10):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/entrypoints.yaml`
  - `tests/entrypoints/openai/responses/__init__.py`
  - `tests/entrypoints/openai/responses/test_errors.py`
  - `tests/entrypoints/openai/responses/test_function_call_parsing.py`
  - `tests/entrypoints/openai/responses/test_harmony.py`
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`
  - `tests/entrypoints/openai/responses/test_parsable_context.py`
  - `tests/entrypoints/openai/responses/test_simple.py`

### Summary

**What changed and why**  
This PR isolates flaky Responses API tests (particularly tool calling) into a separate CI pipeline step. The changes modify three Buildkite configuration files to add a dedicated `Entrypoints Integration Test (Responses API)` step and update existing OpenAI entrypoint test commands to exclude the `responses` directory, preventing retries from rerunning these flaky tests.

**Technical impact**  
The CI pipeline now runs Responses API tests independently, reducing the cost of retrying flaky tests that previously caused broader entrypoint test failures. This improves test stability and resource efficiency by isolating problematic tests without altering the underlying test logic or functionality.

**Potential risks**  
If the new dedicated step fails, it may not block other critical tests, potentially allowing regressions in the Responses API to go unnoticed. Additionally, the import path adjustments in test files (`...utils` to `....utils`) could break test execution if relative imports are not correctly resolved in all environments.

**Key insights**  
Developers should monitor the new Responses API test step for flakiness and consider adding appropriate blocking grades to ensure failures are addressed. The import path changes should be verified in the test environment to prevent module resolution errors. This isolation strategy is a good practice for managing flaky tests but requires ongoing maintenance to ensure coverage remains comprehensive.

---

## 10. [[Model Runner V2] Simplify InputBuffers](https://github.com/vllm-project/vllm/pull/32111)


### Base Information

- **PR Number:** #32111
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-01-11 01:17:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32111/files) (8):**
  - `vllm/v1/worker/gpu/block_table.py`
  - `vllm/v1/worker/gpu/buffer_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/states.py`
  - `vllm/v1/worker/gpu/structured_outputs.py`

### Summary

**What changed and why**  
This PR modernizes GPU buffer management by replacing `UvaBackedGpuTensor` with plain GPU tensors and introducing a new `UvaBufferPool` for staging CPU→UVA→GPU copies. The changes simplify input preparation across the GPU worker, removing redundant abstractions and unifying buffer handling through UVA-backed staging.

**Technical impact**  
The refactor centralizes buffer management via `UvaBufferPool`, which handles concurrent CPU-to-UVA copies and provides direct GPU tensor access. This reduces coupling between CPU and GPU views, streamlines data flow (e.g., `query_start_loc` is now computed on-device), and updates multiple components (attention metadata, kernels, sampling) to consume tensors directly.

**Potential risks**  
- The removal of `.gpu`/`.cpu` coupling may introduce subtle bugs if callers still expect synchronized views.  
- Dynamic reallocation in `StagedWriteTensor.write_contents` could cause race conditions if not properly synchronized.  
- Changes to kernel interfaces (e.g., passing tensors instead of UVA-backed objects) require validation across all call sites.

**Key insights**  
- The shift to `UvaBufferPool` improves concurrency and reduces unnecessary copies, but developers must ensure all staging buffers are correctly sized and managed.  
- Kernel and metadata interfaces now rely on plain tensors; verify that all downstream consumers (e.g., attention backends) are updated.  
- Monitor performance for regressions in UVA transfer overhead, especially in high-concurrency scenarios.

---

## 11. [[Misc] Make `scipy` as optional audio/benchmark dependency](https://github.com/vllm-project/vllm/pull/32096)


### Base Information

- **PR Number:** #32096
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-11 00:18:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32096/files) (3):**
  - `requirements/common.txt`
  - `setup.py`
  - `vllm/multimodal/audio.py`

### Summary

**What changed and why**  
The PR makes `scipy` an optional dependency by removing it from common requirements and adding it to the `audio` and `bench` extras in `setup.py`. In `audio.py`, `scipy.signal` is imported via a try-except block with a placeholder, and the lazy import inside `resample_audio_scipy` is replaced with the pre-imported alias. This reduces the default installation footprint, as `scipy` is only needed for audio resampling in Phi4-MM.

**Technical impact**  
These changes decouple `scipy` from the core dependencies, aligning it with other audio-related packages like `librosa`. The `resample_audio_scipy` function now relies on the globally imported `scipy_signal` alias, which will raise an informative error if `scipy` is missing when the function is called. This maintains functionality for users who install the `audio` extra while slimming requirements for others.

**Potential risks**  
If `scipy` is not installed and `resample_audio_scipy` is invoked (e.g., via Phi4-MM audio processing), it will raise an `ImportError` at runtime. The placeholder approach may produce less clear error messages compared to a direct import. Additionally, any other parts of the codebase that indirectly depend on `scipy` could break if not covered by the optional extra.

**Key insights**  
Developers should ensure that any use of `scipy` in the codebase is guarded by similar optional imports or clearly documented as requiring the `audio` extra. Testing should verify that Phi4-MM audio functionality works correctly with the `audio` extra installed and fails gracefully without it. Consider updating documentation to highlight the new optional dependency.

---

## 12. [[KVConnector] OffloadingConnector: Fix bug in handling of preemptions](https://github.com/vllm-project/vllm/pull/29870)


### Base Information

- **PR Number:** #29870
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-11 00:05:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29870/files) (7):**
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `tests/v1/kv_offload/test_worker.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/base.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`
  - `vllm/v1/kv_offload/worker/cpu_gpu.py`
  - `vllm/v1/kv_offload/worker/worker.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR fixes a bug in the OffloadingConnector where preempted requests could cause data corruption or incomplete KV cache transfers. The changes ensure that when requests are preempted, any in-flight store operations for those requests are properly completed or flushed before their GPU blocks are overwritten. This prevents stale data from being stored and allows re-computed blocks to be stored correctly.

**Technical impact**  
The changes introduce a new `handle_preemptions()` method in the KV connector base class, which is called from `gpu_model_runner` before blocks are overwritten. The OffloadingConnector scheduler now marks in-flight stores as complete for preempted requests, while the worker side submits deferred store jobs and waits for them. The CPU↔GPU offloading handler now tracks CUDA events per job and implements a blocking `wait()` method to ensure synchronization.

**Potential risks**  
If the `wait()` implementation does not properly synchronize CUDA events, it could lead to race conditions or deadlocks. The changes assume that preempted request IDs are correctly propagated through the system; any mismatch could cause missed flushes. Additionally, the added blocking waits might introduce latency spikes if many preemptions occur simultaneously.

**Key insights**  
Developers should verify that the `wait()` method in all offloading handlers correctly synchronizes transfers. The preemption handling logic is critical for data integrity—ensure it is triggered at the right time (before block overwrites). The test updates provide a good foundation, but consider adding stress tests for concurrent preemptions and large-scale transfers.

---

