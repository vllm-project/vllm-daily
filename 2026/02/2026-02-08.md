# vLLM Merged PR Report

**Report Date:** 2026-02-08 PST

**Total Merged PRs:** 11

---

## 1. [[BugFix] Fix `fastsafetensors` TP all procs using all GPUs](https://github.com/vllm-project/vllm/pull/34070)


### Base Information

- **PR Number:** #34070
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-08 23:15:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34070/files) (1):**
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
This PR fixes a GPU memory waste issue in multi-GPU deployments when using `fastsafetensors`. Previously, all worker processes initialized CUDA contexts on all visible GPUs, causing significant memory overhead. The changes disable GDS (GPU Direct Storage) mode for tensor parallelism cases (TP > 1) and adjust device assignment to use the current device instead of the process group rank.

**Technical impact**  
The fix modifies the weight loading logic to avoid unnecessary CUDA context creation across all GPUs. By setting `nogds=True` when `pg.size() > 1`, it prevents the GDS subsystem from initializing on every GPU. Additionally, device assignment now uses `current_platform.current_device()` instead of `pg.rank()`, aligning with the actual GPU mapping and reducing memory waste.

**Potential risks**  
Disabling GDS could theoretically impact I/O performance for weight loading, though the PR notes no negative impact in tests. There’s a risk if GDS provides benefits in specific hardware configurations not covered in testing. The fallback retry logic when GDS is not enabled adds robustness but may mask underlying issues if GDS fails silently.

**Key insights**  
The fix is critical for efficient multi-GPU memory usage and paves the way for enabling `fastsafetensors` by default. Developers should verify that weight loading performance remains acceptable in their deployment environments. Consider monitoring for any regressions in I/O-bound scenarios, especially with large models or high-speed storage.

---

## 2. [[Frontend][last/5] Make pooling entrypoints request schema consensus.](https://github.com/vllm-project/vllm/pull/31127)


### Base Information

- **PR Number:** #31127
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-08 22:42:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31127/files) (24):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `docs/features/multimodal_inputs.md`
  - `docs/serving/openai_compatible_server.md`
  - `examples/pooling/classify/vision_classification_online.py`
  - `examples/pooling/embed/template/dse_qwen2_vl.jinja`
  - `examples/pooling/embed/template/vlm2vec_phi3v.jinja`
  - `examples/pooling/embed/template/vlm2vec_qwen2vl.jinja`
  - `examples/pooling/embed/vision_embedding_offline.py`
  - `examples/pooling/embed/vision_embedding_online.py`
  - `examples/pooling/pooling/vision_language_pooling.py`
  - `examples/pooling/score/vision_rerank_api_online.py`
  - `examples/pooling/score/vision_reranker_offline.py`
  - `examples/pooling/score/vision_score_api_online.py`
  - `tests/entrypoints/pooling/classify/test_offline.py`
  - `tests/entrypoints/pooling/embed/test_online_vision.py`
  - `tests/renderers/test_hf.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/utils/print_utils.py`

### Summary

**What changed and why**  
This PR refactors pooling entrypoints to request schema consensus, addressing issue #33813. The changes reorganize pooling-related examples, consolidate protocol field definitions, and update documentation to reflect a unified schema approach. Key modifications include renaming and restructuring example files, moving common parameters to a base protocol, and updating test configurations.

**Technical impact**  
The refactoring centralizes common pooling parameters (like `mm_processor_kwargs` and `cache_salt`) into `PoolingBasicRequestMixin`, reducing duplication across embedding, classification, and scoring protocols. This improves maintainability and ensures consistent behavior across pooling tasks. The reorganization of examples separates embedding, classification, and scoring into distinct directories, clarifying their purposes and simplifying future updates.

**Potential risks**  
Moving parameters to a base mixin could inadvertently affect downstream request validation if any protocol-specific overrides were previously in place. The renaming of example files and updates to documentation paths may break external references or scripts that rely on old file locations. Additionally, changes to test pipelines (e.g., updating test commands) must be validated to avoid regressions in CI/CD.

**Key insights**  
Developers should verify that all pooling entrypoints correctly inherit the base mixin and that no protocol-specific parameters are missing. The consolidation of parameters promotes cleaner code but requires careful review of any custom implementations. Updated example paths in documentation and tests must be consistently applied to avoid broken links or runtime errors.

---

## 3. [[Tiny] Rename encoder budget file to more specific name](https://github.com/vllm-project/vllm/pull/34103)


### Base Information

- **PR Number:** #34103
- **Author:** [reaganjlee](https://github.com/reaganjlee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-08 19:48:19
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34103/files) (5):**
  - `vllm/lora/model_manager.py`
  - `vllm/multimodal/encoder_budget.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The PR renames `vllm/multimodal/budget.py` to `vllm/multimodal/encoder_budget.py` and updates all import statements accordingly. This change aligns the filename more specifically with its purpose—managing encoder-related budgets—as suggested in a prior code review discussion.

**Technical impact**  
This is a straightforward refactoring with no functional changes. The update ensures consistency across the codebase by using a more descriptive module name, which improves clarity but does not affect runtime behavior or architecture.

**Potential risks**  
The risk is minimal since only the filename and imports are modified. However, any external scripts or tools that directly reference the old module path would break unless they are updated. Additionally, if the file was previously imported via dynamic or indirect means, those references might need adjustment.

**Key insights**  
This change enhances code maintainability by using a more precise naming convention. Developers should verify that all references to the old module are updated, including in documentation, configuration files, or any automated tooling. Such renames are low-risk but require thorough validation to avoid hidden import errors.

---

## 4. [[bug-fix] supported_tasks is breaking backward compatibility at init_app_state](https://github.com/vllm-project/vllm/pull/34027)


### Base Information

- **PR Number:** #34027
- **Author:** [kouroshHakha](https://github.com/kouroshHakha)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-08 17:46:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34027/files) (1):**
  - `vllm/entrypoints/openai/api_server.py`

### Summary

**What changed and why**  
The changes make the `supported_tasks` parameter optional in `build_app` and `init_app_state` functions, defaulting to `("generate",)` when not provided. This addresses a backward compatibility break where `supported_tasks` became mandatory, disrupting existing integrations. A deprecation warning is added to notify users that the parameter will be required in future versions.

**Technical impact**  
This restores backward compatibility for applications that integrate with vLLM at the `init_app_state` layer without explicitly passing `supported_tasks`. The `engine_client` remains the source of truth for supported tasks, but the fallback ensures existing code continues to function. The warning guides users toward future compliance.

**Potential risks**  
If users ignore the deprecation warning, their code may break in future releases when the parameter becomes mandatory. The fallback to `("generate",)` might not match the actual capabilities of the `engine_client`, potentially leading to mismatches between advertised and available tasks if not updated.

**Key insights**  
Developers should update their integrations to explicitly pass `supported_tasks` from `engine_client.supported_tasks` to avoid future breaks and ensure task alignment. The deprecation warning provides a clear migration path. Review all call sites of `build_app` and `init_app_state` to ensure they handle `supported_tasks` appropriately.

---

## 5. [[Release 2.10] Update to Torch 2.10 - final release](https://github.com/vllm-project/vllm/pull/30525)


### Base Information

- **PR Number:** #30525
- **Author:** [atalman](https://github.com/atalman)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-08 13:51:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30525/files) (17):**
  - `.buildkite/image_build/image_build.yaml`
  - `CMakeLists.txt`
  - `cmake/external_projects/triton_kernels.cmake`
  - `pyproject.toml`
  - `requirements/build.txt`
  - `requirements/cuda.txt`
  - `requirements/rocm-build.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/compile/test_aot_compile.py`
  - `tests/compile/test_dynamic_shapes_compilation.py`
  - `tests/kernels/moe/test_shared_fused_moe_routed_transform.py`
  - `vllm/compilation/compiler_interface.py`
  - `vllm/compilation/decorators.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/batch_invariant.py`
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`

### Summary

**What changed and why**  
This PR upgrades the project from PyTorch 2.9.1 to 2.10.0, including associated libraries (torchvision, torchaudio, triton). It updates version constraints across configuration files, CI/CD pipelines, Docker builds, and test suites to align with the new release. The changes also address breaking API changes in triton_kernels by implementing replacement functions for removed routing utilities.

**Technical impact**  
The upgrade enables access to new PyTorch 2.10 features and improvements, but requires careful synchronization of all dependency versions. The triton_kernels update from v3.5.0 to v3.6.0 introduces API changes that necessitated custom replacement functions (`legacy_routing`, `legacy_routing_from_bitmatrix`). Build and test pipelines now target PyTorch's test index for pre-release stability, and compilation tests have been updated to reference the stable 2.10.0 version instead of development builds.

**Potential risks**  
The triton_kernels API changes could introduce subtle behavioral differences in MoE routing logic. Using the test index (`--prerelease=allow`) may expose the project to less stable PyTorch builds. One test has been temporarily disabled due to failures with PyTorch 2.10.0, indicating possible compatibility issues. The increased Docker build timeout suggests potential longer build durations.

**Key insights**  
Developers should verify that the custom routing implementations match the original behavior, particularly for MoE layers. The disabled test (#33995) should be prioritized for investigation. All downstream dependencies should be validated against PyTorch 2.10.0, and the team should monitor for any performance regressions or compilation issues introduced by the new version.

---

## 6. [Add support for ModelOpt MXFP8 dense models](https://github.com/vllm-project/vllm/pull/33786)


### Base Information

- **PR Number:** #33786
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-08 11:16:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33786/files) (6):**
  - `docs/features/quantization/modelopt.md`
  - `vllm/config/model.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/mxfp8_utils.py`

### Summary

**What changed and why**  
This PR adds support for ModelOpt MXFP8 quantization for dense models. The changes introduce a new quantization method `modelopt_mxfp8` that handles MXFP8-serialized checkpoints, including configuration detection, weight loading, and linear operation implementation. Documentation and configuration files are updated accordingly.

**Technical impact**  
The implementation integrates MXFP8 as a new quantization backend with proper validation for serialized checkpoints. It leverages existing ModelOpt infrastructure but adds specific handling for MXFP8's block-scaled format (32-element blocks) and E4M3 weight encoding. The changes are backward compatible and do not affect existing quantization methods.

**Potential risks**  
- MXFP8 requires Blackwell (SM100) or newer GPUs; older hardware will fail or fallback to emulation.
- No MoE support yet—attempting to use MXFP8 with MoE models will raise `NotImplementedError`.
- The format is experimental and may change, as noted in the warning log.
- Performance tests show ~23% lower throughput compared to BF16, which could impact user expectations.

**Key insights**  
- Developers must ensure checkpoints are properly serialized with ModelOpt; dynamic quantization is not supported.
- The implementation uses emulation backend initially; future hardware acceleration may improve performance.
- Validation includes strict dtype and shape checks for weights and scales, ensuring compatibility with the block-scaled format.

---

## 7. [glm 4.6 fused tuned inference config for B200](https://github.com/vllm-project/vllm/pull/32958)


### Base Information

- **PR Number:** #32958
- **Author:** [navmarri14](https://github.com/navmarri14)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-08 10:55:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32958/files) (1):**
  - `vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=NVIDIA_B200,dtype=fp8_w8a8.json`

### Summary

**What changed and why**  
A new JSON configuration file was added to provide pre-tuned kernel parameters for the fused MoE (Mixture of Experts) operation targeting the GLM-4.6 MoE architecture on NVIDIA B200 GPUs with FP8 quantization. This configuration eliminates the need for just-in-time (JIT) tuning during startup, reducing initialization latency and ensuring optimal performance for specific model parameters (E=160, N=384, TP=4).

**Technical impact**  
The addition provides a static, performance-optimized kernel configuration for a previously unsupported hardware/model combination. This improves startup time by bypassing heuristic-based or JIT kernel selection, and ensures consistent, efficient execution of the fused MoE layer for GLM-4.6 variants under tensor parallelism of 4 on B200 GPUs.

**Potential risks**  
The configuration is highly specific to the given parameters (E=160, N=384, TP=4, dtype=fp8_w8a8, B200). Using it outside these exact conditions may lead to suboptimal performance or errors. Additionally, any future changes to the underlying Triton compiler version (3.5.1) or GPU architecture could necessitate re-tuning.

**Key insights**  
This is a performance-tuning addition that follows vLLM's established pattern for hardware-specific kernel configurations. Developers should note the narrow applicability of this file and ensure it is only loaded for matching runtime conditions. Consider documenting the supported parameter matrix to prevent misuse.

---

## 8. [[torch.compile] Add an option to force-enable the MOE cold start optimization](https://github.com/vllm-project/vllm/pull/33735)


### Base Information

- **PR Number:** #33735
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-08 10:42:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33735/files) (3):**
  - `vllm/config/compilation.py`
  - `vllm/config/vllm.py`
  - `vllm/forward_context.py`

### Summary

**What changed and why**  
The changes introduce a tri-state configuration (`True`/`False`/`None`) for the MOE cold start optimization, replacing a previous boolean that was automatically disabled for speculative decoding. This allows users to force-enable the optimization when they are certain it applies to their speculative decoding models, while maintaining safe defaults.

**Technical impact**  
The optimization's behavior is now explicitly controlled via the `fast_moe_cold_start` field, which defaults to `None` to preserve backward compatibility. The resolution logic in `vllm.py` sets it to `False` only when speculative decoding is active (unless explicitly overridden), and the forward context logic is simplified to directly use the resolved boolean value.

**Potential risks**  
If users force-enable the optimization (`fast_moe_cold_start=True`) in speculative decoding scenarios where the draft model contains MOE layers, it could lead to silent correctness issues. The documentation warns about this, but there is no runtime validation to prevent misuse.

**Key insights**  
This change provides flexibility for advanced users while keeping the default behavior safe. Developers should carefully verify that their speculative decoding models meet the optimization's assumptions before enabling it. Consider adding a warning log when the flag is explicitly set to `True` with speculative decoding enabled.

---

## 9. [[BugFix] Change support no act and mul for marlin](https://github.com/vllm-project/vllm/pull/34088)


### Base Information

- **PR Number:** #34088
- **Author:** [TomerBN-Nvidia](https://github.com/TomerBN-Nvidia)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-08 09:18:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34088/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`

### Summary

**What changed and why**  
The PR changes a single boolean flag from `False` to `True` in the Marlin MoE experts implementation. This enables the `_supports_no_act_and_mul` flag, indicating that the Marlin fused MoE kernel already supports the "no-activation-and-multiply" optimization path, which was previously not exposed.

**Technical impact**  
This change allows the system to utilize a more efficient computation path in Marlin's fused MoE kernels when applicable, potentially improving performance for models using Mixture of Experts (MoE) by avoiding unnecessary activation and multiplication operations. It aligns the configuration flag with the kernel's actual capabilities.

**Potential risks**  
The main risk is if the kernel's claimed support for the no-act-and-mul path is incomplete or buggy in certain edge cases (e.g., specific data types, tensor shapes, or hardware configurations). This could lead to incorrect computations or runtime errors when the optimization is triggered.

**Key insights**  
Developers should verify that the Marlin kernel's no-act-and-mul implementation is thoroughly tested across all supported scenarios. Consider adding targeted tests for this path if not already present. This is a low-risk change if the kernel support is indeed mature, but it should be monitored in performance benchmarks and validation runs.

---

## 10. [[Revert] Fix performance regression for GLM-4.7-GPTQ decode and MTP acceptance rate](https://github.com/vllm-project/vllm/pull/33771)


### Base Information

- **PR Number:** #33771
- **Author:** [aabbccddwasd](https://github.com/aabbccddwasd)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-08 08:13:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33771/files) (1):**
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
This PR reverts a performance regression by changing how sequence lengths are accessed in the FlashInfer attention backend. The change replaces a `.cpu()` call with direct access to a pre-computed `seq_lens_cpu` attribute, restoring proper kernel selection (fused_triton) for unquantized MTP layers in the GLM-4.7-GPTQ model.

**Technical impact**  
The modification eliminates an unnecessary CPU tensor copy during attention backend construction, which was causing degraded decode performance and incorrect kernel dispatch. This restores the original throughput (95 tps) and improves the Multi-Token Prediction acceptance rate by approximately 30% for the affected model configuration.

**Potential risks**  
The change assumes `common_attn_metadata.seq_lens_cpu` is always available when `needs_seq_lens_cpu` is true, which could cause issues if the metadata structure changes or the attribute is missing in certain execution paths. Additionally, reverting this specific fix might reintroduce issues that the original `.cpu()` call was intended to solve, such as async mode compatibility.

**Key insights**  
Performance optimizations in hot paths like attention backends require careful validation of tensor access patterns. The benchmark data clearly shows the significant impact of unnecessary device-to-host copies on throughput. Developers should verify that pre-computed CPU attributes are consistently maintained across all metadata initialization paths to prevent similar regressions.

---

## 11. [Add embedding input functionality for disabled modalities [remake]](https://github.com/vllm-project/vllm/pull/32493)


### Base Information

- **PR Number:** #32493
- **Author:** [reaganjlee](https://github.com/reaganjlee)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-08 04:57:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32493/files) (10):**
  - `tests/entrypoints/llm/test_mm_embeds_only.py`
  - `tests/multimodal/test_processing.py`
  - `vllm/config/multimodal.py`
  - `vllm/entrypoints/chat_utils.py`
  - `vllm/multimodal/budget.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/dummy_inputs.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR enables embedding input functionality when modalities are disabled (limit=0) by modifying validation logic. Previously, setting `limit_mm_per_prompt={"image": 0}` would disable all multimodal processing. Now, with `enable_mm_embeds=True`, precomputed embeddings bypass count validation for limit=0 modalities, allowing embedding inputs while saving memory by not loading encoder modules.

**Technical impact**  
The changes affect multiple components: validation logic now skips count checks for embeddings when limit=0, budget calculation distinguishes between tower modalities (requiring encoder) and embed-only modalities, and encoder profiling is skipped in embed-only mode. This maintains backward compatibility—raw media inputs still fail validation when limit=0.

**Potential risks**  
If `enable_mm_embeds=True` is set without proper limit configuration, users might inadvertently allow embedding inputs while expecting full encoder behavior. Edge cases where embeddings have incorrect shapes could cause crashes, as noted in the config warning. The separation of tower vs. embed-only modalities in budget calculations adds complexity that must remain synchronized across components.

**Key insights**  
Developers should ensure `enable_mm_embeds=True` is only used with trusted inputs due to crash risks. The memory savings from not loading encoders are significant (demonstrated in tests), but this mode requires careful limit configuration. Integration tests comprehensively cover the new behavior and should be referenced when modifying related code.

---

