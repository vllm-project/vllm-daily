# vLLM Merged PR Report

**Report Date:** 2026-02-17 PST

**Total Merged PRs:** 28

---

## 1. [[Quantization] - Added uses_meta_device_weights to quant config](https://github.com/vllm-project/vllm/pull/34645)


### Base Information

- **PR Number:** #34645
- **Author:** [Josephasafg](https://github.com/Josephasafg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 23:43:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34645/files) (3):**
  - `vllm/model_executor/layers/quantization/base_config.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
Added a `uses_meta_device` boolean flag to quantization configuration classes to indicate when a quantization method creates weights on the meta device for online quantization. This provides a generic mechanism to check if dummy weights should be skipped during initial model loading, replacing a hardcoded check for FP8 quantization.

**Technical impact**  
The change decouples the weight initialization logic from specific quantization backends, making the system more extensible for future online quantization methods. It ensures that weights created on the meta device are properly handled in `process_weights_after_loading`, reducing peak memory usage during model loading.

**Potential risks**  
If a new quantization method sets `uses_meta_device=True` but does not properly implement `process_weights_after_loading`, weights may remain uninitialized. The check iterates over all modules, which could have a minor performance impact for very large models, though this is likely negligible.

**Key insights**  
This refactor improves code maintainability by replacing backend-specific checks with a polymorphic flag. Developers adding new online quantization methods should ensure they set `uses_meta_device=True` and correctly implement the weight processing logic. Consider adding validation or documentation to prevent misuse of the flag.

---

## 2. [[Bugfix] fix activation in cpu_fused_moe_torch call](https://github.com/vllm-project/vllm/pull/34696)


### Base Information

- **PR Number:** #34696
- **Author:** [michalowski-arm](https://github.com/michalowski-arm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 23:39:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34696/files) (1):**
  - `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`

### Summary

**What changed and why**  
The change fixes a bug where `cpu_fused_moe_torch` was incorrectly passed an `activation` enum object instead of its string value. The function expects a string argument, so `activation.value` is now used to extract the underlying string representation.

**Technical impact**  
This ensures compatibility between the enum-based interface and the underlying C++/Torch implementation that processes activation functions as strings. Without this fix, the activation parameter would be misinterpreted, potentially causing runtime errors or incorrect behavior in the fused MoE (Mixture of Experts) layer.

**Potential risks**  
If the `activation` enum lacks a `value` attribute or if the string value doesn't match expected activation names in the downstream implementation, it could lead to failures. Additionally, similar issues might exist elsewhere in the codebase where enums are passed to low-level functions.

**Key insights**  
Always verify that enum types are properly unwrapped when passing to functions expecting primitive types. Consider adding type annotations or runtime checks to catch such mismatches early. Review other calls to `cpu_fused_moe_torch` or similar functions to ensure consistent parameter handling.

---

## 3. [[Bugfix] Fix prefix creation for Qwen3.5](https://github.com/vllm-project/vllm/pull/34723)


### Base Information

- **PR Number:** #34723
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 23:39:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34723/files) (1):**
  - `vllm/model_executor/models/qwen3_5.py`

### Summary

**What changed and why**  
The fix addresses incorrect prefix handling for Qwen3.5 models by modifying how the `prefix` parameter is constructed in the model initialization. Specifically, it ensures that when loading checkpoints like `Qwen/Qwen3.5-397B-A17B` (which have a `model.language_model.layers.0` naming pattern), the prefix does not incorrectly add an extra "model" module, preventing mismatches in quantization ignore lists.

**Technical impact**  
These changes adjust the prefix logic in `Qwen3_5ForConditionalGeneration` and `Qwen3_5MoeForConditionalGeneration` to default to `prefix="model"`, and modify the conditional prefix assignment in `Qwen3_5VLForConditionalGeneration` to avoid duplicating "model" in the module path. This ensures correct parameter name mapping between Hugging Face checkpoints and vLLM's internal representation, which is critical for operations like quantization that rely on precise parameter name matching.

**Potential risks**  
If the prefix logic incorrectly identifies when "model" is already present, it could lead to parameter loading failures for other Qwen3.5 variants or future model versions. Additionally, the hardcoded default `prefix="model"` might not be compatible with all checkpoint structures, potentially breaking backward compatibility with previously supported models.

**Key insights**  
The fix is targeted and necessary for correct quantization support with specific Qwen3.5 checkpoints. Developers should verify that this prefix adjustment works for all Qwen3.5 model variants and consider adding unit tests to cover different checkpoint naming patterns. Monitoring model loading and quantization processes after this change is recommended to ensure no regressions occur.

---

## 4. [[Bugfix] Fix quant RMS norm fusion for quantization with TMA-aligned scales](https://github.com/vllm-project/vllm/pull/33255)


### Base Information

- **PR Number:** #33255
- **Author:** [ElizaWszola](https://github.com/ElizaWszola)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 23:35:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33255/files) (12):**
  - `csrc/ops.h`
  - `csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu`
  - `csrc/quantization/fused_kernels/layernorm_utils.cuh`
  - `csrc/quantization/w8a8/fp8/per_token_group_quant.cu`
  - `csrc/torch_bindings.cpp`
  - `tests/compile/fusions_e2e/test_tp1_quant.py`
  - `tests/kernels/core/test_fused_quant_layernorm.py`
  - `vllm/_custom_ops.py`
  - `vllm/compilation/passes/fusion/matcher_utils.py`
  - `vllm/compilation/passes/fusion/rms_quant_fusion.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`
  - `vllm/utils/deep_gemm.py`

### Summary

**What changed and why**  
This PR fixes quantization RMS norm fusion for TMA-aligned scales, addressing failures introduced by a previous change. The modifications enable proper handling of transposed scales with outer stride >1, ensuring correct kernel indexing and fusion pattern matching when scales are TMA-aligned (e.g., padded for memory alignment).

**Technical impact**  
The changes extend kernel functions (`compute_dynamic_per_token_scales`, `norm_and_quant`) to support an `outer_scale_stride` parameter, adjusting scale indexing for transposed layouts. Fusion patterns are updated to pass TMA-alignment flags through quant operations, and tests now include TMA-aligned scale configurations. This maintains performance (benchmarks show comparable TTFT/TPOT) while restoring fusion for aligned scales.

**Potential risks**  
- The `outer_scale_stride` logic assumes transposed scales when stride >1; incorrect tensor layouts could cause misindexing.  
- TMA-alignment padding may waste memory if not managed carefully (e.g., when `hidden_size // group_size` is already aligned).  
- The fix adds complexity to fusion pattern definitions, increasing maintenance burden.

**Key insights**  
- Developers must ensure scale tensors are correctly strided when using TMA-aligned scales; the kernel now validates `scales.stride(1) > 1` implies `is_scale_transposed`.  
- Fusion matching now requires explicit scale tensors in patterns, simplifying TMA-aligned scale handling but altering pattern signatures.  
- Test coverage expands to include TMA-alignment edge cases, though some redundant configurations are skipped to save time.

---

## 5. [[Model Runner V2] A bit more PP simplification](https://github.com/vllm-project/vllm/pull/34766)


### Base Information

- **PR Number:** #34766
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-17 21:39:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34766/files) (2):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/pp_utils.py`

### Summary

**What changed and why**  
The changes simplify pipeline parallel (PP) communication logic by removing duplicate rank checks. In `model_runner.py`, the code now directly unpacks the tuple returned from `pp_receive` instead of checking for `None`, and moves the broadcast condition inside the PP check. In `pp_utils.py`, the functions now use assertions to enforce rank expectations instead of early returns.

**Technical impact**  
These changes make the PP communication flow more explicit and less error-prone by enforcing rank expectations at function boundaries. The `pp_receive` function now always returns a tuple (non-last ranks) or asserts (last rank), eliminating the need for callers to handle `None` returns. Similarly, `pp_broadcast` asserts it's only called from the last rank.

**Potential risks**  
If assertions fail (e.g., `pp_broadcast` called from non-last rank), the program will crash immediately rather than silently skip operations. This could expose incorrect rank logic in edge cases or during future modifications. The changes assume the existing rank logic is correct and don't add fallback behavior.

**Key insights**  
The simplification improves code clarity by making rank responsibilities explicit. Developers should ensure all call sites respect the new contract: only last ranks call `pp_broadcast`, only non-last ranks call `pp_receive`. Consider adding documentation or type hints to reinforce these preconditions, as the runtime assertions provide the primary safety net.

---

## 6. [[CI/Build] Remove use of `skip_v1`](https://github.com/vllm-project/vllm/pull/34699)


### Base Information

- **PR Number:** #34699
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-17 20:19:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34699/files) (6):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `.buildkite/test_areas/samplers.yaml`
  - `pyproject.toml`
  - `tests/detokenizer/test_disable_detokenization.py`

### Summary

**What changed and why**  
This PR removes the `skip_v1` pytest marker from the codebase and CI configuration. The changes eliminate the marker from test commands in Buildkite pipelines and remove its definition from `pyproject.toml`. Additionally, the PR fixes an oversight where detokenizer tests were not being executed in CI by adding explicit test commands for them.

**Technical impact**  
The removal of `skip_v1` simplifies test execution by eliminating conditional test filtering based on this marker. This reduces configuration complexity and ensures all relevant tests run uniformly across CI pipelines. Adding explicit detokenizer test commands corrects a gap in test coverage, improving overall CI reliability.

**Potential risks**  
If any tests were legitimately skipped due to incompatibility with "v1" (presumably a legacy system or component), they may now fail unexpectedly. The PR description mentions one specific CI failure (#34560) will be handled separately, but other hidden dependencies on `skip_v1` could surface. There's also a risk that the detokenizer tests, now being run, might expose previously undetected issues.

**Key insights**  
This cleanup improves CI maintainability by removing obsolete test markers. Developers should verify that all tests passing with `skip_v1` removed are indeed compatible with the current system. The addition of detokenizer tests is a positive step toward comprehensive test coverage. Future work should address the specific failure mentioned and monitor CI results closely after this change.

---

## 7. [[ROCm][CI] Removed hard-coded attn backend requirement for Qwen VL](https://github.com/vllm-project/vllm/pull/34753)


### Base Information

- **PR Number:** #34753
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-17 19:59:53
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34753/files) (1):**
  - `tests/models/multimodal/generation/test_common.py`

### Summary

**What changed and why**  
Removed the hard-coded ROCm-specific attention backend (`ROCM_AITER_FA`) for Qwen VL tests, allowing the test to fall back to the default attention backend. This change was made because the AITER FA backend sacrifices exact logprob matching for performance, causing test failures on MI355 hardware.

**Technical impact**  
The test will now use the default attention backend instead of forcing a backend that prioritizes performance over accuracy. This ensures logprobs match expectations across platforms, improving test reliability while maintaining broader compatibility.

**Potential risks**  
If the default backend lacks optimizations for ROCm hardware, test performance may degrade. There is also a risk that other ROCm-specific issues could surface if the default backend behaves differently, though this is mitigated by the test's focus on correctness over speed.

**Key insights**  
This change prioritizes test correctness over hardware-specific performance, which is appropriate for validation tests. Developers should ensure that any performance-critical ROCm optimizations are validated separately, and consider adding backend-specific tests if needed for performance regression tracking.

---

## 8. [[Core] Fix SSRF bypass via backslash-@ URL parsing inconsistency](https://github.com/vllm-project/vllm/pull/34743)


### Base Information

- **PR Number:** #34743
- **Author:** [russellb](https://github.com/russellb)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-17 19:53:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34743/files) (2):**
  - `tests/multimodal/media/test_connector.py`
  - `vllm/multimodal/media/connector.py`

### Summary

**What changed and why**  
The fix addresses an SSRF bypass vulnerability (GHSA-v359-jj2v-j536) where backslash-@ sequences in URLs were parsed inconsistently between urllib3 (used for validation) and aiohttp/yarl (used for HTTP requests). By passing the urllib3-normalized URL (`url_spec.url`) to the HTTP client instead of the raw user-provided string, both layers now interpret the URL consistently, preventing attackers from bypassing domain allowlist checks.

**Technical impact**  
This change ensures that the domain validation logic and the actual HTTP request use the same URL representation, closing a security gap. The normalization converts backslashes to `%5C` in the path, making malicious host manipulation ineffective. The system now consistently rejects URLs that attempt to exploit parsing differences.

**Potential risks**  
Normalized URLs with encoded backslashes may result in 404 errors if servers expect literal backslashes, though this is acceptable for security. There is a minor risk of breaking existing integrations that rely on non-standard URL formats, but such usage would already be insecure. The fix assumes urllib3's parsing is authoritative, which should hold for standard compliance.

**Key insights**  
Always use normalized URLs after validation to prevent parsing discrepancies between libraries. This vulnerability highlights the importance of consistent URL interpretation across different layers of the stack. Developers should audit similar patterns where raw user input is passed to downstream HTTP clients without normalization.

---

## 9. [[torch.compile] Turn on silu+fp4 quant fusion by default for O1+](https://github.com/vllm-project/vllm/pull/34718)


### Base Information

- **PR Number:** #34718
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-17 19:29:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34718/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The change modifies the `enable_act_fusion` function to automatically enable activation fusion for NVFP4-quantized models. Previously, fusion was only enabled when specific custom ops (`silu_and_mul` or `quant_fp8`) were active. Since FP4 quantization always uses custom operations that Inductor cannot fuse, this ensures the fusion is applied by default for FP4 models at optimization levels O1 and above.

**Technical impact**  
This enables the SiLU-Mul + NVFP4 quant fusion by default for eligible models, improving inference performance without affecting functionality. The fusion is a pointwise operation with minimal startup overhead, as confirmed by the provided benchmarks showing neutral to positive performance impact across different model sizes and scenarios.

**Potential risks**  
The fusion may not benefit MoE models if `fused_moe` is wrapped, as noted in the PR description. Additionally, enabling fusion by default could introduce subtle behavior changes for FP4 models if there are edge cases where the fusion is incompatible, though the tests indicate stability. The change assumes `cfg.model_config` is non-null when checking `is_nvfp4_quantized()`, which could risk `None` access if the config is improperly initialized.

**Key insights**  
Developers should note that this optimization is now automatic for FP4 models at O1+, reducing manual configuration. Verify that MoE models with wrapped `fused_moe` are not adversely affected. Ensure model configurations are properly initialized to avoid `None` access errors when checking quantization status.

---

## 10. [[BugFix] [Build] fix string literals comparison in indexer_k_quant_and_cache calling site](https://github.com/vllm-project/vllm/pull/34653)


### Base Information

- **PR Number:** #34653
- **Author:** [hongxiayang](https://github.com/hongxiayang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 19:19:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34653/files) (1):**
  - `csrc/cache_kernels.cu`

### Summary

**What changed and why**  
The fix addresses a compilation error in Clang 20+ where string literal comparison using `==` fails when passing a C-style string pointer. Instead of directly passing `"fp8_e4m3"` to the `DISPATCH_BY_KV_CACHE_DTYPE` macro, a `static const std::string` variable is declared, enabling proper `operator==` comparison.

**Technical impact**  
This change ensures compatibility with stricter C++ standards enforcement in newer Clang versions, preventing compilation failures. The macro `DISPATCH_BY_KV_CACHE_DTYPE` likely relies on string comparison to dispatch based on data types, and using `std::string` guarantees correct comparison semantics.

**Potential risks**  
Introducing a `static const` variable in a CUDA kernel function could lead to initialization order issues if used across translation units, though it's likely safe within a single function scope. Additionally, the macro's internal logic might assume lightweight string handling, so using `std::string` could introduce minimal overhead, though negligible.

**Key insights**  
Always use `std::string` for string comparisons in C++ to avoid pointer comparison pitfalls. When fixing compiler-specific issues, verify that the solution doesn't introduce hidden dependencies or performance regressions. Consider if similar issues exist elsewhere in the codebase where string literals are compared directly.

---

## 11. [[Renderer] Move MM Hash parsing into Renderer](https://github.com/vllm-project/vllm/pull/34711)


### Base Information

- **PR Number:** #34711
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 19:18:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34711/files) (16):**
  - `tests/renderers/test_process_multi_modal_uuids.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/deepseek_vl2.py`
  - `vllm/model_executor/models/h2ovl.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/multimodal.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/parse.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/renderers/base.py`

### Summary

**What changed and why**  
This PR moves multi-modal UUID parsing from the processor to the renderer, normalizing `MultiModalUUIDDict` to `MultiModalUUIDItems` earlier in the pipeline. It removes `tokenization_kwargs` from MM hash calculation since they don't affect MM data, and cleans up MM data parsing by replacing `_is_empty` checks with modality selection.

**Technical impact**  
The changes centralize UUID handling in the renderer, simplifying processor interfaces. The hash calculation now excludes `tokenization_kwargs`, making caching more accurate for MM data. The parsing logic is more robust with explicit modality selection and better handling of empty data cases.

**Potential risks**  
The removal of `tokenization_kwargs` from hash calculation could affect cache behavior if any MM processors actually depend on these kwargs. The validation logic changes might handle edge cases differently, particularly around empty data and UUID mismatches. The interface changes across multiple model files require careful integration testing.

**Key insights**  
Developers should verify that MM caching still works correctly across different models, especially for edge cases with empty data or mixed UUID scenarios. The new `parse_mm_uuids` function provides a standardized way to normalize UUID inputs. The `select()` method on `MultiModalDataItems` offers cleaner modality filtering for processor operations.

---

## 12. [[CI] Remove unused precompiled wheel args from image build](https://github.com/vllm-project/vllm/pull/34767)


### Base Information

- **PR Number:** #34767
- **Author:** [amrmahdi](https://github.com/amrmahdi)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-17 18:58:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34767/files) (2):**
  - `.buildkite/image_build/image_build.sh`
  - `.buildkite/image_build/image_build.yaml`

### Summary

**What changed and why**  
Removed unused arguments `VLLM_USE_PRECOMPILED` and `VLLM_MERGE_BASE_COMMIT` from the image build script and pipeline configuration. These values were hardcoded to defaults and are no longer passed by the pipeline generator, simplifying the interface and aligning with prior cleanup efforts.

**Technical impact**  
The script now accepts fewer positional arguments (reduced from 7-8 to 5-6), shifting `IMAGE_TAG` to position `$5`. This reduces complexity and eliminates unused environment variables in the build process, though downstream Docker builds must no longer depend on these removed variables.

**Potential risks**  
If any Docker build stages or external scripts still reference `VLLM_USE_PRECOMPILED` or `VLLM_MERGE_BASE_COMMIT` environment variables, they may fail or behave unexpectedly. The argument shift could also break any external calls to the script that haven’t been updated to match the new signature.

**Key insights**  
Ensure all dependent build configurations and Dockerfiles are audited for references to the removed variables. Update any external documentation or automation that invokes the script to reflect the new argument order. This cleanup improves maintainability but requires validation of the full CI/CD chain.

---

## 13. [[Attention] Refactor `check_and_update_config`](https://github.com/vllm-project/vllm/pull/33600)


### Base Information

- **PR Number:** #33600
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 17:06:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33600/files) (4):**
  - `vllm/config/cache.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/platforms/cuda.py`
  - `vllm/v1/attention/backend.py`

### Summary

**What changed and why**  
The PR refactors `check_and_update_config` in the CUDA platform to eliminate duplicate logic for selecting attention backends and block sizes. Instead of maintaining separate logic, it now delegates to the existing `AttentionSelectorConfig` and `select_attention_backend` functions. This centralizes backend selection logic, simplifies maintenance, and improves user experience with clearer validation and warnings.

**Technical impact**  
The changes centralize attention backend and block size selection logic into a single code path. The `CacheConfig.block_size` type changes from a strict `Literal` to a general `int`, allowing more flexibility. The system now properly validates user-specified `--block-size` against backend compatibility and provides helpful warnings when suboptimal backends are chosen due to block size constraints.

**Potential risks**  
Removing the `BlockSize` literal validation could allow invalid block sizes to pass through type checking, though runtime validation remains. The refactored logic must correctly handle all edge cases previously covered by the duplicated code, particularly for hybrid attention-mamba models and sparse attention scenarios. The warning system for suboptimal backend selection adds new user-facing output that needs to be clear and actionable.

**Key insights**  
This refactoring significantly improves code maintainability by eliminating duplication. Developers should note that block size validation now happens at runtime through backend compatibility checks rather than static typing. The new warning system helps users understand performance implications of their configuration choices. Future changes to backend selection should be made in the selector module rather than platform-specific code.

---

## 14. [[Feature] Decode Context Parallel support for GPU model runner v2](https://github.com/vllm-project/vllm/pull/34179)


### Base Information

- **PR Number:** #34179
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-17 16:27:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34179/files) (5):**
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/block_table.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
This PR adds Decode Context Parallel (DCP) support to the GPU model runner v2, enabling KV cache sharding across multiple GPUs during decoding. The changes modify attention utilities, block table management, CUDA graph preparation, and model runner initialization to handle DCP-specific calculations like local sequence lengths and slot mappings.

**Technical impact**  
The implementation introduces DCP-aware slot mapping computation, where KV cache blocks are virtually sharded across DCP ranks. This affects how block tables and token positions are translated to physical GPU memory locations. The changes are backward compatible (DCP size = 1 defaults to original behavior) and integrate with existing CUDA graph capture mechanisms.

**Potential risks**  
Performance is currently slightly slower than V1, as noted in the PR description. The slot mapping kernel now includes additional branching for DCP rank checks, which may impact throughput. Edge cases with interleaved KV cache configurations (`cp_kv_cache_interleave_size`) need thorough validation. The `prepare_dcp_local_seq_lens` function assumes proper synchronization; asynchronous copies could cause race conditions if not carefully managed.

**Key insights**  
Developers should treat this as a foundational implementation with optimization opportunities in follow-up PRs. The DCP integration is designed to be transparent when disabled. Special attention is needed for CUDA graph safety—persistent buffers like `dcp_local_seq_lens` must be correctly zeroed. Testing should expand beyond GSM8K to include varied DCP configurations and interleaving settings.

---

## 15. [[Model Runner V2] Further simplification for PP](https://github.com/vllm-project/vllm/pull/34724)


### Base Information

- **PR Number:** #34724
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-17 15:18:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34724/files) (3):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/pp_handler.py`
  - `vllm/v1/worker/gpu/pp_utils.py`

### Summary

**What changed and why**  
The PR replaces the `PPHandler` class with simpler utility functions `pp_broadcast` and `pp_receive` in `pp_utils.py`. This eliminates the need to instantiate and manage a handler object, streamlining pipeline parallelism (PP) communication by converting class methods into standalone functions.

**Technical impact**  
The changes reduce code complexity by removing the `PPHandler` class and its initialization logic. The utility functions directly handle broadcast/receive operations, and a notable optimization combines `num_sampled` and `num_rejected` tensors into a single broadcast to reduce communication overhead. The model runner now imports and calls these functions directly, simplifying the PP flow.

**Potential risks**  
The combined tensor broadcast assumes `num_sampled` and `num_rejected` have the same dtype and shape, which could break if future changes alter their formats. Removing the class may affect extensibility if additional PP state or methods are needed later. The functions rely on `get_pp_group()` for device info, which must be correctly initialized in all PP contexts.

**Key insights**  
This refactor aligns with simplification goals but should ensure the combined tensor approach is documented and tested for edge cases. Developers should verify that `num_sampled` and `num_rejected` remain compatible in future updates. Consider adding type hints or assertions to safeguard the tensor structure in `pp_utils`.

---

## 16. [[Kernel] Triton-based Top-k and Top-p sampler kernels](https://github.com/vllm-project/vllm/pull/33538)


### Base Information

- **PR Number:** #33538
- **Author:** [cakeng](https://github.com/cakeng)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-17 15:14:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33538/files) (6):**
  - `benchmarks/benchmark_topk_topp.py`
  - `tests/entrypoints/instrumentator/test_basic.py`
  - `tests/v1/sample/test_topk_topp_sampler.py`
  - `vllm/utils/math_utils.py`
  - `vllm/v1/sample/ops/topk_topp_sampler.py`
  - `vllm/v1/sample/ops/topk_topp_triton.py`

### Summary

**What changed and why**  
This PR introduces a Triton-based kernel for combined Top-k and Top-p sampling, replacing the previous PyTorch sorting implementation for larger batch sizes. The kernel uses a pivot-based truncation algorithm to reduce search space, improving performance while maintaining correctness. A comprehensive benchmark script and extensive unit tests are added to validate performance and correctness against the PyTorch baseline.

**Technical impact**  
The new kernel significantly accelerates Top-k and Top-p operations, especially for larger batch sizes and vocabularies, as shown in the benchmark data (e.g., up to 30x speedup in some scenarios). It introduces an ~80 MiB VRAM overhead for temporary buffers and uses a more accurate Top-p termination condition. The implementation automatically switches between Triton and PyTorch backends based on batch size, ensuring optimal performance across different workloads.

**Potential risks**  
The additional VRAM usage (~80 MiB) could impact memory-constrained environments. The pivot-based algorithm may introduce subtle numerical differences in Top-p results due to floating-point precision, though tests allow small tolerances. Edge cases with many -inf logits (e.g., from grammar masks) require careful handling to avoid NaN propagation. The kernel’s performance on Top-p-only cases is sometimes slower than the previous PR (#32558), as noted in the description.

**Key insights**  
Developers should be aware of the memory trade-off and verify that the VRAM overhead is acceptable for their deployment scale. The benchmark results show the Triton kernel excels in mixed Top-k/Top-p and large-batch scenarios but may underperform in Top-p-only cases. The extensive test suite provides confidence in correctness, but numerical differences in Top-p results should be monitored in production. Consider batch-size thresholds when tuning for specific workloads.

---

## 17. [[Bugfix][MTP][Sparse MLA] Allow sparse MLA with MTP to run with FULL cudagraphs](https://github.com/vllm-project/vllm/pull/34457)


### Base Information

- **PR Number:** #34457
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-17 11:01:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34457/files) (2):**
  - `docs/design/cuda_graphs.md`
  - `vllm/v1/attention/backends/mla/indexer.py`

### Summary

**What changed and why**  
The PR modifies `DeepseekV32IndexerMetadataBuilder` to report `UNIFORM_BATCH` CUDA graph support instead of `UNIFORM_SINGLE_TOKEN_DECODE`, enabling full CUDA graph capture for sparse MLA models with MTP speculative decoding. It also adds validation to reject `num_speculative_tokens > 1` and clamps block table indices to prevent OOB kernel access.

**Technical impact**  
This change allows sparse MLA models using MTP with `num_speculative_tokens=1` to leverage full CUDA graphs, significantly improving throughput (benchmarks show ~2.2x token throughput). The kernel constraint (`next_n ≤ 2`) is now enforced via explicit error handling rather than silent crashes.

**Potential risks**  
The block table clamping for padded requests assumes zero-length sequences produce no meaningful kernel output—this relies on kernel behavior consistency. Any future kernel changes that process padded entries could lead to incorrect results. The validation only prevents `num_speculative_tokens > 1` but does not guard against other unsupported MTP configurations.

**Key insights**  
Developers should note that sparse MLA with MTP now supports full CUDA graphs but is limited to single-token speculation. The performance gains are substantial, but the clamping workaround should be documented as a kernel assumption. Consider adding runtime checks for `num_speculative_tokens=0` MTP edge cases.

---

## 18. [[CI] Fix flaky test_parsable_context](https://github.com/vllm-project/vllm/pull/34717)


### Base Information

- **PR Number:** #34717
- **Author:** [sfeng33](https://github.com/sfeng33)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-17 10:42:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34717/files) (1):**
  - `tests/entrypoints/openai/responses/test_parsable_context.py`

### Summary

**What changed and why**  
The PR fixes a flaky test that assumed a deterministic output sequence from the model. It replaces hardcoded index-based assertions with structural validation to accommodate variable numbers of reasoning/tool-call rounds before the final message, addressing non-deterministic behavior in the Qwen3-8B model even at temperature=0.

**Technical impact**  
The test now robustly validates the presence of required elements (at least one `mcp_call`, a final `message` with correct answer) regardless of their positions in the output sequence. This makes the test more resilient to model output variations while maintaining correctness verification for the MCP tool-call functionality.

**Potential risks**  
The relaxed assertions (`len(mcp_calls) >= 1`, `len(output_messages) >= 3`) could potentially mask regressions if the model starts producing fewer valid outputs. The test no longer verifies the exact sequence pattern, which might be important for some integration scenarios.

**Key insights**  
This fix demonstrates good practice for testing LLM outputs—validating structure rather than exact sequences. Developers should consider similar approaches when testing non-deterministic systems. However, consider adding bounds checking or more specific validation if the exact number of tool-call rounds becomes important for downstream processing.

---

## 19. [[BugFix] Fix sp tests](https://github.com/vllm-project/vllm/pull/34716)


### Base Information

- **PR Number:** #34716
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-17 09:07:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34716/files) (1):**
  - `tests/compile/correctness_e2e/test_sequence_parallel.py`

### Summary

**What changed and why**  
The change replaces the `--enforce-eager` flag with `-cc.cudagraph_mode=none` in a test utility function. This fixes a test that was incorrectly comparing numerics between compilation modes, when the actual intent was to test with CUDA graphs enabled versus disabled.

**Technical impact**  
This correction ensures the test properly isolates the variable under test (CUDA graphs) by using the correct compiler configuration flag. The test now accurately compares results between `cudagraph_mode=default` (or enabled) and `cudagraph_mode=none`, rather than comparing compiled vs. non-compiled execution which can have legitimate numeric differences.

**Potential risks**  
The risk is minimal as it's a test-only change. However, if other tests or code paths rely on the `--enforce-eager` flag for similar comparisons, they may also need updating. There's also a risk that the test might still have hidden dependencies on other compilation differences beyond CUDA graphs.

**Key insights**  
Always ensure test configurations precisely control the specific feature being tested. When testing compiler features like CUDA graphs, use the dedicated configuration flags rather than broader compilation toggles. Developers should verify that similar tests use the correct, granular flags for the features they intend to validate.

---

## 20. [Fixed whisper CPU test that does not spawn properly.](https://github.com/vllm-project/vllm/pull/34324)


### Base Information

- **PR Number:** #34324
- **Author:** [almayne](https://github.com/almayne)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 06:46:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34324/files) (1):**
  - `tests/models/multimodal/generation/test_whisper.py`

### Summary

**What changed and why**  
Removed the `@create_new_process_for_each_test("spawn")` decorator from the `test_models` function in the Whisper CPU test. This change prevents the test from spawning or forking new processes, which was causing the test to not run properly as described in issue #34323.

**Technical impact**  
The test will now execute in the same process as the test runner, changing its execution context and potentially affecting how resources are managed. This ensures the test is actually invoked and its assertions are evaluated, rather than being silently skipped or failing due to process creation issues.

**Potential risks**  
Removing process isolation could lead to test contamination if the test modifies global state or shared resources. There is also a risk that the underlying issue with the decorator might affect other tests, and this fix only addresses the symptom for this specific test case.

**Key insights**  
This is a targeted fix for a test execution problem, not a functional code change. Developers should verify that the test's behavior remains correct without process isolation and consider if similar decorators in other tests might have the same issue. The fix prioritizes test reliability over execution isolation.

---

## 21. [[CI][Nixl] Add CrossLayer KV layout tests](https://github.com/vllm-project/vllm/pull/34615)


### Base Information

- **PR Number:** #34615
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-17 05:35:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34615/files) (1):**
  - `.buildkite/test_areas/distributed.yaml`

### Summary

**What changed and why**  
A new CI test step was added to validate the CrossLayer KV layout feature for the NixlConnector in distributed environments. This test runs on 4 GPUs with the `CROSS_LAYERS_BLOCKS=True` environment variable enabled, aiming to verify accuracy as this experimental feature progresses toward being enabled by default.

**Technical impact**  
This change extends the existing distributed testing pipeline by adding a parallel test configuration specifically for the cross-layer optimization. It ensures the feature works correctly in multi-GPU setups before broader deployment, without modifying any production code or affecting existing test flows.

**Potential risks**  
The test depends on specific environment variables and scripts (`config_sweep_accuracy_test.sh`) which must be properly implemented and maintained. If the underlying cross-layer feature has undiscovered race conditions or GPU memory issues, they may only surface in this 4-GPU distributed configuration.

**Key insights**  
This is a proactive testing addition for a feature in development, demonstrating good CI/CD practices. Developers should ensure the test script handles the `CROSS_LAYERS_BLOCKS` flag correctly and monitor for any intermittent failures that could indicate distributed synchronization problems.

---

## 22. [[Renderer] Move InputPreprocessor into Renderer (2/2)](https://github.com/vllm-project/vllm/pull/34560)


### Base Information

- **PR Number:** #34560
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 05:29:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34560/files) (32):**
  - `tests/entrypoints/llm/test_chat.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/renderers/test_process_multi_modal_uuids.py`
  - `tests/samplers/test_beam_search.py`
  - `tests/v1/engine/test_process_multi_modal_uuids.py`
  - `vllm/beam_search.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/realtime/serving.py`
  - `vllm/entrypoints/openai/responses/context.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/serve/disagg/serving.py`
  - `vllm/entrypoints/serve/tokenize/serving.py`
  - `vllm/inputs/data.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/model_executor/models/funasr.py`
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/platforms/interface.py`
  - `vllm/renderers/base.py`
  - `vllm/renderers/inputs/preprocess.py`
  - `vllm/utils/tqdm_utils.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
This PR moves multimodal input preprocessing from the standalone `InputPreprocessor` into the `Renderer`, centralizing prompt transformation logic. The `InputPreprocessor` is retained for backward compatibility but now delegates to the `Renderer`. All APIs (except Score API) now pass `ProcessorInputs` to the LLM engine, and the beam search implementation is simplified to avoid duplicate multimodal processing. A bug with extra audio BOS/EOS tokens in Qwen2-Audio is also fixed.

**Technical impact**  
The architecture shifts preprocessing responsibility to the `Renderer`, reducing duplication and streamlining the data flow. APIs now consistently use `ProcessorInputs`, which improves type safety and reduces ad‑hoc conversions. Beam search no longer re‑processes multimodal data for each beam, improving performance. The change also enables better caching and request‑ID handling for multimodal inputs.

**Potential risks**  
The refactor touches many entrypoints (32 files), increasing the risk of regression in less‑common code paths (e.g., speech‑to‑text, pooling, real‑time audio). The removal of `skip_v1` marks for beam‑search tests assumes V1 engine now fully supports it, which must be validated. Backward compatibility relies on the `InputPreprocessor` wrapper, which could hide subtle behavioral differences if not thoroughly tested.

**Key insights**  
Developers should verify that all multimodal models (especially audio/video) behave correctly after the change. The new `ProcessorInputs` type should be used consistently when interfacing with the engine. Beam‑search performance for multimodal prompts should improve, but edge cases (e.g., encoder‑decoder models) require attention. The fix for Qwen2-Audio tokens highlights the importance of validating tokenization logic in multimodal processors.

---

## 23. [[CI][BugFix] ShellCheck cleanup to remove baseline and preserve runtime behavior](https://github.com/vllm-project/vllm/pull/34514)


### Base Information

- **PR Number:** #34514
- **Author:** [junuxyz](https://github.com/junuxyz)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-17 04:22:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34514/files) (55):**
  - `.buildkite/image_build/image_build_cpu.sh`
  - `.buildkite/image_build/image_build_cpu_arm64.sh`
  - `.buildkite/image_build/image_build_hpu.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-chartqa-vllm-vlm-baseline.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-mmlupro-vllm-baseline.sh`
  - `.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh`
  - `.buildkite/scripts/annotate-rocm-release.sh`
  - `.buildkite/scripts/cache-rocm-base-wheels.sh`
  - `.buildkite/scripts/cherry-pick-from-milestone.sh`
  - `.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh`
  - `.buildkite/scripts/hardware_ci/run-cpu-test.sh`
  - `.buildkite/scripts/hardware_ci/run-hpu-test.sh`
  - `.buildkite/scripts/hardware_ci/run-npu-test.sh`
  - `.buildkite/scripts/hardware_ci/run-xpu-test.sh`
  - `.buildkite/scripts/push-nightly-builds.sh`
  - `.buildkite/scripts/run-multi-node-test.sh`
  - `.buildkite/scripts/run-prime-rl-test.sh`
  - `.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh`
  - `.buildkite/scripts/scheduled_integration_test/qwen30b_a3b_fp8_block_ep_eplb.sh`
  - `.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh`
  - `.buildkite/scripts/tpu/docker_run_bm.sh`
  - `.buildkite/scripts/tpu/run_bm.sh`
  - `.buildkite/scripts/upload-nightly-wheels.sh`
  - `.buildkite/scripts/upload-release-wheels-pypi.sh`
  - `.buildkite/scripts/upload-rocm-wheels.sh`
  - `benchmarks/auto_tune/auto_tune.sh`
  - `benchmarks/auto_tune/batch_auto_tune.sh`
  - `benchmarks/run_structured_output_benchmark.sh`
  - `examples/online_serving/disaggregated_encoder/disagg_1e1p1d_example.sh`
  - `examples/online_serving/disaggregated_encoder/disagg_1e1pd_example.sh`
  - `examples/online_serving/disaggregated_prefill.sh`
  - `examples/online_serving/disaggregated_serving/kv_events.sh`
  - `examples/online_serving/disaggregated_serving/mooncake_connector/run_mooncake_connector.sh`
  - `examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh`
  - `examples/online_serving/elastic_ep/bench.sh`
  - `examples/online_serving/elastic_ep/serve_deepseek_v2.sh`
  - `examples/online_serving/multi-node-serving.sh`
  - `examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh`
  - `examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh`
  - `examples/pooling/embed/openai_embedding_long_text/service.sh`
  - `tests/standalone_tests/python_only_compile.sh`
  - `tests/v1/ec_connector/integration/run_epd_correctness_test.sh`
  - `tests/v1/kv_connector/nixl_integration/config_sweep_accuracy_test.sh`
  - `tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh`
  - `tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh`
  - `tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh`
  - `tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh`
  - `tools/ep_kernels/elastic_ep/install_eep_libraries.sh`
  - `tools/ep_kernels/install_python_libraries.sh`
  - `tools/flashinfer-build.sh`
  - `tools/install_deepgemm.sh`
  - `tools/pre_commit/shellcheck.baseline`
  - `tools/pre_commit/shellcheck.sh`
  - `tools/vllm-rocm/generate-rocm-wheels-root-index.sh`
  - `tools/vllm-tpu/build.sh`

### Summary

**What changed and why**  
This PR removes the ShellCheck baseline file and associated comparison logic from the pre-commit hook, while applying ShellCheck fixes across 55 shell scripts. The changes are split into three commits: behavior-affecting fixes (e.g., fixing dead-branch patterns, incorrect variable references, and argv-splitting issues), mechanical lint cleanups (e.g., quoting variables, using arrays for safe argument passing), and removal of the baseline gating mechanism.

**Technical impact**  
The removal of the baseline shifts ShellCheck from a "warn on new violations" model to a strict zero-tolerance policy, ensuring all existing scripts comply with ShellCheck best practices. The fixes improve script robustness—especially around argument handling, variable expansion, and error handling—reducing subtle bugs and improving portability across shell environments. The pre-commit hook now fails immediately on any ShellCheck violation.

**Potential risks**  
Some changes (e.g., switching from `wc -l` to `grep -c` or using arrays) could alter behavior in edge cases, such as empty input or whitespace in arguments. The removal of baseline gating means any future ShellCheck violations will block commits, potentially increasing developer friction. The extensive changes across CI, benchmarking, and test scripts require careful validation to ensure no regression in multi-platform workflows (e.g., GPU/CPU/TPU testing).

**Key insights**  
Prioritize reviewing the "fix script behavior" commit first, as it contains correctness-critical changes. The shift to strict ShellCheck enforcement will improve long-term script maintainability but requires team awareness of new commit-time checks. Developers should adopt array-based argument passing and explicit error handling patterns when writing or modifying shell scripts.

---

## 24. [Fix docs build warning](https://github.com/vllm-project/vllm/pull/34686)


### Base Information

- **PR Number:** #34686
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 02:31:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34686/files) (1):**
  - `vllm/v1/worker/gpu/pp_handler.py`

### Summary

**What changed and why**  
Removed an outdated `device` parameter from the docstring of `maybe_receive_sampled_tokens`. The method's argument specification had previously changed, but the documentation wasn't updated, causing a documentation build warning.

**Technical impact**  
This change aligns the docstring with the actual method signature, eliminating a discrepancy that could confuse developers and ensuring the documentation builds without warnings. It has no impact on runtime behavior or functionality.

**Potential risks**  
Low risk, as it's purely a documentation fix. However, if the `device` parameter is still relevant elsewhere or was incorrectly removed from the method earlier, this change might obscure that oversight.

**Key insights**  
Always update docstrings when modifying method signatures to maintain documentation accuracy. Consider verifying that the `device` parameter removal from the method itself was intentional and consistent with the codebase's design.

---

## 25. [[Bugfix] Fix 'remove_instance_endpoint' method logic in disagg_proxy_demo](https://github.com/vllm-project/vllm/pull/32922)


### Base Information

- **PR Number:** #32922
- **Author:** [ChenqianCao](https://github.com/ChenqianCao)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-17 02:06:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32922/files) (1):**
  - `examples/online_serving/disaggregated_serving/disagg_proxy_demo.py`

### Summary

**What changed and why**  
Fixed a logic bug in the `remove_instance_endpoint` method where prefill instance removal incorrectly checked and cycled `self.decode_instances` instead of `self.prefill_instances`. This ensures proper removal and routing updates for both prefill and decode instance types.

**Technical impact**  
The correction ensures that instance removal updates the correct internal lists and cyclers, preventing routing errors and maintaining consistent load balancing across prefill and decode instances in disaggregated serving scenarios.

**Potential risks**  
If any prefill instances were previously removed incorrectly, the system may have experienced undefined behavior or crashes. The fix assumes no lingering state corruption from prior incorrect removals, so a restart of the proxy may be necessary if issues persist.

**Key insights**  
Always verify that operations on distinct data structures reference the correct variables, especially in conditional logic. This bug highlights the importance of careful code review for symmetric operations across different instance types to prevent subtle routing failures.

---

## 26. [Remove dead bitsandbytes CxB code from 8-bit inference path](https://github.com/vllm-project/vllm/pull/34633)


### Base Information

- **PR Number:** #34633
- **Author:** [TimDettmers](https://github.com/TimDettmers)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 01:49:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34633/files) (1):**
  - `vllm/model_executor/layers/quantization/bitsandbytes.py`

### Summary

**What changed and why**  
Removed dead code in the `_apply_8bit_weight()` method that attempted to access the deprecated `MatmulLtState.CxB` attribute. This attribute has always been `None` since bitsandbytes v0.45.0 (December 2024), making the code branch unreachable. The change proactively cleans up deprecated API usage ahead of its full removal in the next bitsandbytes release.

**Technical impact**  
This is a no-op change with no functional impact on 8-bit inference. The removed code was never executed under current bitsandbytes versions, so system behavior remains unchanged. The cleanup ensures compatibility with both current bitsandbytes (where `CxB` emits a `FutureWarning`) and future versions (where it will be removed entirely).

**Potential risks**  
Minimal risk since the code was dead. However, if any environment uses an older bitsandbytes version (<0.45.0) where `CxB` was functional, this removal could theoretically affect behavior—though such versions are over a year old and likely unsupported. The conditional check `matmul_states[i].CxB is not None` would have prevented execution anyway.

**Key insights**  
Proactively removing deprecated code reduces technical debt and prevents future breakage. Developers should verify that all bitsandbytes dependencies are ≥0.45.0 to ensure compatibility. This change highlights the importance of tracking upstream library deprecations and cleaning up unused code paths promptly.

---

## 27. [Revert "[Models] Fuse Qwen3.5 GDN's qkvz_proj and ba_proj"](https://github.com/vllm-project/vllm/pull/34683)


### Base Information

- **PR Number:** #34683
- **Author:** [ZJY0516](https://github.com/ZJY0516)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-17 01:29:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34683/files) (3):**
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/models/qwen3_5.py`
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
This PR reverts a previous optimization that fused Qwen3.5 GDN's qkvz_proj and ba_proj layers. The reversion was necessary because the fusion caused incorrect model outputs (random symbols) during evaluation, as evidenced by the failing lm_eval test. The changes restore the original unfused architecture with separate linear projections.

**Technical impact**  
The code reverts to using separate ColumnParallelLinear layers for qkv, z, b, and a projections instead of merged layers. This affects weight loading logic in linear.py by removing support for tuple-based shard IDs and simplifying the fused module loading path. The model architecture now matches the original Qwen3.5 implementation with distinct projection layers.

**Potential risks**  
The main risk is performance regression due to increased memory operations from separate projections instead of fused ones. There's also a risk of inconsistent weight loading between different model versions if the stacked_params_mapping isn't properly maintained. The removal of tuple shard ID support could affect other models that might use this feature.

**Key insights**  
The fusion optimization was premature and introduced functional regressions. Developers should ensure comprehensive testing across different model configurations before implementing architectural optimizations. The weight loading system is sensitive to projection layer arrangements, and changes require careful validation. Future optimization attempts should include more robust testing with actual model evaluations.

---

## 28. [[Ray] Propagate third-party env vars to Ray workers via prefix matching](https://github.com/vllm-project/vllm/pull/34383)


### Base Information

- **PR Number:** #34383
- **Author:** [kouroshHakha](https://github.com/kouroshHakha)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-17 01:08:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34383/files) (5):**
  - `.buildkite/test_areas/misc.yaml`
  - `tests/test_ray_env.py`
  - `vllm/envs.py`
  - `vllm/ray/ray_env.py`
  - `vllm/v1/executor/ray_executor.py`

### Summary

**What changed and why**  
The changes introduce prefix-based environment variable propagation to Ray workers, replacing the previous hardcoded approach. This fixes issues where third-party integration env vars (like `LMCACHE_*`, `NCCL_*`) were silently dropped, causing subtle failures such as cache mismatches in KV offloading. The solution adds built-in prefix matching and user-extensible configuration via two new environment variables.

**Technical impact**  
The architecture now supports dynamic, extensible env var propagation through a four-layer collection strategy: registered vLLM vars, prefix-matched vars, individual extra vars, and caller-supplied vars. This eliminates the need for hardcoded lists and allows integrations to work without code changes. The Ray executor is simplified by removing its `ADDITIONAL_ENV_VARS` since `HF_` and `HUGGING_FACE_` prefixes are now covered by defaults.

**Potential risks**  
Prefix matching could inadvertently propagate sensitive env vars if prefixes are too broad (e.g., `SECRET_`). Users might incorrectly assume the configuration replaces defaults rather than extending them, though the additive behavior is documented. There is a minor performance overhead from scanning all environment variables for prefix matches, but this is negligible given the typical number of env vars.

**Key insights**  
The additive design ensures backward compatibility while enabling deploy-time extensibility—critical for supporting new integrations without requiring vLLM releases. Developers should note that exclusions (`RAY_NON_CARRY_OVER_ENV_VARS` and `exclude_vars`) take precedence over all inclusion rules. When adding custom prefixes, verify they are specific enough to avoid leaking unintended variables.

---

