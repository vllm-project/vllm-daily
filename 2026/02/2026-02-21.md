# vLLM Merged PR Report

**Report Date:** 2026-02-21 PST

**Total Merged PRs:** 18

---

## 1. [[Bugfix] Fix Qwen3/Qwen3.5 Reasoning Parser](https://github.com/vllm-project/vllm/pull/34779)


### Base Information

- **PR Number:** #34779
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-21 23:15:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34779/files) (3):**
  - `tests/reasoning/test_qwen3_reasoning_parser.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/reasoning/qwen3_reasoning_parser.py`

### Summary

**What changed and why**  
The PR fixes Qwen3/Qwen3.5 reasoning parser compatibility issues. The primary issue was that Qwen3.5 models place `<think>` in the prompt rather than generating it, causing the parser to fail when requiring both `<think>` and `</think>` in generated output. Additional fixes ensure proper handling of streaming paths when thinking is disabled (`enable_thinking=False`).

**Technical impact**  
The parser now supports both Qwen3 (generates `<think>`) and Qwen3.5 (prompt contains `<think>`) model families. Streaming logic in `serving.py` now correctly checks `prompt_is_reasoning_end_arr` before invoking the parser, preventing misrouted content when thinking is disabled. The architecture maintains backward compatibility while adapting to template differences.

**Potential risks**  
Edge cases where `<think>` appears partially in multi-token deltas could still cause parsing issues, though tests cover many scenarios. The streaming logic now has multiple conditional checks that could become complex to maintain. Changes to token grouping in speculative decoding might interact unexpectedly with the new multi-token delta handling.

**Key insights**  
Developers should note that Qwen3.5 models now work correctly with reasoning extraction. The fix emphasizes that the serving layer must handle thinking-disabled cases before calling the parser. Test coverage has significantly improved with new multi-token delta scenarios, which is crucial for real-world streaming behavior.

---

## 2. [[Model Runner V2] Enable CUDA graph for Eagle3](https://github.com/vllm-project/vllm/pull/35040)


### Base Information

- **PR Number:** #35040
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-21 21:42:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35040/files) (2):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The changes enable CUDA graph support for models that output auxiliary hidden states (like Eagle3). The `CudaGraphManager` class now accepts a `use_aux_hidden_state_outputs` flag and handles both single-tensor and tuple outputs from the model during graph capture and replay.

**Technical impact**  
This extends CUDA graph functionality to support models with multiple output tensors, maintaining performance optimizations while adding flexibility. The changes affect graph capture, buffer allocation, and output handling, ensuring compatibility with existing single-output models through conditional logic.

**Potential risks**  
If `use_aux_hidden_state_outputs` is incorrectly set, buffer allocation or output unpacking may fail. The type hint ignore in `model_runner.py` could mask type mismatches. Edge cases with empty auxiliary tensors or mismatched tensor counts during replay need validation.

**Key insights**  
The implementation cleanly extends CUDA graphs to multi-output models with minimal disruption. Developers should ensure the flag is set accurately per model type and verify auxiliary tensor handling in all execution paths. Consider adding runtime checks for auxiliary tensor consistency.

---

## 3. [Fix apply_top_k_top_p_triton called by non-cuda logits Tensor](https://github.com/vllm-project/vllm/pull/35030)


### Base Information

- **PR Number:** #35030
- **Author:** [xli](https://github.com/xli)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 21:11:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35030/files) (1):**
  - `vllm/v1/sample/ops/topk_topp_sampler.py`

### Summary

**What changed and why**  
The change adds a `logits.is_cuda` check to the condition guarding the call to `apply_top_k_top_p_triton`. This ensures the Triton kernel is only invoked when the input tensor is on a CUDA device, preventing an assertion failure inside the kernel that explicitly requires CUDA tensors.

**Technical impact**  
This fix maintains the performance optimization of using the Triton kernel for larger batch sizes (>=8) but now correctly restricts its use to CUDA tensors only. For non-CUDA tensors or smaller batches, the function will fall back to the PyTorch implementation, ensuring cross-device compatibility.

**Potential risks**  
If the Triton kernel has other hidden device-specific assumptions beyond the explicit CUDA check, those could still cause issues. There's also a minor risk that the condition `logits.shape[0] >= 8` might be empirically derived and could affect performance on edge-case batch sizes when combined with the new device check.

**Key insights**  
Always validate device compatibility when conditionally dispatching to hardware-accelerated kernels. The fix is minimal and correct, but developers should ensure the PyTorch fallback path is well-tested for CPU and other non-CUDA backends to guarantee functional parity.

---

## 4. [[Benchmark] Use `sns.relplot` for plotting](https://github.com/vllm-project/vllm/pull/35027)


### Base Information

- **PR Number:** #35027
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 20:26:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35027/files) (1):**
  - `vllm/benchmarks/sweep/plot.py`

### Summary

**What changed and why**  
The PR replaces the lower-level `sns.FacetGrid` + `map_dataframe(sns.lineplot)` pattern with the higher-level `sns.relplot` API for creating line plots. This change simplifies the plotting code by using a single, more declarative seaborn function that internally handles facet grid creation and line plotting.

**Technical impact**  
The refactoring consolidates two separate API calls into one, reducing code complexity and potential for misconfiguration. The functional behavior remains identical—producing faceted line plots with optional error bars—but the implementation now follows seaborn's recommended modern API pattern, which may offer better maintainability and future compatibility.

**Potential risks**  
There's a minor risk if `sns.relplot` has different default behavior or parameter interpretation compared to the previous explicit `FacetGrid` setup, though the parameters appear correctly mapped. The legend handling changed slightly (explicit `g.add_legend()` calls were removed), which could affect legend appearance or positioning in edge cases.

**Key insights**  
This is a clean refactoring that improves code readability and aligns with seaborn best practices. Developers should verify that the generated plots are visually identical, particularly checking legend placement and facet titles. The change demonstrates how higher-level plotting APIs can reduce boilerplate while maintaining functionality.

---

## 5. [[New Model] Add ColModernVBERT](https://github.com/vllm-project/vllm/pull/34558)


### Base Information

- **PR Number:** #34558
- **Author:** [athrael-soju](https://github.com/athrael-soju)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-21 20:23:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34558/files) (9):**
  - `docs/models/supported_models.md`
  - `examples/pooling/score/colmodernvbert_rerank_online.py`
  - `tests/models/multimodal/pooling/test_colmodernvbert.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/colmodernvbert.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/colmodernvbert.py`

### Summary

**What changed and why**  
Added native support for ColModernVBERT, a 250M multimodal late-interaction retrieval model that combines a SigLIP vision encoder with a ModernBERT text encoder. The implementation includes a custom configuration parser, model architecture with a pixel‑shuffle connector, and integration into vLLM’s multimodal embedding framework. This enables text‑and‑image retrieval using ColBERT‑style per‑token embeddings and MaxSim scoring.

**Technical impact**  
The changes extend vLLM’s multimodal embedding capabilities by registering a new model type (`ColModernVBertForRetrieval`) that reuses existing components (`SiglipVisionModel`, `ModernBertEmbeddings`, `ModernBertLayer`). The model supports both text‑only and multimodal inputs via the existing pooling/reranking endpoints, and its late‑interaction scoring aligns with vLLM’s existing MaxSim utilities. Integration follows the established patterns for multimodal models (processor, dummy inputs, registry updates).

**Potential risks**  
The pixel‑shuffle connector assumes square input images and a fixed reduction factor; non‑square or unusually sized images may cause shape mismatches. The model relies on an external `Idefics3ImageProcessor` for image preprocessing, which could introduce version‑dependency issues. Weight loading depends on custom mapping logic that must stay synchronized with the Hugging Face checkpoint structure.

**Key insights**  
The implementation successfully reuses vLLM’s existing SigLIP and ModernBERT components, minimizing code duplication. Developers should note that image inputs are prepended as a block of `image_token_id` tokens, and the connector reduces vision‑token sequence length by a factor of 16 (pixel‑shuffle factor 4). Ensure the Hugging Face checkpoint includes the nested `vlm_config` structure expected by the custom configuration parser.

---

## 6. [[CI] Bump mteb version to `mteb[bm25s]>=2, <3` for pooling model unit tests](https://github.com/vllm-project/vllm/pull/34961)


### Base Information

- **PR Number:** #34961
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 20:23:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34961/files) (4):**
  - `requirements/nightly_torch_test.txt`
  - `requirements/rocm-test.txt`
  - `requirements/test.txt`
  - `tests/models/language/pooling_mteb_test/mteb_score_utils.py`

### Summary

**What changed and why**  
This PR updates the MTEB library dependency from version 1.x/2.1.2 to version 2.x (specifically `mteb[bm25s]>=2, <3`) across test requirements files. The change enables BM25 functionality and ensures compatibility with pooling model unit tests. A minor code addition ensures tasks load their data before evaluation.

**Technical impact**  
The upgrade introduces a major version change (v1 to v2) with potential API differences. Adding the `[bm25s]` extra installs additional dependencies for BM25 retrieval tasks. The data loading check prevents runtime errors by ensuring tasks are properly initialized before evaluation in the test suite.

**Potential risks**  
Major version upgrades may introduce breaking changes in MTEB's API that could affect test behavior. The explicit data loading might mask underlying issues if tasks consistently fail to load data automatically. Dependency conflicts could arise if other packages have incompatible requirements with MTEB v2.

**Key insights**  
This is a backward-incompatible change requiring thorough testing of all MTEB-related functionality. The PR author wisely separated this version bump from functional changes for easier rollback if issues emerge. Developers should verify that all MTEB integration tests pass and watch for any deprecation warnings during test execution.

---

## 7. [[CI] Stabilizing ROCm amd-ci signal and minor name fix in upstream](https://github.com/vllm-project/vllm/pull/35008)


### Base Information

- **PR Number:** #35008
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-21 20:01:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35008/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/distributed.yaml`
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
This PR makes three minor changes to stabilize ROCm CI signals and fix naming inconsistencies. It removes the "Blocking" grade from an AMD test step to allow non-blocking execution, corrects a label typo in distributed tests, and adds the `tensorizer` dependency to ROCm test requirements for example tests.

**Technical impact**  
The changes are configuration-level adjustments with minimal technical impact. Removing the blocking grade allows the AMD test step to run without blocking the pipeline, while the dependency addition ensures ROCm tests have necessary packages for tensorizer-related example tests. The label fix improves clarity in test reporting.

**Potential risks**  
Removing the blocking grade could allow failing tests to pass through CI undetected if not properly monitored. Adding `tensorizer` may introduce version compatibility issues with existing ROCm dependencies, though this is low risk given it's a test-only requirement.

**Key insights**  
These are low-risk maintenance changes focused on CI stability and clarity. Developers should verify that non-blocking AMD tests are still adequately monitored for failures. The tensorizer addition aligns ROCm test environments with other platforms, ensuring consistent test coverage.

---

## 8. [[Model Runner V2] Support attention group](https://github.com/vllm-project/vllm/pull/35036)


### Base Information

- **PR Number:** #35036
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-21 16:42:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35036/files) (5):**
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/cudagraph.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/speculator.py`

### Summary

**What changed and why**  
This PR introduces support for attention groups in the Model Runner V2 architecture. The changes replace direct usage of `AttentionMetadataBuilder` lists with a new `AttentionGroup` abstraction that groups layers by attention backend and KV cache specification. This enables more efficient workspace buffer sharing and better organization of attention metadata across layers.

**Technical impact**  
The architecture now groups attention layers by backend type and KV cache spec within each KV cache group. This allows shared workspace buffers (like FlashInfer's workspace) across compatible layers, reducing memory overhead. The changes affect attention metadata initialization, CUDA graph capture, and speculator integration across multiple components.

**Potential risks**  
The transition from direct builder lists to grouped abstraction could introduce subtle bugs if layer grouping logic fails. Workspace buffer sharing assumes compatible configurations across grouped layers, which may break if assumptions change. The fallback logic in CUDA graph mode when graphs aren't captured yet (added in `cudagraph_utils.py`) could mask underlying capture issues.

**Key insights**  
Developers should verify that layer grouping logic correctly identifies compatible attention backends and KV cache specs. The workspace buffer sharing optimization is backend-specific (FlashInfer) and may need extension for other backends. The CUDA graph fallback is a safety mechanism but should be monitored for unintended performance impacts.

---

## 9. [[Model Bash][DSR1] Add selective dynamic shape marking for CustomOp](https://github.com/vllm-project/vllm/pull/34900)


### Base Information

- **PR Number:** #34900
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-21 16:28:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34900/files) (2):**
  - `vllm/model_executor/custom_op.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
The changes introduce selective dynamic shape marking for CustomOp to optimize kernel compilation. Previously, `CustomOp.maybe_compile` used `dynamic=True`, making all tensor dimensions symbolic. This caused model-constant dimensions (like num_heads) to become runtime kernel arguments with expensive integer divisions. Now, ops can optionally specify `dynamic_arg_dims` during registration to mark only truly dynamic dimensions, while compiling with `dynamic=False` to specialize constant dimensions.

**Technical impact**  
This modifies the compilation strategy for CustomOp instances. When `dynamic_arg_dims` is provided, the system uses `torch._dynamo.mark_dynamic` on specified tensor dimensions and compiles with `dynamic=False`, allowing constant folding and strength reduction for non-dynamic dimensions. Ops without this parameter retain the original `dynamic=True` behavior, maintaining backward compatibility. The `_DecodeConcatQuantFP8` op is updated to mark only batch dimension 0 as dynamic.

**Potential risks**  
If `dynamic_arg_dims` incorrectly specifies dimensions (e.g., marking a truly dynamic dimension as static), it could cause recompilation overhead or runtime errors. The implementation assumes tensor arguments are positional; keyword arguments in `bound.arguments` are handled, but complex signature binding could have edge cases. There's also a risk of integer division for negative dimension indexing if the tensor's rank is unexpected.

**Key insights**  
The optimization provides significant performance gains (19.5% throughput increase) by converting expensive runtime divisions into compile-time constants. Developers should carefully analyze which tensor dimensions are truly dynamic per op to maximize benefits. This pattern can be extended to other CustomOps with known static dimensions. Ensure `dynamic_arg_dims` uses correct dimension indices, considering negative indexing support.

---

## 10. [[Model Runner V2] Support Eagle3 (no CUDA graph)](https://github.com/vllm-project/vllm/pull/35029)


### Base Information

- **PR Number:** #35029
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-21 12:55:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35029/files) (7):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/__init__.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/__init__.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/cudagraph.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/eagle3_utils.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/speculator.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/utils.py`

### Summary

**What changed and why**  
This PR adds support for EAGLE3 speculative decoding without CUDA graphs, including minor refactoring. Key changes include: introducing an `eagle` directory for better organization, adding EAGLE3-specific utility functions, and updating the model runner to handle auxiliary hidden states required by EAGLE3. The refactoring also improves handling of pipeline parallelism and speculative decoding initialization.

**Technical impact**  
The changes extend the speculative decoding framework to support EAGLE3, which requires auxiliary hidden states from the target model. The model runner now conditionally processes these states and propagates them to the speculator. The refactoring centralizes EAGLE model loading and configuration, improving code modularity. Pipeline parallelism is now explicitly considered during speculative decoding setup, with EAGLE3 currently unsupported in pipeline-parallel configurations.

**Potential risks**  
EAGLE3 is incompatible with pipeline parallelism, raising a `ValueError` if both are enabled—this could break existing configurations. The introduction of auxiliary hidden states adds complexity to the model execution flow, potentially affecting performance or correctness if not handled consistently across all execution modes (CUDA graph, piecewise, eager). The refactoring of model loading and speculative decoding initialization may introduce regressions if the conditional logic (e.g., `self.speculator is not None` vs. `self.do_spec_decode`) is not applied uniformly.

**Key insights**  
Developers should note that EAGLE3 requires careful integration due to its auxiliary hidden state requirements and pipeline parallelism restrictions. The refactoring improves code organization but necessitates thorough testing across all speculative decoding methods and execution modes. The conditional checks for `self.speculator` and `self.use_aux_hidden_state_outputs` should be consistently used to avoid null reference issues. Ensure that any future speculative decoding methods align with the updated initialization pattern.

---

## 11. [[CI/Build] Fix gRPC version mismatch](https://github.com/vllm-project/vllm/pull/35013)


### Base Information

- **PR Number:** #35013
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-21 11:14:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35013/files) (3):**
  - `requirements/rocm.txt`
  - `requirements/test.in`
  - `requirements/test.txt`

### Summary

**What changed and why**  
The PR fixes a gRPC version mismatch by ensuring consistent gRPC library versions across the codebase. It adds explicit version pins for `grpcio`, `grpcio-reflection`, and `grpcio-tools` (all at 1.78.0) to `requirements/rocm.txt` and `requirements/test.in`, while removing the standalone `grpcio-tools` line that previously referenced `build.txt`. This resolves a build failure likely caused by incompatible gRPC library versions.

**Technical impact**  
These changes enforce version consistency for all gRPC-related packages in both ROCm and test environments, preventing potential runtime errors or build failures due to version conflicts. The modifications to `requirements/test.txt` are automatically generated from `test.in`, reflecting the new explicit dependencies and their transitive relationships.

**Potential risks**  
If other requirement files (like `build.txt` or `common.txt`) specify different gRPC versions, inconsistencies could still arise. The fix assumes version 1.78.0 is compatible with all system components; testing should verify no regressions in gRPC-dependent features, especially for ROCm-specific functionality.

**Key insights**  
Always pin related library suites to identical versions to avoid subtle compatibility issues. Consider auditing other requirement files (`build.txt`, `common.txt`) to ensure full gRPC version alignment. The automated update to `test.txt` demonstrates the importance of maintaining the source `.in` files correctly.

---

## 12. [[Frontend] Add automatic language detection for Whisper transcription](https://github.com/vllm-project/vllm/pull/34342)


### Base Information

- **PR Number:** #34342
- **Author:** [spacecheck](https://github.com/spacecheck)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 04:49:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34342/files) (5):**
  - `tests/entrypoints/openai/test_transcription_validation_whisper.py`
  - `tests/models/multimodal/generation/test_whisper.py`
  - `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/whisper.py`

### Summary

**What changed and why**  
This PR adds automatic language detection for Whisper transcription when no language parameter is provided. It introduces a new protocol `SupportsExplicitLanguageDetection` that Whisper implements, allowing the system to detect the spoken language via a single-token generation step using `<\|startoftranscript\|>` as the decoder prompt. The feature falls back to English if detection fails and adds minimal overhead (~30ms).

**Technical impact**  
The changes extend the transcription pipeline with a model-agnostic language detection mechanism. A new `_detect_language` method orchestrates detection by delegating prompt construction and output parsing to model classes that implement the protocol. This maintains backward compatibility—existing models like Voxtral are unaffected, and future models can opt in by implementing the interface. The detection uses constrained sampling to ensure only valid language tokens are generated.

**Potential risks**  
If `parse_language_detection_output` receives an unsupported token (e.g., due to tokenizer mismatches), it may raise an assertion error, causing the request to fail. The fallback to English occurs only if detection fails, but the exact failure conditions (e.g., network timeouts during detection) should be clearly defined. Edge cases like very short or noisy audio could lead to incorrect language detection.

**Key insights**  
The implementation cleanly separates concerns by using a protocol, making it easy to extend to other models. Developers should ensure that any new model implementing `SupportsExplicitLanguageDetection` provides robust token ID mapping and output parsing. The constrained sampling via `allowed_token_ids` is a best practice that prevents invalid outputs. Consider adding more comprehensive error handling for detection failures beyond assertions.

---

## 13. [[Bugfix] Gate 256-bit instructions to CUDA 12.9+](https://github.com/vllm-project/vllm/pull/34791)


### Base Information

- **PR Number:** #34791
- **Author:** [huydhn](https://github.com/huydhn)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 04:48:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34791/files) (1):**
  - `csrc/activation_kernels.cu`

### Summary

**What changed and why**  
The change adds a CUDA version check (`CUDA_VERSION >= 12090`) alongside the existing compute capability check (`__CUDA_ARCH__ >= 1000`) in two inline assembly functions (`ld256` and `st256`). This gates the use of 256-bit vector load/store instructions to CUDA 12.9+ only, fixing a build failure on CUDA 12.8 where these instructions are unsupported.

**Technical impact**  
The 256-bit instructions are now conditionally enabled only when both the GPU architecture (SM 10.0+) and CUDA toolkit version (12.9+) support them. This maintains backward compatibility with CUDA 12.8, which is still widely used (e.g., in PyTorch CI). The fallback path uses 128-bit instructions for older CUDA versions.

**Potential risks**  
If the CUDA version macro (`CUDA_VERSION`) is not consistently defined in all build environments, it could lead to incorrect code paths (e.g., missing the feature when it should be enabled). Additionally, the fallback to 128-bit instructions may slightly reduce memory throughput on supported hardware when built with CUDA 12.8.

**Key insights**  
Always validate that new CUDA features are gated by both architecture and toolkit version checks. Consider adding a compile-time assertion or documentation note about the CUDA 12.9+ requirement for optimal performance. Ensure the `CUDA_VERSION` macro is reliably available across all build configurations.

---

## 14. [[Benchmark] Improve benchmarks](https://github.com/vllm-project/vllm/pull/35012)


### Base Information

- **PR Number:** #35012
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-21 02:31:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35012/files) (4):**
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/sweep/plot.py`
  - `vllm/benchmarks/sweep/plot_pareto.py`
  - `vllm/benchmarks/sweep/serve_sla.py`

### Summary

**What changed and why**  
The PR optimizes benchmark code by moving `parser_fn` lookup outside the loop for multimodal datasets to avoid repeated dictionary lookups, fixes SLA data recording by ensuring past data is appended to `sla_data`, logs serve and bench parameters for better debugging, and improves import error handling by isolating each plotting library import.

**Technical impact**  
These changes improve performance for multimodal dataset sampling by reducing O(n) dictionary lookups to O(1), ensure SLA history is properly maintained across iterations, enhance debugging visibility with parameter logging, and provide more precise error messages for missing dependencies in plotting utilities.

**Potential risks**  
The change from `tokenizer(prompt).input_ids` to `tokenizer.encode(prompt)` could introduce subtle differences if the tokenizer has different default behaviors. The isolated import statements might slightly increase code verbosity but improve clarity. The SLA data fix assumes `past_iter_data` structure matches current expectations.

**Key insights**  
Performance optimization is well-targeted at hot loops in dataset sampling. The tokenizer method change should be validated for consistency across different tokenizer implementations. The improved import error handling provides better developer experience by identifying exactly which module is missing during plotting operations.

---

## 15. [[Doc] Fix example of eagle3](https://github.com/vllm-project/vllm/pull/34960)


### Base Information

- **PR Number:** #34960
- **Author:** [petrpechman](https://github.com/petrpechman)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-21 01:57:54
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34960/files) (1):**
  - `docs/features/speculative_decoding/eagle.md`

### Summary

**What changed and why**  
The PR updates a documentation example by changing the speculative decoding method from `"eagle"` to `"eagle3"` in a configuration block. This aligns the example with a newer or updated speculative method name, ensuring documentation accuracy.

**Technical impact**  
This change has no impact on the codebase or runtime behavior since it only affects documentation. However, it ensures users following the example will correctly configure the `LLM` with the intended speculative decoding method.

**Potential risks**  
If the `"eagle3"` method is not yet supported or is incorrectly named in the actual code, users copying the example could encounter configuration errors. Additionally, inconsistent documentation across other files might lead to confusion.

**Key insights**  
Always verify that documentation examples match the current API and supported features. Consider cross-referencing other documentation or code to ensure consistency, and if this reflects a broader rename, ensure all related examples are updated.

---

## 16. [[Core] Minor structured-output related scheduler optimization](https://github.com/vllm-project/vllm/pull/34765)


### Base Information

- **PR Number:** #34765
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 01:38:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34765/files) (1):**
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The changes optimize structured output handling in the scheduler by avoiding unnecessary computations. Specifically, they skip extra loops when there are no structured output requests and prevent bitmask computation for prefill chunks that don't require structured output processing.

**Technical impact**  
These optimizations reduce computational overhead in two key areas: the scheduling update loop no longer iterates over requests unnecessarily, and the grammar bitmask generation skips prefill chunks entirely. This improves performance during inference, especially in workloads with mixed prefill/decode phases or without structured output requirements.

**Potential risks**  
The main risk involves edge cases where prefill chunks might legitimately require structured output processing - though the logic suggests this shouldn't occur. Additionally, the condition `request.use_structured_output and not request.is_prefill_chunk` appears in two places, creating maintenance risk if this logic needs future modification.

**Key insights**  
These are sensible micro-optimizations that eliminate unnecessary work. Developers should verify that structured output is never needed during prefill chunks, as assumed by the changes. Consider centralizing the `use_structured_output and not is_prefill_chunk` logic to a helper method to maintain consistency and simplify future updates.

---

## 17. [[PD] Change kv_load_failure_policy Default from "recompute" to "fail"](https://github.com/vllm-project/vllm/pull/34896)


### Base Information

- **PR Number:** #34896
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 01:34:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34896/files) (5):**
  - `docs/features/nixl_connector_usage.md`
  - `examples/offline_inference/kv_load_failure_recovery/decode_example.py`
  - `tests/v1/kv_connector/unit/test_kv_load_failure_recovery.py`
  - `tests/v1/kv_connector/unit/utils.py`
  - `vllm/config/kv_transfer.py`

### Summary

**What changed and why**  
The default value for `kv_load_failure_policy` has been changed from `"recompute"` to `"fail"` across the codebase. This change addresses user confusion where misconfigured deployments could silently degrade performance due to failed KV cache transfers, as discussed in issue #32200. The update ensures failures are immediately visible rather than hidden behind inefficient recomputation.

**Technical impact**  
This change affects all deployments using the KV connector without explicit configuration, making them fail fast on KV load errors instead of falling back to local recomputation. Documentation, examples, and tests have been updated to reflect the new default, ensuring consistency and preventing performance jitter from hidden recomputation on decode instances.

**Potential risks**  
Existing deployments relying on the old `"recompute"` default may experience increased request failures if KV load issues occur, potentially breaking workflows that previously tolerated silent recomputation. The change also requires updates to any external documentation or configurations that assume the previous default behavior.

**Key insights**  
Developers should explicitly set `kv_load_failure_policy="recompute"` in configurations if they depend on the previous behavior. This change prioritizes operational transparency over silent degradation, aligning with production best practices. Ensure all deployment scripts and CI tests are updated to handle the new default appropriately.

---

## 18. [[ROCm] Enable bitsandbytes quantization support on ROCm](https://github.com/vllm-project/vllm/pull/34688)


### Base Information

- **PR Number:** #34688
- **Author:** [Abdennacer-Badaoui](https://github.com/Abdennacer-Badaoui)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-21 00:34:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34688/files) (8):**
  - `docs/features/quantization/bnb.md`
  - `requirements/nightly_torch_test.txt`
  - `requirements/rocm-test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/models/test_transformers.py`
  - `vllm/model_executor/layers/quantization/bitsandbytes.py`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
This PR enables BitsAndBytes quantization support on ROCm GPUs, including gfx9 architectures. The changes update the required bitsandbytes version to 0.49.2, remove the gfx9 warp size limitation guard, and add version checks that differentiate between ROCm and non-ROCm platforms.

**Technical impact**  
The codebase now supports BitsAndBytes quantization uniformly across all ROCm GPU architectures, eliminating the previous gfx9 restriction. The version requirement is raised to 0.49.2, which includes upstream fixes for ROCm compatibility, and a centralized version-checking function ensures consistent validation.

**Potential risks**  
Users must upgrade bitsandbytes to 0.49.2; older versions will cause import errors. The removal of the gfx9 guard assumes the upstream fix is fully stable—any residual hardware-specific issues could surface. The version check logic now differs by platform (0.49.2 for ROCm vs. 0.48.1 for others), which may complicate maintenance if future updates require further divergence.

**Key insights**  
Always verify that the upstream bitsandbytes dependency (0.49.2) is correctly installed on ROCm systems. The centralized `_check_bitsandbytes_version()` function improves maintainability, but developers should monitor for any performance regressions on gfx9 hardware. Ensure testing covers diverse ROCm architectures to confirm compatibility.

---

