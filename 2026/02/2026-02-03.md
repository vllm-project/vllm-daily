# vLLM Merged PR Report

**Report Date:** 2026-02-03 PST

**Total Merged PRs:** 41

---

## 1. [[Metrics] Add labeled prompt token metrics for P/D disaggregation](https://github.com/vllm-project/vllm/pull/33290)


### Base Information

- **PR Number:** #33290
- **Author:** [ZhanqiuHu](https://github.com/ZhanqiuHu)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-02-03 23:46:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33290/files) (5):**
  - `tests/v1/metrics/test_stats.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/metrics/loggers.py`
  - `vllm/v1/metrics/stats.py`

### Summary

**What changed and why**  
This PR adds labeled Prometheus metrics to distinguish prompt token sources in P/D disaggregated deployments. Currently, decode instances report inflated prompt throughput by counting all prompt tokens as "computed," even when tokens are transferred from prefill instances or cached locally. The changes introduce a `PromptTokenStats` class to track tokens by source (`local_compute`, `external_kv_transfer`, `local_cache_hit`) and adjust metrics accordingly.

**Technical impact**  
The architecture now includes a new data structure (`PromptTokenStats`) that breaks down prompt tokens by source, replacing the simple `num_prompt_tokens` counter. Metrics logging has been updated to emit labeled counters (`vllm:prompt_tokens_by_source_total`) and additional counters for cached and recomputed tokens. This enables accurate monitoring of actual compute work versus transferred or cached work, improving observability in disaggregated setups.

**Potential risks**  
The `-1` adjustment for fully cached prompts (to force recomputation of the last token) introduces a subtle invariant: `num_cached_tokens` may be one less than the prompt length when all tokens are cached. This could cause confusion if not documented. Additionally, the new `num_external_computed_tokens` field must be correctly propagated through the engine output and scheduler; any misalignment could skew metrics.

**Key insights**  
Developers should note that `num_prompt_tokens` now returns the total prompt tokens (for backward compatibility), but prompt throughput calculations should use `prompt_token_stats.computed` to reflect actual compute. The labeled metrics provide critical visibility into P/D disaggregation efficiency, but ensure all components (e.g., connectors) correctly populate `num_external_computed_tokens`.

---

## 2. [[Hardware][AMD][CI] Refactor AMD tests to properly use BuildKite parallelism](https://github.com/vllm-project/vllm/pull/32745)


### Base Information

- **PR Number:** #32745
- **Author:** [mawong-amd](https://github.com/mawong-amd)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-03 22:51:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32745/files) (2):**
  - `.buildkite/scripts/hardware_ci/run-amd-test.sh`
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
This PR refactors AMD CI tests to leverage BuildKite's native parallelism instead of custom sharding logic. The changes remove complex manual parallelization code from the test runner script and update multiple test steps to use single-GPU machines (`mi325_1`) instead of 8-GPU machines (`mi325_8`). This aligns AMD CI with NVIDIA CI best practices and reduces resource waste by allocating only the GPUs needed per shard.

**Technical impact**  
The removal of custom sharding logic (lines 189–226 in `run-amd-test.sh`) shifts parallelism management to BuildKite's step-level concurrency. This simplifies the CI pipeline, improves maintainability, and ensures sharded tests run on appropriate hardware (1 GPU per shard). The updated YAML configuration changes the agent pool for several test suites, enabling efficient resource utilization.

**Potential risks**  
If the companion CI-infra PR (#274) isn't merged first, the pipeline may fail due to missing parallelism support. Additionally, removing the custom sharding logic could affect edge cases where shard management was tightly coupled with GPU assignment, though the PR description indicates successful test runs.

**Key insights**  
Developers must merge the dependent CI-infra PR before this one to avoid pipeline failures. The changes significantly reduce CI costs by optimizing GPU allocation. Future test additions should rely on BuildKite's parallelism features rather than custom sharding implementations.

---

## 3. [[Deprecation] Deprecate profiling envs](https://github.com/vllm-project/vllm/pull/33722)


### Base Information

- **PR Number:** #33722
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 21:58:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33722/files) (2):**
  - `docs/usage/security.md`
  - `vllm/envs.py`

### Summary

**What changed and why**  
Removed deprecated environment variables for profiling configuration (`VLLM_TORCH_CUDA_PROFILE`, `VLLM_TORCH_PROFILER_DIR`, etc.) from the codebase and updated documentation to reflect that profiling is now enabled via the `--profiler-config` command-line argument. This change finalizes the deprecation schedule for the old environment-based profiling controls.

**Technical impact**  
The profiling functionality is no longer configurable via environment variables, requiring users to migrate to the `--profiler-config` CLI argument. The removal simplifies the configuration system and centralizes profiling settings. Existing code that relied on these environment variables will break unless updated.

**Potential risks**  
Users with existing scripts or deployments using the old environment variables will experience failures or loss of profiling functionality. The documentation update helps, but there is a risk of disruption if users miss the migration notice. The change assumes all profiling use cases are covered by the new `--profiler-config` interface.

**Key insights**  
This is a clean-up change that removes technical debt and consolidates configuration methods. Developers must audit their deployment scripts and configurations to replace any usage of the deprecated environment variables with the `--profiler-config` argument. The update to the security documentation correctly emphasizes that profiler endpoints should never be enabled in production.

---

## 4. [[Deprecation] Remove `_get_data_parser` in MM processor](https://github.com/vllm-project/vllm/pull/33757)


### Base Information

- **PR Number:** #33757
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 21:51:52
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33757/files) (1):**
  - `vllm/multimodal/processing/processor.py`

### Summary

**What changed and why**  
The change removes the deprecated `_get_data_parser` method compatibility code in the `BaseMultiModalProcessor` constructor. Previously, it logged a deprecation warning and fell back to the old method; now it raises an explicit `ValueError` with a clear migration message, directing users to override `BaseProcessingInfo.build_data_parser` instead. This is necessary because recent changes made the old compatibility path non-functional.

**Technical impact**  
This update simplifies the initialization logic by removing the conditional fallback path, ensuring all processors consistently use `self.info.get_data_parser()`. It enforces the new API immediately (via an error) rather than allowing deprecated code to run, which aligns with the architectural shift where MM data parsing no longer depends on the processor.

**Potential risks**  
Existing code that still overrides `_get_data_parser` will now break at runtime with a `ValueError` instead of receiving a warning. This could disrupt workflows if users haven't migrated to the new `build_data_parser` method. The error message is clear, but the abrupt change may require immediate updates in dependent projects.

**Key insights**  
Developers must update any custom processors to override `BaseProcessingInfo.build_data_parser` instead of `_get_data_parser`. The deprecation timeline has accelerated—the warning is replaced by a hard error, so immediate action is required. Ensure all multimodal processing code is tested against this change to avoid runtime failures.

---

## 5. [[Feature] Enable `TRITON_ATTN` for Batch Invariance](https://github.com/vllm-project/vllm/pull/33688)


### Base Information

- **PR Number:** #33688
- **Author:** [frankwang28](https://github.com/frankwang28)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 21:27:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33688/files) (4):**
  - `docs/features/batch_invariance.md`
  - `tests/v1/determinism/utils.py`
  - `vllm/model_executor/layers/batch_invariant.py`
  - `vllm/v1/attention/ops/triton_unified_attention.py`

### Summary

**What changed and why**  
This PR enables `TRITON_ATTN` support for batch invariance by modifying the Triton unified attention kernel to use the 2D execution path when batch invariance is active. It also updates documentation and test utilities to include `TRITON_ATTN` as a validated backend.

**Technical impact**  
The changes ensure that when batch invariance is enabled, the Triton attention backend uses the 2D kernel path, which maintains deterministic behavior across batch sizes. This aligns `TRITON_ATTN` with other supported backends like `FLASH_ATTN` for invariance testing and production use.

**Potential risks**  
The 2D kernel may have different performance characteristics than the 3D kernel, potentially affecting throughput or latency. Additionally, the warning message now categorizes `TRITON_ATTN` as "decode-invariant," but thorough validation on Hopper GPUs is missing, which could hide hardware-specific issues.

**Key insights**  
Developers should verify performance impacts of the 2D kernel in production workloads. The update to `decode_invariant_backends` ensures proper warnings, but testing on Hopper GPUs remains a gap. Future changes should consider adding performance benchmarks for the Triton backend under batch invariance.

---

## 6. [[Refactor] Remove unused dead code](https://github.com/vllm-project/vllm/pull/33718)


### Base Information

- **PR Number:** #33718
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 21:25:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33718/files) (3):**
  - `vllm/collect_env.py`
  - `vllm/compilation/fx_utils.py`
  - `vllm/config/utils.py`

### Summary

**What changed and why**  
This PR removes three unused utility functions: `run_and_return_first_line` from environment collection, `find_specified_fn_maybe` and `find_specified_fn` from FX graph utilities, and `contains_object_print`, `assert_hashable`, and `handle_deprecated` from configuration utilities. The purpose is to clean up dead code that is no longer referenced elsewhere in the codebase.

**Technical impact**  
The removal reduces code complexity and maintenance overhead without affecting functionality, as these functions were not being called. It also eliminates unused imports (e.g., `regex` in `utils.py`), which may slightly improve import times and reduce potential dependency conflicts.

**Potential risks**  
Low risk, as the functions are confirmed unused. However, if any external or downstream code indirectly depended on these utilities (e.g., via dynamic imports or reflection), it could break. The `handle_deprecated` function removal might affect future deprecation handling if it was intended for reuse.

**Key insights**  
This is a straightforward cleanup that improves code hygiene. Developers should verify no internal or external dependencies exist via thorough searches (e.g., `grep`) before removing code. Consider adding a linter or CI check to detect unused functions proactively to prevent dead code accumulation.

---

## 7. [[Bugfix] Define router_logits_dtype for remaining MoE models](https://github.com/vllm-project/vllm/pull/33737)


### Base Information

- **PR Number:** #33737
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 21:24:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33737/files) (6):**
  - `vllm/model_executor/models/afmoe.py`
  - `vllm/model_executor/models/bailing_moe.py`
  - `vllm/model_executor/models/flex_olmo.py`
  - `vllm/model_executor/models/longcat_flash.py`
  - `vllm/model_executor/models/mimo_v2_flash.py`
  - `vllm/model_executor/models/step3p5.py`

### Summary

**What changed and why**  
This PR systematically adds explicit `router_logits_dtype` parameters to six Mixture-of-Experts (MoE) model implementations. The changes ensure consistency with a previous kernel support check that requires router logits to be defined in `torch.float32` when the underlying gate or router logic operates in float32 precision.

**Technical impact**  
These changes enforce proper dtype alignment between router computations and the fused MoE kernel, preventing potential dtype mismatches that could cause runtime errors or incorrect behavior. The modifications are minimal and focused, maintaining backward compatibility while ensuring each model correctly propagates its router precision requirements.

**Potential risks**  
If any model's forward pass logic incorrectly assumes a different dtype than what is now explicitly set, it could lead to subtle numerical discrepancies. Additionally, the changes rely on correct interpretation of each model's existing dtype handling (e.g., `self.router_dtype`, `self.gate_dtype`), so any oversight in those original implementations could propagate.

**Key insights**  
This is a necessary cleanup PR that standardizes dtype configuration across MoE models. Developers should verify that the assigned `router_logits_dtype` matches each model's actual router computation dtype. Future MoE model additions must include this parameter to avoid kernel compatibility issues.

---

## 8. [Save startup benchmark results as a list of values](https://github.com/vllm-project/vllm/pull/33629)


### Base Information

- **PR Number:** #33629
- **Author:** [huydhn](https://github.com/huydhn)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-03 20:37:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33629/files) (2):**
  - `vllm/benchmarks/lib/utils.py`
  - `vllm/benchmarks/startup.py`

### Summary

**What changed and why**  
The PR fixes a data format inconsistency in startup benchmark results by ensuring all benchmark values are stored as lists. Previously, single values were passed directly instead of being wrapped in lists, which violated the expected format used by other benchmarks like throughput. The changes wrap four average time metrics in lists and add a type validation check.

**Technical impact**  
This ensures consistency across all benchmark modules by enforcing a uniform list format for benchmark values. The type validation in `convert_to_pytorch_benchmark_format` will now catch format errors early, improving data integrity and compatibility with downstream processing that expects lists.

**Potential risks**  
The type check may break existing code that passes non-list values, though this is intentional to enforce consistency. There's a minor risk if other benchmark modules still pass non-list values, but the validation will now surface those issues. No performance impact is expected since only data structure wrapping is involved.

**Key insights**  
Always validate data formats at API boundaries to catch inconsistencies early. This fix aligns with the existing pattern in throughput benchmarks, demonstrating the importance of consistent data structures across similar modules. Developers should audit other benchmark modules to ensure they also comply with the list format requirement.

---

## 9. [[MM] Align the prefix of MMEncoderAttention with Attention](https://github.com/vllm-project/vllm/pull/33750)


### Base Information

- **PR Number:** #33750
- **Author:** [shen-shanshan](https://github.com/shen-shanshan)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-03 20:07:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33750/files) (17):**
  - `vllm/model_executor/models/aimv2.py`
  - `vllm/model_executor/models/blip.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/glm_ocr.py`
  - `vllm/model_executor/models/idefics2_vision_model.py`
  - `vllm/model_executor/models/intern_vit.py`
  - `vllm/model_executor/models/interns1_vit.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/openpangu_vl.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/step_vl.py`

### Summary

**What changed and why**  
This PR standardizes the prefix parameter passed to `MMEncoderAttention` constructors across 17 multimodal model files. The change appends `.attn` to the existing prefix value, aligning it with the naming convention used in the base `Attention` class.

**Technical impact**  
The modification ensures consistent naming for attention layer parameter storage and retrieval, particularly for weight loading and state management. This affects how attention weights are keyed in model checkpoints and may impact compatibility with existing saved states that rely on specific parameter naming patterns.

**Potential risks**  
If any existing checkpoints or serialized models depend on the previous prefix format (without `.attn`), weight loading could fail or produce mismatches. The change might also affect downstream tooling that parses parameter names, requiring updates to maintain compatibility.

**Key insights**  
Developers should verify that all affected models load correctly with updated checkpoints and that no breaking changes occur in deployment pipelines. Consider adding backward compatibility logic if needed, and ensure documentation reflects the new naming convention for attention layers.

---

## 10. [[CPU] Split attention dispatch by head_dim alignment](https://github.com/vllm-project/vllm/pull/32161)


### Base Information

- **PR Number:** #32161
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 19:37:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32161/files) (6):**
  - `cmake/cpu_extension.cmake`
  - `csrc/cpu/cpu_attn.cpp`
  - `csrc/cpu/cpu_attn_amx.hpp`
  - `csrc/cpu/cpu_attn_neon.hpp`
  - `csrc/cpu/generate_cpu_attn_dispatch.py`
  - `tests/kernels/attention/test_cpu_attn.py`

### Summary

**What changed and why**  
The changes introduce a split dispatch mechanism for CPU attention kernels based on head dimension alignment. Head dimensions divisible by 32 (e.g., 64, 128) can use all ISA implementations (AMX/NEON/VEC/VEC16), while those divisible only by 16 but not 32 (e.g., 80, 112) are restricted to VEC16. This prevents unsupported ISA combinations from being instantiated during compilation, fixing NEON and AMX compilation errors for non-32-multiple head sizes.

**Technical impact**  
The refactoring replaces manual dispatch macros with a generated header (`cpu_attn_dispatch_generated.h`) that encodes head_dim and ISA into a single integer for switch-based dispatch. This reduces template bloat by avoiding redundant instantiations and re-enables static alignment assertions in AMX and NEON kernels. The build process now automatically generates the dispatch header via a Python script integrated into CMake.

**Potential risks**  
If the generated dispatch header is not updated when new head dimensions or ISAs are added, runtime errors may occur. The encoding scheme assumes head_dim fits within 56 bits and ISA types within 8 bits, which is reasonable but could become a constraint with extreme configurations. Additionally, the split dispatch could lead to performance differences for head sizes like 80/112, as they are limited to VEC16 even on capable hardware.

**Key insights**  
Developers should run the generation script when modifying supported head dimensions or ISA implementations. The test suite has been expanded to include VEC16-only head sizes (80, 112), ensuring coverage for the restricted dispatch path. This approach cleanly separates alignment requirements and sets a pattern for maintainable, generated dispatch logic in performance-critical code.

---

## 11. [[1/N] Initial Implementation of Parser for ResponsesAPI](https://github.com/vllm-project/vllm/pull/32712)


### Base Information

- **PR Number:** #32712
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-03 18:59:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32712/files) (10):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/parser/__init__.py`
  - `vllm/parser/abstract_parser.py`
  - `vllm/parser/minimax_m2_parser.py`
  - `vllm/parser/parser_manager.py`

### Summary

**What changed and why**  
This PR introduces a unified `Parser` abstraction to consolidate reasoning and tool-call parsing logic, replacing separate `ReasoningParser` and `ToolParser` managers. It adds a `ParserManager` for lazy loading and registration, with `MiniMaxM2Parser` as the first implementation. The changes refactor the OpenAI serving endpoints (`responses` and `chat_completion`) to use the new parser interface, aiming to simplify future integration (e.g., for GPT-OSS and ChatCompletions).

**Technical impact**  
The architecture now centralizes parser management, reducing duplication and enabling a single entry point for both reasoning and tool parsing. The `ParserManager` supports lazy loading (similar to existing patterns) and delegates to legacy parsers when a unified parser isn’t available. This sets the foundation for extending parsing capabilities without modifying each serving endpoint individually.

**Potential risks**  
- The `_WrappedParser` fallback mechanism may introduce subtle behavioral differences if the delegation logic doesn’t perfectly mirror previous separate parser interactions.  
- Error handling in the new manager could obscure root causes if lazy loading fails (e.g., missing module paths).  
- The refactor touches multiple serving endpoints; any regression in parser initialization could affect both `responses` and `chat_completion` APIs.

**Key insights**  
- The design elegantly bridges old and new parsers via `DelegatingParser` and `_WrappedParser`, ensuring backward compatibility.  
- Developers should register new model-specific parsers in `vllm/parser/__init__.py` and leverage the lazy registration pattern.  
- Future work should validate that streaming and non-streaming extraction behaves identically across all integrated parsers.

---

## 12. [[Bugfix] Fix torchrun PP broadcast deadlock with async scheduling](https://github.com/vllm-project/vllm/pull/33701)


### Base Information

- **PR Number:** #33701
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-03 18:17:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33701/files) (3):**
  - `tests/distributed/test_torchrun_example.py`
  - `tests/distributed/test_torchrun_example_moe.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The fix addresses a deadlock that occurred when using `torchrun` with pipeline parallelism (PP) and async scheduling enabled. The issue was that sampled tokens were being broadcast unnecessarily after PP outputs had already been broadcast during logits computation. The code now skips the redundant broadcast when `broadcast_pp_output` is true, and the associated test workarounds disabling async scheduling have been removed.

**Technical impact**  
This change resolves a synchronization deadlock in the PP communication pattern under async scheduling. It refines the conditional logic in the sampling phase to avoid duplicate broadcasts, ensuring correct token distribution across PP ranks when using external launchers like `torchrun`. The system now supports async scheduling in PP configurations without deadlocks.

**Potential risks**  
If the `broadcast_pp_output` flag is incorrectly set or the PP group state is misconfigured, it could lead to missing broadcasts and token mismatches across ranks. The change assumes that the broadcast during logits computation is always sufficient, which may not hold for all PP implementations or custom configurations. Edge cases involving hybrid parallelism or dynamic PP group changes should be validated.

**Key insights**  
The fix is minimal and targeted, addressing the root cause by aligning broadcast logic with the existing PP output broadcast. Developers should ensure that `broadcast_pp_output` is accurately configured for their deployment setup. This change enables async scheduling for PP, potentially improving throughput, and the removed test FIXMEs confirm the issue is resolved.

---

## 13. [[Frontend][4/n] Make pooling entrypoints request schema consensus \| ScoreRequest](https://github.com/vllm-project/vllm/pull/33060)


### Base Information

- **PR Number:** #33060
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-03 17:48:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33060/files) (8):**
  - `examples/pooling/score/vision_rerank_api_online.py`
  - `examples/pooling/score/vision_score_api_online.py`
  - `tests/entrypoints/pooling/score/test_online_score.py`
  - `tests/entrypoints/pooling/score/test_online_score_vision.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/pooling/score/utils.py`

### Summary

**What changed and why**  
This PR extends the rerank and score APIs to support `list[ScoreMultiModalParam]` as inputs, addressing issue #32378. It introduces new type aliases (`ScoreInput`, `ScoreInputs`, `ScoreData`) and a validation function (`validate_score_input`) to unify handling of multimodal and text inputs across both APIs. Example scripts and tests are updated to demonstrate the new list-based input formats.

**Technical impact**  
The changes centralize input validation and preprocessing logic, reducing code duplication between `llm.py` and `serving.py`. The new `validate_score_input` function ensures consistent handling of string, multimodal dictionary, and list inputs, while maintaining backward compatibility. The architecture now better separates raw input types from processed score data, improving type safety and clarity.

**Potential risks**  
The refactoring touches core scoring logic in both synchronous (`llm.py`) and asynchronous (`serving.py`) paths, which could introduce regressions if validation or data transformation fails. Edge cases with mixed input types (e.g., a list containing both strings and multimodal dicts) need careful testing. The `ScoreData` type may still be ambiguous between a string and a list of content parts, requiring clear documentation.

**Key insights**  
Developers should adopt the new `validate_score_input` function for any future scoring-related features to ensure consistency. The updated type aliases (`ScoreInputs`, `ScoreData`) clarify the expected data flow. Thorough testing of multimodal list inputs is critical, especially for embedding models which currently raise `NotImplementedError` for non-string inputs.

---

## 14. [[BugFix][Spec Decoding] Fix negative accepted tokens metric crash](https://github.com/vllm-project/vllm/pull/33729)


### Base Information

- **PR Number:** #33729
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-03 15:34:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33729/files) (2):**
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The fix addresses a crash when speculative decoding produces empty output tokens. Previously, when `generated_token_ids` was empty, the calculation `num_accepted = len([]) - 1 = -1` would cause a Prometheus counter to receive a negative value, triggering a ValueError. The change ensures spec decoding stats are only recorded when both draft tokens exist and tokens were actually generated.

**Technical impact**  
This prevents a runtime crash in edge cases where no tokens are generated during a speculative decoding step (e.g., due to request abortion or errors). The system now gracefully handles these scenarios by skipping metric recording, maintaining service availability while preserving existing metric collection for normal execution paths.

**Potential risks**  
Skipping metric recording in these edge cases could slightly skew acceptance rate calculations if such events occur frequently, though the PR description notes they should be infrequent. There's also a theoretical risk that other code paths might depend on these stats being present, but the test validates that stats are correctly set to None.

**Key insights**  
The fix is minimal and surgical—changing only the condition check. The comprehensive test case validates the fix and documents the edge case behavior. Developers should be aware that empty token generation during speculative decoding is now a handled condition, and future metric definitions may need to explicitly decide how to account for these zero-output steps.

---

## 15. [[Dependency] Remove comments of ray in dependency files](https://github.com/vllm-project/vllm/pull/33351)


### Base Information

- **PR Number:** #33351
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 15:30:48
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33351/files) (2):**
  - `requirements/cuda.txt`
  - `requirements/rocm.txt`

### Summary

**What changed and why**  
The PR removes explanatory comments from the `ray[cgraph]` dependency lines in both `requirements/cuda.txt` and `requirements/rocm.txt`. The comments previously stated that Ray Compiled Graph is "required for pipeline parallelism in V1," which is misleading because vLLM v1 can use a multiprocessing backend without Ray. This change clarifies that Ray is only necessary when users explicitly opt for the Ray executor backend.

**Technical impact**  
This is a documentation-only change that does not affect runtime dependencies or installation behavior. The `ray[cgraph]` package remains a required dependency in these files, so installations will continue to include it unless dependency specifications are altered in a future update. The change reduces potential confusion for developers reading the requirements files.

**Potential risks**  
There is minimal risk since only comments were removed. However, if developers rely on these comments to understand when Ray is needed, they might now lack context about its purpose. This could lead to incorrect assumptions unless they consult external documentation or the linked issue (#33445) for clarification.

**Key insights**  
The change highlights that Ray is not universally required for pipeline parallelism, aligning the documentation with actual backend flexibility. Developers should be aware that future work may make Ray an optional dependency, and they should follow issue #33445 for related discussions. For now, Ray remains a hard dependency in CUDA/ROCm environments.

---

## 16. [[Bugfix] Fix sparse MLA metadata building](https://github.com/vllm-project/vllm/pull/33579)


### Base Information

- **PR Number:** #33579
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 15:29:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33579/files) (1):**
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
The fix addresses a regression in sparse MLA (Multi-Latent Attention) metadata handling introduced by PR #33284. The previous refactoring moved logic from the backend to the layer but failed to properly handle sparse backends, causing an `AttributeError` because `FlashMLASparseMetadata` lacks `num_decodes` and related attributes. The changes restructure the token classification logic to separate sparse and non-sparse implementations.

**Technical impact**  
This refactoring changes how tokens are categorized for attention computation. Instead of relying on `attn_metadata` attributes (which sparse backends don't have), it now determines `num_mqa_tokens` and `num_mha_tokens` based on whether the implementation is sparse. Sparse implementations treat all tokens as MQA (Multi-Query Attention), while non-sparse implementations use the metadata to separate decode (MQA) from prefill (MHA) tokens.

**Potential risks**  
The assertion `assert attn_metadata.decode is not None` is now only checked for non-sparse implementations, which could mask issues if sparse implementations incorrectly receive decode metadata. There's also a risk that the token splitting logic (`num_mqa_tokens = q.size(0)`) for sparse backends might not align with actual token semantics during prefill phases.

**Key insights**  
The core issue was assuming all MLA implementations share the same metadata structure. The fix properly isolates sparse backend handling by removing dependencies on metadata attributes they don't provide. Developers should ensure sparse backends only receive appropriate metadata and validate that treating all tokens as MQA doesn't affect quality during prefill operations.

---

## 17. [[Bugfix] Disable TRTLLM FP8 MoE if router_logits_dtype==float32 and routing_method!=DeepSeekV3](https://github.com/vllm-project/vllm/pull/33613)


### Base Information

- **PR Number:** #33613
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 13:26:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33613/files) (5):**
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/models/minimax_m2.py`

### Summary

**What changed and why**  
This PR fixes a bug where the FlashInfer TRTLLM FP8 MoE kernel was incorrectly selected for models using float32 router logits with non-DeepSeekV3 routing methods. The issue occurred with models like MiniMaxAI/MiniMax-M2.1, causing accuracy problems. The fix adds proper validation to disable TRTLLM FP8 MoE when `router_logits_dtype==float32` and `routing_method!=DeepSeekV3`, and centralizes router logits/bias dtype conversion logic.

**Technical impact**  
The changes modify the MoE backend selection logic to properly exclude incompatible configurations, preventing accuracy degradation. The router logits and bias dtype conversion is now handled within the kernel implementation rather than at the call sites, improving code maintainability. The MiniMax-M2 model is explicitly configured to use float32 router logits to ensure correct behavior.

**Potential risks**  
If the FlashInfer kernel adds support for float32 router logits with non-DeepSeekV3 routing in the future (tracked in flashinfer-ai/flashinfer#2469), this validation may become overly restrictive. The changes assume DeepSeekV3 routing always requires float32 conversion, which could break if routing method requirements change. There's also a risk of silent performance degradation if models fall back to slower MoE implementations.

**Key insights**  
The fix properly isolates kernel compatibility checks from application logic, making the codebase more maintainable. Developers should be aware that float32 router logits are only supported with DeepSeekV3 routing for the TRTLLM FP8 kernel. When adding new MoE models, ensure router_logits_dtype is correctly specified based on the routing method requirements.

---

## 18. [[Voxtral Realtime] Change name](https://github.com/vllm-project/vllm/pull/33716)


### Base Information

- **PR Number:** #33716
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 13:03:29
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33716/files) (4):**
  - `examples/online_serving/openai_realtime_client.py`
  - `examples/online_serving/openai_realtime_microphone_client.py`
  - `tests/entrypoints/openai/test_realtime_validation.py`
  - `tests/models/multimodal/generation/test_voxtral_realtime.py`

### Summary

**What changed and why**  
The PR updates all references from "mistralai/Voxtral-Mini-3B-Realtime-2602" to "mistralai/Voxtral-Mini-4B-Realtime-2602" across example scripts and test files. This reflects a model name change, likely indicating an upgrade from a 3B to a 4B parameter version of the Voxtral realtime model.

**Technical impact**  
These changes ensure that the example client code and test suites correctly reference the updated model name. The modifications are purely cosmetic and do not affect the underlying functionality, architecture, or system behavior—they only update string constants used for model identification.

**Potential risks**  
If the new "4B" model is not backward compatible or has different input/output specifications, existing tests or client code might fail despite the name-only change. There is also a risk if the model is not publicly available or accessible under the new name, which would break example execution and test runs.

**Key insights**  
Developers should verify that the new model is correctly deployed and accessible. It's recommended to run the updated test suites to ensure compatibility. Consider if any model-specific parameters (like context window or tokenizer) need adjustment due to the parameter count change.

---

## 19. [[MISC] Fix Tensor Parallelism for Quantized Mamba Models with n_groups=1](https://github.com/vllm-project/vllm/pull/33257)


### Base Information

- **PR Number:** #33257
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-03 12:10:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33257/files) (1):**
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`

### Summary

**What changed and why**  
The PR fixes tensor parallelism (TP) support for quantized Mamba models when `n_groups=1`. Previously, custom weight loaders for group replication were only implemented for non-quantized layers, causing failures with TP>1. The changes unify the initialization path to always use `ColumnParallelLinear` with custom weight loaders, extending support to quantized layers via the `weight_loader` property on `BasevLLMParameter` subclasses.

**Technical impact**  
This refactors the Mamba mixer layer to handle both quantized and non-quantized cases through a single code path, removing the conditional branching based on `n_groups % tp_size`. The custom weight loader now supports `ModelWeightParameter` (used for quantization) by checking `isinstance(self.in_proj.weight, BasevLLMParameter)` and directly assigning the loader, while standard `Parameter` objects use the existing `set_weight_attrs` approach.

**Potential risks**  
The removal of the assertion `(n_groups % self.tp_size == 0) or self.tp_size == 1 or quant_config is None` could allow invalid configurations for quantized models when `n_groups > 1` but not divisible by TP size. Additionally, directly manipulating `weight_loader` attributes on `BasevLLMParameter` subclasses assumes stable API compatibility, which may break if the base class changes.

**Key insights**  
The fix elegantly extends weight loader support to quantized layers by leveraging polymorphism. Developers should ensure that the `BasevLLMParameter` check remains robust across future refactoring. The validation shows no performance regression, but the removed assertion should be replaced with a clearer runtime check or documentation to prevent misuse.

---

## 20. [Turn `@config` into a `dataclass_transform`](https://github.com/vllm-project/vllm/pull/31541)


### Base Information

- **PR Number:** #31541
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 09:41:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31541/files) (32):**
  - `tests/engine/test_arg_utils.py`
  - `tests/test_config.py`
  - `tests/tools/test_config_validator.py`
  - `tests/v1/e2e/test_spec_decode.py`
  - `tests/v1/structured_output/test_backend_guidance.py`
  - `tools/pre_commit/validate_config.py`
  - `vllm/config/__init__.py`
  - `vllm/config/attention.py`
  - `vllm/config/cache.py`
  - `vllm/config/compilation.py`
  - `vllm/config/device.py`
  - `vllm/config/ec_transfer.py`
  - `vllm/config/kv_events.py`
  - `vllm/config/kv_transfer.py`
  - `vllm/config/load.py`
  - `vllm/config/lora.py`
  - `vllm/config/model.py`
  - `vllm/config/multimodal.py`
  - `vllm/config/observability.py`
  - `vllm/config/parallel.py`
  - `vllm/config/pooler.py`
  - `vllm/config/profiler.py`
  - `vllm/config/scheduler.py`
  - `vllm/config/speculative.py`
  - `vllm/config/speech_to_text.py`
  - `vllm/config/structured_outputs.py`
  - `vllm/config/utils.py`
  - `vllm/config/vllm.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/transformers_utils/configs/speculators/algos.py`
  - `vllm/transformers_utils/configs/speculators/base.py`
  - `vllm/v1/spec_decode/draft_model.py`

### Summary

**What changed and why**  
The PR converts the `@config` decorator into a `dataclass_transform` that automatically creates a Pydantic dataclass with `extra="forbid"` as the default. This eliminates the need for dual `@config` and `@dataclass` decorators on configuration classes, simplifying usage to just `@config`. It also fixes a bug where `decoding_config` was incorrectly passed and resolves kwargs generation issues for speculative configs.

**Technical impact**  
Configuration classes now inherit Pydantic dataclass behavior (e.g., validation, serialization) directly via `@config`, reducing boilerplate. The default `extra="forbid"` enforces strict field validation, preventing silent acceptance of unknown parameters. This change affects all config classes across the codebase, requiring updates to imports, decorators, and field definitions (e.g., replacing `dataclasses.field` with `pydantic.Field`).

**Potential risks**  
The stricter validation (`extra="forbid"`) may break existing code that passes unexpected fields to configs. The removal of `replace` methods from some config classes (e.g., `ParallelConfig`) could affect code relying on them, though a new `replace` utility is provided. Changes to speculative config kwargs generation could introduce regressions if not all edge cases are covered.

**Key insights**  
Developers should now use only `@config` for new configuration classes and replace `dataclasses.field` with `pydantic.Field` where needed. The `replace` function from `vllm.config.utils` should be used instead of custom `replace` methods. Ensure all config instantiations avoid extra kwargs to comply with the new validation. Review speculative decoding and related tests for compatibility.

---

## 21. [[torch.compile] Significantly speed up cold start times](https://github.com/vllm-project/vllm/pull/33641)


### Base Information

- **PR Number:** #33641
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-03 09:12:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33641/files) (3):**
  - `tests/compile/test_cold_start.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/compiler_interface.py`

### Summary

**What changed and why**  
The PR changes vLLM's torch.compile cold start caching from per-subgraph-index to per-subgraph-hash. Previously, each subgraph (even if identical) was compiled separately, creating redundant artifacts. Now, subgraphs are hashed using torch.compile internals (sanitizing node names), and the cache maps `(runtime_shape, graph_hash, backend_name)` to compiled artifacts, ensuring only unique subgraphs are compiled.

**Technical impact**  
This reduces compilation artifacts from N (number of subgraphs) to the number of unique subgraphs (e.g., from 33 to 3 for typical vLLM models). The cache key change (`graph_index` → `graph_hash`) enables deduplication, and a new `loaded_cache_entries` dictionary prevents redundant in-memory loading. The compiler interface's `load` method no longer needs `graph_index`.

**Potential risks**  
The hash relies on torch.compile internals (`AOTAutogradCachePickler`), which may change in future PyTorch versions, risking cache invalidation or errors. The hash must remain agnostic to node names; any oversight could cause unnecessary recompilation. Removing `graph_index` from the compiler interface could break custom backends if they depend on it.

**Key insights**  
Cold start times improve significantly (e.g., ~42% for llama-3.1-70b). Developers should verify the hash stability across PyTorch updates and ensure custom compilers adapt to the removed `graph_index` parameter. The change sets a foundation for further optimizations in PyTorch's `standalone_compile` cache hits.

---

## 22. [[Attention][FA3] Update FA3 to include new swizzle optimization](https://github.com/vllm-project/vllm/pull/23465)


### Base Information

- **PR Number:** #23465
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 08:08:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/23465/files) (3):**
  - `cmake/external_projects/vllm_flash_attn.cmake`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`

### Summary

**What changed and why**  
This PR updates the FlashAttention (FA3) backend to incorporate a new swizzle optimization from the upstream flash-attention repository. It updates the git commit hash to include the optimization, and modifies two attention backend files to correctly size the `scheduler_metadata` tensor for CUDA graph compatibility, ensuring it accounts for the larger buffer size required by the optimization.

**Technical impact**  
The changes align vLLM with the latest FlashAttention improvements, potentially improving performance as shown in the benchmark (slight reductions in TTFT and TPOT). The `scheduler_metadata` tensor is now sized as `max(max_cudagraph_size, max_num_seqs) * 4 + 1` to prevent buffer overflows, which is critical for stable execution under CUDA graphs.

**Potential risks**  
If `max_cudagraph_size` is not properly configured or is excessively large, memory usage could increase unnecessarily. The dependency on a specific upstream commit (`2adfc8c2177c5b0e8ddeedfd5a8990d80eb496ff`) may introduce instability if that commit has unresolved issues. Edge cases where `max_cudagraph_size` is zero or undefined require careful handling (addressed with `or 0`).

**Key insights**  
Always verify that `max_cudagraph_size` is set appropriately in the compilation config to balance memory and performance. The benchmark shows marginal but consistent improvements, validating the optimization. Developers should monitor for any regressions in memory usage or CUDA graph capture failures after this update.

---

## 23. [[P/D] rework mooncake connector and introduce its bootstrap server](https://github.com/vllm-project/vllm/pull/31034)


### Base Information

- **PR Number:** #31034
- **Author:** [dtcccc](https://github.com/dtcccc)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 08:08:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31034/files) (9):**
  - `docs/features/disagg_prefill.md`
  - `docs/features/mooncake_connector_usage.md`
  - `examples/online_serving/disaggregated_serving/README.md`
  - `examples/online_serving/disaggregated_serving/mooncake_connector/mooncake_connector_proxy.py`
  - `examples/online_serving/disaggregated_serving/mooncake_connector/run_mooncake_connector.sh`
  - `vllm/distributed/kv_transfer/kv_connector/factory.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake/__init__.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake/mooncake_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake/mooncake_utils.py`

### Summary

**What changed and why**  
This PR reworks the Mooncake connector to introduce a centralized bootstrap server architecture. The primary goals are to improve Time-To-First-Token (TTFT) performance by enabling concurrent prefill and decode operations, and to lay groundwork for future features like layerwise transfer. The changes replace previous workarounds with a cleaner design where prefiller workers register with a bootstrap server, allowing proxy and decoder workers to query connection information dynamically.

**Technical impact**  
The architecture shifts from direct peer-to-peer coordination to a centralized registration model. A new bootstrap server runs on the prefiller side, managing worker metadata (DP/TP/PP ranks and addresses). This decouples discovery from transfer logic, simplifying connector state management. The proxy script is updated to leverage this server, and the connector’s internal metadata structures are refactored to use transfer IDs and engine IDs for better request tracking. Performance improvements of ~6ms in TTFT are observed.

**Potential risks**  
Centralized bootstrap server introduces a single point of failure—if it crashes, new worker registrations and queries will fail. The refactored metadata handling (e.g., `reqs_to_recv` now keyed by engine ID and DP rank) increases complexity and could lead to subtle bugs if DP rank mappings are inconsistent. The removal of workarounds depends on upstream fixes (#33037, #32937), so regressions could occur if those fixes are incomplete or reverted.

**Key insights**  
Developers should ensure the bootstrap server is highly available and properly configured across all prefiller instances. The new proxy script (`mooncake_connector_proxy.py`) must be used for correct operation. Testing should focus on edge cases like worker restarts and heterogeneous TP/PP configurations (currently rejected). The performance gains justify the architectural shift, but thorough integration testing is recommended to validate stability under load.

---

## 24. [[Bugfix] Fix startup hang for Granite Speech](https://github.com/vllm-project/vllm/pull/33699)


### Base Information

- **PR Number:** #33699
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-03 07:57:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33699/files) (1):**
  - `vllm/multimodal/budget.py`

### Summary

**What changed and why**  
The change moves the initialization of the processor and cache inside the existing `set_default_torch_num_threads()` context manager. This ensures that processor initialization—which can involve PyTorch operations—also runs with controlled thread settings, preventing a startup hang observed with certain models like Granite Speech.

**Technical impact**  
This modification guarantees that all PyTorch-related initialization steps (cache creation, processor instantiation, and modality limit extraction) execute under the same thread configuration. It maintains the existing behavior for other components while specifically resolving hangs during model initialization tests.

**Potential risks**  
If `set_default_torch_num_threads()` has side effects (e.g., global state changes), extending its scope could inadvertently affect subsequent operations within the `__init__` method. Additionally, any exceptions raised during processor initialization now occur within the context manager, which might alter error handling or resource cleanup.

**Key insights**  
The fix highlights that processor initialization can be as sensitive to thread settings as other model components. Developers should ensure that all PyTorch-dependent initialization phases are wrapped in thread control contexts when dealing with multimodal models. Consider verifying that the context manager’s scope does not introduce performance overhead or unintended constraints in other scenarios.

---

## 25. [[Voxtral models] Skip warm-up to skip confusing error message in warm-up](https://github.com/vllm-project/vllm/pull/33576)


### Base Information

- **PR Number:** #33576
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 07:22:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33576/files) (3):**
  - `vllm/entrypoints/openai/translations/speech_to_text.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_realtime.py`

### Summary

**What changed and why**  
The changes introduce a `skip_warmup_audio_preprocessing` flag to bypass audio preprocessing warm-up for Voxtral models. This is a temporary workaround because Transformers' `cached_get_processor` currently throws a `TypeError` for MistralCommonTokenizer backends due to a missing `tokenizer_path` argument.

**Technical impact**  
The warm-up logic now checks for the flag before attempting to initialize the processor, preventing the error. This avoids confusing users with non-fatal error messages during server startup, but may result in higher latency for the first audio transcription request on Voxtral models.

**Potential risks**  
Skipping warm-up could cause unexpected latency spikes for initial requests in production. Additionally, this workaround masks the underlying issue in `cached_get_processor`, which may affect other models or future updates if not resolved.

**Key insights**  
This is a tactical fix that should be accompanied by a long-term solution to address the root cause in the processor initialization logic. Developers should monitor performance and consider re-enabling warm-up once the Transformers library support is improved.

---

## 26. [[MM] Pass `prefix` parameter to MMEncoderAttention](https://github.com/vllm-project/vllm/pull/33674)


### Base Information

- **PR Number:** #33674
- **Author:** [shen-shanshan](https://github.com/shen-shanshan)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 06:47:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33674/files) (15):**
  - `vllm/model_executor/models/aimv2.py`
  - `vllm/model_executor/models/blip.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/idefics2_vision_model.py`
  - `vllm/model_executor/models/intern_vit.py`
  - `vllm/model_executor/models/interns1_vit.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/step_vl.py`

### Summary

**What changed and why**  
This PR adds a `prefix` parameter to the `MMEncoderAttention` constructor across 15 multimodal model implementations. The purpose is to enable OOT (out-of-tree) `MMEncoderAttention` operations to extract `layer_index` from the prefix for performance optimization in a related PR.

**Technical impact**  
The changes are minimal and backward compatible, only adding an optional parameter that doesn't affect existing functionality. This enables future optimizations in the vllm-ascend integration by providing layer identification information to the attention mechanism. The consistent pattern across all models ensures uniform behavior.

**Potential risks**  
The main risk is ensuring all call sites properly pass the `prefix` parameter, though the changes show consistent implementation. There's a theoretical risk of breaking existing OOT implementations if they don't handle the new parameter gracefully, but the PR description indicates this has no effect on current code.

**Key insights**  
This is a preparatory change for performance optimization in the vllm-ascend integration. Developers should ensure downstream OOT implementations properly utilize the `prefix` parameter when available. The changes follow a clean, consistent pattern across all affected models, making future maintenance straightforward.

---

## 27. [[Bugfix] Do not add extra \n for image-only cases when constructing multimodal text prompts.](https://github.com/vllm-project/vllm/pull/33647)


### Base Information

- **PR Number:** #33647
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 06:43:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33647/files) (1):**
  - `vllm/entrypoints/chat_utils.py`

### Summary

**What changed and why**  
The change modifies the `_get_full_multimodal_text_prompt` function to avoid adding an extra newline (`\n`) when the `text_prompt` is empty (e.g., in image-only cases). Previously, the function always joined `missing_placeholders` and `text_prompt` with a newline, which could result in a trailing newline when `text_prompt` was an empty string.

**Technical impact**  
This fix ensures that prompts containing only image placeholders (no text) are constructed without an extraneous newline at the end. This maintains cleaner prompt formatting and prevents potential subtle issues in downstream processing where the exact prompt string matters, such as in tokenization or model input validation.

**Potential risks**  
If there are existing workflows or tests that depend on the previous behavior (including the trailing newline), they may break. Additionally, the change assumes `text_prompt` being falsy (empty string) is the only scenario for image-only cases; other falsy values or whitespace-only strings might not be handled as intended.

**Key insights**  
Always validate edge cases like empty strings when constructing strings with separators. Consider adding a test specifically for the image-only prompt scenario to prevent regression. Reviewers should ensure no downstream code relies on the previous newline behavior.

---

## 28. [Feat/add nemotron nano v3 tests](https://github.com/vllm-project/vllm/pull/33345)


### Base Information

- **PR Number:** #33345
- **Author:** [shaharmor98](https://github.com/shaharmor98)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-03 05:52:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33345/files) (6):**
  - `.buildkite/lm-eval-harness/configs/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16.yaml`
  - `.buildkite/lm-eval-harness/configs/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8.yaml`
  - `.buildkite/lm-eval-harness/configs/models-large-hopper.txt`
  - `.buildkite/lm-eval-harness/configs/models-large.txt`
  - `tests/config/base_model_arch_groundtruth.json`
  - `tests/config/test_model_arch_config.py`

### Summary

**What changed and why**  
This PR adds testing configurations for the NVIDIA Nemotron-3 Nano 30B model variants (BF16 and FP8) to the lm-eval-harness CI pipeline. It includes new YAML configuration files for both model variants, updates the model lists for large-scale testing, and extends the base model architecture ground truth and trust remote code allowlist to support the new model.

**Technical impact**  
The changes integrate the Nemotron-3 Nano model into the existing evaluation framework, enabling automated performance benchmarking on the GSM8K task. The FP8 configuration specifically enables FlashInfer MoE FP8 optimizations via environment variables, which may affect inference performance and memory usage during testing.

**Potential risks**  
The FP8 variant relies on experimental environment variables (`VLLM_USE_FLASHINFER_MOE_FP8`, `VLLM_FLASHINFER_MOE_BACKEND`) that could cause instability if the underlying vLLM version changes or if the FlashInfer backend is unavailable. Additionally, the model's trust_remote_code requirement introduces a security consideration, though it's already managed in the allowlist.

**Key insights**  
Developers should verify that the CI environment supports the required FlashInfer dependencies for FP8 testing. The performance thresholds (exact_match values) should be monitored for consistency across CI runs, as they serve as regression benchmarks. Consider documenting these new model configurations in the project's supported models documentation for future reference.

---

## 29. [[Bugfix][Async][Connector] avoid vllm-side double free during async scheduling + request abort + async KV cache transfer](https://github.com/vllm-project/vllm/pull/33377)


### Base Information

- **PR Number:** #33377
- **Author:** [KuntaiDu](https://github.com/KuntaiDu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 05:50:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33377/files) (1):**
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The fix modifies the request completion check in `update_from_output()` to also consider `request.is_finished()` alongside a `None` check. This prevents a double-free bug where aborted requests undergoing async KV cache transfer could be processed twice in `get_finished`, because they aren't set to `None` during finalization.

**Technical impact**  
This change ensures robust handling of request abortion in async scheduling with KV cache transfers. It maintains compatibility with existing abort logic while extending protection to cases where `delay_free_blocks=True`, preventing double frees that could corrupt memory or cause crashes.

**Potential risks**  
If `is_finished()` has edge cases or race conditions during async operations, some requests might still be mishandled. Additionally, the fix assumes `is_finished()` accurately reflects completion during KV transfer; any oversight here could lead to missed frees or premature skips.

**Key insights**  
Always verify request state with both existence and completion checks when dealing with async resource management. Developers should ensure `is_finished()` is thread-safe and consistently updated during all async phases, and consider adding assertions or logs for aborted requests in similar pathways.

---

## 30. [Document NixlConnector backend selection via kv_connector_extra_config](https://github.com/vllm-project/vllm/pull/33552)


### Base Information

- **PR Number:** #33552
- **Author:** [KrxGu](https://github.com/KrxGu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 05:49:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33552/files) (1):**
  - `docs/features/nixl_connector_usage.md`

### Summary

**What changed and why**  
This PR adds documentation for selecting NIXL transport backends (like UCX or LIBFABRIC) via the existing `--kv-transfer-config` parameter. It addresses a documentation gap by clearly showing users how to configure the backend without relying on environment variables, using both JSON and CLI dotted-key syntax.

**Technical impact**  
The change is purely documentation and has no impact on the codebase's runtime behavior or architecture. It improves user experience by providing clear, actionable examples for a configuration mechanism that already exists, making the feature more discoverable and usable.

**Potential risks**  
There is a minor risk if the documented syntax or parameter (`kv_connector_extra_config.backends`) is incorrect or unsupported in certain versions, which could lead to user configuration errors. The note about backend availability depending on the NIXL build is crucial, as users might expect all backends to be universally available.

**Key insights**  
The documentation successfully fills a known gap and standardizes configuration guidance. Developers should ensure the examples are kept in sync with the actual CLI and JSON parsing logic. For future work, consider adding this configuration detail to any automated schema validation or help text for the `--kv-transfer-config` flag.

---

## 31. [Fix Gemma3n audio encoder for Transformers v5](https://github.com/vllm-project/vllm/pull/33673)


### Base Information

- **PR Number:** #33673
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 05:49:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33673/files) (1):**
  - `vllm/model_executor/models/gemma3n_mm.py`

### Summary

**What changed and why**  
The PR modifies the Gemma3n audio encoder to handle different output formats from Transformers v4 and v5. Transformers v5 changed multimodal feature methods to return `BaseModelOutputWithPooling` (a dictionary-like object) instead of a tuple, breaking vLLM's expectation of a tuple return from `audio_tower.forward`.

**Technical impact**  
This change adds version compatibility logic, allowing the model to work with both Transformers v4 and v5. The audio processing now conditionally extracts `last_hidden_state` and `audio_mel_mask` from the new output structure while maintaining backward compatibility with the old tuple format.

**Potential risks**  
The fix assumes `audio_mel_mask` is always present in the v5 output object, which could fail if the attribute name differs in future updates. There's also a risk of silent failures if the output type changes unexpectedly (e.g., to a different non-tuple type).

**Key insights**  
This is a necessary compatibility patch that follows the adapter pattern. Developers should consider adding explicit version checks or configuration flags for cleaner long-term maintenance. The change highlights the importance of abstracting external library dependencies to minimize upgrade impacts.

---

## 32. [[Models] Intern-S1-Pro](https://github.com/vllm-project/vllm/pull/33636)


### Base Information

- **PR Number:** #33636
- **Author:** [CUHKSZzxy](https://github.com/CUHKSZzxy)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 05:49:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33636/files) (11):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/layers/rotary_embedding/__init__.py`
  - `vllm/model_executor/layers/rotary_embedding/base.py`
  - `vllm/model_executor/layers/rotary_embedding/fope.py`
  - `vllm/model_executor/models/interns1_pro.py`
  - `vllm/model_executor/models/qwen3_moe.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds support for the Intern-S1-Pro vision-language model, which extends the existing Intern-S1 architecture. The changes include a new model implementation (`InternS1ProForConditionalGeneration`), a custom Fourier Rotary Embedding (FoPE) layer, updates to rotary embedding infrastructure, and integration into the model registry, documentation, and examples.

**Technical impact**  
The addition introduces a new MoE-based vision-language model with specialized rotary embeddings (FoPE) that require deferred cache initialization and head-specific coefficients. The changes extend the rotary embedding factory to support FoPE parameters and modify base classes to optionally skip cache initialization. Existing Qwen3-based VL and MoE components are reused with minor adjustments for deepstack index handling and decoder layer configurability.

**Potential risks**  
The FoPE implementation assumes key tensors are always provided in `forward_native`, which could break compatibility with other rotary embeddings. The custom routing function in the MoE block may not align with vLLM's optimized routing utilities. There is also a risk of incomplete testing, as the registry entry sets `is_available_online=False`, indicating potential offline or restricted model availability.

**Key insights**  
Developers should verify that FoPE's requirement for key tensors does not affect other rotary embedding use cases. The custom MoE routing logic should be reviewed for performance and consolidated with vLLM's routing functions if possible. Ensure thorough offline testing of the full model pipeline, given the online unavailability flag. The changes demonstrate a pattern for extending VL-MoE models with specialized positional encodings.

---

## 33. [Fix Gemma3 GGUF for Transformers v5](https://github.com/vllm-project/vllm/pull/33683)


### Base Information

- **PR Number:** #33683
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-03 04:36:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33683/files) (1):**
  - `vllm/transformers_utils/gguf_utils.py`

### Summary

**What changed and why**  
The change replaces the deprecated `Gemma3Config.from_text_vision_configs()` method with a direct call to `Gemma3Config.__init__()` constructor. This update is necessary because Transformers v5 removed the deprecated helper method, and direct initialization provides identical functionality.

**Technical impact**  
This ensures compatibility with the upcoming Transformers v5 release by removing dependency on a deprecated API. The change maintains the same configuration behavior for Gemma3 GGUF models, as the constructor parameters remain unchanged.

**Potential risks**  
If the `Gemma3Config` constructor signature differs from the deprecated method's parameters, initialization could fail. There's also a risk that other deprecated methods in the codebase might cause similar compatibility issues with Transformers v5.

**Key insights**  
This is a straightforward compatibility fix that follows Hugging Face's deprecation guidance. Developers should audit the codebase for other uses of deprecated Transformers methods to ensure full v5 compatibility. The change demonstrates proper migration from deprecated helper methods to standard constructors.

---

## 34. [Fix offline test for Transformers v5](https://github.com/vllm-project/vllm/pull/33682)


### Base Information

- **PR Number:** #33682
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 04:07:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33682/files) (1):**
  - `tests/entrypoints/offline_mode/test_offline_mode.py`

### Summary

**What changed and why**  
The change addresses an incompatibility in offline tests with Transformers v5, where `tokenization_utils` and `tokenization_utils_fast` have been removed and replaced with aliases. The test's module reloading logic fails with these aliases, so the fix ensures they are deleted from `sys.modules` before reloading, allowing them to be re-aliased on the next import.

**Technical impact**  
This update ensures that offline tests correctly handle Transformers v5's architectural changes, preventing import errors during module reloading. It maintains backward compatibility by preserving the alias mechanism while allowing the test suite to function without requiring manual intervention or test modifications.

**Potential risks**  
If other modules are similarly aliased in future Transformers versions, they may also cause reload failures unless added to the `aliased_modules` list. Additionally, deleting modules from `sys.modules` could inadvertently affect other tests or imports if not carefully scoped to the reloading process.

**Key insights**  
Developers should monitor for similar alias patterns in Transformers updates and consider extending the `aliased_modules` list as needed. This fix highlights the importance of adapting test utilities to library evolution, especially when internal APIs change. Ensure that any future changes to module reloading logic account for alias-based designs.

---

## 35. [[Bugfix] fix qwen3-asr response error](https://github.com/vllm-project/vllm/pull/33644)


### Base Information

- **PR Number:** #33644
- **Author:** [jesse996](https://github.com/jesse996)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 03:33:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33644/files) (1):**
  - `vllm/model_executor/models/qwen3_asr.py`

### Summary

**What changed and why**  
The fix addresses a response error in the Qwen3-ASR model by implementing a missing `get_data_parser` method in the `Qwen3ASRForCausalLM` class and removing the duplicate `_get_data_parser` method from the `Qwen3ASRMultiModalProcessor` class. This resolves an issue where the model likely failed to properly parse audio input data due to an incorrect or missing data parser configuration.

**Technical impact**  
This change ensures the Qwen3-ASR model correctly initializes its multimodal data parser with the required sampling rate and expected hidden size parameters. The architecture now properly separates concerns by having the model class provide the parser configuration, while the processor class inherits its parent's behavior, improving code organization and consistency with the inheritance hierarchy.

**Potential risks**  
The removed `_get_data_parser` method in the processor class might have contained custom logic not present in the parent class. If the parent's implementation differs significantly, it could introduce subtle behavioral changes. Additionally, the new `get_data_parser` method adds a dependency on `_get_expected_hidden_size()`, which must be properly implemented to avoid runtime errors.

**Key insights**  
The fix properly centralizes data parser configuration in the model class, which is the appropriate architectural pattern. Developers should verify that `_get_expected_hidden_size()` returns the correct value for the Qwen3-ASR model variant. Consider adding unit tests for the data parser initialization to ensure both audio sampling rate and hidden size parameters are correctly propagated.

---

## 36. [[Misc] Update default image format of `encode_base64`](https://github.com/vllm-project/vllm/pull/33656)


### Base Information

- **PR Number:** #33656
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-03 03:13:17
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33656/files) (2):**
  - `vllm/multimodal/media/image.py`
  - `vllm/multimodal/utils.py`

### Summary

**What changed and why**  
The PR updates the default image format in `encode_base64` from JPEG to PNG to resolve a backwards-incompatible change announcement. It removes deprecation warnings and cleans up related code, including replacing direct base64 encoding with a utility function for tensors.

**Technical impact**  
Changing the default from JPEG to PNG ensures lossless compression for encoded images, improving fidelity. The removal of the logger import and warning simplifies the code, while replacing manual base64 encoding with `tensor2base64` centralizes tensor serialization logic.

**Potential risks**  
Users relying on the old JPEG default may experience larger base64 payloads due to PNG's lossless compression. The removal of the deprecation warning could surprise users who missed the announcement, though the change is intentional for v0.15 compatibility.

**Key insights**  
This is a breaking change; developers should update any code expecting JPEG-encoded images by explicitly setting `image_format="JPEG"` if needed. The consolidation of base64 encoding into `tensor2base64` promotes maintainability but requires verification that the utility handles all tensor formats correctly.

---

## 37. [[Bugfix] Disable RoutingMethodType.[Renormalize,RenormalizeNaive] for TRTLLM per-tensor FP8 MoE](https://github.com/vllm-project/vllm/pull/33620)


### Base Information

- **PR Number:** #33620
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-03 02:37:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33620/files) (1):**
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`

### Summary

**What changed and why**  
The PR disables `RoutingMethodType.Renormalize` and `RoutingMethodType.RenormalizeNaive` for the TRTLLM per-tensor FP8 MoE backend. This change addresses accuracy issues observed when using these routing methods, as documented in GitHub issue #33532. The methods are temporarily commented out with a note explaining the investigation.

**Technical impact**  
This restricts the available routing methods for the TRTLLM FP8 MoE backend to only `RoutingMethodType.Llama4`. The change directly impacts model accuracy and performance, as shown in the test results where switching to `Fp8MoeBackend.FLASHINFER_CUTLASS` (which presumably uses different routing) significantly improved accuracy from 0.016 to 0.780.

**Potential risks**  
Disabling these routing methods may limit optimization opportunities or compatibility with certain model configurations that previously relied on them. If the issue is specific to FP8 quantization or TRTLLM, other backends or data types might still require these methods, potentially causing inconsistencies across the codebase.

**Key insights**  
The accuracy improvement is substantial, confirming the routing methods were problematic for this specific configuration. Developers should treat this as a temporary workaround; the commented code and linked issue indicate further investigation is needed. Ensure any future changes to routing methods are validated across all supported backends and quantization schemes to prevent similar regressions.

---

## 38. [[Refactor] Clean up pooling serial utils](https://github.com/vllm-project/vllm/pull/33665)


### Base Information

- **PR Number:** #33665
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-03 02:29:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33665/files) (9):**
  - `examples/pooling/embed/embedding_requests_base64_online.py`
  - `examples/pooling/embed/embedding_requests_bytes_online.py`
  - `tests/entrypoints/pooling/embed/test_online.py`
  - `tests/entrypoints/pooling/pooling/test_online.py`
  - `tests/utils_/test_serial_utils.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/pooling/utils.py`
  - `vllm/utils/serial_utils.py`

### Summary

**What changed and why**  
This refactor consolidates pooling serialization utilities by introducing a `DTypeInfo` dataclass to unify dtype mappings, replacing `base64` with faster `pybase64`, and moving entrypoint-specific utilities to a dedicated `vllm/entrypoints/pooling/utils.py` module. The changes simplify serialization logic and improve code organization.

**Technical impact**  
The refactor reduces code duplication by centralizing dtype information in a single dataclass and separates entrypoint-specific serialization functions from general utilities. This improves maintainability and performance (through `pybase64`), while keeping the external API consistent for downstream consumers like examples and tests.

**Potential risks**  
The switch from `base64` to `pybase64` introduces a new dependency that must be properly managed. The refactored serialization functions now return different types (e.g., `list[float]` vs. `str`), which could cause subtle issues if callers rely on internal implementation details. Additionally, the consolidation of dtype mappings into `EMBED_DTYPES` requires careful validation to ensure all supported dtypes are correctly represented.

**Key insights**  
Developers should note that `EMBED_DTYPE_TO_TORCH_DTYPE` is replaced by `EMBED_DTYPES`, which provides richer dtype information via the `DTypeInfo` dataclass. The new `vllm/entrypoints/pooling/utils.py` module now houses entrypoint-specific serialization logic, making the codebase more modular. Ensure `pybase64` is included in the project dependencies.

---

## 39. [[Bugfix][Model] Fix DeepSeek-OCR-2 chat template to include BOS token](https://github.com/vllm-project/vllm/pull/33642)


### Base Information

- **PR Number:** #33642
- **Author:** [l4b4r4b4b4](https://github.com/l4b4r4b4b4)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-03 00:35:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33642/files) (1):**
  - `vllm/transformers_utils/configs/deepseek_vl2.py`

### Summary

**What changed and why**  
The PR fixes empty responses from DeepSeek-OCR-2 models by updating the model type detection logic. Previously, only `DeepseekOCRForCausalLM` was mapped to `model_type="deepseek_ocr"`. Now, `DeepseekOCR2ForCausalLM` is also recognized and mapped to `model_type="deepseek_ocr2"`, ensuring the correct chat template (which includes the BOS token) is selected.

**Technical impact**  
This change ensures that OCR-2 models use a chat template that includes the BOS token, which is required for proper inference. Without it, models produce no output via the `/v1/chat/completions` endpoint. The fix is minimal and only adjusts the model type mapping in the configuration.

**Potential risks**  
If other DeepSeek variants (e.g., future OCR-3 models) are introduced, they may not be covered by this mapping and could exhibit similar issues. Additionally, the reliance on architecture names in the config could break if upstream naming conventions change.

**Key insights**  
Always verify that model-specific tokens (like BOS) are included in chat templates for multimodal models. Consider creating a more extensible mapping strategy (e.g., using a registry) to handle future model variants without requiring code changes.

---

## 40. [Patch Protobuf for CVE 2026-0994](https://github.com/vllm-project/vllm/pull/33619)


### Base Information

- **PR Number:** #33619
- **Author:** [zaristei2](https://github.com/zaristei2)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-03 00:03:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33619/files) (2):**
  - `requirements/build.txt`
  - `requirements/common.txt`

### Summary

**What changed and why**  
Updated the protobuf dependency from an unconstrained version to `>= 6.33.5` in both `requirements/build.txt` and `requirements/common.txt` to address CVE-2026-0994, a security vulnerability in the protobuf library.

**Technical impact**  
This change enforces a minimum protobuf version of 6.33.5 across build and runtime environments, ensuring the security fix is applied. It may affect compatibility with other dependencies that require older protobuf versions, but the patch is necessary for vulnerability mitigation.

**Potential risks**  
If other dependencies in the project have strict upper bounds on protobuf versions below 6.33.5, this could lead to version conflicts. Additionally, the change might inadvertently break existing functionality if the new protobuf version introduces backward-incompatible changes.

**Key insights**  
Always verify that all downstream dependencies are compatible with protobuf >= 6.33.5. Consider adding a version ceiling if needed to prevent future breaking changes. Ensure CI/CD pipelines test the updated dependency matrix thoroughly.

---

## 41. [Patch aiohttp for CVE-2025-69223](https://github.com/vllm-project/vllm/pull/33621)


### Base Information

- **PR Number:** #33621
- **Author:** [zaristei2](https://github.com/zaristei2)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-03 00:02:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33621/files) (3):**
  - `requirements/common.txt`
  - `requirements/rocm-test.txt`
  - `requirements/test.txt`

### Summary

**What changed and why**  
The PR upgrades the aiohttp dependency from version 3.13.0 to version 3.13.3 across three requirement files to address CVE-2025-69223, a security vulnerability. This is a targeted patch to secure the 0.15.1 release.

**Technical impact**  
This change pins aiohttp to a newer, secure minor version. It ensures all environments (common, test, and ROCm-test) consistently use the patched version, preventing the vulnerability from being exploited in development, testing, or production deployments.

**Potential risks**  
The risk is minimal as this is a minor version patch within the same major release, suggesting backward-compatible security fixes. However, any subtle behavioral changes in aiohttp 3.13.3 could theoretically affect asynchronous HTTP operations, though this is unlikely.

**Key insights**  
This is a standard and necessary security update. Developers should ensure the change is propagated and locked in all deployment manifests. It's also a good practice to verify that the fix doesn't introduce any regressions in HTTP client or server functionality within the application.

---

