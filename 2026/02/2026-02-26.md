# vLLM Merged PR Report

**Report Date:** 2026-02-26 PST

**Total Merged PRs:** 52

---

## 1. [[Bugfix] Use 'sum' reduction instead of 'avg' in Async TP reduce-scatter](https://github.com/vllm-project/vllm/pull/33088)


### Base Information

- **PR Number:** #33088
- **Author:** [wangxingran222](https://github.com/wangxingran222)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-26 23:06:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33088/files) (1):**
  - `vllm/compilation/passes/fusion/collective_fusion.py`

### Summary

**What changed and why**  
Changed the reduction operation in Async TP reduce-scatter from "avg" to "sum" for Row Parallel Linear layers. This ensures mathematical correctness, as partial results from different tensor-parallel ranks should be summed, not averaged, to maintain output magnitude equivalence.

**Technical impact**  
This correction aligns the fused collective operation with the expected tensor-parallel behavior for row-wise partitioned linear layers. It affects all models using Async TP with fused matmul-reduce-scatter operations, potentially changing output scaling and gradient magnitudes during training or inference.

**Potential risks**  
The PR notes a ~3% accuracy loss on MMMU-Val with Qwen3-VL-32B and lower GSM8K performance with "sum" versus "avg". This suggests the previous "avg" may have inadvertently compensated for other issues (e.g., normalization, scaling). Multi-modal models with SP + Async TP have known issues that need separate fixes.

**Key insights**  
While "sum" is mathematically correct, the performance regression indicates possible underlying issues in model scaling or implementation. Developers should verify layer normalization and scaling factors in affected models. Consider adding a configurable reduction option if backward compatibility is needed, and prioritize fixing the known multi-modal model issues separately.

---

## 2. [[Model Performance] Add Qwen3MoE tuned MoE configs for H200](https://github.com/vllm-project/vllm/pull/35457)


### Base Information

- **PR Number:** #35457
- **Author:** [chengyinie](https://github.com/chengyinie)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 21:51:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35457/files) (2):**
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=96,device_name=NVIDIA_H200,dtype=fp8_w8a8.json`
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=96,device_name=NVIDIA_H200.json`

### Summary

**What changed and why**  
Two new JSON configuration files have been added to provide tuned Triton kernel parameters for the Qwen3MoE model (with E=128 experts, N=96 selected experts) on NVIDIA H200 GPUs with tensor parallelism (TP)=8. One config is for BF16 precision, and another for FP8 (fp8_w8a8). These tuned configs aim to replace default kernel settings to improve inference performance, addressing issue #34256.

**Technical impact**  
The configurations optimize kernel launch parameters (like BLOCK sizes, warps, and stages) for different batch sizes, which directly influences GPU utilization and memory access patterns. This tuning can significantly boost throughput for MoE layers, especially at larger batch sizes, and enables efficient use of H200's FP8 capabilities. The changes are additive and will be automatically selected by the MoE oracle when the specific model, device, and dtype conditions are met.

**Potential risks**  
The configs are tightly coupled to the specific hardware (H200), model architecture (E=128, N=96), and TP setting (8). Performance gains may not generalize to other GPU models, different tensor parallelism degrees, or other MoE configurations. There is also a risk of regression if the tuning was not validated across all possible operational scenarios or if future Triton version changes affect kernel behavior.

**Key insights**  
These additions demonstrate the importance of hardware-specific and precision-specific tuning for achieving optimal MoE performance. Developers should ensure similar tuning is performed for other GPU architectures and model variants. Consider automating the discovery and validation of such configs to maintain performance as models and hardware evolve.

---

## 3. [[Bug] correct out dtype of rms_norm_gated native path](https://github.com/vllm-project/vllm/pull/35369)


### Base Information

- **PR Number:** #35369
- **Author:** [zufangzhu](https://github.com/zufangzhu)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-26 21:19:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35369/files) (1):**
  - `vllm/model_executor/layers/layernorm.py`

### Summary

**What changed and why**  
The fix addresses a dtype inconsistency in RMSNormGated's native forward path where the output tensor could inadvertently become float32 due to Python's default float type (1e-5) promoting computation. The change ensures the output is cast back to the input activation's dtype before returning, maintaining consistency with RMSNorm-style operations.

**Technical impact**  
This correction ensures that the native implementation's output dtype matches the input tensor's dtype, aligning with the expected behavior of normalization layers. It prevents potential dtype mismatches that could affect downstream operations or cause unexpected type promotions in mixed-precision training scenarios.

**Potential risks**  
If the input tensor `x` is already in float32, the `.to(x.dtype)` call is redundant but harmless. However, there's a minor performance overhead from the extra casting operation. No functional regression is expected as this aligns the native path with the CUDA implementation's behavior.

**Key insights**  
Always ensure normalization operations preserve input dtypes, especially when using Python literals that can trigger implicit type promotion. Consider making `eps` a tensor with appropriate dtype to avoid promotion entirely. This fix is minimal and correct, but reviewing other similar operations in the codebase for the same issue is recommended.

---

## 4. [[Bugfix] disable allreduce_rms_fusion by default when pp size > 1](https://github.com/vllm-project/vllm/pull/35424)


### Base Information

- **PR Number:** #35424
- **Author:** [ZJY0516](https://github.com/ZJY0516)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-26 20:18:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35424/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The PR disables the `allreduce_rms_fusion` optimization by default when pipeline parallelism (PP) size is greater than 1. This addresses a bug where flashinfer incorrectly maps tensor parallelism (TP) ranks to GPU device indices in PP scenarios, causing NCCL "Duplicate GPU detected" errors.

**Technical impact**  
This change prevents the fusion optimization from being applied in TP+PP configurations, avoiding incorrect device mapping that breaks inter-process communication. The system will fall back to a non-fused allreduce path for RMSNorm operations in pipeline-parallel setups.

**Potential risks**  
Disabling this optimization may slightly reduce performance in TP+PP configurations due to increased communication overhead. There's also a risk that similar device mapping issues could exist in other fused operations or when using different parallelism combinations.

**Key insights**  
The fix is a conservative safety measure that prioritizes correctness over performance for edge-case configurations. Developers should be aware that TP+PP deployments won't benefit from this particular optimization until the underlying flashinfer issue is resolved. Consider adding a warning when this optimization is automatically disabled.

---

## 5. [[BugFix] Repo utils debug print patch](https://github.com/vllm-project/vllm/pull/35434)


### Base Information

- **PR Number:** #35434
- **Author:** [pi314ever](https://github.com/pi314ever)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-26 19:50:56
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35434/files) (1):**
  - `vllm/transformers_utils/repo_utils.py`

### Summary

**What changed and why**  
The change fixes a logging bug where a debug statement would crash when a file doesn't exist on Hugging Face Hub. The original `logger.debug` call incorrectly passed an exception object as a second argument instead of using proper string formatting or `exc_info`. The fix replaces `logger.debug("...", e)` with `logger.debug("...:", exc_info=e)` to correctly log the exception details.

**Technical impact**  
This ensures that when `VLLM_LOGGING_LEVEL=debug` is set and a file is missing, the debug log will output the exception traceback without causing a `TypeError`. The logging behavior now matches the expected output format, preserving system stability during error conditions.

**Potential risks**  
The risk is minimal since this only affects debug logging. However, if other similar logging calls exist in the codebase, they might have the same bug and could cause unexpected crashes when debug logging is enabled. The change does not alter any functional logic beyond logging.

**Key insights**  
Always use `exc_info=e` when logging exceptions with Python's logging module to avoid formatting errors. Review other logging statements in the codebase for similar patterns to prevent hidden crashes. This fix highlights the importance of proper exception logging in debug paths.

---

## 6. [[Bug] Fix outdated links in source code](https://github.com/vllm-project/vllm/pull/35314)


### Base Information

- **PR Number:** #35314
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-02-26 19:50:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35314/files) (5):**
  - `.github/mergify.yml`
  - `docs/design/metrics.md`
  - `tools/profiler/print_layerwise_table.py`
  - `tools/profiler/visualize_layerwise_profile.py`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
This PR fixes outdated file paths and documentation links across the codebase. The changes update references to moved or renamed files, such as consolidating structured output examples and correcting the path to an OpenTelemetry README file. The purpose is to ensure documentation, configuration, and error messages point to valid, current resources.

**Technical impact**  
These changes are non-functional and primarily affect developer experience and maintainability. They correct broken links in documentation, error messages, and CI configuration (Mergify), preventing confusion and ensuring automated workflows reference the correct files. The updates to help text in profiling tools improve clarity but do not alter their runtime behavior.

**Potential risks**  
The risk is minimal as these are documentation and configuration updates. However, if the new file paths are incorrect or the referenced files are moved again, it could reintroduce broken links. The Mergify rule change could affect which pull requests trigger specific CI checks if the file pattern is not correctly matched.

**Key insights**  
This is a straightforward hygiene PR that improves codebase health. Developers should verify the new paths are correct and consider adding automated link checking to prevent similar issues. When moving files, it's crucial to update all references systematically, as demonstrated here.

---

## 7. [use 'max_active_experts' for moe lora input size](https://github.com/vllm-project/vllm/pull/33197)


### Base Information

- **PR Number:** #33197
- **Author:** [gnovack](https://github.com/gnovack)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-26 19:50:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33197/files) (2):**
  - `tests/lora/test_moe_lora_align_sum.py`
  - `vllm/lora/punica_wrapper/punica_gpu.py`

### Summary

**What changed and why**  
The PR modifies the fused MoE LoRA kernel to compute the input size parameter `M` (number of blocks) based on `max_active_experts` instead of the total `num_experts`. This addresses inefficiencies at low batch sizes where not all experts can be activated, improving performance for decoding passes and small batches.

**Technical impact**  
The change reduces the allocated memory and computational overhead in the kernel by limiting the padded token count to `topk_ids.numel() * block_size` when the active tokens are fewer than the total experts. This optimizes GPU resource usage and accelerates inference, particularly in low-concurrency scenarios.

**Potential risks**  
If the condition `topk_ids.numel() < num_experts` is incorrectly evaluated (e.g., due to tensor shape mismatches), it could lead to under-allocation of memory. Additionally, the change assumes that the maximum active experts never exceed the total experts, which should hold but warrants validation in edge cases like dynamic expert routing.

**Key insights**  
The fix yields a measurable performance gain (~10% faster token generation in benchmarks) by aligning kernel resource allocation with actual runtime requirements. Developers should ensure that the `topk_ids` tensor accurately represents activated experts and consider applying similar optimizations to other MoE-related kernels.

---

## 8. [[Bugfix] Fix Qwen3NextForCausalLM packed_modules_mapping](https://github.com/vllm-project/vllm/pull/35413)


### Base Information

- **PR Number:** #35413
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-26 19:46:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35413/files) (1):**
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
Added two new entries to the `packed_modules_mapping` dictionary for the Qwen3NextForCausalLM model. The entries map the keys `"in_proj_qkvz"` and `"in_proj_ba"` to their respective module lists, ensuring proper handling of these packed linear modules during operations like quantization or weight loading.

**Technical impact**  
This change ensures that the model's packed linear modules (`in_proj_qkvz` and `in_proj_ba`) are correctly identified and processed by the framework's internal mechanisms. Without these mappings, operations that rely on `packed_modules_mapping` (such as certain quantization techniques or weight initialization) might fail or behave incorrectly for these specific modules.

**Potential risks**  
If the module names in the mapping do not exactly match the actual attribute names defined in the model's layers, it could lead to runtime errors or silent failures during model loading or inference. Additionally, any future refactoring that changes these module names would need to update this mapping accordingly.

**Key insights**  
The change is minimal and focused, directly addressing a missing configuration. Developers should verify that the module names used in the mapping (`["in_proj_qkvz"]` and `["in_proj_ba"]`) are consistent with the actual class attributes throughout the codebase. This fix is likely necessary for full compatibility with the intended model architecture.

---

## 9. [[Misc] Move `GPUModelRunner.prepare_kernel_block_sizes` to utils](https://github.com/vllm-project/vllm/pull/35400)


### Base Information

- **PR Number:** #35400
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 19:42:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35400/files) (3):**
  - `tests/v1/worker/test_gpu_model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
The `prepare_kernel_block_sizes` method and its helper `select_common_block_size` were moved from `GPUModelRunner` to `utils.py`. This refactoring centralizes the logic for calculating kernel block sizes from KV cache configurations, making it reusable outside the model runner and keeping the runner leaner.

**Technical impact**  
The change reduces coupling by extracting stateless utility functions, improving code organization. The model runner now delegates block size calculation to the imported `prepare_kernel_block_sizes` function, while external components can directly use this utility without iterating through physical tensors or instantiating a runner.

**Potential risks**  
The function signature changed slightly: `attn_groups` is now passed as a parameter instead of accessed via `self.attn_groups`. This could introduce errors if callers pass incorrect group indexing. Additionally, any future state-dependent logic added to the original method would break after the move.

**Key insights**  
This is a clean separation of concerns that enhances modularity. Developers should ensure that any new use cases pass `attn_groups` correctly indexed by KV cache group ID. The move also simplifies testing, as demonstrated by the updated test imports.

---

## 10. [[Core]Extract is_last_rank in Ray for tpu to override](https://github.com/vllm-project/vllm/pull/33012)


### Base Information

- **PR Number:** #33012
- **Author:** [Chenyaaang](https://github.com/Chenyaaang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 19:18:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33012/files) (1):**
  - `vllm/v1/executor/ray_utils.py`

### Summary

**What changed and why**  
The change extracts the `get_pp_group().is_last_rank` check into a new method `_is_last_rank()` within the `RayGPUExecutor` class. This refactoring allows TPU-specific implementations to override the method, enabling different logic for determining the last rank in a pipeline parallel group on TPU hardware.

**Technical impact**  
This modification introduces a layer of abstraction for rank detection in pipeline-parallel execution. The core Ray GPU logic remains unchanged, but the codebase now supports future TPU adaptations without modifying the existing GPU execution path, promoting better separation of concerns.

**Potential risks**  
If TPU overrides are not implemented correctly, it could lead to incorrect rank detection in pipeline-parallel setups, potentially causing silent errors in request handling or tensor synchronization. The assertion on line 111 assumes non-last ranks produce no output, which must hold true for any override.

**Key insights**  
This is a clean refactoring that follows the open-closed principle, making the code extensible for new hardware. Developers should ensure any `_is_last_rank()` override maintains the invariant that only the last rank returns meaningful model outputs to avoid breaking pipeline parallelism.

---

## 11. [[compile] Invalidate cache for cpu flags](https://github.com/vllm-project/vllm/pull/35119)


### Base Information

- **PR Number:** #35119
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-26 18:54:12
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35119/files) (1):**
  - `vllm/envs.py`

### Summary

**What changed and why**  
Removed `VLLM_CPU_OMP_THREADS_BIND`, `VLLM_CPU_NUM_OF_RESERVED_CPU`, and `VLLM_CPU_SGL_KERNEL` from the `ignored_factors` set in the compilation cache key generation. These environment variables affect OpenMP thread configuration for CPU kernels, and their previous exclusion could cause incorrect cached kernels to be reused when thread counts change.

**Technical impact**  
The compilation cache key will now properly account for changes in these CPU threading environment variables, ensuring that inductor-compiled kernels are recompiled when thread affinity or count settings change. This prevents performance degradation or incorrect behavior from using kernels compiled for different thread configurations.

**Potential risks**  
If these variables change frequently during normal operation, it could increase compilation overhead due to more frequent cache misses. There's also a risk that other CPU-related environment variables not yet removed from `ignored_factors` could cause similar issues if they affect kernel compilation.

**Key insights**  
This fix addresses a subtle cache invalidation bug where thread configuration changes weren't triggering recompilation. Developers should be aware that any environment variable affecting kernel behavior should be excluded from `ignored_factors`. Consider auditing other variables in `ignored_factors` for similar issues with CPU/GPU kernel compilation dependencies.

---

## 12. [[Bugfix] Emit reasoning_part events in simple streaming path for Resp…](https://github.com/vllm-project/vllm/pull/35184)


### Base Information

- **PR Number:** #35184
- **Author:** [daniel-salib](https://github.com/daniel-salib)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-26 17:49:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35184/files) (2):**
  - `tests/entrypoints/openai/responses/test_simple.py`
  - `vllm/entrypoints/openai/responses/serving.py`

### Summary

**What changed and why**  
Fixed incorrect Server-Sent Events (SSE) emission in the simple streaming path for reasoning-enabled models. The code now correctly emits `ResponseReasoningPartAddedEvent` instead of `ResponseContentPartAddedEvent` for reasoning content and adds the previously missing `ResponseReasoningPartDoneEvent` after reasoning completion. This aligns the simple streaming path with the harmony streaming path's behavior.

**Technical impact**  
These changes ensure consistent SSE event sequences across different streaming implementations, enabling clients to properly parse reasoning and content segments. The fix affects all non-GPT-OSS models using `--reasoning-parser` (e.g., Qwen3, DeepSeek-R1, Llama) by providing the expected event structure for reasoning-aware applications.

**Potential risks**  
If clients were relying on the previous incorrect event types (`content_part.added` for reasoning), they may break after this fix. Additionally, the event sequence now includes extra `reasoning_part.done` events, which could affect client-side event counters or state machines that weren't expecting them.

**Key insights**  
Developers should verify that downstream consumers of these SSE streams are updated to handle the corrected event types and sequences. The added test provides a validation pattern for future changes. Consider documenting the expected event sequences for both streaming paths to prevent similar inconsistencies.

---

## 13. [[CI] Actually run tests/kernels/quantization/test_block_fp8.py in CI](https://github.com/vllm-project/vllm/pull/34274)


### Base Information

- **PR Number:** #34274
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-26 16:58:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34274/files) (3):**
  - `.buildkite/test_areas/kernels.yaml`
  - `tests/kernels/quantization/test_block_fp8.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
The PR enables full execution of the `test_block_fp8.py` test suite in CI by removing the `-k deep_gemm` filter, allowing all tests in the file to run. It also updates test configurations and fixes a kernel compatibility issue by removing Hopper-specific scale transposition logic that is no longer needed.

**Technical impact**  
This change improves test coverage for FP8 quantization kernels by ensuring all test cases run in CI, not just the DeepGEMM subset. The kernel update aligns the test with recent changes to the `cutlass_scaled_mm` function, which now handles scale formats consistently across platforms.

**Potential risks**  
The removal of platform-specific scale transposition (`Bs_cutlass`) could cause failures if the underlying `cutlass_scaled_mm` function still has different format requirements for Hopper (SM90) versus other architectures. The expanded test execution may increase CI runtime and potentially expose previously hidden failures in non-DeepGEMM test paths.

**Key insights**  
The test configuration updates (removing M=84 and N=7748) likely reflect refined kernel constraints or performance characteristics. Developers should verify that the `cutlass_scaled_mm` function indeed has uniform scale format requirements across all supported GPU architectures. The typo fix in the flashinfer.py comment demonstrates good attention to documentation consistency.

---

## 14. [[Performance] Cublas Bf16 Gate with Fp32 Output](https://github.com/vllm-project/vllm/pull/35121)


### Base Information

- **PR Number:** #35121
- **Author:** [roikoren755](https://github.com/roikoren755)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-26 16:51:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35121/files) (9):**
  - `CMakeLists.txt`
  - `csrc/moe/moe_ops.h`
  - `csrc/moe/router_gemm.cu`
  - `csrc/moe/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/router/gate_linear.py`
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
This PR introduces `GateLinear`, a specialized MoE gate linear layer with three-tier GEMM dispatch for router logits: DSV3 kernel (Tier 1) for small batches on SM90+, cuBLAS bf16×bf16→fp32 GEMM (Tier 2) for broader SM90+ support, and a fallback to `F.linear` (Tier 3). It also adds a `force_fp32_compute` parameter for models like NemotronH that require fp32 compute precision. The changes migrate DeepSeek-V2 and NemotronH to use `GateLinear`, removing custom gate implementations.

**Technical impact**  
The new `GateLinear` class centralizes router GEMM optimizations, improving performance on Hopper/Blackwell GPUs (SM90+) with measured speedups of 1.5–3.9x. It maintains backward compatibility via fallback paths and allows dynamic `out_dtype` setting. The architecture now separates kernel dispatch logic from model-specific code, simplifying maintenance.

**Potential risks**  
The specialized kernels are limited to SM90+ GPUs without bias; older architectures (e.g., SM80) may see performance regressions in some workloads. The `force_fp32_compute` flag increases memory usage when storing weights in fp32. Edge cases like large batch sizes (>16) bypass the fastest DSV3 kernel, and incorrect `out_dtype` setting could cause dtype mismatches.

**Key insights**  
Developers should ensure `set_out_dtype()` is called before forward passes if the output dtype depends on runtime conditions. For non-SM90+ hardware, consider future optimizations to avoid performance cliffs. The `GateLinear` design is extensible for adding new kernels, and its use should be preferred over custom gate implementations for new MoE models.

---

## 15. [[Update] Use FlashInfer fast_decode_plan directly instead of replication](https://github.com/vllm-project/vllm/pull/34687)


### Base Information

- **PR Number:** #34687
- **Author:** [askliar](https://github.com/askliar)
- **Merged By:** [pavanimajety](https://github.com/pavanimajety)
- **Merged time:** 2026-02-26 16:31:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34687/files) (2):**
  - `tests/kernels/attention/test_flashinfer.py`
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
This PR updates FlashInfer integration to use `fast_decode_plan` directly instead of replicating its logic, and migrates from positional to keyword arguments for API calls. These changes address breakages caused by FlashInfer 0.6.0 introducing new required parameters (like `o_data_type`), ensuring forward compatibility.

**Technical impact**  
The changes centralize planning logic within FlashInfer's native `fast_decode_plan` function, reducing code duplication and aligning vLLM's implementation with upstream. Keyword arguments provide resilience to future API changes, and the updated test suite validates both warmup and fast-path behavior.

**Potential risks**  
Removing the `seq_lens_cpu` parameter from `fast_plan_decode` could affect callers if not updated. The reliance on FlashInfer's internal `fast_decode_plan` may introduce version coupling, though keyword arguments mitigate this. Edge cases around buffer sizes or mismatched batch sizes in CUDA graph mode require careful validation.

**Key insights**  
Developers should ensure all calls to `fast_plan_decode` omit the `seq_lens_cpu` argument. The keyword-argument pattern should be adopted for other FlashInfer wrapper calls to prevent similar breakages. The enhanced test coverage provides a template for validating future FlashInfer updates.

---

## 16. [[Bugfix] Fix KV Scale loading for MLA Models](https://github.com/vllm-project/vllm/pull/35430)


### Base Information

- **PR Number:** #35430
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-26 15:38:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35430/files) (1):**
  - `vllm/model_executor/layers/quantization/modelopt.py`

### Summary

**What changed and why**  
The PR fixes KV scale loading for MLA models by updating the type check in `get_quant_method` to include `MLAAttention` alongside `Attention`. This ensures that after `MLAAttention` was moved to its own base class, the quantization method for KV cache is correctly identified and applied.

**Technical impact**  
The change ensures that MLA models (like Deepseek R1 FP4) properly load KV scales during quantization, preventing warnings or errors. It maintains backward compatibility for existing attention layers while extending support to the new `MLAAttention` class.

**Potential risks**  
If other attention subclasses are introduced in the future, they may also require explicit inclusion in this check. Additionally, any unintended side effects from the broader `isinstance` check should be monitored, though the risk is low given the focused change.

**Key insights**  
Always verify type checks when refactoring classes into separate hierarchies. Consider using a common base class or registry pattern for attention layers to avoid manual updates. Testing with specific models (as done here) is crucial for validating quantization behavior.

---

## 17. [[ROCm][Quantization] GPT OSS Upstream MoE wmxfp4_afp8 with static scales](https://github.com/vllm-project/vllm/pull/30357)


### Base Information

- **PR Number:** #30357
- **Author:** [maleksan85](https://github.com/maleksan85)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-26 14:50:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30357/files) (3):**
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`

### Summary

**What changed and why**  
This PR adds support for GPT OSS models with mixed MXFP4 weight and AFP8 activation quantization using static scales in ROCm environments. The changes introduce a new Triton kernel path for fused MoE operations when specific quantization conditions are met, leveraging ROCm's aiter ops for optimized routing and computation.

**Technical impact**  
The modifications create a specialized execution path in the fused MoE layer for GPT OSS models with MXFP4/AFP8 quantization, bypassing the default kernel when ROCm aiter is enabled. This introduces a new `QuarkOCP_MX_MoEMethod_OSS` class that handles weight swizzling and static scale management, improving performance for this specific quantization scheme on compatible hardware.

**Potential risks**  
The new code path adds complexity and could lead to maintenance overhead. There's a risk of regression for non-ROCm platforms or different quantization configurations. The static scale approach assumes uniform scales across experts, which may not hold for all models. Memory management changes (deleting original weights) could cause issues if not handled carefully across all execution paths.

**Key insights**  
Developers should note this is a hardware-specific optimization for ROCm with aiter support. The changes are mostly additive but introduce new conditional logic that must be tested across different platforms. The static scale quantization requires careful validation of model compatibility. The PR demonstrates a pattern for integrating custom Triton kernels with existing quantization infrastructure.

---

## 18. [[Kernel][perf] optimize NCCL symm_mem vs custom_AR selection thresholds](https://github.com/vllm-project/vllm/pull/33839)


### Base Information

- **PR Number:** #33839
- **Author:** [pkousha](https://github.com/pkousha)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-26 14:35:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33839/files) (1):**
  - `vllm/distributed/device_communicators/all_reduce_utils.py`

### Summary

**What changed and why**  
The PR replaces a simple size threshold for NCCL symmetric memory allreduce selection with a range-based approach. Previously, NCCL symm_mem was only used for large tensors (≥2MB for 4 GPUs, ≥1MB for 8 GPUs). Benchmark data revealed that symm_mem also outperforms custom allreduce for small tensors (≤16KB), so the new logic enables symm_mem for both small and large sizes while reserving mid-range sizes for custom allreduce.

**Technical impact**  
This change optimizes allreduce performance across tensor sizes by dynamically selecting the best communicator based on empirical benchmarks. The decision logic now uses a configurable "custom_ar_preferred_ranges" dictionary, which improves latency for small tensors and bandwidth for large tensors, while maintaining custom allreduce's advantage in mid-range sizes where its P2P approach has lower overhead.

**Potential risks**  
The new range boundaries are hardware-specific (H100/GB200 benchmarks) and may not generalize to other GPU architectures or network topologies. If the symmetric memory allocator is disabled or fails, the fallback behavior remains unchanged, but any misconfiguration in the range thresholds could lead to suboptimal performance for edge-case tensor sizes near the boundaries.

**Key insights**  
Developers should validate these thresholds on their specific hardware, as performance characteristics may vary. The config is now more flexible—adding new world sizes or adjusting ranges only requires updating the dictionary. Monitoring real-world performance is recommended to ensure the ranges remain optimal across diverse workloads.

---

## 19. [[WideEP] Remove pplx all2all backend](https://github.com/vllm-project/vllm/pull/33724)


### Base Information

- **PR Number:** #33724
- **Author:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-26 14:30:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33724/files) (39):**
  - `.buildkite/test_areas/kernels.yaml`
  - `CMakeLists.txt`
  - `csrc/ops.h`
  - `csrc/quantization/w8a8/cutlass/moe/moe_data.cu`
  - `csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu`
  - `csrc/torch_bindings.cpp`
  - `docker/Dockerfile`
  - `docker/versions.json`
  - `docs/design/fused_moe_modular_kernel.md`
  - `docs/design/moe_kernel_features.md`
  - `docs/governance/committers.md`
  - `docs/serving/expert_parallel_deployment.md`
  - `examples/online_serving/elastic_ep/serve_deepseek_v2.sh`
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/modular_kernel_tools/mk_objects.py`
  - `tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py`
  - `tests/kernels/moe/test_modular_kernel_combinations.py`
  - `tests/kernels/moe/test_pplx_cutlass_moe.py`
  - `tests/kernels/moe/test_pplx_moe.py`
  - `tools/ep_kernels/README.md`
  - `tools/ep_kernels/elastic_ep/install_eep_libraries.sh`
  - `tools/ep_kernels/install_python_libraries.sh`
  - `vllm/_custom_ops.py`
  - `vllm/config/compilation.py`
  - `vllm/config/parallel.py`
  - `vllm/distributed/device_communicators/all2all.py`
  - `vllm/distributed/device_communicators/cuda_communicator.py`
  - `vllm/distributed/eplb/eplb_state.py`
  - `vllm/model_executor/layers/fused_moe/all2all_utils.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`
  - `vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/utils/import_utils.py`

### Summary

**What changed and why**  
This PR removes the PPLX all2all backend and its associated kernels from the codebase. The PPLX kernels have been superseded by the newer pplx-garden project, which has a different interface and is not a drop-in replacement. Removing this backend reduces complexity in the MoE layer.

**Technical impact**  
The removal eliminates the `pplx` all2all backend option, along with all related kernel implementations, tests, documentation, and build dependencies. The function `get_cutlass_pplx_moe_mm_data` has been renamed to `get_cutlass_batched_moe_mm_data` to reflect its more generic batched use case. Any configuration attempting to use the `pplx` backend will now fall back to `allgather_reducescatter` with a warning.

**Potential risks**  
Users who were explicitly using the `pplx` backend will experience a silent fallback to `allgather_reducescatter`, which may affect performance or behavior in multi-node deployments. The removal of tests and documentation could make it harder to understand historical context or debug issues related to batched MoE kernels. There is also a risk of breaking any downstream scripts or configurations that hardcoded the `pplx` backend.

**Key insights**  
Developers should update any deployment scripts or configurations that specify `--all2all-backend pplx` to use an alternative backend like `deepep_low_latency` or `allgather_reducescatter`. The renamed `get_cutlass_batched_moe_mm_data` function now serves a broader batched MoE use case, so ensure any custom kernels using it are updated accordingly. The cleanup significantly reduces maintenance burden but requires attention to user-facing changes.

---

## 20. [[Bugfix] Fix MessageQueue connect_ip for cross-node data parallelism](https://github.com/vllm-project/vllm/pull/35429)


### Base Information

- **PR Number:** #35429
- **Author:** [luccafong](https://github.com/luccafong)
- **Merged By:** [luccafong](https://github.com/luccafong)
- **Merged time:** 2026-02-26 14:08:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35429/files) (2):**
  - `tests/distributed/test_mq_connect_ip.py`
  - `vllm/v1/executor/multiproc_executor.py`

### Summary

**What changed and why**  
The fix addresses a socket binding error in cross-node data parallelism where each data-parallel (DP) group leader's `MessageQueue` incorrectly used the master node's IP (`master_addr`) for ZMQ socket binding. This caused `ZMQError: Cannot assign requested address` for DP leaders on non-master nodes. The solution replaces `master_addr` with the local node's own IP (`get_ip()`) for the `connect_ip` parameter, ensuring each leader binds to its local interface.

**Technical impact**  
This change corrects the socket binding logic for ZMQ communication in distributed setups, enabling proper multi-node data parallelism. The `master_addr` is now reserved only for torch.distributed rendezvous, while `MessageQueue` uses local IPs, aligning with ZMQ's requirement that sockets bind to local interfaces. The added logging helps debug DP group configurations.

**Potential risks**  
If `get_ip()` returns an incorrect or unreachable IP (e.g., due to misconfigured network interfaces), binding may still fail or cause communication issues. The unit tests cover local and non-local IP scenarios but rely on a hardcoded TEST-NET-2 address; environments with restrictive firewalls or unusual network setups might exhibit different failure modes.

**Key insights**  
Always bind ZMQ sockets to local IPs, not remote addresses. The fix is minimal and focused, but developers should verify network configuration in production deployments. The added unit tests provide a robust validation framework for similar binding logic, and the logging enhancement aids in diagnosing DP group leadership during distributed execution.

---

## 21. [add mixed precision support for modelopt](https://github.com/vllm-project/vllm/pull/35047)


### Base Information

- **PR Number:** #35047
- **Author:** [sychen52](https://github.com/sychen52)
- **Merged By:** [pavanimajety](https://github.com/pavanimajety)
- **Merged time:** 2026-02-26 13:56:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35047/files) (4):**
  - `vllm/config/model.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
Added support for ModelOpt mixed precision quantization, enabling models to use different quantization algorithms (e.g., FP8 and NVFP4) per layer. This involves registering a new `modelopt_mixed` quantization method, creating a `ModelOptMixedPrecisionConfig` class to handle per-layer quantization settings, and updating configuration loading logic to parse mixed precision checkpoints.

**Technical impact**  
The changes extend the quantization framework to support heterogeneous quantization strategies within a single model. The new config class delegates layer-specific quantization to existing FP8 and NVFP4 implementations, while the updated weight loader logic ensures compatibility with both new (embedded in `config.json`) and legacy (`hf_quant_config.json`) checkpoint formats. This maintains backward compatibility and integrates seamlessly with the existing ModelOpt quantization infrastructure.

**Potential risks**  
- If the `quantized_layers` mapping is missing or malformed in the checkpoint, model loading may fail or produce incorrect quantization.  
- The fallback logic for loading `quantized_layers` from files could introduce subtle bugs if file paths or naming conventions change.  
- Mixing quantization algorithms may lead to unexpected performance variations if layer-specific configurations are not validated for hardware compatibility.

**Key insights**  
- The implementation cleverly reuses existing FP8 and NVFP4 configs, minimizing code duplication.  
- Developers should ensure that mixed precision checkpoints include a complete `quantized_layers` map and that all referenced algorithms are supported.  
- Testing should verify that per-layer quantization is correctly applied, especially for MoE models where different experts may use different algorithms.

---

## 22. [Nemotron: use per-layer config in NemotronHMLPDecoderLayer for heterogeneous models](https://github.com/vllm-project/vllm/pull/35396)


### Base Information

- **PR Number:** #35396
- **Author:** [danielafrimi](https://github.com/danielafrimi)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-26 13:55:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35396/files) (1):**
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
The change adds logic to retrieve a per-layer configuration for heterogeneous Nemotron models. Instead of always using the global model config, the decoder layer now attempts to fetch a layer-specific config via a `get_nemotron_h_config_for_layer` method if it exists, falling back to the global config otherwise.

**Technical impact**  
This enables support for heterogeneous architectures where different layers may have distinct configurations (e.g., varying intermediate sizes). The change localizes configuration handling to each decoder layer, making the model more flexible and adaptable to mixed-layer designs without altering the overall model structure.

**Potential risks**  
If the `get_nemotron_h_config_for_layer` method is implemented incorrectly or returns an incompatible config object, it could lead to runtime errors or misconfigured layers. Additionally, the fallback to the global config must ensure backward compatibility with homogeneous models that lack per-layer configurations.

**Key insights**  
Developers should verify that any heterogeneous model using this feature provides a correctly implemented `get_nemotron_h_config_for_layer` method. Testing should cover both heterogeneous and homogeneous model paths to ensure seamless operation. This change is a strategic enhancement for supporting advanced model architectures.

---

## 23. [[Performance] Extract KV cache update op from flashinfer forward](https://github.com/vllm-project/vllm/pull/35422)


### Base Information

- **PR Number:** #35422
- **Author:** [ElizaWszola](https://github.com/ElizaWszola)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-26 13:29:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35422/files) (1):**
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
The PR extracts KV cache update operations from the FlashInfer attention forward pass into a separate `do_kv_cache_update` method, similar to a previous refactoring. This change is part of a broader performance optimization effort to modularize cache management and potentially enable more flexible execution patterns.

**Technical impact**  
This refactoring decouples KV cache updates from the main attention computation, aligning the FlashInfer backend with other attention backends. It introduces a `forward_includes_kv_cache_update` flag (set to `False`) to indicate that cache updates are now handled separately, which may allow for future optimizations like overlapping computation and I/O.

**Potential risks**  
The conditional logic for FP8 dtype conversion is now executed only when `kv_sharing_target_layer_name` is `None` and the cache uses FP8, which could introduce subtle behavioral differences if the cache update path changes. Additionally, moving the cache update to a separate method may affect performance if not properly integrated with the scheduler or if there are hidden dependencies on the original execution order.

**Key insights**  
Developers should ensure that all callers of the FlashInfer backend now invoke `do_kv_cache_update` appropriately after the forward pass. The refactoring maintains functional equivalence (as shown by unchanged evaluation metrics) but requires careful validation in distributed or edge-case scenarios to avoid cache corruption or missed updates.

---

## 24. [fix(reasoning): Qwen3ReasoningParser returns truncated output as reasoning](https://github.com/vllm-project/vllm/pull/35230)


### Base Information

- **PR Number:** #35230
- **Author:** [stakeswky](https://github.com/stakeswky)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-26 12:30:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35230/files) (2):**
  - `tests/reasoning/test_qwen3_reasoning_parser.py`
  - `vllm/reasoning/qwen3_reasoning_parser.py`

### Summary

**What changed and why**  
The fix addresses a bug where `Qwen3ReasoningParser.extract_reasoning()` incorrectly returned `(None, model_output)` when the model output was truncated (missing `</think>` token). Since Qwen3 defaults to thinking enabled, a missing `</think>` indicates truncation mid-thought, so the correct return is `(model_output, None)`. The parser now reads the `enable_thinking` flag from `chat_template_kwargs` to disambiguate between truncated reasoning (thinking enabled) and no reasoning (thinking disabled).

**Technical impact**  
This change aligns `Qwen3ReasoningParser` with the base class `BaseThinkingReasoningParser` behavior for truncated outputs, ensuring consistency across reasoning parsers. It also introduces proper handling of the `enable_thinking` flag, matching the pattern used in `DeepSeekV3ReasoningParser`. The fix affects how reasoning and content are extracted in both streaming and non-streaming scenarios, particularly for truncated outputs.

**Potential risks**  
If `chat_template_kwargs` is not properly passed during parser initialization, `self.thinking_enabled` may default to `True`, potentially misclassifying outputs without `</think>` as truncated reasoning when thinking was actually disabled. Additionally, the change assumes that missing `</think>` always indicates truncation when thinking is enabled, which could be ambiguous if the model occasionally omits the end token without truncation.

**Key insights**  
Always pass `chat_template_kwargs` with `enable_thinking` explicitly set when initializing the parser to avoid default behavior mismatches. The updated test suite now covers edge cases like Qwen3.5-style truncation and thinking-disabled scenarios, providing robust validation. Developers should verify that their use cases align with the new truncation logic, especially in streaming contexts where detection is more complex.

---

## 25. [[Model Runner V2] Prepare attn metadata in ModelState [2/N]](https://github.com/vllm-project/vllm/pull/35383)


### Base Information

- **PR Number:** #35383
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-26 11:47:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35383/files) (4):**
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/model_states.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/speculator.py`

### Summary

**What changed and why**  
This PR moves attention metadata preparation from `InputBatch` to `ModelState` as part of a refactoring effort (Model Runner V2). The `InputBatch` dataclass no longer stores `attn_metadata` and `slot_mappings`, and the logic for building these structures is relocated to `ModelState.prepare_attn()`. This centralizes attention-related computations within the model state.

**Technical impact**  
The changes decouple attention metadata from the input batch, shifting responsibility to the model state. This simplifies `InputBatch` and aligns with a clearer separation of concerns—input batching focuses on token/data preparation, while model state handles attention-specific computations. The `execute_model` method now calls `prepare_attn` conditionally, and dummy runs use a separate `prepare_dummy_attn` helper.

**Potential risks**  
Conditional logic in `execute_model` (e.g., `if not (dummy_run and skip_attn_for_dummy_run)`) could lead to edge cases where `attn_metadata` or `slot_mappings` are unexpectedly `None`. The `speculator.py` changes assume callers now pass `attn_metadata` and `slot_mappings` explicitly, which may break if not updated elsewhere. There’s also a risk of inconsistent state if `prepare_attn` is called with mismatched parameters.

**Key insights**  
This refactoring improves modularity but requires careful coordination between components. Developers should ensure all callers of `InputBatch` (like speculative decoding) are updated to use the new `attn_metadata` and `slot_mappings` parameters. The conditional preparation logic should be reviewed for robustness, especially in dummy-run scenarios. Future changes should maintain this separation of concerns.

---

## 26. [[Model Runner V2] Add model states [1/N]](https://github.com/vllm-project/vllm/pull/35350)


### Base Information

- **PR Number:** #35350
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-26 11:20:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35350/files) (4):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/model_states.py`

### Summary

**What changed and why**  
This PR introduces a `ModelState` class to centralize model-specific state management, particularly for M-RoPE (Multi-Head Rotary Position Embedding) functionality. It removes scattered M-RoPE logic from `CudaGraphManager` and `ModelRunner`, consolidating it into the new class. The changes also simplify method signatures by replacing individual tensor parameters with a unified `model_inputs` dictionary.

**Technical impact**  
The refactoring improves code modularity by decoupling M-RoPE state handling from core execution logic. The `CudaGraphManager` no longer needs to know about `uses_mrope`, and `ModelRunner` delegates M-RoPE operations to `ModelState`. This reduces complexity in graph capture and model execution paths, making the system more extensible for future model-specific features.

**Potential risks**  
There is a risk of regression in M-RoPE functionality if the state transitions in `ModelState` are not correctly synchronized with request lifecycle events. The removal of `mrope_positions` from `InputBatch` could break external dependencies, though it appears unused elsewhere. Additionally, the `prepare_inputs` method’s override behavior (where returned values replace defaults) requires careful coordination to avoid missing required inputs.

**Key insights**  
This is a foundational change for a larger refactoring effort (indicated by "[1/N]"). Developers should ensure all M-RoPE interactions now go through `ModelState`. The pattern of using `prepare_inputs` to override default model inputs is powerful but must be documented clearly. Future PRs will likely expand `ModelState` with additional model-specific features.

---

## 27. [[Model Runner V2] Fix error-handling](https://github.com/vllm-project/vllm/pull/35063)


### Base Information

- **PR Number:** #35063
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-26 11:00:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35063/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
Added an `execute_model_state` instance variable to track execution state between `execute_model()` and `sample_tokens()` calls. Modified `sample_tokens()` to return `None` when `execute_model_state` is `None`, ensuring that exceptions in `execute_model()` are properly propagated instead of being masked by assertion failures.

**Technical impact**  
This change decouples the error-handling flow between the two asynchronous stages (model execution and token sampling). The system now gracefully handles failures in `execute_model()` by allowing `sample_tokens()` to return `None` without raising an assertion error, preserving the original exception for debugging.

**Potential risks**  
If `execute_model_state` is incorrectly set or cleared elsewhere, it could lead to silent failures or state leakage across requests. The `_dummy_run()` method also accesses `execute_model_state` and clears it—ensuring consistency between these code paths is critical to avoid race conditions in async/PP scenarios.

**Key insights**  
Always validate state transitions in async pipelines to prevent exception masking. Consider adding logging or metrics when `execute_model_state` is `None` in `sample_tokens()` to improve debuggability. Review other methods that interact with `execute_model_state` to ensure thread safety and proper state isolation.

---

## 28. [[Bugfix] Remove erroneous lower bound on LoRA vocab size constraint](https://github.com/vllm-project/vllm/pull/35354)


### Base Information

- **PR Number:** #35354
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-26 10:44:51
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35354/files) (2):**
  - `tests/lora/test_layers.py`
  - `vllm/lora/layers/logits_processor.py`

### Summary

**What changed and why**  
The fix removes an erroneous lower bound check (`vocab_size <= 32000`) that was inadvertently introduced in a previous PR. This lower bound was not originally enforced, and its addition broke LoRA support for models like Mixtral, which have a vocab size of exactly 32,000. The change restores the original intent of only enforcing an upper bound (vocab size ≤ 258,048).

**Technical impact**  
This correction ensures compatibility with models having vocab sizes ≤ 32,000, reinstating support for Mixtral and similar architectures in LoRA workflows. The code now aligns with the historical behavior where only the upper bound was validated, preventing unnecessary restrictions on model selection.

**Potential risks**  
If there was an original, undocumented requirement for vocab sizes > 32,000, removing the check might expose underlying issues in LoRA implementations for smaller vocabularies. Additionally, the test suite reduction (removing test cases for vocab sizes 512 and 32,000) could mask regressions if future changes reintroduce lower-bound logic.

**Key insights**  
Always verify that conditional logic changes preserve original behavior, especially when modifying chained comparisons. The fix highlights the importance of comprehensive regression testing for boundary values. Developers should ensure test coverage reflects both the intended constraints and the supported model specifications.

---

## 29. [[BugFix] Align fused MoE-LoRA kernel config with actual weight shapes](https://github.com/vllm-project/vllm/pull/34396)


### Base Information

- **PR Number:** #34396
- **Author:** [RunkaiTao](https://github.com/RunkaiTao)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-26 10:03:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34396/files) (1):**
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
The change fixes a configuration key mismatch in fused MoE-LoRA kernels by deriving the `intermediate_size` parameter from actual LoRA weight shapes instead of the layer's static `intermediate_size_per_partition`. This ensures the GPT-OSS gate-up kernel (`w1`/`w3` concatenated weights) uses the correct shape (5888) rather than an incorrect one (2994).

**Technical impact**  
This aligns kernel configuration keys with the true weight dimensions, preventing potential kernel dispatch errors or performance degradation. The fix specifically targets the `w13_lora_b_stacked` weight shape for gate-up operations while leaving down kernels (`w2`) unchanged, maintaining backward compatibility for unaffected paths.

**Potential risks**  
If the weight shape derivation logic fails (e.g., empty stacked tensors), it could lead to incorrect configuration or runtime errors. Additionally, any undiscovered mismatches in other fused MoE-LoRA variants might persist, though the change is localized to the gate-up kernel path.

**Key insights**  
Always validate kernel configuration parameters against actual runtime tensor shapes, especially when weights are concatenated or transformed. Developers should verify similar shape-derived logic elsewhere in the codebase and consider adding safeguards (e.g., shape assertions) to catch mismatches early.

---

## 30. [[Refactor] Remove dead code for attention benchmark script](https://github.com/vllm-project/vllm/pull/35418)


### Base Information

- **PR Number:** #35418
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged time:** 2026-02-26 09:53:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35418/files) (2):**
  - `benchmarks/attention_benchmarks/__init__.py`
  - `benchmarks/attention_benchmarks/common.py`

### Summary

**What changed and why**  
This PR removes dead code from the attention benchmark script, specifically deleting unused mock classes (`MockModelConfig`, `MockParallelConfig`, `MockCompilationConfig`, `MockVLLMConfig`, `MockRunner`) and related imports (`numpy`, `_HAS_ATTENTION_LAYER_BASE` flag). The changes aim to clean up the codebase by eliminating unused components that are no longer needed for benchmarking.

**Technical impact**  
The removal reduces code complexity and maintenance overhead without affecting functionality, as these mock classes and imports were not being utilized. The benchmark script's core functionality remains intact, with only unused helper code being eliminated. This simplifies the module's public API by removing unused exports from `__init__.py`.

**Potential risks**  
Low risk since the code was confirmed dead/unused. However, if any downstream scripts or tests were indirectly relying on these mock classes (e.g., via dynamic imports), they could break. The removal of the `_HAS_ATTENTION_LAYER_BASE` flag might affect debugging or conditional logic if it was used elsewhere, though it appears to be local.

**Key insights**  
This is a straightforward cleanup that improves code hygiene. Developers should verify no external dependencies exist on the removed symbols. Future refactoring should include checking for any residual references in the codebase or documentation. The use of try/except for `AttentionLayerBase` import remains, which is good for handling optional dependencies.

---

## 31. [[Core] Support `min_tokens` with speculative decoding](https://github.com/vllm-project/vllm/pull/32642)


### Base Information

- **PR Number:** #32642
- **Author:** [qianlihuang](https://github.com/qianlihuang)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-02-26 09:31:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32642/files) (7):**
  - `tests/v1/e2e/test_async_scheduling.py`
  - `tests/v1/logits_processors/test_custom_offline.py`
  - `vllm/sampling_params.py`
  - `vllm/v1/sample/logits_processor/__init__.py`
  - `vllm/v1/sample/logits_processor/builtin.py`
  - `vllm/v1/sample/logits_processor/state.py`
  - `vllm/v1/sample/rejection_sampler.py`

### Summary

**What changed and why**  
The PR enables `min_tokens` support for speculative decoding by masking stop tokens during rejection sampling. Previously, `min_tokens` was explicitly disallowed with speculative decoding; now it's permitted while `min_p` and `logit_bias` remain unsupported.

**Technical impact**  
The changes modify the logits processing pipeline to conditionally apply `MinTokensLogitsProcessor` during speculative decoding. A new method `apply_with_spec_decode()` handles stop-token masking across draft positions, ensuring the minimum token requirement is respected before allowing stop tokens.

**Potential risks**  
If `min_tokens` is set higher than the number of draft tokens, the masking logic may not fully enforce the requirement across all draft positions. Edge cases involving very large `min_tokens` values or empty stop-token lists could lead to incorrect behavior. The integration assumes proper synchronization between draft token counts and request states.

**Key insights**  
Developers should note that `min_p` and `logit_bias` remain incompatible with speculative decoding. The implementation efficiently batches stop-token masking using NumPy and PyTorch operations, but performance should be monitored for large batch sizes. Ensure `min_tokens` validation aligns with speculative decoding configurations in production.

---

## 32. [[Perf] Optimize maxsim scores computation for pooling models, 13.9% E2E throughput improvement](https://github.com/vllm-project/vllm/pull/35330)


### Base Information

- **PR Number:** #35330
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-26 09:14:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35330/files) (3):**
  - `tests/entrypoints/pooling/score/test_utils.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/pooling/score/utils.py`

### Summary

**What changed and why**  
The PR optimizes MaxSim score computation for pooling models by moving the calculation from CPU to GPU and implementing a batched version. This replaces the per-pair `compute_maxsim_score` function with a new batched `compute_maxsim_scores` function that processes multiple query-document embedding pairs efficiently using padded mini-batches and GPU acceleration.

**Technical impact**  
This change significantly improves end-to-end throughput by 13.9% (from 141.38 to 161.14 req/s) by leveraging GPU parallelism for similarity computations. The implementation introduces configurable batching parameters (`max_batch_size`, `max_score_matrix_elements`) to manage memory usage while maintaining numerical equivalence with the original CPU implementation, as verified by unit tests.

**Potential risks**  
The batched implementation could increase GPU memory pressure when processing many long sequences simultaneously, though the `max_score_matrix_elements` parameter provides protection. Edge cases with extremely variable sequence lengths within a batch may reduce computational efficiency due to padding overhead. The automatic device selection (`cuda` if available) assumes GPU availability for optimal performance.

**Key insights**  
The optimization demonstrates substantial performance gains through GPU acceleration and batching while maintaining backward compatibility. Developers should monitor memory usage in production and consider tuning `max_batch_size` and `max_score_matrix_elements` based on typical workload characteristics. The unit test provides a good reference for validating numerical correctness across different tensor shapes.

---

## 33. [Add GlmOcrConfig for GLM-OCR model type recognition](https://github.com/vllm-project/vllm/pull/34982)


### Base Information

- **PR Number:** #34982
- **Author:** [hujia177](https://github.com/hujia177)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 09:04:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34982/files) (3):**
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/glm_ocr.py`

### Summary

**What changed and why**  
Added support for GLM-OCR model type by creating custom `GlmOcrConfig` and `GlmOcrVisionConfig` classes that extend `PretrainedConfig`. This addresses a compatibility issue where vLLM's bundled HuggingFace Transformers library (older than v5.1.0) doesn't recognize the `glm_ocr` model_type, preventing proper model loading via `AutoConfig.from_pretrained()`.

**Technical impact**  
The changes enable vLLM to properly load GLM-OCR models by intercepting the `glm_ocr` model_type through the `_CONFIG_REGISTRY` before it reaches HuggingFace's `AutoConfig`. This follows the established pattern used for other custom model configurations like `DotsOCRConfig`, maintaining consistency in the codebase's architecture for handling unsupported model types.

**Potential risks**  
The implementation assumes GLM-OCR uses a ChatGLM text configuration, which may not hold for all variants or future versions. The custom config classes hardcode specific token IDs (image/video start/end tokens) that could change in different model releases. There's also a risk of version mismatch if the actual GLM-OCR model implementation diverges from the assumptions in these config classes.

**Key insights**  
This is a standard pattern for adding support for new model types in vLLM when the bundled transformers library lacks native support. Developers should verify that the token IDs and architecture parameters match the specific GLM-OCR model version being used. Consider adding integration tests with actual GLM-OCR model weights to validate the configuration works correctly in practice.

---

## 34. [[BugFix][kv_offload]: Fix kernel block size detection](https://github.com/vllm-project/vllm/pull/35125)


### Base Information

- **PR Number:** #35125
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-26 08:29:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35125/files) (1):**
  - `vllm/v1/kv_offload/worker/cpu_gpu.py`

### Summary

**What changed and why**  
The fix addresses incorrect kernel block size detection when registering non-cross layers KV-cache, specifically with HND layout. The issue occurred because stride order permutation was incorrectly applied to all cases, but should only be applied when `has_layers_dim=True` (cross layers case).

**Technical impact**  
This change ensures proper alignment between logical and physical tensor layouts by conditionally applying stride order permutation. When `has_layers_dim=False`, the test_shape now remains in its original logical layout, preventing incorrect block dimension identification that could lead to performance degradation or runtime errors.

**Potential risks**  
The conditional logic introduces a new code path that could be missed during testing if cross-layer and non-cross-layer configurations aren't thoroughly tested. There's also a risk that other attention backends might have different stride order requirements not covered by the current implementation.

**Key insights**  
Developers should verify that both cross-layer and non-cross-layer KV-cache configurations work correctly with various attention backends. The fix highlights the importance of distinguishing between logical and physical tensor layouts when working with different memory layouts like HND. Consider adding validation to ensure block_size_idx is always found in test_shape.

---

## 35. [[ROCm] Update the torch version in rocm_build.txt to use the official 2.10 release](https://github.com/vllm-project/vllm/pull/34387)


### Base Information

- **PR Number:** #34387
- **Author:** [SageMoore](https://github.com/SageMoore)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-26 08:28:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34387/files) (1):**
  - `requirements/rocm-build.txt`

### Summary

**What changed and why**  
Updated the PyTorch index URL from ROCm 7.0 test builds to the official ROCm 7.1 release channel, and upgraded the `amdsmi` dependency from version 6.4.3 to 7.0.2. This change aligns the build environment with ROCm 7.1 (used on MI300X hardware) and ensures compatibility with newer AMD system management interfaces.

**Technical impact**  
The switch to the official PyTorch 2.10 release for ROCm 7.1 provides stable, production-ready binaries, reducing reliance on test builds. Upgrading `amdsmi` to 7.0.2 may introduce new APIs or improvements for GPU monitoring and management, potentially enhancing system integration on AMD platforms.

**Potential risks**  
The `amdsmi` upgrade could introduce breaking changes if the new version modifies its interface or behavior, affecting GPU monitoring features. Additionally, any undiscovered incompatibilities between PyTorch 2.10 (ROCm 7.1) and other dependencies might surface in edge-case scenarios or on different ROCm versions.

**Key insights**  
Developers building on ROCm 7.1+ systems should benefit from improved stability and compatibility. It is advisable to verify that the new `amdsmi` version maintains backward compatibility with existing GPU management code. Consider adding a note in documentation specifying that this configuration targets ROCm 7.1 environments.

---

## 36. [[ROCm] Add dynamic mxfp4 quantization for DeepSeek V2 projection layers](https://github.com/vllm-project/vllm/pull/34157)


### Base Information

- **PR Number:** #34157
- **Author:** [dllehr-amd](https://github.com/dllehr-amd)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-26 08:00:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34157/files) (2):**
  - `vllm/model_executor/layers/quantization/quark/quark.py`
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py`

### Summary

**What changed and why**  
This PR adds dynamic MXFP4 quantization for DeepSeek V2 projection layers when using Quark OCP MX quantization. The changes introduce a `dynamic_mxfp4_quant` flag to enable runtime quantization of excluded layers (except lm_head) at model load time, rather than relying on pre-quantized weights. This is specifically targeted at DeepSeek V3 models using FP4 quantization.

**Technical impact**  
The modifications extend the Quark quantization framework to support dynamic quantization for attention projection layers. Key changes include: adding configuration detection in `QuarkConfig.maybe_update_config()`, modifying `get_quant_method()` to apply dynamic quantization to excluded layers, and updating `QuarkOCP_MX` to handle both pre-quantized and dynamically quantized weight paths. This enables more flexible quantization strategies for specific model architectures.

**Potential risks**  
The dynamic quantization path is only enabled for DeepSeek V3 models with FP4 quantization, which could lead to inconsistent behavior if other models have similar configurations. The conditional logic in `get_quant_method()` (checking `"self_attn" not in prefix`) may be too restrictive or miss relevant layers. Additionally, the new `dynamic_mxfp4_quant` flag defaults to `False`, but the PR description states it's enabled by default for DeepSeek V2 models—this inconsistency needs clarification.

**Key insights**  
Developers should verify that the model type check (`model_type == "deepseek_v3"`) aligns with the intended DeepSeek V2 support mentioned in the PR title. The separation of dynamic and pre-quantized weight initialization in `create_weights()` is a clean architectural choice, but ensure that `ModelWeightParameter` is appropriate for all dynamic quantization scenarios. Testing should cover edge cases where layers are incorrectly included or excluded from dynamic quantization.

---

## 37. [[Refactor] Remove dead or duplicate func utils or variables](https://github.com/vllm-project/vllm/pull/35318)


### Base Information

- **PR Number:** #35318
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-02-26 07:57:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35318/files) (9):**
  - `benchmarks/backend_request_func.py`
  - `benchmarks/benchmark_utils.py`
  - `benchmarks/cutlass_benchmarks/utils.py`
  - `benchmarks/disagg_benchmarks/rate_limiter.py`
  - `benchmarks/disagg_benchmarks/request_queue.py`
  - `vllm/model_executor/layers/quantization/ptpc_fp8.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`
  - `vllm/v1/engine/core.py`

### Summary

**What changed and why**  
This PR removes dead or duplicate code across multiple files in the vLLM codebase. The changes consist entirely of deletions, targeting unused constants, functions, classes, and imports within benchmark utilities, quantization modules, and model files. The purpose is to clean up the codebase, reduce maintenance burden, and eliminate potential sources of confusion.

**Technical impact**  
The removal of dead code reduces the overall code footprint and simplifies the project structure. Specifically, it eliminates unused benchmark utilities (like PyTorch benchmark formatting and rate limiting), unused quantization helper functions, and unused constants. This does not affect runtime behavior since the removed code was not in active use, but it makes the codebase easier to navigate and maintain.

**Potential risks**  
The primary risk is inadvertently removing code that might be used indirectly or in future scenarios. For example, `OPENAI_COMPATIBLE_BACKENDS` or the `RateLimiter` class could be referenced in external scripts or planned features. Additionally, removing the `POLLING_TIMEOUT_S` constant could affect code that imports it, though it appears unused in the provided diff.

**Key insights**  
This cleanup is a positive step for code hygiene. Developers should verify that none of the removed symbols are imported elsewhere in the codebase or in dependent projects. Future refactoring efforts should include checks for cross-references and consider adding linters (like `vulture` or `deadcode`) to automate dead code detection. The removal of entire files (`rate_limiter.py`, `request_queue.py`) suggests these components were experimental or superseded.

---

## 38. [[Model] Add nvidia/llama-nemotron-embed-vl-1b-v2 multimodal embedding model](https://github.com/vllm-project/vllm/pull/35297)


### Base Information

- **PR Number:** #35297
- **Author:** [jzakrzew](https://github.com/jzakrzew)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 06:17:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35297/files) (8):**
  - `docs/models/pooling_models.md`
  - `docs/models/supported_models.md`
  - `examples/pooling/embed/template/nemotron_embed_vl.jinja`
  - `tests/models/multimodal/pooling/test_llama_nemotron_vl_embed.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/nemotron_vl.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
Added support for the NVIDIA Llama-Nemotron-Embed-VL-1B-V2 multimodal embedding model. The changes extend the existing Nemotron VL codebase to handle this embedding variant, which uses a SigLIP vision encoder and bidirectional LLaMA backbone for pooling-based embeddings rather than generative chat. A new model class `LlamaNemotronVLForEmbedding` was created, along with configuration handling, a custom chat template, and comprehensive tests.

**Technical impact**  
The model integrates into vLLM's pooling/embedding framework, reusing much of the Nemotron VL infrastructure but with key differences: bidirectional attention, SigLIP vision encoder, and a pooling layer instead of causal LM head. The changes are backward-compatible—existing Nemotron VL chat models remain unaffected. The addition expands vLLM's multimodal embedding capabilities, supporting both text and image inputs via the embeddings API.

**Potential risks**  
The model requires `trust_remote_code=True` due to custom Hugging Face implementations. The bidirectional LLaMA configuration must be correctly set (non-causal) to avoid attention mask issues. The custom chat template is essential for proper prefix handling (`query:`/`passage:`), and misuse could lead to incorrect embeddings. There is also a risk of vision model incompatibility if future SigLIP updates change the interface.

**Key insights**  
Developers should use the provided Jinja template when serving the model to ensure proper prefixing. The model is tested against the original HF implementation, ensuring parity. Note that the embedding API expects a single message per request, unlike chat models. The changes demonstrate a pattern for extending existing multimodal architectures to embedding variants with minimal code duplication.

---

## 39. [[Bug] Fix missing <think> tag after tool call in MiniMax 2.1](https://github.com/vllm-project/vllm/pull/35352)


### Base Information

- **PR Number:** #35352
- **Author:** [stingoChen](https://github.com/stingoChen)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-26 06:11:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35352/files) (1):**
  - `vllm/reasoning/minimax_m2_reasoning_parser.py`

### Summary

**What changed and why**  
The PR fixes a bug where the reasoning end detection logic incorrectly identified the start of reasoning after tool calls. The `is_reasoning_end` method was modified to check for both `<think>` and `</think>` tokens, ensuring that reasoning detection only ends when the closing tag is encountered, not the opening one.

**Technical impact**  
This change ensures that the MiniMax M2.1 model correctly includes the `<think>` tag in reasoning outputs after tool calls, maintaining the expected reasoning structure. It prevents premature termination of reasoning detection, which could disrupt the model's chain-of-thought behavior in tool-augmented interactions.

**Potential risks**  
If the tokenizer's vocabulary does not contain the `<think>` token, `self.start_token_id` could be `None`, potentially causing the check to fail or behave unexpectedly. Additionally, the loop now iterates over all input IDs, which may have minor performance implications for long sequences, though this is likely negligible.

**Key insights**  
Developers should verify that the tokenizer includes both `<think>` and `</think>` tokens in its vocabulary. The fix is minimal and focused, but consider adding a fallback or validation for missing tokens to enhance robustness. This change is critical for maintaining correct reasoning flow in tool-calling scenarios.

---

## 40. [[Misc] Standardize handling of `mm_processor_kwargs.size`](https://github.com/vllm-project/vllm/pull/35284)


### Base Information

- **PR Number:** #35284
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 05:05:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35284/files) (9):**
  - `tests/lora/test_qwenvl.py`
  - `tests/models/multimodal/processing/test_gemma3.py`
  - `tests/models/multimodal/processing/test_qwen2_vl.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`

### Summary

**What changed and why**  
This PR adds backward compatibility for multimodal processor configuration by supporting both old-style `min_pixels`/`max_pixels` parameters and new-style `size` dictionary with `shortest_edge`/`longest_edge` keys. The changes ensure that existing configurations continue to work while aligning with the updated Transformers library interface.

**Technical impact**  
The modifications affect multiple vision-language model implementations (Qwen2-VL, Ernie45-VL, Hunyuan, Keye, PaddleOCR-VL, Qwen3-VL, Gemma3) by unifying the handling of image size constraints. The logic now merges default processor sizes with user overrides, supporting both parameter naming conventions. This improves interoperability across different Transformers versions and custom model repositories.

**Potential risks**  
- Inconsistent behavior may arise if both `size` and `min_pixels`/`max_pixels` are provided simultaneously, as the merging order could lead to ambiguous overrides.  
- The Transformers version checks in tests rely on version parsing, which might fail with non-standard version strings.  
- The conditional key mapping based on `trust_remote_code` in Ernie45-VL and PaddleOCR-VL introduces additional complexity that could cause errors if the flag is misinterpreted.

**Key insights**  
- Developers should avoid mixing old and new parameter styles in the same configuration to prevent undefined behavior.  
- The changes are primarily additive and maintain backward compatibility, but thorough testing with both parameter formats is recommended.  
- Consider centralizing the size parameter normalization logic to reduce duplication across model files and ensure consistent handling.

---

## 41. [[Bugfix] Fix uint32 overflow in Mamba selective scan state pointer arithmetic](https://github.com/vllm-project/vllm/pull/35275)


### Base Information

- **PR Number:** #35275
- **Author:** [Josephasafg](https://github.com/Josephasafg)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-02-26 04:22:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35275/files) (1):**
  - `csrc/mamba/mamba_ssm/selective_scan.h`

### Summary

**What changed and why**  
The PR fixes a uint32 overflow in Mamba's selective scan state pointer arithmetic by changing the `index_t` type from `uint32_t` to `size_t`. This prevents overflow when calculating memory offsets (`cache_index * ssm_states_batch_stride`) for large cache indices, which previously caused SSM state corruption under high concurrency.

**Technical impact**  
This change ensures correct memory addressing for SSM state operations, eliminating silent data corruption that led to incorrect model outputs. The switch to `size_t` aligns with standard practice for memory indexing on 64-bit systems, improving robustness for large-scale or concurrent inference scenarios.

**Potential risks**  
While `size_t` resolves the overflow, developers should verify that all related calculations and data structures consistently use 64-bit types to avoid mismatches. There is a minimal risk of increased memory usage if other dependent types are not aligned, but this is negligible compared to the critical bug fix.

**Key insights**  
This is a critical fix for correctness in memory-bound operations; ensure all pointer arithmetic in the codebase uses `size_t` or equivalent for indexing. Review related kernel code for similar overflow risks, especially in high-concurrency paths, and consider adding bounds checking in debug builds to catch such issues earlier.

---

## 42. [[Bugfix] fix device_name for routing replay](https://github.com/vllm-project/vllm/pull/34336)


### Base Information

- **PR Number:** #34336
- **Author:** [Li-Yongwen](https://github.com/Li-Yongwen)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 04:18:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34336/files) (1):**
  - `vllm/model_executor/layers/fused_moe/routed_experts_capturer.py`

### Summary

**What changed and why**  
The change replaces a hardcoded `"cuda"` device specification with `current_platform.device_type` when initializing a PyTorch tensor buffer. This fixes a bug where routing replay functionality would fail on non-CUDA platforms (e.g., CPU or other accelerators) by ensuring the buffer is allocated on the correct device.

**Technical impact**  
This update makes the routed experts capturer platform-agnostic, aligning it with vLLM's multi-platform support. The buffer now correctly respects the active platform's device type, improving portability and enabling routing replay on non-CUDA backends without code changes.

**Potential risks**  
If `current_platform.device_type` returns an unexpected value (e.g., `None` or an invalid device string), tensor initialization could fail. Additionally, any downstream code assuming CUDA-specific behavior (e.g., GPU-only operations) may need further adjustments to remain cross-platform compatible.

**Key insights**  
Always use platform-aware utilities (like `current_platform`) instead of hardcoded device references to maintain cross-platform compatibility. Review other instances of hardcoded `"cuda"` strings in the codebase to prevent similar issues. Ensure thorough testing on all supported platforms to validate device handling.

---

## 43. [[Bugfix] Fix Qwen2.5-Omni and Qwen3-Omni mixed-modality embed regression](https://github.com/vllm-project/vllm/pull/35368)


### Base Information

- **PR Number:** #35368
- **Author:** [linyueqian](https://github.com/linyueqian)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 03:58:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35368/files) (3):**
  - `tests/models/multimodal/processing/test_qwen2_5_omni_embed.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
This PR fixes a regression where mixed-modality embeddings (audio+image+video) failed for Qwen2.5-Omni and Qwen3-Omni models. The root cause was PR #33605, which changed the non-interleaved path to call `_merge_multimodal_embeddings` directly instead of using the parent class's `embed_input_ids`. The fix restores `super().embed_input_ids()` for the non-interleaved path, ensuring proper embedding assignment.

**Technical impact**  
The changes restore correct embedding placement for mixed-modality inputs by leveraging the parent class's proven merging logic. The interleaved audio-in-video path remains unchanged, preserving its specialized handling. This maintains architectural consistency and ensures both models handle multimodal inputs as intended.

**Potential risks**  
If the parent class's `embed_input_ids` logic changes in the future, these models could regress again. The deepstack processing in Qwen3-Omni relies on side effects (`_set_deepstack_input_embeds`), which could be fragile if the super call order or internal state management changes. Edge cases with empty multimodal embeddings or malformed interleaved sequences should be validated.

**Key insights**  
Always prefer inherited parent methods over direct helper calls when they encapsulate complex, validated logic. The comprehensive test suite added here is crucial for catching regressions in multimodal embedding placement. Developers should ensure any future modifications to `embed_input_ids` in parent or child classes are tested across both interleaved and non-interleaved scenarios.

---

## 44. [Remove `bc-lint`](https://github.com/vllm-project/vllm/pull/35274)


### Base Information

- **PR Number:** #35274
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-26 03:01:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35274/files) (5):**
  - `.github/.bc-linter.yml`
  - `.github/workflows/bc-lint.yml`
  - `vllm/__init__.py`
  - `vllm/_bc_linter.py`
  - `vllm/v1/core/sched/output.py`

### Summary

**What changed and why**  
This PR removes the `bc-lint` (backward compatibility linter) GitHub Actions workflow and all associated code. The linter was added as a trial but only covered three classes, providing minimal value while consuming significant CI resources (~35% of workflow runs, ~15% of workflow minutes). The removal frees up GitHub Actions capacity for more useful workflows.

**Technical impact**  
The removal eliminates a CI workflow that checked for backward compatibility violations in a limited set of classes. No functional code changes occur—only CI configuration and unused decorator imports are removed. The public API surface is slightly reduced by removing the `bc_linter_skip` and `bc_linter_include` decorator exports from `vllm.__init__`.

**Potential risks**  
If the trial linter was actively preventing breaking changes in the three monitored classes, those checks are now absent. However, given the limited coverage and trial status, this risk appears minimal. Developers should ensure other CI checks adequately cover compatibility concerns for critical APIs.

**Key insights**  
This is a clean removal of an underutilized tool that was disproportionately consuming CI resources. The decision is data-driven, based on resource usage metrics. Teams should monitor CI performance post-removal to confirm improved responsiveness and consider implementing a more targeted compatibility strategy if needed in the future.

---

## 45. [[XPU] use fixed UMD version in dockerfile.xpu](https://github.com/vllm-project/vllm/pull/35392)


### Base Information

- **PR Number:** #35392
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 02:54:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35392/files) (1):**
  - `docker/Dockerfile.xpu`

### Summary

**What changed and why**  
The Dockerfile.xpu was modified to replace dynamic package installation from the unverified "kobuk-team/intel-graphics" repository with explicit downloads of fixed-version User Mode Driver (UMD) components. This change addresses CI instability caused by using untested latest UMD versions by pinning to a specific, verified release (25.48.36300.8).

**Technical impact**  
This improves CI reliability by eliminating variability from rolling UMD updates. The build process now downloads and installs specific .deb packages directly from official GitHub releases, ensuring consistent runtime environment composition for XPU workloads. The change also removes the dependency on the third-party PPA.

**Potential risks**  
Manually managing individual .deb packages increases maintenance overhead—future updates require updating each package URL and version. There is a risk of missing transitive dependencies not explicitly listed. The approach may also lead to version incompatibility if other components (e.g., DPC++ compiler) expect different UMD versions.

**Key insights**  
Pinning UMD versions is crucial for CI stability, but consider automating version updates or using a curated repository for easier maintenance. Validate that all necessary runtime dependencies are included, and monitor for any functional regressions in XPU tests due to the version change.

---

## 46. [[Bugfix] [Qwen3.5]Fix Qwen3.5 FP8 quantization: tuple shard_id weight loading](https://github.com/vllm-project/vllm/pull/35289)


### Base Information

- **PR Number:** #35289
- **Author:** [Alibaba-HZY](https://github.com/Alibaba-HZY)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-26 02:26:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35289/files) (1):**
  - `vllm/model_executor/layers/linear.py`

### Summary

**What changed and why**  
The fix addresses a crash in Qwen3.5's FP8 quantization when loading weights with a tuple `shard_id`. The issue occurred because `MergedColumnParallelLinear.weight_loader` didn't support tuple shard IDs, which are used when a single checkpoint weight maps to multiple output shards. The changes modify the weight loader to handle tuple shard IDs correctly, allowing proper weight splitting and loading for fused modules.

**Technical impact**  
These changes enable FP8 quantization for Qwen3.5 models by properly handling tuple shard IDs in the weight loading process. The modifications allow the system to correctly split and load weights that cover multiple output shards simultaneously, which is essential for models using `stacked_params_mapping` with complex weight distributions. This maintains the quantization workflow while ensuring all shards are loaded before FP8 conversion.

**Potential risks**  
The implementation still has limitations with BNB quantization and GGUF formats when using tuple shard IDs, as indicated by the maintained `NotImplementedError` checks. There's a risk of edge cases where the tuple shard ID handling might not align with other model architectures or quantization methods. The changes to the output_sizes calculation logic could potentially affect non-Qwen models if they use similar weight loading patterns.

**Key insights**  
Developers should note that tuple shard ID support is now available for standard FP8 quantization but remains unsupported for BNB quantization and GGUF formats. The fix demonstrates how to properly handle multi-index shard IDs in fused linear layers, which could serve as a reference for similar implementations. When working with models using `stacked_params_mapping`, ensure weight loaders can process both single and tuple shard IDs for full compatibility.

---

## 47. [[Hardware][Powerpc]Enable prefix caching and chunked prefill for ppc64le](https://github.com/vllm-project/vllm/pull/35081)


### Base Information

- **PR Number:** #35081
- **Author:** [Akashcodes732](https://github.com/Akashcodes732)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-26 02:21:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35081/files) (1):**
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
The PR removes POWERPC (ppc64le) from the list of CPU architectures that automatically disable chunked prefill and prefix caching in the V1 backend. Previously, these optimizations were disabled for both POWERPC and RISC-V CPUs; now they remain disabled only for RISC-V.

**Technical impact**  
This change enables prefix caching and chunked prefill optimizations for POWERPC (ppc64le) systems when using the V1 CPU backend. These features can significantly improve inference performance by reusing computed KV cache for repeated prompt prefixes and processing long prompts in chunks.

**Potential risks**  
The main risk is that these optimizations might not have been fully tested or validated on POWERPC architectures, potentially leading to correctness issues or performance regressions. There's also a risk of memory management differences on POWERPC that could affect chunked prefill behavior.

**Key insights**  
The benchmark results show substantial performance improvements with prefix caching enabled (2-3x throughput increase, 3x faster TTFT). Developers should verify that the underlying chunked prefill implementation is indeed compatible with POWERPC's memory model and instruction set. Consider adding architecture-specific tests to ensure long-term compatibility.

---

## 48. [[Benchmarks] Plot benchmark timeline and requests statistics](https://github.com/vllm-project/vllm/pull/35220)


### Base Information

- **PR Number:** #35220
- **Author:** [sducouedic](https://github.com/sducouedic)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-26 02:17:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35220/files) (3):**
  - `setup.py`
  - `vllm/benchmarks/plot.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
This PR adds two new visualization features to the vLLM benchmark suite: an interactive timeline plot (using Plotly) showing request execution with TTFT and ITL segments, and a dataset statistics plot (using Matplotlib) displaying token distributions. These visualizations help users analyze performance bottlenecks, latency patterns, and dataset characteristics more effectively.

**Technical impact**  
The changes introduce optional dependencies (Plotly and Matplotlib) for the "bench" extra, with graceful degradation via `PlaceholderModule` if libraries are missing. A new `plot.py` module centralizes visualization logic, while `serve.py` gains CLI flags (`--plot-timeline`, `--plot-dataset-stats`) and a refactored filename computation function to support plot generation. Plots are saved alongside benchmark JSON results with consistent naming.

**Potential risks**  
The optional dependencies may cause runtime warnings if missing, though the code handles this gracefully. The timeline plot’s data construction assumes all required metrics (start_times, ttfts, itls) are present; missing data could lead to empty plots or warnings. The `construct_timeline_data` function’s time formatting assumes durations under 100 hours, which could break in very long benchmarks.

**Key insights**  
The modular design isolates plotting logic, making it easy to maintain and extend. Developers should ensure benchmark configurations produce the necessary metrics for visualizations. The refactored `compute_result_filename` improves code reuse but must be called before plot generation to avoid path issues. Consider adding unit tests for `plot.py` functions to validate data handling and edge cases.

---

## 49. [[Model] Ring 2.5](https://github.com/vllm-project/vllm/pull/35102)


### Base Information

- **PR Number:** #35102
- **Author:** [ZJY0516](https://github.com/ZJY0516)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-26 02:17:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35102/files) (8):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/model_executor/layers/fla/ops/layernorm_guard.py`
  - `vllm/model_executor/layers/layernorm.py`
  - `vllm/model_executor/layers/mamba/linear_attn.py`
  - `vllm/model_executor/models/bailing_moe_linear.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`

### Summary

**What changed and why**  
This PR adds support for the new Ring-2.5-1T model (a variant of the Ling/BailingMoe architecture) by introducing a new model class `BailingMoeV2_5ForCausalLM`. The changes include a new model implementation file (`bailing_moe_linear.py`), updates to the layer normalization kernel to support different activation functions (SiLU/swish vs. sigmoid), refactoring of linear attention utilities, and updates to documentation and registry files.

**Technical impact**  
The PR extends the existing BailingMoe architecture with a hybrid design that combines MLA (Multi‑Head Latent Attention) for full‑attention layers and linear attention (Mamba‑style) for other layers. The layernorm kernel now supports configurable gating activations, and linear attention operations are modularized into reusable functions. This enables efficient inference for the new model while maintaining compatibility with existing vLLM infrastructure.

**Potential risks**  
- The new activation parameter in layernorm kernels must be correctly propagated through all call sites; missing updates could lead to silent behavioral changes.  
- The hybrid attention design increases complexity—ensuring correct routing between MLA and linear attention layers requires careful validation of the `layer_group_size` logic.  
- The refactored linear attention utilities assume specific metadata fields (e.g., `num_prefills`, `num_decode_tokens`); if these are absent in some execution paths, errors may occur.

**Key insights**  
- The layernorm activation flexibility (swish/sigmoid) is a reusable enhancement for other gated architectures.  
- Modularizing linear attention into `linear_attention_prefill_and_mix` and `linear_attention_decode` improves code maintainability.  
- Developers should verify that all hybrid‑layer indices are correctly aligned with the model’s configuration to avoid attention‑type mismatches.

---

## 50. [[Test] Add tests for n parameter in chat completions API](https://github.com/vllm-project/vllm/pull/35283)


### Base Information

- **PR Number:** #35283
- **Author:** [KrxGu](https://github.com/KrxGu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-26 01:14:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35283/files) (1):**
  - `tests/entrypoints/openai/test_chat.py`

### Summary

**What changed and why**  
This PR adds comprehensive testing for the `n` parameter in the chat completions API to verify that multiple completion choices are correctly returned. It includes both integration tests (end-to-end with a server) and unit tests (protocol layer) covering non-streaming, streaming, seed usage, default behavior, and various `n` values.

**Technical impact**  
The changes validate that the existing V1 engine's `ParentRequest` child request fan-out pattern correctly supports `n>1` via `async_llm.py` and `parallel_sampling.py`. No functional modifications are made to the core logic—only test coverage is expanded, ensuring the API behaves as expected for multiple-choice generation.

**Potential risks**  
The tests rely on non-deterministic outputs (e.g., checking for diverse responses with `temperature>0`), which could lead to flaky failures if randomness is insufficient. Additionally, streaming tests assume all content chunks are properly aggregated per index, which may not hold if the streaming protocol changes.

**Key insights**  
Developers should note that the `n` parameter is already fully supported in the V1 engine, and these tests serve as a regression suite. Ensure any future changes to sampling or request handling maintain compatibility with the test cases, particularly for streaming aggregation and default `n=1` behavior.

---

## 51. [[Bugfix][Hardware][AMD] Gate FP4 ops on gfx950 to prevent MI300X crash](https://github.com/vllm-project/vllm/pull/35250)


### Base Information

- **PR Number:** #35250
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-26 00:11:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35250/files) (1):**
  - `vllm/_aiter_ops.py`

### Summary

**What changed and why**  
Added hardware architecture checks to gate FP4 operations on gfx950 (CDNA4) only. The changes prevent MI300X (gfx942) systems from attempting to execute CDNA4-specific instructions when environment variables enable FP4 BMM or FP4 ASM GEMM features, which would cause illegal instruction crashes.

**Technical impact**  
These changes ensure runtime compatibility checks align with hardware capabilities by adding `on_gfx950()` guards to two feature detection methods. This maintains consistency with existing gating patterns in the codebase and prevents runtime crashes on unsupported architectures while preserving functionality for compatible hardware.

**Potential risks**  
The gating logic depends on correct architecture detection in `on_gfx950()`. Future hardware generations with similar capabilities but different architecture identifiers might be incorrectly excluded. There's also a risk that environment variables could be misinterpreted as enabling features on incompatible hardware if users don't understand the architecture requirements.

**Key insights**  
Always validate hardware compatibility before enabling architecture-specific optimizations. Consider documenting architecture requirements for feature flags to prevent user confusion. The pattern of importing `on_gfx950()` inside methods rather than at module level suggests potential for refactoring to centralize architecture detection imports.

---

## 52. [[ROCm] Add extra step in config initialization to populate custom ops before compilation config init](https://github.com/vllm-project/vllm/pull/34848)


### Base Information

- **PR Number:** #34848
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-26 00:05:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34848/files) (3):**
  - `vllm/config/vllm.py`
  - `vllm/platforms/interface.py`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
Added a new `apply_config_platform_defaults` method to the platform interface, called early in `VllmConfig` initialization. This moves ROCm-specific custom op initialization (for AITER features) before the compilation config is finalized, ensuring fusion passes can utilize these ops. The change also fixes a test failure when AITER is enabled.

**Technical impact**  
This refactors platform-specific config initialization into two phases: early defaults application (`apply_config_platform_defaults`) and later validation (`check_and_update_config`). It ensures custom ops like `+rms_norm` or `+quant_fp8` are available during compilation config setup, enabling proper fusion pass integration. The ROCm implementation now cleanly separates default population from validation logic.

**Potential risks**  
If other platforms implement `apply_config_platform_defaults` incorrectly, they might modify configs in ways that conflict with later validation. The early call order assumes all platform-specific defaults are independent; overlapping modifications could cause race conditions. Additionally, moving `+sparse_attn_indexer` to the defaults phase might affect other platforms if they rely on similar late-stage additions.

**Key insights**  
This change introduces a clearer lifecycle for platform-specific config adjustments. Developers should ensure any new platform implementations follow the same pattern: use `apply_config_platform_defaults` for early, non-validating defaults and `check_and_update_config` for validation and constraints. The ROCm-specific logic is now more maintainable, but cross-platform consistency must be verified.

---

