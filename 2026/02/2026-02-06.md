# vLLM Merged PR Report

**Report Date:** 2026-02-06 PST

**Total Merged PRs:** 42

---

## 1. [Fix spelling errors](https://github.com/vllm-project/vllm/pull/33978)


### Base Information

- **PR Number:** #33978
- **Author:** [sleepcoo](https://github.com/sleepcoo)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 23:58:50
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33978/files) (5):**
  - `tests/kernels/moe/test_cutedsl_moe.py`
  - `tests/kernels/moe/test_moe_align_block_size.py`
  - `tests/reasoning/test_hunyuan_reasoning_parser.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/v1/attention/ops/flashmla.py`

### Summary

**What changed and why**  
This PR fixes spelling errors in variable names and comments across five files. The changes correct "Intialize" to "Initialize" in test comments, "is_availble" to "is_available" in a function name, "AVAILBLE_BACKENDS" to "AVAILABLE_BACKENDS" in a comment, and "NO_REASONING_QUICK_THROUGHT" to "NO_REASONING_QUICK_THOUGHT" in a test variable.

**Technical impact**  
These changes are purely cosmetic and do not affect functionality, as they only modify comments, internal variable names, and a constant used in tests. The code behavior remains identical, but readability and consistency are improved.

**Potential risks**  
The risk is minimal since no functional code is altered. However, the change to `NO_REASONING_QUICK_THROUGHT` could affect test parameterization if other parts of the codebase reference the old spelling, though this appears confined to the test file.

**Key insights**  
Maintaining correct spelling enhances code clarity and professionalism. Developers should ensure that any references to the renamed constant are updated accordingly. Such fixes are low-risk but valuable for long-term maintainability.

---

## 2. [[ROCm][CI] Pinning lm-eval version to resolve multi-modal small eval bug](https://github.com/vllm-project/vllm/pull/34038)


### Base Information

- **PR Number:** #34038
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 22:21:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34038/files) (1):**
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
The change pins `lm-eval[api]` to version `0.4.9.2` (from `>=0.4.9.2`) in the test requirements. This resolves a breaking incompatibility where newer versions of `lm-eval` altered the return signature of `modify_gen_kwargs`, causing a `TypeError` in the VLM evaluation backend when unpacking results into `SamplingParams`.

**Technical impact**  
This fix ensures CI tests for multimodal models (like `Qwen2.5-VL-7B-Instruct`) pass by locking the dependency to a compatible version. It prevents automatic upgrades to newer `lm-eval` versions that introduce incompatible API changes, maintaining stability in the evaluation pipeline for VLMs.

**Potential risks**  
Pinning to an exact version may lead to dependency conflicts or hinder security updates in `lm-eval`. If other components require a newer version, this could cause integration issues. Additionally, the underlying bug in the VLM backend remains unaddressed, making the system fragile to future dependency changes.

**Key insights**  
This is a temporary workaround; the long-term solution should involve updating the VLM backend (`lm_eval/models/vllm_vlms.py`) to handle the new return signature. Consider adding a comment linking to the upstream issue or tracking the fix. Regularly review the pin to avoid technical debt and assess when to upgrade and adapt the backend code.

---

## 3. [[Misc] Make `PlaceholderRange.get_num_embeds` a method](https://github.com/vllm-project/vllm/pull/34035)


### Base Information

- **PR Number:** #34035
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-06 21:30:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34035/files) (9):**
  - `tests/models/multimodal/processing/test_mllama4.py`
  - `tests/multimodal/test_inputs.py`
  - `tests/v1/core/test_encoder_cache_manager.py`
  - `vllm/multimodal/budget.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/request.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The PR converts `PlaceholderRange.get_num_embeds` from a cached property to a regular method. This change eliminates redundancy because `embeds_cumsum` is already cached, and it aligns with Python naming conventions where methods that perform calculations are not prefixed with `get_` when implemented as properties.

**Technical impact**  
This change simplifies the `PlaceholderRange` class by removing the `@cached_property` decorator, reducing overhead. All callers have been updated to invoke it as a method (`get_num_embeds()`), which may slightly affect performance due to the loss of caching, but the impact is minimal since `embeds_cumsum` remains cached.

**Potential risks**  
The removal of caching could lead to repeated computations of `get_num_embeds` in tight loops, though the underlying `embeds_cumsum` cache mitigates this. There is also a risk of missing updates in external codebases or downstream dependencies that might still reference the property without parentheses.

**Key insights**  
This refactor improves code clarity and adherence to conventions. Developers should ensure all usages are updated to method calls. Consider adding a performance monitor if `get_num_embeds` is called frequently in performance-critical paths to verify no regression occurs.

---

## 4. [[Kernel] Add enable_sm120_or_later for SM121 (DGX Spark) CUTLASS support](https://github.com/vllm-project/vllm/pull/33517)


### Base Information

- **PR Number:** #33517
- **Author:** [Code4me2](https://github.com/Code4me2)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 20:28:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33517/files) (2):**
  - `csrc/cutlass_extensions/common.hpp`
  - `csrc/quantization/w8a8/cutlass/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh`

### Summary

**What changed and why**  
Added a new `enable_sm120_family` kernel wrapper to support both SM120 (RTX 5090) and SM121 (DGX Spark GB10) architectures for CUTLASS kernels. The previous `enable_sm120_only` wrapper used an exact architecture match (`__CUDA_ARCH__ == 1200`), which excluded SM121 (arch 1210) despite having identical tensor core capabilities.

**Technical impact**  
This change extends hardware compatibility for CUTLASS-based FP8 GEMM kernels to the broader SM12x family (architectures 1200-1299). The kernel dispatch logic now uses a version range check (`__CUDA_ARCH__ >= 1200 && __CUDA_ARCH__ < 1300`) instead of exact matching, allowing DGX Spark systems to utilize optimized CUTLASS kernels.

**Potential risks**  
The range-based approach could inadvertently enable kernels on future SM12x architectures with different capabilities, though the current check excludes SM130+. There's a risk if SM121 has subtle hardware differences not accounted for in kernel implementations. The change affects only one specific kernel dispatch file, but similar issues may exist elsewhere in the codebase.

**Key insights**  
The solution correctly identifies that SM120 and SM121 share tensor core capabilities, making them compatible for these kernels. Developers should audit other SM120-specific kernel wrappers for similar exact-match issues. Consider whether `enable_sm120_family` should be used consistently or if some kernels truly require exact architecture matching for performance/feature reasons.

---

## 5. [[Revert] Add util `handle_deprecated` back](https://github.com/vllm-project/vllm/pull/33998)


### Base Information

- **PR Number:** #33998
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 20:14:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33998/files) (1):**
  - `vllm/config/utils.py`

### Summary

**What changed and why**  
A utility function `handle_deprecated` has been re-added to `vllm/config/utils.py`. This change addresses a previous reversion, as indicated by the PR title and the linked discussion, to restore functionality for managing deprecated configuration parameters with proper warnings and value migration.

**Technical impact**  
This reintroduces a centralized mechanism for handling deprecated fields in configuration objects. The function automatically logs deprecation warnings and copies values from old parameter names to new ones, ensuring backward compatibility during configuration transitions.

**Potential risks**  
If the function is called without proper validation, it could overwrite existing values in new configuration fields unexpectedly. There's also a risk of inconsistent state if `old_val` is not a valid value for the new parameter(s). The function currently doesn't validate that `new_names` attributes exist on the config object.

**Key insights**  
The implementation correctly handles both single and multiple replacement parameters. Developers should ensure this function is called early in configuration processing, before new parameters are used. Consider adding type validation or existence checks for new attributes to prevent runtime errors.

---

## 6. [fix description in plugin_system.md](https://github.com/vllm-project/vllm/pull/33999)


### Base Information

- **PR Number:** #33999
- **Author:** [guodongxiaren](https://github.com/guodongxiaren)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 19:37:02
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33999/files) (1):**
  - `docs/design/plugin_system.md`

### Summary

**What changed and why**  
The change corrects a documentation example in `plugin_system.md` by updating a comment to reflect that `vllm_add_dummy_model` is a package (with an `__init__.py` file) rather than a standalone Python module. This aligns the documentation with the actual structure implied by the `packages` configuration in the `setup.py` example.

**Technical impact**  
This is a minor documentation fix that improves accuracy and clarity for developers implementing vLLM plugins. It ensures the example correctly references the file path where the `register()` function should be defined, reducing potential confusion during plugin development.

**Potential risks**  
There are no functional risks, as this only affects documentation. However, if the underlying plugin system or package structure differs from what's described, developers might still encounter issues despite the correction.

**Key insights**  
Always verify that documentation examples match the actual code structure and configuration. For plugin systems, precise file paths and entry point definitions are critical to avoid integration errors. Consider reviewing other related documentation for similar inconsistencies.

---

## 7. [[ModelRunner V2] Revert token rank comparison difference for now](https://github.com/vllm-project/vllm/pull/34017)


### Base Information

- **PR Number:** #34017
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 19:11:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34017/files) (1):**
  - `vllm/v1/worker/gpu/sample/logprob.py`

### Summary

**What changed and why**  
The change reverts the token rank comparison from strict inequality (`>`) to inclusive inequality (`>=`) in the Triton kernel that computes token ranks. This aligns the behavior with V1 to ensure output equivalence during testing, while deferring the potential switch to strict inequality to a separate PR since it represents a behavior change.

**Technical impact**  
This modification affects how token ranks are computed during sampling—specifically, tokens with equal logit values will now be assigned the same rank rather than potentially different ranks. This ensures consistency with V1's behavior, which is critical for passing equivalence tests in the ModelRunner V2 migration.

**Potential risks**  
If logits frequently have tied values, using `>=` could lead to more tokens sharing the same rank, which might subtly affect sampling distributions (e.g., in top-k or top-p sampling). Additionally, any downstream logic relying on unique ranks per token could be impacted, though this is likely minimal given the reversion to V1 behavior.

**Key insights**  
The revert is a safe interim step to maintain equivalence with V1, but the team should explicitly decide whether strict or inclusive inequality is semantically correct for ranking. Consider making this configurable if both behaviors are needed, or ensure any future change is applied consistently across V1 and V2 to avoid divergence.

---

## 8. [[Misc] Add backward-compatible import aliases for renamed translations module](https://github.com/vllm-project/vllm/pull/34015)


### Base Information

- **PR Number:** #34015
- **Author:** [kouroshHakha](https://github.com/kouroshHakha)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 19:01:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34015/files) (5):**
  - `vllm/entrypoints/openai/translations/__init__.py`
  - `vllm/entrypoints/openai/translations/api_router.py`
  - `vllm/entrypoints/openai/translations/protocol.py`
  - `vllm/entrypoints/openai/translations/serving.py`
  - `vllm/entrypoints/openai/translations/speech_to_text.py`

### Summary

**What changed and why**  
This PR adds backward-compatible import aliases for a renamed module (`translations` → `speech_to_text`). It creates stub files under the old path that re-export all symbols from the new location while emitting `DeprecationWarning` on import, giving users time to update their code before removal in version 0.17+.

**Technical impact**  
The changes restore compatibility for downstream code that imports from the old module path without altering the underlying implementation. The stubs ensure that imported symbols remain identical to those from the new module, preserving runtime behavior while guiding users toward the updated imports via warnings.

**Potential risks**  
If users suppress or ignore deprecation warnings, they may face abrupt breakage when the stubs are removed in version 0.17+. Additionally, the use of wildcard imports (`import *`) could inadvertently expose private symbols if the source modules are not carefully maintained, though this risk is low given the controlled context.

**Key insights**  
This is a well-executed deprecation strategy that balances compatibility with forward progress. Developers should ensure the warnings are visible in CI/CD pipelines to catch usage early. The removal version (0.17+) should be clearly communicated in release notes to prevent unexpected breaks.

---

## 9. [[Bugfix] Fix _fused_moe_lora_expand signature mismatch](https://github.com/vllm-project/vllm/pull/33821)


### Base Information

- **PR Number:** #33821
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 18:45:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33821/files) (1):**
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`

### Summary

**What changed and why**  
The PR removes the `b_intermediate_cache1` parameter from the `_fused_moe_lora_expand_fake` function to align its signature with `_fused_moe_lora_expand`. This ensures consistency between the real and fake implementations, fixing a signature mismatch introduced when the parameter was removed from the real function.

**Technical impact**  
This change maintains synchronization between the fake and real kernel functions, preventing potential runtime errors or incorrect behavior during graph captures or tests that rely on the fake function. It ensures that both functions have identical parameter lists, which is critical for compatibility in operations like tracing or mocking.

**Potential risks**  
If any downstream code still passes `b_intermediate_cache1` to the fake function, it will now raise an error due to the missing parameter. Additionally, if the real function’s removal of this parameter was not properly validated, there could be hidden side effects in the actual kernel logic that the fake function no longer reflects.

**Key insights**  
Always keep fake functions in sync with their real counterparts to avoid subtle bugs in testing or compilation. Verify that no callers depend on the removed parameter, and consider adding a runtime check or deprecation warning if the parameter was previously public. This fix highlights the importance of maintaining parity between real and fake implementations in kernel operations.

---

## 10. [[CI][AMD]Bugfix] Check that model_config is not None in enable_norm_pad_fusion](https://github.com/vllm-project/vllm/pull/34007)


### Base Information

- **PR Number:** #34007
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-06 18:43:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34007/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
Added a null check for `cfg.model_config` before accessing its `get_hidden_size()` method in the `enable_norm_pad_fusion` function. This prevents `AttributeError` crashes when `model_config` is `None`, which was causing CI test failures in MOE-related test suites.

**Technical impact**  
The change ensures the function safely evaluates its conditional logic by verifying `model_config` exists before attempting to read the hidden size. This maintains existing behavior for valid configurations while gracefully handling edge cases where the configuration may be incomplete or uninitialized.

**Potential risks**  
If `model_config` is `None` in production scenarios where fusion is expected, the function will now return `False` instead of crashing, potentially disabling optimizations silently. This could affect performance on AMD platforms if the null state is unintended elsewhere in the configuration pipeline.

**Key insights**  
Always validate object existence before attribute access in conditional chains. Consider whether `model_config` being `None` indicates a deeper configuration issue that should be resolved earlier. For clarity, log a warning when skipping fusion due to a null config to aid debugging.

---

## 11. [[Bugfix] Fix Whisper tokenization](https://github.com/vllm-project/vllm/pull/34011)


### Base Information

- **PR Number:** #34011
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 18:42:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34011/files) (1):**
  - `vllm/model_executor/models/whisper.py`

### Summary

**What changed and why**  
The fix addresses a bug where Whisper's feature extractor incorrectly truncates audio data to near-zero length. This occurs because Hugging Face's WhisperProcessor forwards all keyword arguments to both the tokenizer and feature extractor. When text-related kwargs like `truncation` and `max_length` are present during audio processing, the feature extractor misinterprets `max_length` as raw audio samples, causing excessive truncation.

**Technical impact**  
This change filters out `truncation` and `max_length` from the tokenizer kwargs when processing audio data, ensuring the feature extractor only receives appropriate parameters. It prevents silent failures where audio input could be truncated to ~0.03 seconds, which would produce empty or degraded model outputs.

**Potential risks**  
If other text-specific kwargs are similarly misinterpreted by the feature extractor, they might still cause issues. Additionally, the fix assumes `truncation` and `max_length` are the only problematic parameters; future Hugging Face updates could introduce new conflicting kwargs. There's also a minor risk if downstream code relies on these kwargs being passed through for audio processing.

**Key insights**  
Always validate that preprocessing parameters are appropriate for the data modality. Consider implementing a more robust allowlist/denylist system for kwargs based on the processor type. Developers should test audio processing with varied input lengths to ensure no unintended truncation occurs.

---

## 12. [[Bugfix] Fix QK Norm+RoPE fusion pattern matching on B200+FP8](https://github.com/vllm-project/vllm/pull/33967)


### Base Information

- **PR Number:** #33967
- **Author:** [ikchifo](https://github.com/ikchifo)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-06 18:27:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33967/files) (5):**
  - `tests/compile/fusions_e2e/models.py`
  - `tests/compile/passes/test_qk_norm_rope_fusion.py`
  - `tests/compile/passes/test_split_coalescing.py`
  - `vllm/compilation/passes/pass_manager.py`
  - `vllm/compilation/passes/utility/split_coalescing.py`

### Summary

**What changed and why**  
This PR fixes a pattern matching issue in the QK Norm+RoPE fusion pass that occurs on B200 hardware with FP8-quantized models. The problem arises when PyTorch Inductor's CSE fails to merge identical `split_with_sizes` calls, resulting in three separate split nodes instead of one with three users. The solution introduces a `SplitCoalescingPass` that merges duplicate split nodes before the fusion pass runs, ensuring the pattern matcher sees the expected graph structure.

**Technical impact**  
The changes add a new utility pass (`SplitCoalescingPass`) that coalesces duplicate `split_with_sizes` nodes based on their input tensor and split sizes. This pass is inserted before the `QKNormRoPEFusionPass` in the pass manager, guaranteeing the fusion can proceed correctly. The fix is transparent to performance—benchmarks show negligible differences—but ensures fusion works on affected hardware/dtype combinations where it previously failed silently.

**Potential risks**  
The pass assumes all users of the split nodes are `getitem` operations; if other user types exist, they may not be handled correctly. There is also a risk that merging splits could interfere with other passes or optimizations that rely on the original graph structure. Additionally, the pass does not verify that the split sizes list comparison (`list(n.args[1]) == list(split_sizes)`) is safe for all possible tensor representations.

**Key insights**  
Developers should note that this is a workaround for an upstream CSE limitation in PyTorch Inductor (issue #174472). The pass is narrowly targeted and should only affect graphs with duplicate splits. When modifying fusion passes or graph transformations, consider similar CSE gaps that could cause pattern matching failures. The fix is validated with a new test that simulates the problematic graph structure, ensuring robustness.

---

## 13. [[Feat][RL] Pause and Resume with keep requests for single engine](https://github.com/vllm-project/vllm/pull/32351)


### Base Information

- **PR Number:** #32351
- **Author:** [hao-aaron](https://github.com/hao-aaron)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-06 16:08:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32351/files) (8):**
  - `examples/offline_inference/pause_resume.py`
  - `tests/v1/engine/test_async_llm.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/serve/rlhf/api_router.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`

### Summary

**What changed and why**  
This PR introduces a new "keep" mode for pause/resume functionality in vLLM's async engine. The changes add a `mode` parameter to pause APIs with three options: "abort" (default), "wait", and the new "keep" mode. "keep" mode freezes in-flight requests in the scheduler queue without aborting them, allowing them to resume generation when the engine is resumed. This enables RL workflows where requests need to be temporarily paused without losing progress.

**Technical impact**  
The implementation adds scheduler pause/resume state management to the core engine (`_scheduler_paused` flag) and exposes async methods (`pause_scheduler_async`, `resume_scheduler_async`) through the client layer. When "keep" mode is used, the engine stops scheduling work but preserves request state, while "abort" and "wait" modes maintain their original behaviors with proper deprecation handling for old parameters. The changes affect the async LLM interface, protocol definitions, and API endpoints.

**Potential risks**  
The "keep" mode is not supported with data-parallel configurations (`--api-server-count > 1`), which could cause confusion in distributed deployments. There's also a risk of resource leakage if requests remain frozen indefinitely during long pauses. The deprecation of `wait_for_inflight_requests` parameter requires careful migration for existing users. Edge cases around concurrent pause/resume calls need thorough testing to avoid deadlocks.

**Key insights**  
Developers should use the new `mode` parameter instead of deprecated `wait_for_inflight_requests`. The "keep" mode is ideal for RL workflows but requires single-engine deployments. Ensure proper cleanup of frozen requests to prevent resource exhaustion. The example script provides a good pattern for verifying pause behavior through timing gaps in token generation.

---

## 14. [[bugfix] [ROCm] Fix premature CUDA initialization in platform detection](https://github.com/vllm-project/vllm/pull/33941)


### Base Information

- **PR Number:** #33941
- **Author:** [kouroshHakha](https://github.com/kouroshHakha)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-06 14:17:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33941/files) (6):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/cuda.yaml`
  - `tests/cuda/scripts/check_device_count_respects_env.py`
  - `tests/cuda/scripts/check_platform_no_cuda_init.py`
  - `tests/cuda/test_platform_no_cuda_init.py`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
The fix addresses premature CUDA initialization during ROCm platform detection by replacing `torch.cuda.get_device_properties()` calls with `amdsmi` queries. This prevents CUDA from being initialized before Ray workers can set `CUDA_VISIBLE_DEVICES`, which previously caused all workers to incorrectly default to GPU 0.

**Technical impact**  
Platform detection functions now use `amdsmi` to retrieve GPU architecture without triggering CUDA initialization, preserving the ability to control GPU visibility via environment variables. The change maintains backward compatibility by falling back to `torch.cuda` if `amdsmi` fails, though this re-introduces the initialization issue in fallback scenarios.

**Potential risks**  
The `amdsmi` dependency may not be available in all environments, leading to fallback paths that still initialize CUDA prematurely. Additionally, the `amdsmi` API could return unexpected values or fail silently, potentially causing incorrect platform detection. Edge cases include systems without ROCm GPUs or with multiple GPU architectures.

**Key insights**  
Developers should ensure `amdsmi` is properly installed in ROCm environments to avoid fallback behavior. The added tests validate that CUDA remains uninitialized after imports and that `device_count()` respects environment variables, providing critical safeguards for multi-GPU setups. This change highlights the importance of lazy initialization in GPU-aware libraries.

---

## 15. [Fix RoutingMethodType logic](https://github.com/vllm-project/vllm/pull/33919)


### Base Information

- **PR Number:** #33919
- **Author:** [dbari](https://github.com/dbari)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 14:03:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33919/files) (9):**
  - `docker/Dockerfile`
  - `docker/Dockerfile.nightly_torch`
  - `docker/versions.json`
  - `requirements/cuda.txt`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_router.py`
  - `vllm/model_executor/layers/fused_moe/router/router_factory.py`

### Summary

**What changed and why**  
This PR fixes routing logic for MoE models by correcting `RoutingMethodType` selection in fused top-k routers and adding a fallback mechanism. Specifically, it addresses a regression where Mistral Large 3 (with `n_group=1` and `topk_group=1`) broke after previous changes because it uses `Renormalize` routing instead of `DeepSeekV3`. The fix ensures proper routing method detection and allows single-group configurations to fall back to non-grouped routers when no valid routing method is found.

**Technical impact**  
The changes introduce a centralized `get_routing_method_type()` function to standardize routing method determination based on `scoring_func`, `top_k`, and `renormalize` parameters. This improves consistency across router implementations. Additionally, the router factory now includes logic to fall back to standard top-k routing when `GroupedTopKRouter` has an unspecified routing method with only one group. The FlashInfer version is also updated to 0.6.3 to support these fixes.

**Potential risks**  
Re-enabling `RoutingMethodType.Renormalize` and `RenormalizeNaive` in FlashInfer TRTLLM kernels could reintroduce accuracy issues that were previously disabled (as noted in issue #33532). The fallback logic assumes single-group configurations are safe to route with standard top-k routers, which may not hold for all edge cases. There's also a temporary workaround adding `activation_type=ActivationType.Swiglu` that should be removed in future FlashInfer updates.

**Key insights**  
Developers should verify that the re-enabled renormalize routing methods don't cause accuracy regressions in affected models. The new `get_routing_method_type()` function provides a single source of truth for routing logic, reducing duplication. When updating dependencies, ensure FlashInfer 0.6.3 is required for these changes to work correctly. The fallback mechanism elegantly handles edge cases where grouped routing isn't necessary.

---

## 16. [[Fix] Fix `logprobs=0` handling for `/inference/v1/generate` endpoint](https://github.com/vllm-project/vllm/pull/34010)


### Base Information

- **PR Number:** #34010
- **Author:** [SumanthRH](https://github.com/SumanthRH)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-06 12:33:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34010/files) (2):**
  - `tests/entrypoints/openai/test_serving_tokens.py`
  - `vllm/entrypoints/serve/disagg/serving.py`

### Summary

**What changed and why**  
The PR fixes inconsistent handling of `logprobs=0` in the `/inference/v1/generate` endpoint. Previously, `logprobs=0` returned no logprobs, but it should return the chosen token's logprob (equivalent to `logprobs=1`). The change aligns this endpoint's behavior with the `/v1/completions` endpoint and Engine APIs.

**Technical impact**  
The fix modifies the condition from `if sampling_params.logprobs:` to `if sampling_params.logprobs is not None:` to correctly handle zero values. Additionally, it updates the logprobs filtering logic to ensure at least one top logprob is returned when `num_output_top_logprobs` is zero. This ensures consistent logprob output across endpoints.

**Potential risks**  
If other parts of the codebase rely on the previous behavior where `logprobs=0` returned no logprobs, they may break. The change also assumes that `num_output_top_logprobs` being `None` is handled elsewhere; if not, it could lead to unexpected behavior. Edge cases with negative or non-integer logprobs values are not addressed.

**Key insights**  
Developers should verify that all endpoints and client code expect the new behavior. The test suite now covers `logprobs=0, 1, 5`, but consider adding tests for edge cases like `logprobs=None` or large values. Ensure documentation is updated to reflect that `logprobs=0` now returns chosen token logprobs.

---

## 17. [[Bugfix] Fix no attribute error of SharedFusedMoE (DeepSeek-V3.1 as test model)](https://github.com/vllm-project/vllm/pull/33993)


### Base Information

- **PR Number:** #33993
- **Author:** [xuebwang-amd](https://github.com/xuebwang-amd)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-06 11:11:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33993/files) (2):**
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/shared_fused_moe.py`

### Summary

**What changed and why**  
The fix addresses an AttributeError in the SharedFusedMoE class initialization. The error occurred because `FusedMoE.__init__()` tried to access `self.shared_experts` before the `SharedFusedMoE` subclass had assigned it. The solution adds a `has_shared_experts` parameter to `FusedMoE.__init__()` and passes it from `SharedFusedMoE` to avoid the premature attribute access.

**Technical impact**  
This change modifies the initialization flow between parent and child classes in the fused MoE layer hierarchy. The `FusedMoE` base class now receives explicit information about shared experts via a parameter rather than inspecting subclass attributes, making the initialization order more robust and decoupled.

**Potential risks**  
The fix assumes `shared_experts is not None` accurately reflects the intended `has_shared_experts` state. If `shared_experts` could be an empty but non-None object (like an empty list), the logic might not behave as expected. Additionally, any other subclasses of `FusedMoE` must now handle the new `has_shared_experts` parameter.

**Key insights**  
The root cause was a classic Python initialization order issue in class inheritance. The fix is clean and avoids more invasive refactoring. Developers should ensure all callers of `FusedMoE` are updated to include the `has_shared_experts` parameter, and consider if similar patterns exist elsewhere in the codebase.

---

## 18. [[Rocm][Bugfix] Fix dtype not same for gemm_a4w4 op](https://github.com/vllm-project/vllm/pull/33734)


### Base Information

- **PR Number:** #33734
- **Author:** [charlifu](https://github.com/charlifu)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-06 11:09:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33734/files) (1):**
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py`

### Summary

**What changed and why**  
The fix addresses a dtype mismatch error in the `gemm_a4w4` operation where the weight tensor (uint8) doesn't match the expected input tensor type (float4_e2m1fn_x2). The change casts the weight tensor to match the data type of the quantized input tensor `x_q` before passing it to the kernel.

**Technical impact**  
This ensures type consistency between the quantized activation (`x_q`) and weight tensors when calling the low-level `gemm_a4w4` kernel. The `view()` operation reinterprets the weight tensor's dtype without changing underlying data, maintaining performance while resolving the compatibility error.

**Potential risks**  
Using `view()` for dtype conversion assumes the tensors have compatible memory layouts and byte sizes. If the weight tensor's underlying data isn't properly formatted for the target dtype, this could cause silent data corruption or runtime errors. The fix doesn't address why the weight tensor originally had an incompatible dtype.

**Key insights**  
Always validate tensor dtype compatibility before kernel calls, especially for custom low-level operations. Consider adding explicit dtype conversion with proper validation rather than relying solely on `view()`. The root cause investigation should verify why the weight tensor's dtype differs from what the quantization scheme produces.

---

## 19. [[Refactor] Remove align block size logic in `moe_permute`](https://github.com/vllm-project/vllm/pull/33449)


### Base Information

- **PR Number:** #33449
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 10:57:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33449/files) (8):**
  - `benchmarks/kernels/benchmark_moe_permute_unpermute.py`
  - `csrc/moe/moe_permute_unpermute_op.cu`
  - `csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu`
  - `csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.h`
  - `csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.inl`
  - `csrc/moe/torch_bindings.cpp`
  - `tests/kernels/moe/test_moe_permute_unpermute.py`
  - `vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py`

### Summary

**What changed and why**  
This PR removes the `align_block_size` logic and related DeepGEMM support from the `moe_permute` kernel. The code was complex and unused in the main branch, as the system currently uses `deepgemm_moe_permute` instead. Benchmark results show the removed logic was slower, and its removal also eliminates a `torch.full` operation for a minor performance gain.

**Technical impact**  
The changes significantly simplify the MoE permute/unpermute kernel by removing dead code paths for block alignment and the `m_indices` tensor generation. The kernel interface is now cleaner with fewer parameters and outputs. This reduces code complexity and maintenance burden, and aligns the implementation with the currently used execution path.

**Potential risks**  
The removal assumes the `align_block_size` logic is truly dead code. If any downstream systems or future configurations rely on this DeepGEMM alignment feature, they will break. The test suite has been updated to remove alignment-related tests, which is appropriate but means there's no regression coverage for that functionality if it's ever revived.

**Key insights**  
This is a positive cleanup that removes technical debt. Developers should verify that no experimental branches or configurations use the removed DeepGEMM alignment feature. The performance improvement, while minor, is a bonus. The simplified code will be easier to understand and modify for future optimizations.

---

## 20. [[Model Runner V2] support apply penalty for spec decode](https://github.com/vllm-project/vllm/pull/33251)


### Base Information

- **PR Number:** #33251
- **Author:** [izhuhaoran](https://github.com/izhuhaoran)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-06 10:56:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33251/files) (4):**
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/penalties.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`

### Summary

**What changed and why**  
This PR adds support for applying repetition/frequency/presence penalties during speculative decoding in Model Runner V2. The core change tracks token positions within speculative decoding sequences (`expanded_local_pos`) and modifies the penalty kernel to account for draft tokens when calculating token frequencies.

**Technical impact**  
The penalty calculation now correctly handles speculative decoding by accumulating counts from draft tokens across multiple positions within a request. This requires passing additional positional information (`expanded_local_pos`, `input_ids`) through the sampling pipeline and modifying the Triton kernel to compute cumulative draft token frequencies. The changes affect data structures in `InputBatch`, penalty application logic, and sampler initialization.

**Potential risks**  
The kernel now uses a static loop over `MAX_SPEC_LEN`, which assumes a fixed maximum speculative length. If actual speculative lengths exceed this, incorrect behavior may occur. Additionally, the increased data passing (`input_ids`, `expanded_local_pos`) through multiple layers could introduce overhead or errors if mismatched shapes occur. Edge cases with zero draft tokens or varying speculative lengths per request need careful validation.

**Key insights**  
Developers should ensure `num_speculative_tokens` is correctly configured to match the maximum speculative length used elsewhere. The kernel's static loop limit must be reviewed if speculative decoding configurations change. Testing should verify penalty behavior consistency between speculative and non-speculative decoding paths, especially for batched requests with mixed draft token counts.

---

## 21. [[DOC] [ROCm] Update docker deployment doc](https://github.com/vllm-project/vllm/pull/33971)


### Base Information

- **PR Number:** #33971
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-06 10:05:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33971/files) (4):**
  - `docs/deployment/docker.md`
  - `docs/getting_started/installation/gpu.cuda.inc.md`
  - `docs/getting_started/installation/gpu.md`
  - `docs/getting_started/installation/gpu.rocm.inc.md`

### Summary

**What changed and why**  
This PR restructures Docker documentation by moving detailed Docker instructions from the deployment section into the GPU-specific installation guides. The main deployment page now references snippets from the installation section, creating a single source of truth for Docker content across different GPU platforms (CUDA, ROCm, Intel).

**Technical impact**  
The changes centralize Docker documentation within GPU-specific installation files, eliminating duplication and improving maintainability. This modular approach allows platform-specific details (like ROCm device flags or CUDA build args) to stay with their respective installation guides while keeping the main deployment page clean and reference-based.

**Potential risks**  
There's a risk of broken links if snippet references (`--8<--`) are misaligned or if future edits to the GPU installation files don't maintain the exact snippet boundaries. Additionally, users accustomed to finding all Docker details in the deployment section may initially overlook the restructured content.

**Key insights**  
This is a documentation refactoring that improves consistency and reduces duplication. Developers should verify all snippet references work correctly and ensure any future Docker-related updates are made in the GPU installation files rather than the deployment page. The change also better aligns documentation structure with the multi-platform support strategy.

---

## 22. [[KV Connector] Add missing method overrides to MultiConnector](https://github.com/vllm-project/vllm/pull/33292)


### Base Information

- **PR Number:** #33292
- **Author:** [eicherseiji](https://github.com/eicherseiji)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-06 09:58:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33292/files) (2):**
  - `tests/v1/kv_connector/unit/test_multi_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py`

### Summary

**What changed and why**  
This PR adds missing method overrides to the `MultiConnector` class to ensure proper delegation to its sub-connectors. The changes include handshake methods for NixlConnector support (`get_handshake_metadata`, `set_xfer_handshake_metadata`) and worker-side methods (`set_host_xfer_buffer_ops`, `handle_preemptions`, `get_finished_count`, `get_kv_connector_kv_cache_events`). A new test validates that all base class methods are overridden, and an integration test confirms `handle_preemptions` delegation.

**Technical impact**  
The updates ensure `MultiConnector` correctly wraps other KV connectors by delegating all required operations, maintaining compatibility with connectors like NixlConnector. The system now properly handles handshake metadata propagation, preemption events, and xPU-specific buffer operations across multiple sub-connectors. The test suite is enhanced to prevent future regressions in method delegation.

**Potential risks**  
The `get_finished_count` method returns `None` as a placeholder, which may need adjustment if sub-connectors later return non-None values (see issue #33400). The `get_kv_connector_kv_cache_events` method remains unimplemented (tracked in PR #31811), which could cause issues if events from multiple connectors need merging. The delegation pattern assumes all sub-connectors implement the same interface, which may not hold for future connector types.

**Key insights**  
Developers should note the added test that enforces all `KVConnectorBase_V1` methods are overridden, reducing the risk of missing delegations. The `get_handshake_metadata` method returns the first non-None metadata from sub-connectors, which may need reconsideration if metadata conflict resolution is required. The unimplemented `get_kv_connector_kv_cache_events` should be prioritized to ensure full functionality.

---

## 23. [[Log] Optimize duplicate startup log](https://github.com/vllm-project/vllm/pull/33944)


### Base Information

- **PR Number:** #33944
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-06 09:49:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33944/files) (3):**
  - `vllm/compilation/backends.py`
  - `vllm/utils/deep_gemm.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The changes replace standard `logger.info` calls with `logger.info_once` across three files to eliminate duplicate startup logs. The primary issue was repeated log messages like "Using V2 Model Runner" appearing multiple times during initialization. The `scope` parameter is consistently added to control log deduplication boundaries.

**Technical impact**  
These changes reduce log noise and improve startup log clarity by ensuring certain informational messages are logged only once per specified scope. The logging behavior becomes more deterministic, with "local" scope preventing duplicates within the same module/context, while maintaining the "global" scope for truly one-time system-wide messages.

**Potential risks**  
The main risk is losing visibility into legitimate repeated operations that should be logged. If `logger.info_once` with "local" scope is called from different execution contexts that should be tracked separately, important diagnostic information could be suppressed. There's also a risk of scope misuse where "local" might be too restrictive or "global" too broad.

**Key insights**  
The pattern shows systematic cleanup of duplicate logs, but developers should carefully consider the appropriate scope for each log statement. "local" scope is appropriate for module-specific initialization messages, while "global" should be reserved for truly system-wide one-time events. When adding new log statements, evaluate whether the message should appear once per context or multiple times for debugging purposes.

---

## 24. [[Bugfix] Fix the issue where tool calling does not work when using fast detokenization with dsv32](https://github.com/vllm-project/vllm/pull/33964)


### Base Information

- **PR Number:** #33964
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-06 09:23:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33964/files) (1):**
  - `vllm/tool_parsers/deepseekv32_tool_parser.py`

### Summary

**What changed and why**  
Added an `adjust_request` method to the DeepSeekV32 tool parser that sets `skip_special_tokens=False` when tools are enabled. This fixes a bug where tool-calling tokens like `<｜DSML｜function_calls>` were being incorrectly skipped during fast detokenization with the DSV32 model, preventing proper tool call parsing.

**Technical impact**  
This change ensures the tokenizer's special token handling is disabled during decoding when tools are active, allowing the model's structured tool-call tokens to be properly preserved and parsed. The fix maintains compatibility with both fast and slow detokenization paths and specifically addresses behavior changes in Transformers 5.x.

**Potential risks**  
The change could potentially affect other special tokens that should legitimately be skipped during decoding when tools are not in use. There's a minor risk of increased token output when `skip_special_tokens=False` is applied globally for the request, though this is limited to tool-enabled scenarios.

**Key insights**  
The fix correctly identifies that DSV32's tool-call tokens aren't marked as special tokens in the tokenizer but still need preservation. Developers should verify this doesn't interfere with other special token handling in the codebase and consider whether this adjustment should be model-specific or applied more broadly to other tool parsers.

---

## 25. [[Docs] Update link to Benchmark CLI documentation](https://github.com/vllm-project/vllm/pull/33254)


### Base Information

- **PR Number:** #33254
- **Author:** [eldarkurtic](https://github.com/eldarkurtic)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-06 08:01:00
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33254/files) (1):**
  - `benchmarks/README.md`

### Summary

**What changed and why**  
Updated a documentation link in the benchmarks README from an old URL (`contributing/benchmarks.html#benchmark-cli`) to a new one (`benchmarking/cli/#benchmark-cli`). This change corrects a broken or outdated link, ensuring users can access the correct Benchmark CLI documentation.

**Technical impact**  
This is a documentation-only change with no impact on the codebase, runtime behavior, or system architecture. It solely improves the accuracy of the developer documentation.

**Potential risks**  
The primary risk is if the new URL path is also incorrect or becomes outdated in the future. There is a minimal risk of a typo in the new link string itself.

**Key insights**  
Always verify that documentation links point to active, correct pages. Consider implementing automated link checking in the CI/CD pipeline to prevent broken links in documentation. For documentation PRs, confirming the target page's content is advisable.

---

## 26. [[XPU][5/N] add wna16 xpu kernel](https://github.com/vllm-project/vllm/pull/33973)


### Base Information

- **PR Number:** #33973
- **Author:** [zufangzhu](https://github.com/zufangzhu)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-06 07:59:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33973/files) (2):**
  - `.buildkite/scripts/hardware_ci/run-xpu-test.sh`
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu.py`

### Summary

**What changed and why**  
This PR adds XPU kernel support for WNA16 compressed tensors (specifically uint4 and uint4b8 quantization types) as part of a series to enhance XPU hardware support. The changes include updating the CI test script to validate the new kernel with a GPTQ-Int4 model and refactoring the XPUwNa16LinearKernel to replace IPEX-based implementation with a custom XPU kernel (`torch.ops._xpu_C.int4_gemm_w4a16`).

**Technical impact**  
The kernel now directly calls a low-level XPU GEMM operation instead of relying on IPEX libraries, reducing external dependencies and potentially improving performance for supported quantization types. The implementation enforces strict alignment requirements (input/output sizes and group sizes must be multiples of 32) and supports BF16/FP16 activations. This change aligns the kernel more closely with vLLM's internal quantization framework.

**Potential risks**  
The removal of IPEX dependency may break compatibility with existing CPU-based workflows that relied on it. Strict size constraints (multiples of 32) could cause failures for models with incompatible dimensions. The new kernel assumes specific tensor layouts (transposed and contiguous), which may introduce subtle bugs if weight tensors are not preprocessed correctly.

**Key insights**  
Developers should verify that all target models meet the new size constraints before using this kernel. The CI test addition is crucial for validating real-world usage with GPTQ models. Consider adding runtime checks for tensor alignment and documenting the transition from IPEX to native XPU ops for users migrating from older versions.

---

## 27. [[Refactor] Consolidate sequence normalization and enc-dec parsing](https://github.com/vllm-project/vllm/pull/33928)


### Base Information

- **PR Number:** #33928
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 07:43:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33928/files) (38):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/renderers/inputs/__init__.py`
  - `tests/renderers/inputs/test_preprocess.py`
  - `tests/renderers/test_completions.py`
  - `tests/renderers/test_mistral.py`
  - `tests/test_inputs.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/utils.py`
  - `vllm/inputs/__init__.py`
  - `vllm/inputs/data.py`
  - `vllm/inputs/parse.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/platforms/interface.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/inputs/__init__.py`
  - `vllm/renderers/inputs/preprocess.py`
  - `vllm/renderers/inputs/tokenize.py`
  - `vllm/renderers/mistral.py`
  - `vllm/renderers/protocol.py`
  - `vllm/renderers/terratorch.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/utils.py`

### Summary

**What changed and why**  
This PR refactors input handling to consolidate sequence normalization and encoder-decoder parsing logic. It introduces new `*DictPrompt` classes to replace the previous `Parsed*Prompt` classes, moving prompt standardization and parsing into a dedicated `vllm.renderer.inputs.parse` module. The changes also rename `render_completion(s)` to `render_prompt(s)` and remove dead code related to encoder-decoder models, while updating documentation and improving type safety across the codebase.

**Technical impact**  
The refactor centralizes input preprocessing logic, reducing duplication and improving maintainability. By introducing standardized `DictPrompt` and `TokPrompt` types, the system now has clearer boundaries between raw inputs, standardized dictionaries, and tokenized prompts. This enables better support for encoder-decoder models and simplifies the renderer interface. The changes affect multiple entrypoints and engine components, requiring updates to type annotations and method calls throughout.

**Potential risks**  
- The widespread type changes (`PromptType \| DictPrompt \| TokPrompt`) may introduce runtime errors if any call sites pass incompatible types.  
- Removing dead code like `build_explicit_enc_dec_prompt` could break external integrations that relied on those utilities.  
- The consolidation of logic into new modules may cause import errors or circular dependencies if not properly managed.  
- Changes to the renderer interface (e.g., `render_completion` → `render_prompt`) could affect downstream custom renderers.

**Key insights**  
- The refactor significantly improves code organization by separating concerns between input parsing, standardization, and tokenization.  
- Developers should update any custom renderers or input processors to use the new `DictPrompt`/`TokPrompt` types and renamed methods.  
- The temporary use of union types (`PromptType \| DictPrompt \| TokPrompt`) indicates a transitional state; future simplifications to `TokPrompt` are planned once all APIs use the Renderer.  
- Thorough testing is essential, especially for encoder-decoder and multi-modal workflows, due to the extensive changes in input handling paths.

---

## 28. [[Model] Support MiniCPM-o 4.5](https://github.com/vllm-project/vllm/pull/33431)


### Base Information

- **PR Number:** #33431
- **Author:** [tc-mb](https://github.com/tc-mb)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 07:29:11
- **Type:** `None`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33431/files) (1):**
  - `vllm/model_executor/models/minicpmo.py`

### Summary



---

## 29. [[Docs] Add sections on process architecture and minimum CPU resources](https://github.com/vllm-project/vllm/pull/33940)


### Base Information

- **PR Number:** #33940
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-02-06 07:26:43
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33940/files) (4):**
  - `docs/assets/design/arch_overview/v1_process_architecture_tp2_dp4.png`
  - `docs/assets/design/arch_overview/v1_process_architecture_tp4.png`
  - `docs/configuration/optimization.md`
  - `docs/design/arch_overview.md`

### Summary

**What changed and why**  
This PR adds documentation to clarify vLLM's multi-process architecture and provide explicit CPU resource recommendations. The changes address user confusion about performance degradation when running with insufficient CPU cores by detailing the process types, their counts, and minimum CPU requirements.

**Technical impact**  
The documentation updates provide critical operational guidance for deployment sizing, directly linking architectural understanding to resource provisioning. This helps users avoid common performance bottlenecks by properly allocating CPU resources based on their GPU count and parallelism configuration.

**Potential risks**  
The recommendations assume typical deployment scenarios and may not cover all edge cases or specialized hardware configurations. Users might misinterpret the distinction between physical cores and vCPUs, leading to under-provisioning in virtualized environments. The diagrams, while helpful, are static and may become outdated with architectural changes.

**Key insights**  
Developers should treat `2 + N` physical CPU cores as an absolute minimum for `N` GPUs, with additional cores recommended for optimal performance. The engine core's busy loop makes it particularly sensitive to CPU starvation. When troubleshooting low GPU utilization, CPU contention should be investigated as a primary bottleneck.

---

## 30. [[ROCm][AITER] Fix AITER import regression for explicit backend selection](https://github.com/vllm-project/vllm/pull/33749)


### Base Information

- **PR Number:** #33749
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-06 07:08:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33749/files) (5):**
  - `tests/kernels/attention/test_aiter_flash_attn.py`
  - `vllm/_aiter_ops.py`
  - `vllm/v1/attention/backends/fa_utils.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
This PR fixes a regression that prevented explicit AITER backend selection on ROCm when the `VLLM_ROCM_USE_AITER` environment variable is unset or set to 0. The issue occurred because availability checks incorrectly incorporated the environment variable, blocking explicit user requests. The fix separates capability detection from user preference, ensuring explicit backend selection works regardless of the environment variable.

**Technical impact**  
The changes introduce a clear separation between AITER availability (platform + library + architecture) and preference (availability + environment variable). This allows the system to honor explicit user configuration via `attention_config` while maintaining the ability to suppress JIT compilation warnings during auto-discovery. The architecture now supports both automatic backend selection (respecting env var) and explicit override (ignoring env var).

**Potential risks**  
The unconditional import of AITER modules when explicitly selected could still trigger JIT compilation warnings if the environment variable is 0, though this is now intentional. There's a risk of subtle behavior differences between auto-discovery and explicit selection paths. The test migration to a two-step gather-then-attend process must accurately reflect production behavior to avoid test coverage gaps.

**Key insights**  
Developers should understand that `is_aiter_found_and_supported()` now only checks technical capability, while `rocm_aiter_ops.is_enabled()` adds the environment variable check for preference. Explicit backend selection bypasses the environment variable entirely. When modifying AITER-related code, ensure both auto-discovery and explicit selection paths are tested with different environment variable settings.

---

## 31. [[FIX] guidance: use max(vocab_size, len(tokenizer)) for n_vocab](https://github.com/vllm-project/vllm/pull/33509)


### Base Information

- **PR Number:** #33509
- **Author:** [FredericOdermatt](https://github.com/FredericOdermatt)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-06 06:23:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33509/files) (1):**
  - `vllm/v1/structured_output/backend_guidance.py`

### Summary

**What changed and why**  
The change replaces `self.vocab_size` with `max(self.vocab_size, len(self.tokenizer))` when passing `n_vocab` to `llguidance_hf.from_tokenizer()`. This addresses a mismatch where some models (e.g., Gemma 3, Llama 3.2) have more tokens in the tokenizer than indicated by the model's `config.vocab_size`, causing llguidance to error.

**Technical impact**  
This ensures compatibility with models where added special tokens expand the tokenizer beyond the base vocabulary size. The `max()` function handles both cases: it preserves the larger `vocab_size` for TP/GPU-padded models (e.g., DeepSeek, Qwen) while preventing failures when `len(tokenizer)` exceeds `vocab_size`.

**Potential risks**  
Using `max()` could inadvertently pass a larger-than-necessary `n_vocab` if `vocab_size` is excessively padded, though this aligns with llguidance's expectation for padded models. Edge cases where `len(tokenizer)` is smaller than `vocab_size` remain unaffected, but the change assumes `vocab_size` is the intended upper bound when larger.

**Key insights**  
The fix elegantly resolves a common Hugging Face inconsistency between config, tokenizer, and actual token counts. Developers should be aware that `config.vocab_size` may not reflect the true tokenizer size, and similar issues could arise in other components relying solely on config values. Consider adding a comment explaining the `max()` logic for future maintainability.

---

## 32. [[Bugfix] Fix models and tests for transformers v5](https://github.com/vllm-project/vllm/pull/33977)


### Base Information

- **PR Number:** #33977
- **Author:** [zucchini-nlp](https://github.com/zucchini-nlp)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 05:47:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33977/files) (12):**
  - `docs/models/supported_models.md`
  - `tests/models/multimodal/generation/test_qwen2_vl.py`
  - `tests/models/multimodal/pooling/test_llava_next.py`
  - `tests/models/multimodal/test_mapping.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/minimax_vl_01.py`
  - `vllm/transformers_utils/processors/bagel.py`
  - `vllm/transformers_utils/processors/hunyuan_vl.py`
  - `vllm/transformers_utils/processors/ovis.py`
  - `vllm/transformers_utils/processors/ovis2_5.py`

### Summary

**What changed and why**  
This PR updates vLLM to be compatible with Transformers v5 while maintaining v4 compatibility. Changes include fixing processor parameter names, adjusting model configurations, updating documentation, and aligning test code with new Transformers APIs.

**Technical impact**  
The changes ensure seamless operation across Transformers v4 and v5 by updating deprecated imports, parameter names, and processor methods. Key updates include replacing `convert_to_rgb` with `do_convert_rgb` in OVIS processors, fixing tied weight handling, and correcting model configuration references (e.g., `pad_token_id` access). These adjustments prevent runtime errors and maintain consistent multimodal processing.

**Potential risks**  
- The removal of `video_processor` from `HunYuanVLProcessor` could break video processing if the parent class relies on it.  
- Changes to processor kwargs (e.g., moving `max_partition` out of `images_kwargs`) may cause silent failures if callers pass these parameters incorrectly.  
- The update to `AudioFlamingo3`'s audio limit from `None` to `1` could restrict functionality for multi-audio inputs unless intentional.

**Key insights**  
- Always verify that parameter renames (e.g., `convert_to_rgb` → `do_convert_rgb`) are consistently applied across all processor calls.  
- Ensure removed parameters (like `video_processor`) are genuinely unused in parent classes to avoid inheritance issues.  
- Test multimodal models thoroughly to confirm audio/video/image processing remains functional across both Transformers versions.

---

## 33. [Update `WeightTransferConfig` to be more standard like the others](https://github.com/vllm-project/vllm/pull/33989)


### Base Information

- **PR Number:** #33989
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 05:15:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33989/files) (2):**
  - `vllm/config/weight_transfer.py`
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
The PR updates the `WeightTransferConfig` class to use the `@config` decorator (a dataclass transform) instead of the explicit `@dataclass` decorator, aligning it with other configuration classes. Additionally, the default value for `weight_transfer_config` in `EngineArgs` is now sourced from a central configuration (`VllmConfig`) via `get_field`, eliminating duplicate default definitions.

**Technical impact**  
These changes centralize default configuration management, reducing duplication and ensuring consistency across the codebase. The `@config` decorator likely provides enhanced serialization/deserialization capabilities or validation, standardizing configuration handling with other similar classes in the project.

**Potential risks**  
If the `@config` decorator does not fully replicate `@dataclass` behavior (e.g., `__post_init__` support or field ordering), it could introduce subtle bugs. Relying on `get_field` from `VllmConfig` assumes that the central configuration is always properly initialized, which might cause issues if defaults are missing or incorrectly defined.

**Key insights**  
Developers should verify that `@config` supports all required dataclass features and that `VllmConfig` provides appropriate defaults. This pattern improves maintainability but requires careful testing to ensure backward compatibility, especially for serialization paths that depend on `WeightTransferConfig`.

---

## 34. [[Docs] Improve documentation](https://github.com/vllm-project/vllm/pull/33799)


### Base Information

- **PR Number:** #33799
- **Author:** [SorenDreano](https://github.com/SorenDreano)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-06 04:57:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33799/files) (4):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/openai_batch/README.md`
  - `vllm/config/model.py`
  - `vllm/entrypoints/llm.py`

### Summary

**What changed and why**  
This PR updates vLLM documentation and code comments to reflect current Hugging Face tooling and paths. It replaces deprecated `huggingface_cli` references with the modern `hf` command and updates token storage location from `~/.huggingface` to `~/.cache/huggingface/token` per Hugging Face's migration guidance.

**Technical impact**  
These changes have no runtime impact on vLLM's functionality since they only affect documentation strings and comments. However, they ensure users receive correct instructions for interacting with Hugging Face Hub, preventing confusion and failed authentication/download attempts when following outdated documentation.

**Potential risks**  
The main risk is incomplete migration if other undocumented references to the old CLI or token path exist elsewhere in the codebase. Users with tokens stored in the old location might experience authentication issues if vLLM's underlying Hugging Face libraries now exclusively check the new path, though this depends on the huggingface_hub library's backward compatibility.

**Key insights**  
This is a well-scoped documentation update that aligns vLLM with Hugging Face's current tooling. Developers should verify no other references to `huggingface-cli` or `~/.huggingface` remain in the codebase, and consider adding a note about token migration for users who may have credentials stored at the old location.

---

## 35. [[Bugfix][Model] Support LoRA on Qwen3 Output Embedding](https://github.com/vllm-project/vllm/pull/29816)


### Base Information

- **PR Number:** #29816
- **Author:** [klshuster](https://github.com/klshuster)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-06 04:25:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29816/files) (6):**
  - `tests/lora/test_qwen3_unembed.py`
  - `vllm/lora/layers/logits_processor.py`
  - `vllm/lora/layers/vocal_parallel_embedding.py`
  - `vllm/lora/model_manager.py`
  - `vllm/model_executor/models/qwen3.py`
  - `vllm/model_executor/models/qwen3_moe.py`

### Summary

**What changed and why**  
This PR adds LoRA support for the output embedding layer (`lm_head`) in Qwen3 dense and MoE models. It simplifies a previous approach by removing zero-padded vocabulary handling, which was already addressed separately. The changes include adding `embedding_modules` mappings to the model classes and adjusting LoRA weight initialization and logit computation to properly handle the `lm_head` layer wrapped by `LogitsProcessorWithLoRA`.

**Technical impact**  
The modifications enable LoRA adapters to affect the model's output logits via the `lm_head` layer, expanding fine-tuning capabilities for Qwen3 models. The `LogitsProcessorWithLoRA` now correctly accesses the underlying `lm_head` base layer, and dummy LoRA weight creation accounts for the wrapped structure. This aligns Qwen3's LoRA support with other models in the codebase.

**Potential risks**  
If the `lm_head` layer is not properly wrapped by `LogitsProcessorWithLoRA` in all scenarios, the fallback logic in `logits_processor.py` could fail. The dummy weight initialization assumes consistent layer attributes, which might break with future architectural changes. The test coverage is limited to synthetic weights and may not catch edge cases with real LoRA adapters.

**Key insights**  
Developers should ensure that any new model adding LoRA support follows the same pattern of defining `embedding_modules`. The `hasattr(lm_head, "base_layer")` check provides a robust way to handle wrapped layers. When extending LoRA to other embedding layers, verify that input/output dimensions are correctly derived, as done for `lm_head` in `model_manager.py`.

---

## 36. [[torch.compile] Reorganize vllm/compilation and tests/compile (0/N for vLLM IR)](https://github.com/vllm-project/vllm/pull/33731)


### Base Information

- **PR Number:** #33731
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 04:19:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33731/files) (47):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/compile.yaml`
  - `.buildkite/test_areas/pytorch.yaml`
  - `tests/compile/backend.py`
  - `tests/compile/correctness_e2e/__init__.py`
  - `tests/compile/correctness_e2e/test_async_tp.py`
  - `tests/compile/correctness_e2e/test_sequence_parallel.py`
  - `tests/compile/fusions_e2e/common.py`
  - `tests/compile/passes/__init__.py`
  - `tests/compile/passes/distributed/__init__.py`
  - `tests/compile/passes/distributed/test_async_tp.py`
  - `tests/compile/passes/distributed/test_fusion_all_reduce.py`
  - `tests/compile/passes/distributed/test_sequence_parallelism.py`
  - `tests/compile/passes/test_functionalization.py`
  - `tests/compile/passes/test_fuse_act_padding.py`
  - `tests/compile/passes/test_fusion.py`
  - `tests/compile/passes/test_fusion_attn.py`
  - `tests/compile/passes/test_noop_elimination.py`
  - `tests/compile/passes/test_pass_manager.py`
  - `tests/compile/passes/test_qk_norm_rope_fusion.py`
  - `tests/compile/passes/test_silu_mul_quant_fusion.py`
  - `tests/compile/test_compile_ranges.py`
  - `tests/compile/test_config.py`
  - `tests/compile/test_graph_partition.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/passes/__init__.py`
  - `vllm/compilation/passes/fusion/__init__.py`
  - `vllm/compilation/passes/fusion/act_quant_fusion.py`
  - `vllm/compilation/passes/fusion/allreduce_rms_fusion.py`
  - `vllm/compilation/passes/fusion/attn_quant_fusion.py`
  - `vllm/compilation/passes/fusion/collective_fusion.py`
  - `vllm/compilation/passes/fusion/matcher_utils.py`
  - `vllm/compilation/passes/fusion/qk_norm_rope_fusion.py`
  - `vllm/compilation/passes/fusion/rms_quant_fusion.py`
  - `vllm/compilation/passes/fusion/rocm_aiter_fusion.py`
  - `vllm/compilation/passes/fusion/sequence_parallelism.py`
  - `vllm/compilation/passes/fx_utils.py`
  - `vllm/compilation/passes/inductor_pass.py`
  - `vllm/compilation/passes/pass_manager.py`
  - `vllm/compilation/passes/utility/__init__.py`
  - `vllm/compilation/passes/utility/fix_functionalization.py`
  - `vllm/compilation/passes/utility/noop_elimination.py`
  - `vllm/compilation/passes/utility/post_cleanup.py`
  - `vllm/compilation/passes/vllm_inductor_pass.py`
  - `vllm/config/compilation.py`
  - `vllm/platforms/interface.py`

### Summary

**What changed and why**  
This PR reorganizes the compilation codebase by moving and renaming files to prepare for the upcoming vLLM IR. The changes are purely structural—no functional modifications—and involve relocating compilation passes, tests, and buildkite configurations into clearer directories (e.g., `vllm/compilation/passes/`, `tests/compile/passes/`). The goal is to create a cleaner separation between passes, utilities, and tests to accommodate future IR-related additions.

**Technical impact**  
The reorganization improves codebase maintainability by grouping related compilation logic (e.g., fusion passes, utility passes) into dedicated subdirectories. Buildkite test pipelines are updated to reflect the new file paths, ensuring CI runs correctly. Import paths across the codebase are consistently adjusted, which may affect downstream developers who directly import from these modules.

**Potential risks**  
- Import errors could arise if any external code references the old module paths (though this is internal code).  
- The large number of file moves (47 files) increases the risk of merge conflicts with concurrent PRs.  
- Subtle issues may surface if any file references (e.g., in logs or dynamic imports) were not updated.

**Key insights**  
- This is a foundational refactoring that sets the stage for vLLM IR integration. Developers should verify their local environments and update any custom imports accordingly.  
- The PR intentionally avoids functional changes, so any behavioral differences in CI should be investigated as potential oversights.  
- Future work on vLLM IR will build upon this structure, so familiarity with the new organization is recommended.

---

## 37. [[CPU][BugFix] Fix loading of w4a8int models with bias](https://github.com/vllm-project/vllm/pull/33582)


### Base Information

- **PR Number:** #33582
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-06 03:59:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33582/files) (1):**
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit.py`

### Summary

**What changed and why**  
The fix addresses a TypeError when loading w4a8int quantized models with bias in MLP layers. Previously, direct tensor assignment to `layer.bias` caused type mismatches; now it properly uses `torch.nn.Parameter` with `requires_grad=False` via a `replace_parameter` helper.

**Technical impact**  
This change ensures compatibility with models containing bias parameters in w4a8int quantization by maintaining correct parameter types during weight loading. It preserves the float32 dtype requirement for bias in Float32/Bfloat16 variants while adhering to PyTorch's parameter constraints.

**Potential risks**  
If `replace_parameter` isn't properly implemented or handles edge cases incorrectly, it could introduce subtle bugs. The change assumes bias conversion to float32 is always appropriate for these quantization variants, which may not hold for all model architectures.

**Key insights**  
Always use proper PyTorch parameter assignment methods when modifying layer parameters. The fix demonstrates good practice by maintaining parameter metadata (requires_grad) while fixing the immediate type error. Consider adding validation for the `replace_parameter` function's behavior in the test plan.

---

## 38. [Bump HF Hub client to get bug fix](https://github.com/vllm-project/vllm/pull/33984)


### Base Information

- **PR Number:** #33984
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 03:25:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33984/files) (2):**
  - `requirements/rocm-test.txt`
  - `requirements/test.txt`

### Summary

**What changed and why**  
Updated the Hugging Face Hub client from version 0.36.1 to 0.36.2 in both `requirements/rocm-test.txt` and `requirements/test.txt` to incorporate a bug fix from PR #3778 in the upstream repository.

**Technical impact**  
This minor version bump ensures the test environments use the patched version, potentially resolving issues related to the specific bug fix. It maintains compatibility since it's a patch-level update within the same minor release.

**Potential risks**  
Low risk given it's a patch release focused on bug fixes, but there's a slight chance of unintended side effects if the fix introduces new issues or interacts unexpectedly with other dependencies in the test environment.

**Key insights**  
Always verify that patch updates align with the specific bug being addressed and consider running targeted tests to confirm the fix resolves the intended issue without regressions.

---

## 39. [[PaddleOCR-VL] Add BC for transformers 5.0 config](https://github.com/vllm-project/vllm/pull/33976)


### Base Information

- **PR Number:** #33976
- **Author:** [zhang-prog](https://github.com/zhang-prog)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-06 02:33:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33976/files) (1):**
  - `vllm/model_executor/models/paddleocr_vl.py`

### Summary

**What changed and why**  
Added backward compatibility handling for transformers 5.0 configs in the PaddleOCR-VL model. The change extracts `text_config` from the Hugging Face config, removes certain unsafe keys, and merges the remaining text configuration into the main config object.

**Technical impact**  
This ensures the model initializes correctly with newer transformers library versions by properly merging nested configuration structures. The update maintains compatibility while preventing potential conflicts from keys that shouldn't be overwritten during the merge process.

**Potential risks**  
If `text_config` is missing expected keys after removal, it could lead to incomplete configuration. The hardcoded `unsafe_keys` list may need updates if future transformers versions introduce new conflicting keys. There's also a risk of silent failures if `text_config.to_dict()` returns an unexpected structure.

**Key insights**  
This is a targeted fix for transformers 5.0 compatibility—developers should verify similar updates aren't needed for other model implementations. Consider adding logging or validation to ensure the merged config contains all required fields. The approach of filtering unsafe keys before merging is a good pattern for similar configuration compatibility issues.

---

## 40. [Consolidate and fix forbidden import `pre-commit` checks](https://github.com/vllm-project/vllm/pull/33982)


### Base Information

- **PR Number:** #33982
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 01:47:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33982/files) (5):**
  - `.pre-commit-config.yaml`
  - `tools/pre_commit/check_forbidden_imports.py`
  - `tools/pre_commit/check_pickle_imports.py`
  - `tools/pre_commit/check_triton_import.py`
  - `tools/pre_commit/enforce_regex_import.py`

### Summary

**What changed and why**  
This PR consolidates three separate pre-commit checks (`enforce_regex_import.py`, `check_triton_import.py`, and `check_pickle_imports.py`) into a single unified script, `check_forbidden_imports.py`. The primary motivation was to fix a critical bug where the regex and triton checks were ineffective in CI environments because they relied on parsing `git diff` output, which is empty during CI runs. The new consolidated script correctly consumes the file list passed by the `pre-commit` framework.

**Technical impact**  
The changes significantly improve the reliability and maintainability of the forbidden import checks. The architecture shifts from three independent, diff-based scripts to a single, data-driven script that processes all staged files directly. This ensures the checks work consistently across local development and CI environments. The configuration is now centralized in a `CHECK_IMPORTS` dictionary, making it easier to add new forbidden import rules or modify existing ones.

**Potential risks**  
A key risk is the potential for false positives or negatives due to the new regex patterns, especially for complex import statements. The consolidation also removes the previous `git diff` parsing logic, which could have caught imports in non-Python files or in specific git states; the new script only checks files passed by `pre-commit`. Additionally, the `allowed_files` sets are now duplicated within the new script, so any future updates to the allowlist must be made in this single location.

**Key insights**  
Developers should note that the forbidden import checks are now fully functional in CI. The new script's structure is more maintainable, but care must be taken when updating the `CHECK_IMPORTS` configuration, particularly the regex patterns and allowlists. The removal of three separate files simplifies the pre-commit hook configuration, reducing cognitive overhead and potential configuration errors.

---

## 41. [support view_from_cpu_tensor on XPU](https://github.com/vllm-project/vllm/pull/33868)


### Base Information

- **PR Number:** #33868
- **Author:** [xinyu-intel](https://github.com/xinyu-intel)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-06 00:34:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33868/files) (4):**
  - `tests/kernels/core/test_uva.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/utils/torch_utils.py`
  - `vllm/v1/worker/gpu/buffer_utils.py`

### Summary

**What changed and why**  
The PR adds XPU support for obtaining accelerator views from CPU tensors via Unified Virtual Addressing (UVA). It renames `get_cuda_view_from_cpu_tensor` to `get_accelerator_view_from_cpu_tensor` and adds a conditional branch to handle XPU devices, enabling compatibility with ModelRunnerV2 on XPU platforms.

**Technical impact**  
This change abstracts device-specific UVA operations behind a unified interface, making the codebase more extensible for additional accelerators. The function now dynamically selects the appropriate backend (CUDA or XPU) based on the current platform, reducing hardcoded device assumptions and improving cross-platform support.

**Potential risks**  
The XPU path relies on `torch.ops._C.get_xpu_view_from_cpu_tensor`, which may not be available in all PyTorch builds or could have different behavior than the CUDA equivalent. Additionally, the renaming of the function requires updates across the codebase, which appears incomplete if other files still reference the old name.

**Key insights**  
Developers should verify that `torch.ops._C.get_xpu_view_from_cpu_tensor` exists and behaves identically to the CUDA version. Ensure all references to the old function name are updated to prevent runtime errors. Consider adding a fallback or validation for unsupported platforms to improve robustness.

---

## 42. [Fix `main` pre-commit](https://github.com/vllm-project/vllm/pull/33975)


### Base Information

- **PR Number:** #33975
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-06 00:08:06
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33975/files) (1):**
  - `vllm/model_executor/models/voyage.py`

### Summary

**What changed and why**  
The change replaces the standard `re` module import with `regex` module import in `voyage.py`. This fixes a pre-commit issue that was missed in a previous PR (#33720), likely because the `regex` module offers enhanced functionality needed for this specific model implementation.

**Technical impact**  
This is a minimal dependency change that affects only the Voyage model implementation. The `regex` module provides advanced regex features (like Unicode property support and fuzzy matching) not available in Python's standard `re` module, which may be required for text processing in this model.

**Potential risks**  
If the `regex` package isn't properly installed in all environments, this could cause import errors. There's also a risk of subtle behavioral differences between `re` and `regex` modules if the code relies on specific regex engine behaviors.

**Key insights**  
Always verify that new dependencies are properly documented in requirements files. Consider adding a comment explaining why `regex` is preferred over `re` for this specific use case to help future maintainers understand the dependency choice.

---

