# vLLM Merged PR Report

**Report Date:** 2026-02-14 PST

**Total Merged PRs:** 11

---

## 1. [[KV Connector] Add temporary, off-by-default `VLLM_DISABLE_REQUEST_ID_RANDOMIZATION` workaround](https://github.com/vllm-project/vllm/pull/34415)


### Base Information

- **PR Number:** #34415
- **Author:** [eicherseiji](https://github.com/eicherseiji)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 23:26:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34415/files) (2):**
  - `vllm/envs.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
A new environment variable `VLLM_DISABLE_REQUEST_ID_RANDOMIZATION` has been added as a temporary workaround. This disables the random suffix appended to request IDs introduced in v0.14.0, which broke several KV connectors (P2P NCCL, MoRIIO, Mooncake) that rely on matching request IDs across instances.

**Technical impact**  
When enabled, the system reverts to pre-v0.14.0 behavior by using externally-provided request IDs without modification. This allows affected KV connectors to function but reintroduces the risk of duplicate request IDs, which the randomization was designed to prevent. The change is localized to the request ID assignment logic in the input processor.

**Potential risks**  
If the environment variable is enabled, duplicate external request IDs could cause request collisions, leading to failures or subtle correctness errors in caching or request handling. The workaround is temporary and will be removed in a future release, so dependent connectors must be updated to handle randomized IDs.

**Key insights**  
This is a stopgap solution for unmaintained or uncovered KV connectors. Developers should treat this as a migration aid and update connectors to support randomized request IDs. The warning log ensures users are aware of the temporary nature and risks. Consider adding CI coverage for these connectors to prevent similar regressions.

---

## 2. [[Bugfix] Handle num_expert_group=None in flashinfer block-scale FP8 MoE](https://github.com/vllm-project/vllm/pull/34494)


### Base Information

- **PR Number:** #34494
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 23:25:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34494/files) (2):**
  - `tests/kernels/moe/test_flashinfer.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`

### Summary

**What changed and why**  
Added a guard to convert `num_expert_group=None` to `0` in `flashinfer_fused_moe_blockscale_fp8`, matching existing handling for `topk_group`. This fixes a crash when using DeepSeekV3 routing with FP8 block-scale quantization on models like MiniMax-M2.1 that don’t use grouped top-k routing. A regression test was added to validate the fix on Blackwell GPUs.

**Technical impact**  
Ensures consistent parameter handling across FP8 MoE kernels, preventing kernel-level assertion failures when `num_expert_group` is `None`. The change aligns behavior with the sister function `fi_trtllm_fp8_per_tensor_moe` and maintains compatibility with models using sigmoid scoring without grouped routing.

**Potential risks**  
The test requires SM100+ (Blackwell) GPUs, limiting validation on other platforms. If `num_expert_group=0` is misinterpreted elsewhere in the routing logic, it could affect expert selection behavior. The fix assumes `0` is a safe default for ungrouped routing, but this may need validation for other routing methods.

**Key insights**  
Always validate `None` defaults for optional routing parameters in MoE kernels. The pattern of converting `None` to `0` for grouped routing parameters should be applied consistently across all fused MoE implementations. Consider adding unit tests for edge cases on available hardware to catch similar issues early.

---

## 3. [[BUGFIX] Fix accuracy regression for NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4 with TP>1](https://github.com/vllm-project/vllm/pull/34476)


### Base Information

- **PR Number:** #34476
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 23:25:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34476/files) (1):**
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`

### Summary

**What changed and why**  
The fix reverts a problematic unification of tensor parallelism (TP) sharding paths for MambaMixer2, restoring the original dual-path approach. Specifically, it reintroduces `MergedColumnParallelLinear` for the case where `n_groups % tp_size == 0`, while keeping the custom `ColumnParallelLinear` with `mamba_v2_sharded_weight_loader` for the `n_groups == 1` case. This corrects a weight-scale misalignment in quantized models (e.g., NVFP4, FP8) that caused accuracy regressions.

**Technical impact**  
This change ensures proper alignment between sharded weights and their corresponding scale parameters in quantized Mamba models under TP>1. By using `MergedColumnParallelLinear` for divisible group cases, all parameters (weights and scales) are sharded consistently per component, preserving model accuracy. The architecture now correctly handles both divisible-group and single-group scenarios without breaking quantized model support.

**Potential risks**  
The dual-path logic increases code complexity and could lead to maintenance overhead if future modifications are not applied to both branches. There is also a risk that the custom weight loader for the `n_groups == 1` case might not handle all quantized parameter types (e.g., additional scales) if the model configuration evolves. Edge cases with `n_groups > 1` but not divisible by `tp_size` remain unaddressed.

**Key insights**  
The core issue was that scale parameters in quantized models were sharded contiguously while weights used component-wise sharding, causing misalignment. Developers should ensure that any future changes to TP sharding consider all parameter types, especially for quantized models. Consider adding validation to verify weight-scale alignment during model loading, and document the sharding strategy for each linear layer type to prevent similar regressions.

---

## 4. [[bugfix] Fix critical bug when reporting for all paths where handler.create_error_response is used](https://github.com/vllm-project/vllm/pull/34516)


### Base Information

- **PR Number:** #34516
- **Author:** [kizill](https://github.com/kizill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 23:24:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34516/files) (9):**
  - `vllm/entrypoints/openai/chat_completion/api_router.py`
  - `vllm/entrypoints/openai/completion/api_router.py`
  - `vllm/entrypoints/openai/responses/api_router.py`
  - `vllm/entrypoints/pooling/classify/api_router.py`
  - `vllm/entrypoints/pooling/embed/api_router.py`
  - `vllm/entrypoints/pooling/pooling/api_router.py`
  - `vllm/entrypoints/pooling/score/api_router.py`
  - `vllm/entrypoints/serve/disagg/api_router.py`
  - `vllm/entrypoints/serve/tokenize/api_router.py`

### Summary

**What changed and why**  
The changes fix a critical bug where error responses were incorrectly returning HTTP 200 status codes. Previously, when an exception occurred, `handler.create_error_response(e)` was returned directly, bypassing the subsequent error-handling logic that sets the appropriate HTTP error code. Now, the error response is assigned to a variable (e.g., `generator` or `result`) so the code can check if it's an `ErrorResponse` and return it with the correct status code.

**Technical impact**  
This ensures that all API endpoints consistently return proper HTTP error codes (e.g., 4xx or 5xx) instead of 200 when errors occur. The fix maintains the existing error-handling flow by allowing the `isinstance(generator, ErrorResponse)` check to execute, which then wraps the error in a `JSONResponse` with the correct status code from `result.error.code`.

**Potential risks**  
If `handler.create_error_response` returns a non-`ErrorResponse` type (unlikely but possible), the error may still be mishandled. Additionally, the changes assume that all modified endpoints follow the same pattern—any future additions must replicate this structure to avoid regression.

**Key insights**  
The fix is minimal and consistent across nine files, demonstrating a systematic approach. Developers should verify that `create_error_response` always returns an `ErrorResponse` object and consider adding a type assertion or test to enforce this contract. This bug highlights the importance of centralized error handling in API routes.

---

## 5. [[CI][Entrypoints] Validate detokenize token IDs to prevent int64 overflow causing 500](https://github.com/vllm-project/vllm/pull/34468)


### Base Information

- **PR Number:** #34468
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 23:08:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34468/files) (1):**
  - `vllm/entrypoints/serve/tokenize/protocol.py`

### Summary

**What changed and why**  
The change adds a Pydantic field constraint to `DetokenizeRequest.tokens` to validate that each token ID is within the range `[0, 2^63 - 1]`. This prevents an int64 overflow that previously caused a 500 Internal Server Error when a token ID of `2^63` (9223372036854775808) was sent to the `/detokenize` endpoint.

**Technical impact**  
This moves error handling from a runtime overflow (resulting in a 500) to a validation-layer rejection (returning a proper 422 status). The constraint ensures token IDs align with the expected 64-bit signed integer range, improving API robustness and user feedback.

**Potential risks**  
The validation relies on a hard-coded maximum (`2**63 - 1`), which may become outdated if the underlying token representation changes. Additionally, the TODO note highlights a dependency concern—currently importing `torch.iinfo` would introduce an unnecessary torch dependency in this protocol file.

**Key insights**  
Always validate input boundaries at the API layer to prevent internal server errors. Consider centralizing constant definitions (like max token ID) to avoid magic numbers and ensure consistency across the codebase. Address the TODO by extracting the constant from a shared configuration or utility module.

---

## 6. [[Kernels] Fix Helion GPU utils to use platform-agnostic device name API](https://github.com/vllm-project/vllm/pull/34537)


### Base Information

- **PR Number:** #34537
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 20:29:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34537/files) (1):**
  - `vllm/kernels/helion/utils.py`

### Summary

**What changed and why**  
The PR replaces hardcoded `torch.cuda` calls in `vllm/kernels/helion/utils.py` with vLLM's platform-agnostic `current_platform.get_device_name()` API. This change ensures compatibility with AMD/ROCm platforms, prevents premature CUDA context initialization, and leverages existing abstractions for device name retrieval.

**Technical impact**  
The code now uses a unified interface that works across CUDA, ROCm, and CPU platforms, improving portability. It also removes a hidden dependency on CUDA initialization timing, aligning with vLLM's deferred context strategy. The fallback to `device_id=0` with a warning maintains functionality but highlights potential multi-device issues.

**Potential risks**  
Defaulting to `device_id=0` without proper context may return incorrect device names in multi-GPU or multi-node environments. The warning log is helpful but doesn't resolve the underlying ambiguity. Additionally, any downstream code relying on the exact format of `torch.cuda.get_device_properties().name` may need validation, though the canonicalization function handles variations.

**Key insights**  
Always use platform abstractions (`current_platform`) for device operations to ensure cross-platform support. Consider enhancing the API to require explicit `device_id` or infer it safely from execution context. Review other kernel utilities for similar hardcoded CUDA dependencies to maintain consistency.

---

## 7. [[Model Runner V2] Minor cleanup for Sampler](https://github.com/vllm-project/vllm/pull/34563)


### Base Information

- **PR Number:** #34563
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-14 18:29:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34563/files) (2):**
  - `vllm/v1/worker/gpu/sample/sampler.py`
  - `vllm/v1/worker/gpu/sample/states.py`

### Summary

**What changed and why**  
The changes refactor sampling logic by moving temperature, min_p, and top_k/top_p application from the sampler.py module into the SamplingStates class. This centralizes sampling operations and adds early exit optimizations to skip kernel launches when parameters don't require processing.

**Technical impact**  
This improves code organization by encapsulating sampling operations within the SamplingStates class, making the sampler.py cleaner and more focused. The optimizations reduce unnecessary GPU kernel launches when temperature=1.0 or min_p=0.0, potentially improving performance for common cases where these parameters are at default values.

**Potential risks**  
The early exit conditions using NumPy array operations (np.all) could add CPU overhead for small batch sizes, though they save GPU work. There's a subtle behavior change: min_p now always attempts to apply (with early exit) instead of conditionally checking do_min_p first, which is fine but changes execution flow.

**Key insights**  
The refactoring follows good encapsulation principles and adds meaningful optimizations. Developers should verify the NumPy array operations don't become bottlenecks for very small batches. The consistent pattern of passing idx_mapping and idx_mapping_np to all methods suggests these parameters are fundamental to the sampling architecture.

---

## 8. [[Hybrid] Enable mamba prefix cache "align" mode with async scheduling](https://github.com/vllm-project/vllm/pull/33997)


### Base Information

- **PR Number:** #33997
- **Author:** [tdoublep](https://github.com/tdoublep)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-14 13:15:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33997/files) (4):**
  - `tests/v1/e2e/test_mamba_prefix_cache.py`
  - `vllm/config/vllm.py`
  - `vllm/v1/core/single_type_kv_cache_manager.py`
  - `vllm/v1/worker/mamba_utils.py`

### Summary

**What changed and why**  
The PR enables Mamba prefix caching "align" mode to work with async scheduling when speculative decoding is active. Previously, async scheduling caused inaccurate `num_computed_tokens` tracking in the scheduler (due to draft tokens that may be rejected), leading to incorrect block allocation/freeing in `MambaManager`. The fix adjusts `MambaManager` logic to conservatively handle overestimated token counts and updates test expectations to match new timing.

**Technical impact**  
Changes allow async scheduling and Mamba prefix caching to coexist, improving system performance by removing a previous restriction. The `MambaManager` now accounts for speculative token uncertainty by adjusting `num_computed_tokens` (subtracting speculative blocks) and using `num_scheduled_tokens` for block calculation, ensuring blocks aren’t freed prematurely. Validation logic in `vllm.py` is removed to permit the combination.

**Potential risks**  
The conservative approach may temporarily over-allocate blocks (holding them longer than strictly necessary), slightly increasing memory usage during speculative decoding. Test changes indicate block allocation/freeing timing shifts, which could affect edge cases if not fully validated. The fix assumes draft token rejection is the only source of inaccuracy; other async scheduling discrepancies could remain.

**Key insights**  
Developers should note that Mamba prefix caching with async scheduling is now supported, but memory patterns may differ. The solution avoids deep scheduler/model-runner synchronization, preserving async benefits. Ensure tests cover varied speculative acceptance rates to verify block management correctness. Monitor memory usage in production to confirm no regressions.

---

## 9. [[Renderer] Move InputPreprocessor into Renderer (1/2)](https://github.com/vllm-project/vllm/pull/34510)


### Base Information

- **PR Number:** #34510
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 10:14:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34510/files) (39):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/renderers/test_completions.py`
  - `tests/renderers/test_mistral.py`
  - `tests/v1/e2e/test_streaming_input.py`
  - `tests/v1/streaming_input/test_async_llm_streaming.py`
  - `vllm/benchmarks/mm_processor.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/realtime/serving.py`
  - `vllm/inputs/__init__.py`
  - `vllm/inputs/data.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/ovis.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/voxtral_realtime.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/renderers/base.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/mistral.py`
  - `vllm/renderers/params.py`
  - `vllm/renderers/terratorch.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/metrics/stats.py`

### Summary

**What changed and why**  
This PR moves multimodal tokenization logic from `InputPreprocessor` into the `Renderer` class. The primary goal is to centralize tokenization defaults within the Renderer, allowing each multimodal model to define its own tokenization parameters via `BaseProcessingInfo.get_default_tok_params`. This ensures consistent handling of `add_special_tokens` across online/offline APIs and resolves accuracy discrepancies. Additionally, `StreamingInput` is relocated to avoid circular imports, and miscellaneous cleanup is performed (e.g., renaming `close` to `shutdown`).

**Technical impact**  
Tokenization and truncation are now delegated to the Renderer, which provides default `TokenizeParams` for both completion and chat endpoints. Multimodal models override defaults via `get_default_tok_params`, ensuring proper special token handling. The Renderer also manages multimodal processor initialization and cache statistics, simplifying `InputPreprocessor` and `InputProcessor` responsibilities. This refactor reduces duplication and aligns tokenization behavior across APIs.

**Potential risks**  
- Incorrect tokenization defaults in new multimodal models could break generation or cause silent errors.  
- The move of `StreamingInput` may affect external imports; however, it’s re-exported in `engine/protocol.py`.  
- Changes to `TokenizeParams.with_kwargs` (now keyword-only) require updates in callers (e.g., `entrypoints/llm.py`).  
- Multimodal cache stats aggregation relies on correct delta calculation; misalignment could skew metrics.

**Key insights**  
- Renderer now serves as the single source of truth for tokenization parameters, improving consistency.  
- Developers adding new multimodal models must implement `get_default_tok_params` to control `add_special_tokens` and other tokenization options.  
- The `contains_only_strings` validation shift to `InputProcessor` ensures multimodal hash integrity closer to processing.  
- Test updates reflect the new Renderer initialization pattern (`from_config`), which should be adopted in all new tests.

---

## 10. [[ROCm][CI] Guard sparse MLA backend imports for ROCm compatibility in tests](https://github.com/vllm-project/vllm/pull/34538)


### Base Information

- **PR Number:** #34538
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-14 07:32:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34538/files) (1):**
  - `tests/v1/attention/test_sparse_mla_backends.py`

### Summary

**What changed and why**  
The change adds a conditional import guard for `FlashInferMLASparseBackend` in the test file to prevent `ImportError` on ROCm platforms. Since this backend depends on `flashinfer` (a CUDA-only library), the unconditional import was causing test collection to fail entirely on ROCm.

**Technical impact**  
This modification ensures the test file can be loaded and executed on ROCm systems by dynamically building the `_SPARSE_BACKENDS` list with only available backends. It maintains test coverage for CUDA while gracefully degrading on ROCm, aligning with the existing platform check that skips tests for non-CUDA platforms.

**Potential risks**  
If the `try`/`except` block is placed incorrectly or the dynamic backend list logic has flaws, tests might incorrectly skip or fail on CUDA where the backend should be available. Additionally, the TODO comment indicates future integration of `ROCMAiterMLASparseBackend` for ROCm, which could introduce compatibility issues if not properly validated.

**Key insights**  
Always guard platform-specific imports to prevent module-level failures, especially in test files. This pattern improves cross-platform compatibility without sacrificing existing functionality. Developers should ensure dynamic backend selection logic is robust and consider adding validation for the ROCm sparse backend once integrated.

---

## 11. [[Bugfix] Fix Qwen3.5 config loading](https://github.com/vllm-project/vllm/pull/34554)


### Base Information

- **PR Number:** #34554
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-14 03:56:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34554/files) (2):**
  - `vllm/transformers_utils/configs/qwen3_5.py`
  - `vllm/transformers_utils/configs/qwen3_5_moe.py`

### Summary

**What changed and why**  
The PR fixes a regression in Qwen3.5 config loading by moving the assignment of `pad_token_id`, `bos_token_id`, `eos_token_id`, and `tie_word_embeddings` to after the `super().__init__()` call. This prevents Transformers v4's `PretrainedConfig.__init__` from overwriting these values with its own defaults.

**Technical impact**  
This ensures that custom token IDs and embedding tying settings are preserved when loading Qwen3.5 configurations. The change maintains compatibility with Transformers v4 while correcting a degradation introduced in a previous incomplete port.

**Potential risks**  
If other config classes have similar issues with Transformers v4 defaults, they may still be affected. The fix assumes that all relevant attributes are only set after the parent initializer; any missing attributes could lead to inconsistent state.

**Key insights**  
Always set config attributes after calling `super().__init__()` when parent classes may override them. Review other config files for similar patterns to prevent regressions. This is a critical fix for correct model initialization and token processing.

---

