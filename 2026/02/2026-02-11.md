# vLLM Merged PR Report

**Report Date:** 2026-02-11 PST

**Total Merged PRs:** 41

---

## 1. [[bugfix] refactor FunASR's _get_data_parser](https://github.com/vllm-project/vllm/pull/34397)


### Base Information

- **PR Number:** #34397
- **Author:** [AllenDou](https://github.com/AllenDou)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-11 23:26:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34397/files) (1):**
  - `vllm/model_executor/models/funasr.py`

### Summary

**What changed and why**  
The changes refactor the `_get_data_parser` method from `FunASRMultiModalProcessor` to `FunASRProcessingInfo` to align with a base class refactoring in v0.16, where `BaseMultiModalProcessor._get_data_parser` was moved to `BaseProcessingInfo.build_data_parser`. Additionally, the `skip_prompt_length_check` property was removed to enforce prompt length validation.

**Technical impact**  
This centralizes data parser creation within the processing info class, improving consistency with the updated base class architecture. Removing the prompt length check override ensures that audio prompt lengths are validated, which could affect processing of padded encoder prompts.

**Potential risks**  
If encoder prompts are genuinely padded and require length check skipping, removing `skip_prompt_length_check` could introduce validation errors. The refactor assumes the base class's `build_data_parser` method is properly overridden or compatible.

**Key insights**  
Developers should verify that audio prompt length validation does not break existing functionality with padded prompts. Ensure `BaseProcessingInfo.build_data_parser` is correctly implemented in the base class to avoid runtime issues. This change improves architectural alignment but requires careful testing of audio processing workflows.

---

## 2. [[Bugfix] Fix Sparse24 Compressed Tensors models](https://github.com/vllm-project/vllm/pull/33446)


### Base Information

- **PR Number:** #33446
- **Author:** [kylesayrs](https://github.com/kylesayrs)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 23:15:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33446/files) (3):**
  - `csrc/sparse/cutlass/sparse_scaled_mm_entry.cu`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
This PR fixes a regression in sparse24 compressed tensor models introduced by a previous check that assumed the `config_groups` field always exists. The changes add a conditional check for `config_groups` before filtering Attention quantization groups and adjust sparse24 kernel compatibility to require exact Hopper architecture (SM90) instead of forward compatibility.

**Technical impact**  
The fix ensures sparse24 models load correctly by handling missing `config_groups` gracefully, preventing crashes during model initialization. The stricter compute capability check (SM90 only) aligns kernel usage with actual hardware support, avoiding potential runtime errors on newer architectures.

**Potential risks**  
The change to require exact SM90 may break forward compatibility if sparse24 kernels are later updated for newer architectures. Skipping quantization tests in CI due to CC<90 could allow similar regressions to slip through if test coverage isn't improved.

**Key insights**  
Always validate optional fields before access in quantization configs. Consider enhancing CI to run quantization tests on appropriate hardware or with mocked capabilities. Document sparse24's hardware requirements clearly to avoid confusion about forward compatibility.

---

## 3. [Fix DeepSeek-OCR tensor validation for all size variants](https://github.com/vllm-project/vllm/pull/34085)


### Base Information

- **PR Number:** #34085
- **Author:** [yichuan-w](https://github.com/yichuan-w)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-11 22:50:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34085/files) (1):**
  - `vllm/model_executor/models/deepseek_ocr.py`

### Summary

**What changed and why**  
The fix addresses a hardcoded `base_size` value (1024) in the `_parse_and_validate_image_input()` method that was causing tensor dimension mismatches for DeepSeek-OCR model variants like Tiny (512) and Small (640). The method now dynamically reads the spatial dimension from the input tensor's shape and properly provides the previously missing `image_size` binding.

**Technical impact**  
This change enables the DeepSeek-OCR processor to correctly handle all model size variants by aligning tensor validation with the actual input dimensions. The vision encoders (SAM and CLIP) already support arbitrary resolutions through positional encoding interpolation, so this fix ensures the validation logic matches the encoder capabilities.

**Potential risks**  
If the `pixel_values` tensor has unexpected dimensions (e.g., non-square images or malformed batches), the validation may still fail or produce incorrect `image_size` values. The conditional logic for `images_crop` assumes it matches the main tensor's structure, which could lead to inconsistencies if they're misaligned.

**Key insights**  
Always derive configuration values from actual runtime data rather than hardcoded constants, especially when supporting multiple model variants. The fix correctly leverages the existing encoder flexibility, but developers should ensure consistent tensor shapes across all image-related inputs (`pixel_values`, `images_crop`, `images_spatial_crop`) to prevent new edge cases.

---

## 4. [[BugFix] Fix DP chunking](https://github.com/vllm-project/vllm/pull/34379)


### Base Information

- **PR Number:** #34379
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-11 22:44:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34379/files) (1):**
  - `vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`

### Summary

**What changed and why**  
The fix addresses a bug in DP (data parallelism) chunking for models with shared experts. Previously, when chunking hidden states for processing, the shared expert input wasn't being chunked, leading to a tensor size mismatch (e.g., 256 vs 4096). The changes ensure that `full_shared_input` is properly chunked alongside `full_hidden_states` and `full_router_logits`.

**Technical impact**  
This corrects the execution path for models with shared experts in distributed settings, ensuring tensor dimensions align during chunked processing. The fix modifies the internal chunking logic to handle shared expert inputs consistently, preventing runtime errors and restoring proper functionality for tests like `test_dbo_dp_ep_gsm8k`.

**Potential risks**  
If `full_shared_input` is provided but its dimensions don't match `full_hidden_states`, similar mismatches could still occur. Additionally, the change assumes `shared_input` (after chunking) defaults to `staged_hidden_states` when `None`, which might not be appropriate for all model configurations.

**Key insights**  
Always ensure all relevant inputs are chunked uniformly in distributed processing. Review other similar chunking functions in the codebase to prevent analogous oversights. The fix is minimal and targeted, but thorough testing across various model architectures with shared experts is recommended.

---

## 5. [[Refactor] Pass Renderer to Input Processor](https://github.com/vllm-project/vllm/pull/34329)


### Base Information

- **PR Number:** #34329
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 19:38:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34329/files) (20):**
  - `tests/entrypoints/openai/test_serving_responses.py`
  - `tests/models/language/generation/test_hybrid.py`
  - `tests/models/language/pooling/test_auto_prefix_cache_support.py`
  - `tests/v1/e2e/test_pooling_chunked_prefill.py`
  - `tests/v1/sample/test_logprobs.py`
  - `vllm/config/pooler.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/models/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/serve/tokenize/serving.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/output_processor.py`

### Summary

**What changed and why**  
This refactor passes the Renderer object to the InputProcessor constructor instead of having InputProcessor create its own. The primary purpose is to prepare for upcoming Renderer refactoring and improve consistency between offline and online APIs. Notably, it removes direct `max_model_len` access from online serving components, requiring them to access it via `model_config.max_model_len` instead.

**Technical impact**  
The architecture now centralizes Renderer creation in engine classes (LLMEngine and AsyncLLMEngine), which then pass it to InputProcessor and other components. This creates clearer dependency injection patterns and eliminates duplicate Renderer instantiation. Test files were updated to access configuration through `vllm_config` rather than directly from engine internals, improving encapsulation.

**Potential risks**  
The change modifies core initialization flows across multiple engine and serving components, which could introduce subtle initialization order issues. The removal of `max_model_len` caching in serving classes might cause minor performance regression due to repeated attribute access. There's also a risk of incomplete test updates since many test files required modifications to access configuration correctly.

**Key insights**  
This refactor establishes a cleaner separation of concerns where engines own the Renderer lifecycle. Developers should now access `max_model_len` exclusively through `model_config.max_model_len` rather than cached instance variables. The changes demonstrate a move toward more consistent dependency injection patterns that will facilitate future Renderer refactoring work.

---

## 6. [[Refactor] Move validation to params definitions](https://github.com/vllm-project/vllm/pull/34362)


### Base Information

- **PR Number:** #34362
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 19:33:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34362/files) (3):**
  - `vllm/pooling_params.py`
  - `vllm/sampling_params.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The PR moves validation logic for `SamplingParams` from `InputProcessor` to the `SamplingParams` class itself, aligning with the existing pattern used by `PoolingParams`. This centralizes validation within the parameter classes, improving consistency and separation of concerns.

**Technical impact**  
The `SamplingParams` class now includes a comprehensive `verify()` method that handles validation for logprobs, logit bias, allowed token IDs, speculative decoding compatibility, and structured outputs. This reduces `InputProcessor` complexity by removing 235 lines of validation code, making the processor more focused on input parsing and delegation.

**Potential risks**  
- The `verify()` method now requires additional parameters (`model_config`, `speculative_config`, `structured_outputs_config`, `tokenizer`), which could increase coupling.  
- Validation errors may surface later if `verify()` is not called consistently across all code paths.  
- The removal of validation from `InputProcessor` could affect error handling if the new method is not integrated correctly in all request flows.

**Key insights**  
- This refactor enhances code organization by colocating validation with parameter definitions, similar to `PoolingParams`.  
- Developers must ensure `SamplingParams.verify()` is invoked wherever `InputProcessor._validate_params` was previously used.  
- The change simplifies future maintenance of validation logic but requires careful testing to ensure no regression in error detection or user-facing behavior.

---

## 7. [[Bug Fix] Fix `naive_block_assignment` always defaulting to False due to arg misalignment](https://github.com/vllm-project/vllm/pull/33848)


### Base Information

- **PR Number:** #33848
- **Author:** [RunkaiTao](https://github.com/RunkaiTao)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-11 19:30:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33848/files) (2):**
  - `vllm/lora/layers/fused_moe.py`
  - `vllm/lora/punica_wrapper/punica_base.py`

### Summary

**What changed and why**  
Fixed a bug where the `naive_block_assignment` parameter was incorrectly defaulting to `False` due to argument misalignment. The issue occurred because a function call was passing `naive_block_assignment` as a positional argument when it should have been a keyword argument, while the function signature was missing this parameter entirely.

**Technical impact**  
The fix ensures that the `naive_block_assignment` parameter is properly passed through the function call chain. This corrects the block assignment logic for LoRA-based Mixture of Experts (MoE) layers, which significantly improves performance as shown in the benchmark results with ~30% better throughput and reduced latency across all metrics.

**Potential risks**  
The change modifies a critical path for LoRA MoE execution, so any regression could impact multi-LoRA serving performance. The parameter name `naive_block_assignment` suggests there might be alternative block assignment strategies, and incorrect handling could affect load balancing across experts.

**Key insights**  
Always use keyword arguments when calling functions with optional parameters to prevent argument misalignment. The performance improvement demonstrates that proper block assignment is crucial for LoRA MoE efficiency. Developers should verify that all function signatures and calls remain synchronized when adding or modifying parameters.

---

## 8. [[Bugfix] Fix MTP accuracy for GLM-5](https://github.com/vllm-project/vllm/pull/34385)


### Base Information

- **PR Number:** #34385
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-11 19:08:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34385/files) (1):**
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
The change adds logic to share the target model's `lm_head` with each MTP layer's `shared_head.head` module. This fixes a bug where models like GLM-5, whose checkpoints don't duplicate the `lm_head` weight in the MTP layer, produce NaN logits during speculative decoding because `shared_head.head` remains uninitialized.

**Technical impact**  
This ensures consistent weight sharing across all speculative decoding components, matching the behavior that models like DeepSeek-V3.2 achieve automatically through checkpoint duplication. The fix directly impacts MTP's `compute_logits` function, which relies on `shared_head.head` rather than the top-level `self.model.lm_head`.

**Potential risks**  
The modification assumes the target model's `lm_head` is compatible with all MTP layers' `shared_head.head` modules. If model architectures vary significantly (e.g., different head dimensions or configurations), this forced sharing could cause shape mismatches or unexpected behavior. Additionally, the iterative approach over layers may have performance implications for models with many MTP layers.

**Key insights**  
Always verify weight sharing mechanisms when integrating new model architectures into speculative decoding. The fix highlights the importance of understanding how different model checkpoints store and reference shared components. Developers should consider adding architecture validation to prevent silent failures when `shared_head.head` is missing or incompatible.

---

## 9. [[Refactor] Replace `activation: str` with `MoEActivation` enum](https://github.com/vllm-project/vllm/pull/33843)


### Base Information

- **PR Number:** #33843
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 17:29:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33843/files) (48):**
  - `benchmarks/kernels/benchmark_cutlass_moe_fp8.py`
  - `benchmarks/kernels/benchmark_moe.py`
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/test_cpu_fused_moe.py`
  - `tests/kernels/moe/test_cutlass_moe.py`
  - `tests/kernels/moe/test_deepep_deepgemm_moe.py`
  - `tests/kernels/moe/test_deepep_moe.py`
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/kernels/moe/test_flashinfer_moe.py`
  - `tests/kernels/moe/test_modular_oai_triton_moe.py`
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/moe/test_pplx_cutlass_moe.py`
  - `tests/kernels/moe/test_triton_moe_no_act_mul.py`
  - `tests/kernels/moe/utils.py`
  - `tests/kernels/utils.py`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/activation.py`
  - `vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/fallback.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_batched_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/utils.py`
  - `vllm/model_executor/layers/fused_moe/xpu_fused_moe.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gguf.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/moe_wna16.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/mxfp4_utils.py`
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
This PR introduces a `MoEActivation` enum to replace raw string usage for activation functions in MoE (Mixture of Experts) layers. The change centralizes activation definitions, validation, and mapping to custom ops, addressing the need to support diverse activation types (including non-gated variants) across different MoE kernels. String parameters are retained in external interfaces (e.g., model definitions) for compatibility, but internally they are converted to the enum.

**Technical impact**  
The refactor standardizes activation handling across 48 files, improving type safety and reducing string-based errors. It introduces a single source of truth for activation support checks (`is_gated` property, `custom_op_name` mapping) and consolidates activation logic into a new `activation.py` module. This affects kernel selection, workspace calculations, and activation application, ensuring consistent behavior across CPU/GPU kernels and quantization backends.

**Potential risks**  
- String-to-enum conversion may fail for unsupported activation strings, potentially breaking existing model configurations.  
- The removal of string constants (e.g., `SILU_NO_MUL`) from `utils.py` could impact downstream code that imports them directly.  
- Inconsistent handling of activation strings in external interfaces (e.g., `FusedMoE.__init__`) could lead to mismatches if validation is missed.

**Key insights**  
- The enum provides clear validation and discoverability for supported activations, reducing maintenance overhead.  
- Developers should use `MoEActivation.from_str()` for backward compatibility and `activation.value` when strings are required (e.g., custom ops).  
- Ensure all new activation-dependent logic checks `activation.is_gated` instead of string parsing to align with the new design.

---

## 10. [[ci] Integrate AMD tests into CI](https://github.com/vllm-project/vllm/pull/33626)


### Base Information

- **PR Number:** #33626
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-11 16:54:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33626/files) (6):**
  - `.buildkite/hardware_tests/amd.yaml`
  - `.buildkite/test_areas/basic_correctness.yaml`
  - `.buildkite/test_areas/entrypoints.yaml`
  - `.buildkite/test_areas/models_basic.yaml`
  - `.buildkite/test_areas/models_language.yaml`
  - `.buildkite/test_areas/samplers.yaml`

### Summary

**What changed and why**  
This PR integrates AMD hardware testing into the CI pipeline by enabling six AMD test jobs. It introduces a new `mirror` configuration interface for AMD tests, replacing the previous `mirror_hardwares` approach, and establishes dependencies on a dedicated AMD image build step.

**Technical impact**  
The changes transition from a generic experimental hardware mirroring system to a structured AMD-specific configuration. This creates a clear separation between AMD and other hardware test paths, improves dependency management through explicit `depends_on` relationships, and allows for customized command execution on AMD devices (e.g., filtering tests with `-m 'not skip_v1'` in samplers).

**Potential risks**  
The removal of `mirror_hardwares` from multiple test areas without apparent replacement could break existing experimental test flows if they relied on that configuration. There's also a risk of inconsistent AMD test coverage since the `mirror` block is only added to specific test areas, potentially leaving gaps. The hardcoded device `mi325_1` may limit flexibility for future AMD hardware variations.

**Key insights**  
Developers should verify that all intended test suites are covered by the new AMD `mirror` configuration, as several files lost the `mirror_hardwares` directive without replacement. The dependency chain now explicitly requires the `image-build-amd` step, ensuring proper image availability. Consider making the AMD device type configurable to support future hardware iterations.

---

## 11. [[ROCm][CI] Revert Test Groups From mi325_8 to mi325_1 Agent Pool In AMD CI](https://github.com/vllm-project/vllm/pull/34384)


### Base Information

- **PR Number:** #34384
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-11 15:52:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34384/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
This PR reverts seven test groups in the AMD CI configuration from the `mi325_8` agent pool to the `mi325_1` pool. The change corrects a previous mistake where tests requiring only one GPU were incorrectly assigned to an 8-GPU agent pool, optimizing resource allocation.

**Technical impact**  
The adjustment ensures that single-GPU tests run on appropriate hardware, freeing up the multi-GPU pool (`mi325_8`) for tests that actually require multiple GPUs. This improves CI efficiency and reduces unnecessary resource consumption without affecting test execution logic.

**Potential risks**  
If any of the reverted tests actually require multiple GPUs (e.g., for distributed or multi-GPU scenarios), they may fail due to insufficient hardware. Additionally, the `mi325_1` pool could experience increased queue times if it becomes overloaded with these additional test groups.

**Key insights**  
Always verify GPU requirements when assigning CI agent pools to avoid resource waste. Consider adding a validation step in CI configuration reviews to catch similar issues. Monitor test stability after this change to confirm no regressions occur.

---

## 12. [[Bugfix] Fix some issues with MoERunner PR #32344](https://github.com/vllm-project/vllm/pull/34371)


### Base Information

- **PR Number:** #34371
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 14:33:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34371/files) (2):**
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`

### Summary

**What changed and why**  
The changes relocate the `ensure_moe_quant_config_init()` call from `FusedMoE.forward_native` into the `_moe_forward` and `_moe_forward_shared` helper functions. This restores the initialization pattern to be closer to its original state (when it was inside a custom op) to resolve torch.compile compatibility issues. Additionally, the `gate` property and `is_internal_router` logic are fixed to properly respect the `use_overlapped` flag.

**Technical impact**  
Moving the quantization config initialization into the runner's forward helper functions ensures it is called in the correct execution context for torch.compile, potentially fixing graph breaks or compilation errors. The gate property fix corrects the conditional exposure of the internal router, which is critical for the correct execution path in overlapped and non-overlapped routing scenarios.

**Potential risks**  
The added TODO comment suggests this is a temporary fix pending a "MK migration," which could mean future changes might remove these calls, requiring careful coordination. If the `use_overlapped` flag is incorrectly set or toggled at runtime, it could lead to unexpected `None` returns from the `gate` property or incorrect routing behavior.

**Key insights**  
The core issue was a torch.compile incompatibility caused by initialization logic placement. The fix strategically moves initialization to the point of use within the runner. Developers should note the temporary nature of the quantization config fix and ensure the `use_overlapped` flag is managed correctly, as it now directly controls the availability of the internal gate/router.

---

## 13. [Fix CI failure - Flashinfer Kernel tests](https://github.com/vllm-project/vllm/pull/34316)


### Base Information

- **PR Number:** #34316
- **Author:** [wzhao18](https://github.com/wzhao18)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 14:17:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34316/files) (3):**
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/kernels/moe/test_flashinfer_moe.py`
  - `tests/kernels/moe/test_pplx_cutlass_moe.py`

### Summary

**What changed and why**  
Three test files were modified to add the `num_logical_experts` parameter when constructing `FusedMoEConfig` objects. This change addresses CI failures by ensuring the configuration matches an updated interface that now requires this parameter.

**Technical impact**  
The changes align test code with an updated `FusedMoEConfig` constructor that now expects `num_logical_experts` alongside `num_local_experts`. This ensures the tests correctly instantiate the configuration and validate the Flashinfer kernel functionality without breaking the CI pipeline.

**Potential risks**  
If `num_logical_experts` is set incorrectly (e.g., mismatched with `num_local_experts` or `num_experts`), it could lead to logical errors in MOE routing or performance degradation. The third file uses `num_experts` for `num_logical_experts`, which may be intentional but should be verified for consistency.

**Key insights**  
Always update tests when dependencies change their interfaces. Verify that `num_logical_experts` values are semantically correct—particularly in `test_pplx_cutlass_moe.py` where it differs from `num_local_experts`. Consider adding a comment explaining the relationship between these parameters if not obvious.

---

## 14. [[Bugfix] Fix more multimodal tests for transformers V5](https://github.com/vllm-project/vllm/pull/34334)


### Base Information

- **PR Number:** #34334
- **Author:** [zucchini-nlp](https://github.com/zucchini-nlp)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-11 13:02:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34334/files) (5):**
  - `tests/models/multimodal/processing/test_common.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/glmasr_utils.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`

### Summary

**What changed and why**  
This PR fixes multimodal test failures for transformers V5 compatibility, specifically addressing an Idefics3 processor issue where kwargs were being filtered out incorrectly. The changes include bug fixes for GLMASR (audio mask key handling and conv length calculation), LFM2_VL (special token handling), Qwen2VL/Tarsier2 (processor initialization), and test configuration updates.

**Technical impact**  
The fixes ensure proper multimodal processor behavior across different model architectures by correcting key name mismatches, aligning audio length calculations with HuggingFace implementations, preventing special token duplication, and fixing processor initialization chains. These changes maintain compatibility with transformers V5 while preserving existing functionality.

**Potential risks**  
The GLMASR mask key change from "input_feature_mask" to "input_features_mask" could break backward compatibility if other code depends on the old key name. The audio conv length formula change might produce different output lengths in edge cases. The LFM2_VL special token removal could affect prompt formatting if special tokens were intentionally included.

**Key insights**  
These changes highlight the importance of consistent key naming across transformers versions and proper kwargs propagation in processor initialization. Developers should verify that all multimodal models use the correct mask key names and that processor initialization properly passes all required arguments. The test exclusion for LFM2_VL suggests this model may need additional fixes for full transformers V5 compatibility.

---

## 15. [[Bugfix] send None sentinel on final commit so server properly sends transcription.done](https://github.com/vllm-project/vllm/pull/33963)


### Base Information

- **PR Number:** #33963
- **Author:** [pjs102793](https://github.com/pjs102793)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-11 13:01:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33963/files) (2):**
  - `tests/entrypoints/openai/test_realtime_validation.py`
  - `vllm/entrypoints/openai/realtime/connection.py`

### Summary

**What changed and why**  
The fix addresses a deadlock where the server fails to send `transcription.done` during real-time audio streaming when the client sends a final commit. Previously, setting `_is_input_finished = True` without sending a sentinel to `audio_queue` left `audio_stream_generator` blocked indefinitely. The solution sends a `None` sentinel to the queue, mirroring the pattern used in `cleanup()`.

**Technical impact**  
This change removes the `_is_input_finished` flag and its associated conditional check in `_run_generation`, simplifying the control flow. The `None` sentinel now directly signals the end of audio input, ensuring `audio_stream_generator` can exit cleanly and trigger the `transcription.done` event. The architecture now relies solely on queue messaging for termination.

**Potential risks**  
If the `None` sentinel is sent prematurely or multiple times (e.g., in both `cleanup()` and the commit handler), it could cause early termination or queue corruption. Additionally, removing the flag-based check might affect edge cases where the queue is empty but the connection is still active, though the sentinel approach should handle this.

**Key insights**  
The fix aligns with existing sentinel patterns, promoting consistency. Developers should ensure sentinel usage is idempotent and coordinated with other cleanup paths. Testing should verify that streaming works across varied timing conditions, especially with real-time delays, to prevent regression.

---

## 16. [[Benchmarks] Reduce ready checker log verbosity](https://github.com/vllm-project/vllm/pull/34349)


### Base Information

- **PR Number:** #34349
- **Author:** [tomasruizt](https://github.com/tomasruizt)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-11 12:57:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34349/files) (1):**
  - `vllm/benchmarks/lib/ready_checker.py`

### Summary

**What changed and why**  
The ready checker's `wait_for_endpoint()` function was modified to log only the last line of error tracebacks instead of the full traceback. This change reduces log verbosity and improves readability by focusing on the actual exception message rather than repetitive stack traces during retries.

**Technical impact**  
This change reduces log output size significantly while preserving the essential error information needed for debugging. The system behavior remains unchanged—it still retries every 5 seconds—but now produces cleaner, more focused warning messages that are easier to parse in console output or log files.

**Potential risks**  
If the error message spans multiple lines or contains critical debugging information in earlier stack frames, developers might lose context for diagnosing complex connectivity issues. The simple string splitting approach (`rsplit("\n", 1)[-1]`) assumes standard Python traceback formatting and could behave unexpectedly with non-standard error representations.

**Key insights**  
This is a sensible optimization for production-like scenarios where verbose logging can obscure other important log events. Consider adding a debug-level log option that retains full tracebacks for deeper investigation when needed, and ensure the error extraction logic handles edge cases like empty errors or single-line messages gracefully.

---

## 17. [[GPT-OSS] Remove unnecessary contiguous](https://github.com/vllm-project/vllm/pull/34337)


### Base Information

- **PR Number:** #34337
- **Author:** [elvischenv](https://github.com/elvischenv)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-11 12:29:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34337/files) (1):**
  - `vllm/model_executor/models/gpt_oss.py`

### Summary

**What changed and why**  
Removed a single `v.contiguous()` call in the GPT-OSS attention forward pass. The change eliminates an unnecessary tensor copy operation that was likely introduced for compatibility or safety but is now deemed redundant for correct execution.

**Technical impact**  
This micro-optimization reduces memory operations during inference, potentially improving latency and throughput. Benchmark results show a slight improvement in token throughput (2315.68 vs 2127.95 tok/s) and reduced median inter-token latency (67.48 vs 73.60 ms), indicating more efficient memory access patterns.

**Potential risks**  
If downstream operations (e.g., in the attention kernel) implicitly rely on `v` being contiguous, this could cause subtle errors or performance degradation in edge cases. The risk is low if the attention implementation already handles non-contiguous tensors correctly, but cross-validation with varied input shapes is advised.

**Key insights**  
The removal is a valid optimization when the tensor layout is already contiguous or the attention kernel supports non-contiguous strides. Developers should verify that no other part of the model expects contiguous `v` tensors and consider applying similar optimizations to other models after thorough testing.

---

## 18. [[ROCm] [CI] fix test_unrecognized_env](https://github.com/vllm-project/vllm/pull/34350)


### Base Information

- **PR Number:** #34350
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-11 10:50:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34350/files) (1):**
  - `tests/config/test_config_generation.py`

### Summary

**What changed and why**  
The test `test_unrecognized_env` was modified to use `monkeypatch` for environment variable manipulation instead of directly modifying `os.environ`. This change prevents interference from pre-existing `VLLM_*` environment variables (like `VLLM_TEST_GROUP_NAME` used in AMD CI) that were causing the test to fail by triggering the validation error it was designed to test.

**Technical impact**  
The test is now more isolated and reliable, as it explicitly cleans up any unrecognized `VLLM_*` environment variables before running. Using `monkeypatch` ensures proper test isolation and prevents side effects between test runs or from the external CI environment.

**Potential risks**  
If the cleanup loop incorrectly removes a valid `VLLM_*` environment variable that is part of `environment_variables`, it could mask issues with the validation logic. Additionally, the test now assumes `monkeypatch` fixture is available, which is standard in pytest but could be a dependency concern in alternative test runners.

**Key insights**  
Always use test isolation techniques like `monkeypatch` when modifying global state such as environment variables. The fix correctly addresses a CI-specific issue while maintaining the test's original purpose. Developers should ensure the cleanup logic only targets truly unrecognized variables by verifying against the `environment_variables` set.

---

## 19. [[Bugfix] Enable attn quantization of Llama-4 by correctly permuting scales for rope (int8, fp8)](https://github.com/vllm-project/vllm/pull/34243)


### Base Information

- **PR Number:** #34243
- **Author:** [eldarkurtic](https://github.com/eldarkurtic)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-11 10:24:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34243/files) (1):**
  - `vllm/model_executor/models/llama4.py`

### Summary

**What changed and why**  
The fix addresses a critical bug where weight permutations applied to Llama-4's `q_proj` and `k_proj` layers for rotary positional embeddings (RoPE) were not being applied to their corresponding quantization scales. This mismatch caused severe accuracy degradation when quantizing attention layers. The change extends the permutation logic to include quantization scales for INT8 and FP8 per-channel schemes, enabling accurate attention quantization for Llama-4 models.

**Technical impact**  
This correction allows Llama-4 models (including dense variants like Llama-Guard-4) to have their attention layers quantized without accuracy collapse, maintaining competitive performance (e.g., GSM8k scores restored from ~5.38 to ~91.13). The modification specifically handles scale tensors for compressed-tensors quantization formats (INT8/FP8) and NVFP4 scales, ensuring weight-scale alignment post-permutation.

**Potential risks**  
The fix currently only supports INT8 and FP8 per-channel quantization; INT4 support is deferred to a follow-up PR, which may temporarily leave INT4 quantization inaccurate for Llama-4 attention layers. Additionally, the added conditional logic increases complexity in the permutation helper, which could introduce subtle bugs if extended to other quantization formats or model architectures.

**Key insights**  
Developers should ensure that any future quantization schemes or weight transformations consistently apply the same permutations to both weights and their associated scales. The accuracy collapse previously attributed to attention layer sensitivity was actually a correctable implementation oversight, highlighting the importance of verifying tensor alignment in quantized models. When implementing INT4 support, careful handling of packed weight layouts will be required.

---

## 20. [[Bugfix] fix default is_neox_style to be True for deepseekv3.2](https://github.com/vllm-project/vllm/pull/34353)


### Base Information

- **PR Number:** #34353
- **Author:** [xyDong0223](https://github.com/xyDong0223)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-11 10:20:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34353/files) (1):**
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
The change modifies the default value for `is_neox_style` in the DeepSeekV2 model's rope initialization. Previously, when `indexer_rope_interleave` was not present in the config, it defaulted to `True`, making `is_neox_style=False`. Now it defaults to `False`, making `is_neox_style=True` when the attribute is absent. This fixes a bug where the default rotary embedding style was incorrect.

**Technical impact**  
This change ensures the rotary positional embeddings (RoPE) use the neox style by default for DeepSeekV2 models when `indexer_rope_interleave` is not explicitly configured. This aligns the model's behavior with expected defaults and prevents potential performance degradation or incorrect outputs due to mismatched RoPE implementations.

**Potential risks**  
If any existing configurations or model checkpoints rely on the previous default behavior (non-neox style), this change could introduce compatibility issues or require retraining. There's also a risk if other parts of the codebase assume different default RoPE styles for similar models.

**Key insights**  
The fix correctly addresses a configuration inversion bug. Developers should verify that all DeepSeekV2 model deployments use explicit `indexer_rope_interleave` settings in their configs to ensure consistent behavior. Consider adding a configuration validation step during model loading to catch similar default value mismatches early.

---

## 21. [[Multimodal] Expose `mm_processor_kwargs` for `DummyInputsBuilder`](https://github.com/vllm-project/vllm/pull/34330)


### Base Information

- **PR Number:** #34330
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 09:37:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34330/files) (72):**
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/bagel.py`
  - `vllm/model_executor/models/bee.py`
  - `vllm/model_executor/models/blip2.py`
  - `vllm/model_executor/models/chameleon.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/deepseek_ocr2.py`
  - `vllm/model_executor/models/deepseek_vl2.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/funasr.py`
  - `vllm/model_executor/models/funaudiochat.py`
  - `vllm/model_executor/models/fuyu.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/interns1.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/kanana_v.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/kimi_k25.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/llava_next_video.py`
  - `vllm/model_executor/models/llava_onevision.py`
  - `vllm/model_executor/models/midashenglm.py`
  - `vllm/model_executor/models/minicpmo.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/nvlm_d.py`
  - `vllm/model_executor/models/ovis.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_asr.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen_vl.py`
  - `vllm/model_executor/models/rvl.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/skyworkr1v.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/multimodal.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/processing/dummy_inputs.py`
  - `vllm/multimodal/registry.py`

### Summary

**What changed and why**  
This PR adds a `mm_processor_kwargs` parameter to the `get_dummy_mm_data` and `get_dummy_processor_inputs` methods across 72 multimodal model implementations. The change ensures that user-provided processor configuration (e.g., `longest_edge` for video comprehension) is reflected during dummy input generation, leading to accurate profiling of multimodal token counts.

**Technical impact**  
The modifications propagate processor configuration (like image/video size parameters) through the dummy input pipeline, aligning profiling with runtime behavior. This affects cache budgeting and performance estimation for models with configurable multimodal processing, particularly Qwen3-VL-style models with long-video support.

**Potential risks**  
- Inconsistent handling across models: some implementations (e.g., audio models) now pass kwargs to feature extractors, while others ignore the parameter.  
- Missing validation of `mm_processor_kwargs` could lead to runtime errors if invalid parameters are supplied.  
- The widespread changes increase the risk of regression in unimodal or unaffected model paths.

**Key insights**  
- Developers must ensure any new multimodal model implementations include the `mm_processor_kwargs` parameter in dummy input methods.  
- Consider adding centralized validation for processor kwargs to prevent configuration errors.  
- Monitor profiling accuracy for edge cases where kwargs significantly alter token counts (e.g., extreme size values).

---

## 22. [[Model Runner V2] Init cuda graph pool when necessary](https://github.com/vllm-project/vllm/pull/33217)


### Base Information

- **PR Number:** #33217
- **Author:** [xinyu-intel](https://github.com/xinyu-intel)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-11 09:12:13
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33217/files) (2):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py`

### Summary

**What changed and why**  
The PR modifies two CUDA graph utility files to initialize the CUDA graph pool (`torch.cuda.graph_pool_handle()`) only when CUDA graph mode is active (`cudagraph_mode != CUDAGraphMode.NONE`). Previously, the pool was always initialized, which could cause issues on platforms where `torch.cuda.graph_pool_handle()` is unavailable or unsupported.

**Technical impact**  
This change ensures compatibility with environments lacking full CUDA graph support by deferring pool initialization until it's actually needed. The `self.pool` attribute is now set to `None` by default, avoiding potential runtime errors during class instantiation on incompatible platforms.

**Potential risks**  
If `self.pool` is accessed when `cudagraph_mode` is `NONE`, it could lead to `None`-related errors if not properly guarded. Additionally, any downstream code assuming `self.pool` is always a valid handle may need adjustments to handle the `None` case gracefully.

**Key insights**  
Always validate the availability of platform-specific features before initialization to enhance cross-platform compatibility. Developers should ensure all usages of `self.pool` check for `None` or are only called when CUDA graph mode is active. Consider adding a runtime assertion or documentation clarifying the valid states of `self.pool`.

---

## 23. [[CI][BugFix] Fix silent failure in shellcheck hook and baseline exist…](https://github.com/vllm-project/vllm/pull/32458)


### Base Information

- **PR Number:** #32458
- **Author:** [junuxyz](https://github.com/junuxyz)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-11 09:03:49
- **Type:** `None`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32458/files) (2):**
  - `tools/pre_commit/shellcheck.baseline`
  - `tools/pre_commit/shellcheck.sh`

### Summary



---

## 24. [[Docs] Fix typo ("defult") and double spacing](https://github.com/vllm-project/vllm/pull/34348)


### Base Information

- **PR Number:** #34348
- **Author:** [SorenDreano](https://github.com/SorenDreano)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 09:02:27
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34348/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
Fixed a typo in documentation where "defult" was corrected to "default" and "-02" was updated to "-O2" (the correct flag for the optimization level). Additionally, removed a double space after "See" for proper formatting.

**Technical impact**  
This is a documentation-only change within a configuration class docstring. It does not affect runtime behavior, system functionality, or any code logic—only improves readability and accuracy of the documentation.

**Potential risks**  
No functional risks since no code logic was modified. The only minor risk is if the documentation is automatically parsed by tools expecting the exact previous text, but this is highly unlikely.

**Key insights**  
Always ensure documentation accuracy as it guides users and developers. While low-risk, such fixes enhance professionalism and prevent confusion (e.g., users might incorrectly use `-02` instead of `-O2`). Maintain consistency in spelling and formatting across all docs.

---

## 25. [[ROCm] [aiter] Split KV cache update for AiterFlashAttention](https://github.com/vllm-project/vllm/pull/33681)


### Base Information

- **PR Number:** #33681
- **Author:** [kliuae](https://github.com/kliuae)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-11 08:26:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33681/files) (1):**
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
The PR extracts KV cache update logic from the attention forward pass into a separate `do_kv_cache_update` method in the ROCm AiterFlashAttention backend. This change supports a larger refactoring effort (#32335) to decouple cache updates from attention computation, enabling more flexible scheduling and optimization. The update handles both flash and shuffled KV cache layouts, controlled by the same existing flag.

**Technical impact**  
This refactoring modularizes the KV cache update process, making the attention forward method cleaner and more focused on computation. The backend now sets `forward_includes_kv_cache_update: bool = False`, indicating that cache updates are handled separately. This aligns with a broader architectural shift toward separating cache management from attention kernels, which could improve maintainability and enable future optimizations like asynchronous updates.

**Potential risks**  
The extraction assumes that `attn_metadata` and scaling tensors (`k_scale`, `v_scale`) are accessible via `get_attention_context`. If this context is not correctly set in all execution paths (e.g., during certain profiling runs), cache updates might be skipped incorrectly. Additionally, the shuffled layout path now explicitly asserts that scales are non-null, which could introduce crashes if metadata is incomplete.

**Key insights**  
Developers should verify that `get_attention_context` reliably provides the required metadata across all attention scenarios, including cross-attention and profiling. The change is a stepping stone for #32335, so future PRs may further adjust the cache update interface. Ensure that the separation does not introduce performance regressions, especially for FP8 and shuffled layouts, by monitoring benchmarks.

---

## 26. [[Bugfix]: Fix ROCm fusion attn test; use AttentionBackend utils to create kv cache](https://github.com/vllm-project/vllm/pull/33948)


### Base Information

- **PR Number:** #33948
- **Author:** [Rohan138](https://github.com/Rohan138)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-11 08:12:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33948/files) (1):**
  - `tests/compile/passes/test_fusion_attn.py`

### Summary

**What changed and why**  
The PR refactors the KV cache initialization in a test to use existing `AttentionBackend` utility methods (`get_kv_cache_shape` and `get_kv_cache_stride_order`) instead of hardcoding backend-specific tensor shapes and strides. It also fixes a typo that was causing ROCm tests to be skipped (`BACKENDS` → `BACKENDS_FP8`).

**Technical impact**  
This change centralizes KV cache creation logic, reducing code duplication and aligning test behavior with production code. The test now dynamically adapts to different attention backends (ROCM, Triton, FlashInfer) via backend-defined shape/stride configurations, improving maintainability and consistency.

**Potential risks**  
If the backend utility methods (`get_kv_cache_shape`, `get_kv_cache_stride_order`) have bugs or return unexpected values, test failures could be misleading. The `try-except` fallback for stride order may mask missing backend implementations, potentially hiding integration issues.

**Key insights**  
Always reuse production utilities in tests to ensure alignment and reduce maintenance overhead. Verify that the fallback logic for `kv_cache_stride_order` is safe across all backends, and consider adding validation for shape/stride outputs to catch backend regressions early.

---

## 27. [Don't try and run GLM-ASR with remote code](https://github.com/vllm-project/vllm/pull/34352)


### Base Information

- **PR Number:** #34352
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 08:09:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34352/files) (1):**
  - `tests/models/registry.py`

### Summary

**What changed and why**  
Removed the `trust_remote_code=True` parameter from the test configuration for the `GlmAsrForConditionalGeneration` model. This change is necessary because the GLM-ASR model architecture has been officially integrated into the Transformers library (starting from version 5.0.0rc2), so it no longer requires the `trust_remote_code` flag to load.

**Technical impact**  
This update aligns the test configuration with the upstream Transformers library, ensuring the test will use the native, trusted implementation once the minimum version requirement (`"5.0.0"`) is met. It simplifies the model loading process for this specific architecture within the test suite.

**Potential risks**  
The primary risk is that the test is currently gated behind a Transformers version (`v5.0.0`) that may not yet be in widespread use or fully stable in the dependent environment. If the test suite runs against an older Transformers version, the model loading could fail because the native implementation is unavailable.

**Key insights**  
This is a routine maintenance change that follows upstream library integrations. Developers should verify their local or CI Transformers version meets the `min_transformers_version` requirement before expecting this test to pass. The removal of `trust_remote_code` is a positive step towards improved security and maintainability.

---

## 28. [Reapply [Attention][FA3] Update FA3 to include new swizzle optimization](https://github.com/vllm-project/vllm/pull/34043)


### Base Information

- **PR Number:** #34043
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 07:07:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34043/files) (6):**
  - `cmake/external_projects/vllm_flash_attn.cmake`
  - `tests/v1/cudagraph/test_cudagraph_dispatch.py`
  - `vllm/forward_context.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/cudagraph_dispatcher.py`

### Summary

**What changed and why**  
This PR reapplies a previous FlashAttention 3 (FA3) optimization update that was reverted, now with corrected metadata sizing. The changes update the FA3 dependency version, fix scheduler_metadata allocation to properly account for FA3's swizzle optimization requirements, and refactor cudagraph dispatching logic to handle batch descriptors more accurately.

**Technical impact**  
The scheduler_metadata tensor size calculation now correctly aligns with FA3's internal requirements (1 + round_up(batch_size, 4) * 4), preventing potential out-of-bounds memory access. Cudagraph dispatching logic is simplified by removing the `relax_for_mixed_batch_cudagraphs()` method and using direct dataclass transformations, ensuring proper key matching for FULL vs. PIECEWISE modes based on exact `num_reqs` requirements.

**Potential risks**  
If the FA3 dependency update introduces breaking changes in the flash-attention API, it could affect attention kernel execution. The removal of the relaxation method might introduce subtle bugs if any code paths still depend on its behavior. Incorrect batch descriptor transformations in dispatch logic could lead to mismatched cudagraph keys and runtime errors.

**Key insights**  
Developers should verify that the new FA3 commit (5824e6e) maintains API compatibility with vLLM's integration. The metadata size fix is critical for memory safety—ensure all attention backends using FA3 follow the same calculation pattern. The cudagraph dispatcher changes improve clarity but require careful testing across all cudagraph modes (FULL, PIECEWISE, etc.) to confirm correct behavior.

---

## 29. [Responses harmony system message structured](https://github.com/vllm-project/vllm/pull/34268)


### Base Information

- **PR Number:** #34268
- **Author:** [Kimahriman](https://github.com/Kimahriman)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 05:14:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34268/files) (2):**
  - `tests/entrypoints/openai/responses/test_harmony.py`
  - `vllm/entrypoints/openai/responses/serving.py`

### Summary

**What changed and why**  
The PR fixes an issue where the OpenAI responses API would fail when a structured system message (content as a list of objects) was passed. The fix modifies `_extract_system_message_from_request` to handle both string and structured content formats, specifically extracting text from `input_text` type objects. A corresponding test case was added to verify the behavior.

**Technical impact**  
This change enhances the robustness of the harmony system by supporting the OpenAI-compatible structured message format, ensuring backward compatibility with string-based system prompts while extending support for more complex content structures. The system now correctly processes system messages regardless of whether they are provided as plain text or as structured content arrays.

**Potential risks**  
If the structured content contains multiple `input_text` objects, only the first one is used, which may lead to incomplete system prompt extraction. Additionally, the fix assumes a specific structure (`type: "input_text"`) and may not handle other content types (e.g., images, tool calls) that could appear in system messages. The test’s word-count assertion (3–8 words) is somewhat lenient and may not catch subtle regressions in prompt adherence.

**Key insights**  
Developers should ensure that any future enhancements to structured content support consider all possible content types. The extraction logic could be refactored to be more extensible if additional structured formats are introduced. The added test provides a good baseline but should be complemented with edge-case tests for malformed or mixed content structures.

---

## 30. [[NVIDIA][test] Tests for flashinfer TRTLLM BF16 MoE](https://github.com/vllm-project/vllm/pull/33715)


### Base Information

- **PR Number:** #33715
- **Author:** [Linda-Stadter](https://github.com/Linda-Stadter)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-11 04:38:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33715/files) (7):**
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-fi-cutlass.yaml`
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/moe/test_unquantized_backend_selection.py`
  - `tests/quantization/test_blackwell_moe.py`
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`

### Summary

**What changed and why**  
This PR adds comprehensive testing for the newly integrated FlashInfer TRTLLM BF16 MoE backend. It includes unit tests for the backend selection logic, integration tests for the TRTLLM kernel, and updates to existing evaluation configurations to use the FlashInfer CUTLASS backend to avoid intermittent issues.

**Technical impact**  
The changes enhance test coverage for the MoE backend selection and the TRTLLM BF16 kernel, ensuring correct behavior across different platforms and configurations. The updated evaluation configs switch to a more stable backend (CUTLASS) for E2E tests, which should improve reliability. The backend selection logic now includes an additional environment variable (`VLLM_FLASHINFER_MOE_BACKEND`) to control between latency (TRTLLM) and throughput (CUTLASS) modes.

**Potential risks**  
The test for the TRTLLM backend is restricted to Blackwell GPUs (SM10x), which may limit validation on other architectures. The tolerance thresholds in the kernel test (`atol=1e-1, rtol=0.85`) are relatively high, which could mask numerical precision issues. Additionally, the backend selection depends on multiple environment variables, increasing configuration complexity.

**Key insights**  
Developers should note that the TRTLLM backend is now gated by both `VLLM_USE_FLASHINFER_MOE_FP16` and `VLLM_FLASHINFER_MOE_BACKEND=latency`. The tests provide a clear blueprint for validating new MoE backends. Ensure that any future changes to the backend selection logic maintain compatibility with the updated logging and environment variable checks.

---

## 31. [Make JAIS compatible with Transformers v5](https://github.com/vllm-project/vllm/pull/34264)


### Base Information

- **PR Number:** #34264
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-11 04:30:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34264/files) (1):**
  - `vllm/model_executor/models/jais.py`

### Summary

**What changed and why**  
The change removes an assertion that checked if `config.add_cross_attention` was False. This attribute was removed from the Transformers library in version 5, and since JAIS configs never explicitly used it, the assertion is no longer valid.

**Technical impact**  
This is a compatibility update that allows the JAIS model implementation to work with Transformers v5. The removal of this assertion prevents a runtime error when loading JAIS models with the newer library version, as the attribute no longer exists.

**Potential risks**  
If any JAIS checkpoints were somehow configured with cross-attention enabled (though the description states they aren't), this safety check is now removed. There's also a minor risk if the attribute is reintroduced in a future Transformers version with different semantics.

**Key insights**  
This is a necessary maintenance update for library compatibility. Developers should ensure their JAIS model checkpoints are validated under Transformers v5. Consider adding a version check or try-except block if maintaining compatibility with multiple Transformers versions is required.

---

## 32. [Make Qwen3VL compatible with Transformers v5](https://github.com/vllm-project/vllm/pull/34262)


### Base Information

- **PR Number:** #34262
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-11 04:13:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34262/files) (2):**
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`

### Summary

**What changed and why**  
This PR updates Qwen3VL models to be compatible with Transformers v5 by moving the `tie_word_embeddings` attribute to a more appropriate location. The changes relocate a vision-specific validation check from `Qwen3LLMModel`/`Qwen3MoeLLMModel` to `Qwen3VLForConditionalGeneration`/`Qwen3MoeVLForConditionalGeneration`, where the full vision configuration is accessible. Additionally, the language model constructors now receive the text-specific config via `vllm_config.with_hf_config(config.text_config)`.

**Technical impact**  
The refactoring ensures that the vision configuration validation occurs at the correct abstraction level, improving code organization and aligning with Transformers v5's structural changes. By passing only the text config to the language model, the implementation maintains separation of concerns and prevents unnecessary exposure of vision config to text-only components. This also resolves potential compatibility issues with the updated `tie_word_embeddings` location in multimodal models.

**Potential risks**  
If the `vision_config` is not properly initialized or missing in the parent class, the validation check could raise an `AttributeError`. There's also a risk that the `start_layer` property might not be correctly accessible from `self.language_model` in the new location, especially under pipeline parallelism conditions. The changes assume that `config.text_config` exists and is correctly structured, which could fail if the HF config format deviates.

**Key insights**  
Developers should verify that the `vision_config` attribute is consistently available in the conditional generation classes. The relocation of the validation check improves modularity but requires careful testing under pipeline parallelism scenarios. Ensure that the `with_hf_config` method correctly isolates the text config without breaking other model properties or quantization settings.

---

## 33. [[Bugfix][CPU] Fix llama4 inference on CPU](https://github.com/vllm-project/vllm/pull/34321)


### Base Information

- **PR Number:** #34321
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-11 03:07:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34321/files) (6):**
  - `.gitignore`
  - `csrc/cpu/cpu_fused_moe.cpp`
  - `csrc/cpu/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`
  - `vllm/v1/worker/cpu_worker.py`

### Summary

**What changed and why**  
This PR fixes llama4 inference on CPU by addressing two issues: 1) Adding support for `skip_weighted` parameter in CPU fused MoE kernels to handle router weight application correctly, and 2) Fixing CPU affinity assignment logic for distributed CPU inference scenarios. The changes ensure proper weight handling in MoE layers and correct CPU core distribution across data parallel ranks.

**Technical impact**  
The modifications extend the CPU fused MoE implementation to support `skip_weighted` mode (only for topk=1), which allows router weights to be applied to inputs before computation rather than after. The CPU affinity logic now correctly accounts for data parallel dimensions, ensuring proper core allocation when using multiple processes. These changes affect CPU inference performance and correctness for MoE models.

**Potential risks**  
The `skip_weighted` parameter introduces a new code path that's only validated for topk=1 scenarios, potentially causing issues if used with topk>1. The CPU affinity changes assume consistent process count calculations across different configurations, which might break in edge cases. The modifications to core allocation logic could impact performance if the CPU topology assumptions don't hold.

**Key insights**  
Developers should note that `skip_weighted` is now supported but limited to topk=1 on CPU. The CPU affinity fix addresses distributed inference scenarios where data parallelism is involved. When testing MoE models on CPU, verify that router weight behavior matches expectations, especially for llama4 architecture. The changes maintain backward compatibility for existing configurations.

---

## 34. [[Docs] Reduce time spent generating API docs](https://github.com/vllm-project/vllm/pull/34255)


### Base Information

- **PR Number:** #34255
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 02:56:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34255/files) (25):**
  - `mkdocs.yaml`
  - `vllm/config/model.py`
  - `vllm/engine/async_llm_engine.py`
  - `vllm/engine/llm_engine.py`
  - `vllm/inputs/data.py`
  - `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`
  - `vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/trtllm_moe.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/models/blip2.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/llava_next.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/platforms/interface.py`
  - `vllm/plugins/__init__.py`
  - `vllm/plugins/io_processors/interface.py`
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
This PR optimizes API documentation generation by reducing build time by over 60%. Two main changes were made: updating the `git-revision-date-localized` plugin to exclude auto-generated API files (saving ~20s) and removing the `show_if_no_docstring: true` configuration (saving ~300s). Additionally, 23 docstrings were added to previously undocumented classes and functions to improve documentation coverage.

**Technical impact**  
The changes significantly reduce documentation build times from ~520s to ~200s locally. The `git-revision-date-localized` exclusion prevents unnecessary processing of auto-generated files, while removing `show_if_no_docstring` eliminates the generation of empty documentation pages. The added docstrings ensure critical classes remain documented despite the configuration change.

**Potential risks**  
Removing `show_if_no_docstring` could cause some undocumented but important classes to be excluded from documentation. However, the PR proactively adds docstrings to 23 key classes to mitigate this risk. The git revision date exclusions are safe since these files are always regenerated at build time.

**Key insights**  
The optimization demonstrates that documentation configuration can have substantial performance impact. Developers should ensure important classes have docstrings to remain visible in documentation. The pattern of adding docstrings alongside configuration changes is a good practice for maintaining documentation quality while improving build performance.

---

## 35. [Patch protobuf for CVE-2026-0994](https://github.com/vllm-project/vllm/pull/34253)


### Base Information

- **PR Number:** #34253
- **Author:** [eicherseiji](https://github.com/eicherseiji)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 02:25:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34253/files) (2):**
  - `requirements/build.txt`
  - `requirements/common.txt`

### Summary

**What changed and why**  
The changes pin the `protobuf` dependency to exclude versions vulnerable to CVE-2026-0994. The constraints allow protobuf 5 (>= 5.29.6) and protobuf 6 (>= 6.33.5) while explicitly blocking the vulnerable range 6.30.0–6.33.4. This applies the same fix already present in a release branch to the main development branch.

**Technical impact**  
These modifications update the dependency resolution rules in both `requirements/build.txt` and `requirements/common.txt`. The system will now reject installation of any protobuf version within the specified vulnerable ranges, forcing the use of either a secure protobuf 5.x version or a patched protobuf 6.x version. This is a preventative security measure with no functional impact on the application code itself.

**Potential risks**  
The primary risk is dependency resolution conflicts if other packages require protobuf versions within the excluded ranges. The complex version constraint syntax increases the chance of human error during future updates. There is also a minor risk that the fix in the v0.15.1 release branch might diverge if further changes are made there independently.

**Key insights**  
This is a critical security patch that should be merged promptly. Developers must ensure all environment setups and CI/CD pipelines respect these new constraints. When updating protobuf in the future, carefully review the constraint syntax to maintain security while allowing necessary version upgrades. Consider adding a comment in other requirement files if they indirectly depend on protobuf.

---

## 36. [[Frontend] Exploit tokenizers "new stream" in FastIncrementalDetokenizer](https://github.com/vllm-project/vllm/pull/34217)


### Base Information

- **PR Number:** #34217
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-11 02:03:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34217/files) (1):**
  - `vllm/v1/engine/detokenizer.py`

### Summary

**What changed and why**  
The PR updates the FastIncrementalDetokenizer to use tokenizers version 0.22.0+ for native prefill support via DecodeStream's `ids` parameter, simplifying initialization by directly passing prompt tokens. It also includes minor code cleanups like removing manual prompt suffix logic and streamlining conditional returns.

**Technical impact**  
This reduces initialization overhead by eliminating the manual loop to find a safe prompt suffix and leveraging the tokenizer's built-in prefill capability. The version requirement bump ensures compatibility with the new DecodeStream feature, while code simplifications improve readability without altering functional behavior.

**Potential risks**  
Upgrading the tokenizers version dependency to 0.22.0 may break environments with older versions, requiring updates. The removal of the fallback logic for handling replacement characters (`�`) assumes the native prefill handles edge cases correctly, which could affect decoding quality for problematic token sequences.

**Key insights**  
Developers must ensure tokenizers >=0.22.0 is installed. The changes are performance-focused but warrant testing with diverse prompts to validate decoding accuracy. The streamlined code is a positive step, but monitor for any regressions in incremental detokenization output.

---

## 37. [[Doc] Update Marlin support matrix for Turing](https://github.com/vllm-project/vllm/pull/34319)


### Base Information

- **PR Number:** #34319
- **Author:** [iori2333](https://github.com/iori2333)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-11 01:03:41
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34319/files) (2):**
  - `docs/features/quantization/README.md`
  - `docs/features/quantization/fp8.md`

### Summary

**What changed and why**  
This PR updates documentation to reflect new Marlin quantization support for NVIDIA Turing (SM75) GPUs. The changes modify the support matrix to indicate Turing support with an asterisked note about MXFP4 limitations, and update technical descriptions to include Turing in the supported compute capabilities for weight-only FP8 via Marlin kernels.

**Technical impact**  
The documentation now accurately reflects that Marlin's weight-only FP8 (W8A16) quantization is supported on Turing (compute capability 7.5+) and Ampere GPUs, expanding the officially supported hardware range. This aligns the docs with the underlying code changes from #29901, providing correct guidance to users about compatible hardware.

**Potential risks**  
The asterisk notation for Turing in the matrix could be overlooked, leading users to incorrectly assume full Marlin support (including MXFP4) on Turing. The documentation does not specify whether other Marlin variants (GPTQ/AWQ) are now supported on Turing or if the change is FP8-specific, which may cause confusion.

**Key insights**  
Developers should ensure the asterisked footnote about MXFP4 is prominently displayed to prevent user errors. The documentation team should consider clarifying whether GPTQ/AWQ Marlin support also extends to Turing, or if this update is exclusively for FP8 Marlin. These changes are documentation-only and carry no code execution risk.

---

## 38. [[Misc] Bump `fastsafetensors` version for latest fixes](https://github.com/vllm-project/vllm/pull/34273)


### Base Information

- **PR Number:** #34273
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 00:30:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34273/files) (4):**
  - `requirements/nightly_torch_test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `setup.py`

### Summary

**What changed and why**  
The PR updates the `fastsafetensors` dependency from version `>=0.1.10` to `>=0.2.2` across four requirement files. This change specifically incorporates a fix from upstream PR #46, which reduces unnecessary memory overhead on rank 0 in multi-GPU deployments.

**Technical impact**  
Upgrading to `fastsafetensors>=0.2.2` improves memory efficiency during model loading in distributed GPU environments, particularly benefiting rank 0 processes. The change ensures consistent versioning across development, testing, and production dependency specifications.

**Potential risks**  
While the update targets a memory optimization, any new version could introduce regressions or compatibility issues with existing workflows. The pinned version in `test.txt` may cause conflicts if other dependencies rely on different `fastsafetensors` APIs. The removal of the torch dependency comment for `fastsafetensors` is cosmetic but should be verified for accuracy.

**Key insights**  
This is a targeted dependency upgrade addressing a known performance issue in multi-GPU setups. Developers should validate that the new version maintains compatibility with all model loading paths and monitor for any unexpected behavior in distributed inference scenarios. The explicit comment in `test.in` provides clear rationale for future maintainers.

---

## 39. [[torch.compile] Enable AR+rms fusion by default available for `-O2`](https://github.com/vllm-project/vllm/pull/34299)


### Base Information

- **PR Number:** #34299
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 00:30:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34299/files) (2):**
  - `vllm/config/compilation.py`
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
This PR enables AR+RMS (AllReduce + RMSNorm) fusion by default for optimization level `-O2` and above. The changes remove the explicit `eliminate_noops` setting from optimization levels (making it default to `True`) and add conditional logic to enable AR+RMS fusion when specific hardware and software requirements are met (TP>1, Hopper+ GPU, FlashInfer installed).

**Technical impact**  
The AR+RMS fusion optimization is now automatically applied for eligible configurations, potentially improving inference performance by 5-22% as shown in benchmarks. The `eliminate_noops` optimization is now always enabled by default across all optimization levels, simplifying the configuration. The fusion is gated behind platform checks to ensure compatibility.

**Potential risks**  
The fusion depends on specific hardware (Hopper+ GPUs) and software (FlashInfer), which could cause runtime errors or silent fallbacks if dependencies are missing. The performance improvement may vary across different model architectures and hardware setups. Removing explicit `eliminate_noops` settings could affect debugging or performance analysis if no-op elimination needs to be disabled.

**Key insights**  
Developers should verify their deployment environment meets the fusion requirements (CUDA, TP>1, Hopper GPU, FlashInfer). The performance gains are significant but hardware-dependent. Consider adding runtime validation or fallback mechanisms if the fusion cannot be applied. The change simplifies configuration by making `eliminate_noops` a default behavior.

---

## 40. [[Chore] Move `BaseRenderer` to `base.py`](https://github.com/vllm-project/vllm/pull/34308)


### Base Information

- **PR Number:** #34308
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 00:29:52
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34308/files) (8):**
  - `vllm/renderers/__init__.py`
  - `vllm/renderers/base.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/mistral.py`
  - `vllm/renderers/registry.py`
  - `vllm/renderers/terratorch.py`

### Summary

**What changed and why**  
The PR moves the `BaseRenderer` class from `protocol.py` to `base.py` and updates all import statements accordingly. The change is justified because `BaseRenderer` is no longer a protocol class, suggesting it has evolved into a concrete base class or abstract class.

**Technical impact**  
This is a refactoring change that improves code organization by placing the base class in a more appropriately named file. All dependent modules now import from `base.py` instead of `protocol.py`, maintaining functional consistency while clarifying the class's role in the architecture.

**Potential risks**  
The risk is minimal since this is primarily a file move and import update. However, if any external code directly imports `BaseRenderer` from the old location (`protocol.py`), it will break. The PR should ensure no such external dependencies exist.

**Key insights**  
This refactoring enhances code clarity by aligning file names with the actual purpose of classes. Developers should verify that all references to `BaseRenderer` are updated, including in documentation or configuration files, and consider deprecating or removing the now-empty `protocol.py` if it's no longer needed.

---

## 41. [[XPU][9/N] clean up existing ipex code/doc](https://github.com/vllm-project/vllm/pull/34111)


### Base Information

- **PR Number:** #34111
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-11 00:27:15
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34111/files) (10):**
  - `docker/Dockerfile.cpu`
  - `docs/getting_started/installation/gpu.xpu.inc.md`
  - `tests/quantization/test_cpu_wna16.py`
  - `tests/quantization/test_ipex_quant.py`
  - `vllm/_xpu_ops.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/sparse_attn_indexer.py`
  - `vllm/platforms/cpu.py`
  - `vllm/v1/attention/backends/fa_utils.py`
  - `vllm/v1/attention/ops/paged_attn.py`

### Summary

**What changed and why**  
This PR cleans up Intel-specific IPEX references by replacing them with the unified `xpu_ops` naming convention, as part of a broader effort to consolidate XPU backend code. It removes outdated IPEX documentation, updates installation instructions to reference `vllm-xpu-kernels`, and renames a test file to reflect a more generic CPU quantization focus.

**Technical impact**  
The changes standardize the XPU backend interface by deprecating `ipex_ops` in favor of `xpu_ops`, improving code consistency and reducing platform-specific fragmentation. Documentation updates align with newer dependencies (OneAPI 2025.3 and `vllm-xpu-kernels`), while test adjustments ensure proper coverage for CPU/XPU quantization paths.

**Potential risks**  
Renaming `_ipex_ops.py` to `_xpu_ops.py` may break any external imports or scripts that directly reference the old module. The removal of `test_ipex_quant.py` could reduce test coverage for IPEX-specific quantization features unless those cases are covered elsewhere. Inconsistent logging (e.g., "Using xpu backend on XPU") may cause confusion compared to more descriptive backend names.

**Key insights**  
Developers should update any direct imports from `vllm._ipex_ops` to `vllm._xpu_ops`. The shift to `vllm-xpu-kernels` as a dependency requires ensuring compatibility with existing deployment workflows. Verify that all IPEX-specific quantization tests are adequately covered in the renamed `test_cpu_wna16.py` or other suites to maintain testing integrity.

---

