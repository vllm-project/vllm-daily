# vLLM Merged PR Report

**Report Date:** 2026-02-10 PST

**Total Merged PRs:** 52

---

## 1. [[model] support FunASR model](https://github.com/vllm-project/vllm/pull/33247)


### Base Information

- **PR Number:** #33247
- **Author:** [AllenDou](https://github.com/AllenDou)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 23:37:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33247/files) (7):**
  - `docs/models/supported_models.md`
  - `examples/online_serving/openai_transcription_client.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/funasr.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/processors/__init__.py`
  - `vllm/transformers_utils/processors/funasr_processor.py`

### Summary

**What changed and why**  
This PR adds support for the FunASR automatic speech recognition (ASR) model to vLLM. It introduces a new model implementation (`funasr.py`), a corresponding processor (`funasr_processor.py`), and updates documentation, registry, and client examples to enable serving and transcribing audio with FunASR models.

**Technical impact**  
The changes extend vLLM's multimodal capabilities to include a new ASR architecture. The model integrates with existing interfaces (`SupportsMultiModal`, `SupportsTranscription`) and leverages vLLM's distributed parallelism and multimodal processing pipelines. The client example is updated to allow configurable repetition penalty, improving transcription flexibility.

**Potential risks**  
The custom audio frontend and feature extraction may introduce performance overhead or compatibility issues with certain audio formats. The model's reliance on specific preprocessing (CMVN, LFR) and convolutional attention mechanisms could affect inference stability or memory usage. Additionally, the large new codebase (~1.5k lines) increases maintenance burden and requires thorough testing.

**Key insights**  
Developers should verify audio input compatibility and monitor resource usage when serving FunASR models. The repetition penalty parameter in the client example should be tuned per model for optimal accuracy. Future work should include unit tests and benchmarking to ensure performance aligns with other ASR models in vLLM.

---

## 2. [[CPU] Enable FP16 (Half dtype) support for s390x](https://github.com/vllm-project/vllm/pull/34116)


### Base Information

- **PR Number:** #34116
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-10 22:41:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34116/files) (3):**
  - `csrc/cpu/cpu_attn_impl.hpp`
  - `csrc/cpu/cpu_types_vxe.hpp`
  - `csrc/cpu/mla_decode.cpp`

### Summary

**What changed and why**  
This PR enables FP16 (Half dtype) model inference support for s390x (IBM Z) architecture. Previously, FP16 was disabled on s390x, forcing users to use BF16 or FP32. The changes add custom vectorized bit-manipulation routines for FP16↔FP32 conversion and define FP16 vector types, aligning s390x with other architectures.

**Technical impact**  
The modifications extend the CPU vectorization layer to support FP16 operations on s390x, allowing models to run with reduced memory footprint and potentially improved performance. This change integrates FP16 into the existing dispatch system and vector operation templates, ensuring consistency with other data types (Float, BF16) while leveraging s390x-specific vector intrinsics.

**Potential risks**  
Custom bit-manipulation for FP16 conversion may introduce subtle rounding differences compared to native hardware support, potentially affecting numerical reproducibility. The implementation assumes IEEE 754 compliance and may not handle all edge cases (e.g., denormals) identically to other platforms. Additionally, the lack of native PyTorch FP16 support on s390x could lead to integration issues with future PyTorch updates.

**Key insights**  
Developers should verify FP16 numerical accuracy across diverse inputs and monitor performance regressions. The conversion logic is complex and should be thoroughly tested with edge-case values (NaN, Inf, subnormals). Consider adding compile-time assertions or unit tests to ensure bit-exact behavior matches other architectures where possible.

---

## 3. [[Bugfix] Fix weight naming in Qwen3.5](https://github.com/vllm-project/vllm/pull/34313)


### Base Information

- **PR Number:** #34313
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-10 21:37:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34313/files) (1):**
  - `vllm/model_executor/models/qwen3_5.py`

### Summary

**What changed and why**  
The change fixes a typo in the weight naming for the Qwen3.5 model's linear projection layer. Specifically, it corrects the prefix from `"in_proj_ba"` to `"in_proj_b"` when initializing the `in_proj_a` linear layer, ensuring consistent and correct parameter naming.

**Technical impact**  
This correction ensures that the model's weight loading and initialization align with the expected naming conventions, preventing potential mismatches or failures when loading pretrained weights. It maintains the architectural integrity of the Qwen3.5 implementation without altering its functional behavior.

**Potential risks**  
If pretrained weights were saved with the incorrect prefix (`"in_proj_ba"`), this change could cause weight loading errors unless the weights are renamed or the model is retrained. Additionally, any downstream code relying on the old naming pattern may break.

**Key insights**  
Developers should verify that existing pretrained checkpoints are compatible with this naming correction. Consider adding a weight mapping or fallback mechanism to handle legacy checkpoints. This fix highlights the importance of consistent naming in model definitions to avoid silent failures during weight initialization.

---

## 4. [[Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides](https://github.com/vllm-project/vllm/pull/34279)


### Base Information

- **PR Number:** #34279
- **Author:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-10 21:15:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34279/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The PR fixes integer overflow in fused MoE kernels by changing stride parameters from implicit int32 to explicit `tl.int64` types. This prevents pointer arithmetic overflow when tensor offsets exceed int32 max (~2.1B), which previously caused illegal memory access crashes with large tensors when chunking was disabled.

**Technical impact**  
This change ensures correct memory addressing for large-scale MoE operations by using 64-bit strides, aligning with the existing pattern in `fused_batched_moe.py`. It enables the removal of the chunking workaround (`VLLM_FUSED_MOE_CHUNK_SIZE`) and supports larger problem sizes without performance degradation or numerical differences.

**Potential risks**  
While the fix addresses overflow, developers should verify that all callers pass strides as 64-bit integers to avoid type mismatches. Edge cases with extremely large strides (beyond 64-bit range) remain theoretically possible but impractical. The change may affect compatibility with older Triton versions if `tl.int64` support is inconsistent.

**Key insights**  
Always use explicit 64-bit strides in Triton kernels when tensor dimensions or strides can exceed int32 limits. This fix is a prerequisite for simplifying the fused MoE implementation by removing chunking. Ensure thorough testing across varying tensor sizes to confirm no regressions in performance or accuracy.

---

## 5. [[ModelBash][DSR1 NVFp4] Avoid Bf16 Bias Cast](https://github.com/vllm-project/vllm/pull/34298)


### Base Information

- **PR Number:** #34298
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-10 21:00:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34298/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
The PR removes explicit bfloat16 conversion of `e_score_correction_bias` (routing bias) before passing it to the FlashInfer TRT-LLM FP4 MoE kernel. Previously, the bias was cast to bfloat16 if present, but now it is passed directly without conversion. This avoids an unnecessary cast on the inference hotpath, potentially improving performance.

**Technical impact**  
The change eliminates a redundant type conversion in the MoE routing logic, which may reduce overhead during token generation. The kernel likely already handles mixed precision internally or expects the bias in its original dtype. This aligns with performance optimization goals for high-throughput inference scenarios.

**Potential risks**  
If the downstream kernel strictly requires bfloat16 bias, passing the original dtype could cause precision mismatches or runtime errors. Additionally, the change assumes the bias tensor's existing dtype is compatible—if it differs unexpectedly (e.g., float32 in some configurations), it might affect numerical stability or kernel behavior.

**Key insights**  
This is a targeted performance optimization that simplifies the code path. Developers should verify that the FlashInfer kernel supports the bias dtype as provided (e.g., via documentation or tests). Ensure the bias tensor's dtype is consistent across all model configurations to avoid silent regressions.

---

## 6. [Threshold fix wvSplitk for occasional CI fails](https://github.com/vllm-project/vllm/pull/34013)


### Base Information

- **PR Number:** #34013
- **Author:** [amd-hhashemi](https://github.com/amd-hhashemi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-10 19:59:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34013/files) (1):**
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`

### Summary

**What changed and why**  
The PR modifies tolerance thresholds in a ROCm wvSplitK FP8 kernel test to address occasional CI failures. It replaces `assert torch.allclose` with `torch.testing.assert_close` and introduces a special case for large-K matrices without xnorm, applying looser tolerances (atol=0.07, rtol=5e-2) when k >= 32 * 1024.

**Technical impact**  
These changes make the test more robust to numerical variations in FP8 computations, particularly for large matrix dimensions. The switch to `torch.testing.assert_close` provides better error reporting, while the adjusted tolerances prevent flaky test failures without compromising test validity for the targeted kernel behavior.

**Potential risks**  
Overly relaxed tolerances could mask genuine regressions in the wvSplitK kernel implementation. The specific threshold for large-K (32 * 1024) may need validation to ensure it covers all edge cases that cause CI instability. There's also a risk that the fix addresses symptoms rather than root causes of numerical instability.

**Key insights**  
Developers should verify that the tolerance values are empirically justified by the kernel's expected numerical error bounds. Consider adding a comment explaining why these specific thresholds were chosen. Monitor test stability over multiple CI runs to ensure the fix is effective without being permissive.

---

## 7. [[Bugfix] Fix benchmark_moe.py inplace assertion with torch >= 2.9](https://github.com/vllm-project/vllm/pull/34149)


### Base Information

- **PR Number:** #34149
- **Author:** [mgehre-amd](https://github.com/mgehre-amd)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-10 19:58:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34149/files) (1):**
  - `benchmarks/kernels/benchmark_moe.py`

### Summary

**What changed and why**  
The benchmark script was unconditionally passing `inplace=True` to fused expert kernels, causing an assertion failure in PyTorch ≥2.9 after a previous change moved in‑place operation guards to entry‑point assertions. The fix respects the `disable_inplace()` flag by setting `inplace = not disable_inplace()` and passing that variable to both `deep_gemm_experts` and `fused_experts` calls.

**Technical impact**  
This aligns the benchmark’s behavior with production code (e.g., the oracle layer) and ensures the benchmark runs without crashing on newer PyTorch versions. The change is minimal and only affects the in‑place parameter, leaving the benchmark’s performance‑measurement logic intact.

**Potential risks**  
If `disable_inplace()` returns `True` in environments where the benchmark previously relied on in‑place operations, memory usage or performance characteristics could shift slightly. However, this matches the actual kernel behavior, so the benchmark remains representative. No other functional or architectural risks are introduced.

**Key insights**  
Always synchronize benchmark configurations with the corresponding production code paths to avoid assertion mismatches after internal API changes. The fix is straightforward and follows the existing pattern used elsewhere in the codebase, ensuring consistency. Developers should verify that any similar benchmarks or tests are updated when kernel entry‑point contracts change.

---

## 8. [[Plugin] Simplify IO Processor Plugin interface](https://github.com/vllm-project/vllm/pull/34236)


### Base Information

- **PR Number:** #34236
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 19:47:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34236/files) (9):**
  - `docs/design/io_processor_plugins.md`
  - `tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py`
  - `tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/types.py`
  - `tests/plugins_tests/test_io_processor_plugins.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/plugins/io_processors/interface.py`
  - `vllm/utils/collection_utils.py`

### Summary

**What changed and why**  
This PR simplifies the IO Processor Plugin interface by unifying offline and online API handling. Key changes include renaming `parse_request` to `parse_data` to directly handle `request.data`, deprecating `output_to_response` in favor of automatic response construction, and splitting `validate_or_generate_params` into separate `merge_sampling_params` and `merge_pooling_params` methods for clearer type annotations. All modifications maintain backward compatibility until v0.19.

**Technical impact**  
The changes streamline plugin development by removing the distinction between offline and online APIs, reducing boilerplate code. The interface now provides default implementations with deprecation warnings, easing migration. The refactoring also improves type safety by separating sampling and pooling parameter handling, which simplifies integration in both the LLM entrypoint and serving layer.

**Potential risks**  
Existing plugins relying on the deprecated methods (`parse_request`, `validate_or_generate_params`, `output_to_response`) will emit deprecation warnings but remain functional until v0.19. However, plugins that customarily set `request_id` in `output_to_response` may see behavioral changes if the automatic response construction does not preserve this field. The removal of `as_iter` utility could affect other parts of the codebase if it was used elsewhere.

**Key insights**  
Developers should update plugins to use the new method names and rely on the simplified response handling. The split parameter methods (`merge_sampling_params` and `merge_pooling_params`) enhance clarity and type checking. Ensure that any custom `request_id` handling is reviewed, as the automatic response construction may not replicate previous behavior. The changes align the plugin interface more closely with standard vLLM patterns, reducing maintenance overhead.

---

## 9. [[XPU][7/N] enable xpu fp8 moe](https://github.com/vllm-project/vllm/pull/34202)


### Base Information

- **PR Number:** #34202
- **Author:** [zufangzhu](https://github.com/zufangzhu)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-10 19:33:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34202/files) (4):**
  - `requirements/xpu.txt`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/xpu_fused_moe.py`

### Summary

**What changed and why**  
This PR enables FP8 Mixture of Experts (MoE) support for XPU devices by updating the XPU kernel dependency and adding a new `XPUExpertsFp8` class. The changes include adding XPU as a supported backend in the FP8 MoE selection logic and implementing the necessary class inheritance and configuration.

**Technical impact**  
The update allows XPU devices to utilize FP8 precision for MoE layers, potentially improving performance and memory efficiency. The changes integrate XPU into the existing FP8 MoE backend selection framework, ensuring compatibility with the modular kernel architecture. The kernel dependency is upgraded to version 0.1.2, which likely includes necessary FP8 support.

**Potential risks**  
The new `XPUExpertsFp8` class inherits from `XPUExperts` but only sets `is_fp8 = True`, which may lead to subtle behavioral differences if the base class logic doesn't fully account for FP8-specific handling. There's a risk of regression if the updated XPU kernel (v0.1.2) introduces breaking changes or bugs. The hardcoded scale variable assignments (`self.w1_scale`, `self.w2_scale`) assume proper initialization elsewhere, which could cause issues if not set.

**Key insights**  
Developers should verify that the XPU kernel v0.1.2 is stable and thoroughly tested for FP8 operations. The `is_fp8` flag usage in the base `apply` method should be reviewed to ensure correct FP8 execution paths. Consider adding validation for scale tensors in `XPUExpertsFp8` initialization to prevent runtime errors.

---

## 10. [[Kernel] Apply 256bit LDG/STG To Activation Kernels](https://github.com/vllm-project/vllm/pull/33022)


### Base Information

- **PR Number:** #33022
- **Author:** [AstroVoyager7](https://github.com/AstroVoyager7)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 19:31:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33022/files) (1):**
  - `csrc/activation_kernels.cu`

### Summary

**What changed and why**  
The kernel was modified to support 256-bit load/store operations (LDG/STG) for Nvidia Blackwell GPUs, while maintaining backward compatibility with older architectures. This is achieved through new vectorized data structures (u32x8_t), architecture-specific inline assembly for 256-bit operations, and compile-time dispatch using template specialization.

**Technical impact**  
These changes enable higher memory bandwidth utilization on Blackwell GPUs by doubling the vector width from 128-bit to 256-bit for aligned memory accesses. The implementation uses template metaprogramming (VecTraits, PackedTraits) to select optimal vector sizes and operations at compile time, ensuring efficient execution across different GPU architectures without runtime overhead.

**Potential risks**  
The reliance on inline assembly (`ld.global.nc.v8.u32`, `st.global.v8.u32`) may reduce portability and complicate debugging. The 32-byte alignment requirement (`is_32byte_aligned`) could cause performance degradation or fallback to scalar paths if memory allocations are not properly aligned. Additionally, the increased code complexity from template expansions may impact compilation times and code maintainability.

**Key insights**  
Developers should ensure that memory buffers passed to these kernels are 32-byte aligned to maximize performance on Blackwell GPUs. The changes are architecture-aware and will automatically use 128-bit operations on older GPUs, but thorough testing across different hardware configurations is recommended. Consider adding static assertions or runtime checks to validate alignment assumptions in debug builds.

---

## 11. [[Bugfix][DeepSeek-V3.2] fix fp8 kvcache type cast](https://github.com/vllm-project/vllm/pull/33884)


### Base Information

- **PR Number:** #33884
- **Author:** [kebe7jun](https://github.com/kebe7jun)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 19:31:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33884/files) (1):**
  - `csrc/cache_kernels.cu`

### Summary

**What changed and why**  
The fix addresses an issue with FP8 KV cache type casting by expanding the acceptable data types for the `src_cache` tensor. Previously, the function only accepted `uint8` tensors, but now it also supports `float8_e4m3fn` and `float8_e5m2` FP8 formats. This change ensures compatibility with different FP8 representations used in the KV cache.

**Technical impact**  
This modification enables the KV cache gathering and upconversion kernel to handle multiple FP8 data types directly, eliminating the need for external type conversions before kernel invocation. The kernel now properly interprets the underlying byte data regardless of which specific FP8 format is used, maintaining the same bfloat16 output format for downstream processing.

**Potential risks**  
The reinterpret cast from float8 pointers to uint8 pointers assumes compatible memory layouts between the FP8 types and uint8, which is generally safe but relies on implementation details. There's also a risk that other FP8 variants beyond e4m3fn and e5m2 could be introduced in the future, requiring additional updates to the type checking logic.

**Key insights**  
The solution elegantly handles multiple FP8 formats by treating them all as byte arrays at the kernel level, minimizing code duplication. Developers should ensure that any new FP8 data types added to the system are explicitly added to the type checking condition. The change maintains backward compatibility while extending functionality for FP8 KV cache support.

---

## 12. [[Misc] Clean up validation logic in input processor](https://github.com/vllm-project/vllm/pull/34144)


### Base Information

- **PR Number:** #34144
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 19:29:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34144/files) (3):**
  - `tests/v1/engine/test_process_multi_modal_uuids.py`
  - `vllm/multimodal/encoder_budget.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
The changes simplify validation logic for prompt length in multimodal models by centralizing the skip_prompt_length_check flag. Previously, this check was conditionally applied within validation methods; now it's determined during initialization from the multimodal processor's info and stored as an instance variable. This cleanup removes redundant conditional branches and aligns with the assumption that encoder-decoder models are always multimodal.

**Technical impact**  
The refactoring reduces code duplication and improves maintainability by separating prompt length validation from other input validation steps. The validation logic now uses a consistent max_prompt_len based on prompt_type (decoder uses max_model_len, encoder uses mm_encoder_cache_size). The removal of skip_tokenizer_init in tests reflects updated assumptions about multimodal model initialization.

**Potential risks**  
If the skip_prompt_length_check flag is incorrectly set by a multimodal processor, it could bypass necessary length validations. The refactored validation may not handle edge cases where encoder inputs are empty for non-multimodal encoder-decoder models (though the PR assumes these are always multimodal). Changes to max_prompt_len logic could affect error messages for length violations.

**Key insights**  
Developers should verify that all multimodal processors correctly expose the skip_prompt_length_check flag. The simplified validation flow improves readability but centralizes critical logic—ensure thorough testing of multimodal input scenarios. The removal of skip_tokenizer_init in tests suggests broader changes to model initialization assumptions that may affect other test configurations.

---

## 13. [[WideEP] Fix nvfp4 DeepEP High Throughput All2All backend](https://github.com/vllm-project/vllm/pull/33738)


### Base Information

- **PR Number:** #33738
- **Author:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 19:15:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33738/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
The change modifies the `_supports_parallel_config` function to reject the `FLASHINFER_TRTLLM` backend when using DeepEP high-throughput (HT) All2All kernels. This prevents a `NotImplementedError` that occurs because `FLASHINFER_TRTLLM` requires `dispatch_router_logits()`—a method not implemented in DeepEP HT's modular kernel interface.

**Technical impact**  
When DeepEP HT is enabled, the backend selection will now fall back to `FLASHINFER_CUTLASS` instead of `FLASHINFER_TRTLLM`. This ensures compatibility with the existing DeepEP HT implementation, which only supports the modular kernel path via `dispatch()`.

**Potential risks**  
If `FLASHINFER_CUTLASS` has performance or functional limitations compared to `FLASHINFER_TRTLLM` for certain workloads, this change could impact efficiency. Additionally, the fix assumes `FLASHINFER_CUTLASS` is always a viable fallback; any issues with that backend would now become critical.

**Key insights**  
This is a targeted compatibility fix that addresses a clear runtime crash. Developers should verify that `FLASHINFER_CUTLASS` meets performance expectations in DeepEP HT scenarios. Consider documenting the backend incompatibility and exploring whether `dispatch_router_logits()` could be added to DeepEP HT in the future.

---

## 14. [[torch.compile] Stop doing unnecessary FakeTensorProp in PiecewiseCompileInterpreter](https://github.com/vllm-project/vllm/pull/34093)


### Base Information

- **PR Number:** #34093
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 19:15:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34093/files) (2):**
  - `tests/compile/fullgraph/test_simple.py`
  - `vllm/compilation/backends.py`

### Summary

**What changed and why**  
The PR removes unnecessary FakeTensor propagation in PiecewiseCompileInterpreter by directly extracting example values from the graph metadata instead of re-running FakeTensor propagation through submodules. This optimization eliminates redundant computation during torch.compile's graph capture phase.

**Technical impact**  
This change improves compilation performance by avoiding redundant FakeTensor propagation through already-faked submodules. The modified code directly accesses pre-existing `example_value` metadata from graph nodes, reducing computational overhead during graph partitioning and compilation.

**Potential risks**  
The main risk is assuming `example_value` metadata always exists and is correctly populated on graph output nodes. If certain graph transformations don't properly maintain this metadata, the code could fail or produce incorrect results. The test changes adding unbacked symint handling suggest this was considered.

**Key insights**  
The optimization provides significant performance gains (~2s on llama-3.1-70B) by eliminating redundant tensor propagation. Developers should ensure graph metadata consistency when modifying compilation pipelines. The test expansion with unbacked symints validates the change handles dynamic shapes correctly.

---

## 15. [[Redo] Add `--trust-remote-code` to dataset bench args](https://github.com/vllm-project/vllm/pull/34251)


### Base Information

- **PR Number:** #34251
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 19:10:12
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34251/files) (2):**
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
The PR moves the `--trust-remote-code` argument from the `serve.py` CLI parser to the `datasets.py` CLI parser. This is a redo of a previous change (#34208) to address a potential conflict with another PR (#34188), likely ensuring the argument is correctly associated with dataset loading operations rather than the general serving command.

**Technical impact**  
This change repositions the `--trust-remote-code` flag to be specific to dataset benchmarking commands. It ensures that when loading datasets from Hugging Face that require remote code execution (like custom tokenizers or processing scripts), the flag is available in the appropriate command-line interface. The flag is removed from the general `vllm serve` command arguments.

**Potential risks**  
If `vllm serve` or other serving commands actually require the `--trust-remote-code` flag for model loading (which often uses the same Hugging Face infrastructure), its removal could break those workflows. The PR description explicitly asks to verify that `vllm serve` isn't broken, indicating this is a known risk that needs validation.

**Key insights**  
This appears to be a correction to properly scope the `--trust-remote-code` flag to dataset operations. Developers should confirm whether the flag is needed elsewhere in the codebase (particularly for model loading in serving scenarios) to avoid regression. The change highlights the importance of argument placement in modular CLI designs.

---

## 16. [[Bugfix] Fix Worker.load_model context-manager composition for sleep mode](https://github.com/vllm-project/vllm/pull/34021)


### Base Information

- **PR Number:** #34021
- **Author:** [tianshu-Michael-yu](https://github.com/tianshu-Michael-yu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 19:07:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34021/files) (1):**
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The PR fixes a Python context-manager composition bug in `Worker.load_model()` where `with A and B:` was incorrectly used instead of `with A, B:`. The original syntax evaluates to a single context manager (`B` when `A` is truthy), causing `_maybe_get_memory_pool_context(tag="weights")` to never be entered. This ensures weight allocations are properly tracked when cuMem sleep mode is enabled.

**Technical impact**  
This change ensures that memory allocations for model weights are correctly tracked and managed by the sleep mode system. Without this fix, weight offloading behavior would be inconsistent, potentially leading to memory leaks or suboptimal memory usage during sleep mode operations.

**Potential risks**  
The risk is minimal as this is a syntax correction with no functional changes outside the intended context-manager behavior. However, if `_maybe_get_memory_pool_context` or `set_current_vllm_config` have side effects beyond memory tracking, those could now be triggered correctly, which might expose latent issues in dependent code.

**Key insights**  
Always use tuple syntax (`with A, B:`) for multiple context managers in Python, not logical operators (`and`). This bug highlights the importance of understanding Python's context-manager evaluation semantics. Developers should review other uses of `with` statements in the codebase for similar patterns.

---

## 17. [[Misc] Add run one batch script that supports profiling](https://github.com/vllm-project/vllm/pull/32968)


### Base Information

- **PR Number:** #32968
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 18:29:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32968/files) (1):**
  - `examples/offline_inference/run_one_batch.py`

### Summary

**What changed and why**  
A new script `run_one_batch.py` was added to the offline inference examples. This script allows running a single batch of inference with configurable batch size and prompt parameters, and optionally enables profiling of prefill, decode, or both phases using vLLM's profiling capabilities.

**Technical impact**  
This addition provides a convenient utility for performance testing and profiling in controlled batch scenarios. It leverages existing vLLM components (`LLM`, `EngineArgs`, `ProfilerConfig`) without modifying core logic, making it a standalone tool that integrates seamlessly with the existing architecture.

**Potential risks**  
The script hardcodes `max_tokens` to 16 (`DEFAULT_MAX_TOKENS`), which may not suit all profiling needs. If `--profile-dir` is empty when profiling is enabled, it raises a `ValueError`, but this is only checked after argument parsing. The prompt generation logic may produce repetitive or truncated prompts that might not reflect realistic workloads.

**Key insights**  
This script is valuable for benchmarking and profiling but should be documented as a development/analysis tool rather than for production use. Consider making `max_tokens` configurable via CLI for flexibility. Ensure users understand that profiling directories must exist or be writable to avoid runtime errors.

---

## 18. [[ROCm][CI] Fix test_sequence_parallel.py location in AMD CI pipeline](https://github.com/vllm-project/vllm/pull/34280)


### Base Information

- **PR Number:** #34280
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-10 17:08:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34280/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The change updates a single test path in the AMD CI pipeline from `distributed/test_sequence_parallel.py` to `compile/correctness_e2e/test_sequence_parallel.py`. This fixes an oversight from a previous PR (#33731) that moved the test file but missed updating this specific location in the `.buildkite/test-amd.yaml` configuration.

**Technical impact**  
This ensures the AMD CI pipeline's "Distributed Tests (2 GPUs)" step correctly executes the relocated test file. Without this fix, the CI job would fail because the old file path no longer exists, potentially causing false CI failures for AMD-related changes.

**Potential risks**  
The risk is minimal as this is a straightforward path correction. However, if the test file was moved with other structural changes (like import modifications or dependency updates), those might not be reflected here, though the original PR should have handled those. There's also a risk that other overlooked references to the old path might exist elsewhere in the codebase or CI configurations.

**Key insights**  
Always perform a comprehensive search for all references when moving files, especially in CI configuration files which are easy to miss. Consider using `grep` or IDE refactoring tools to find all usages. This fix is critical for maintaining CI stability for AMD GPU testing.

---

## 19. [[MoE Refactor] Introduce MoERunner abstraction and move execution logic from FusedMoE to DefaultMoERunner](https://github.com/vllm-project/vllm/pull/32344)


### Base Information

- **PR Number:** #32344
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-10 16:51:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32344/files) (25):**
  - `docs/design/moe_kernel_features.md`
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/utils.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_method_base.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/runner/__init__.py`
  - `vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`
  - `vllm/model_executor/layers/fused_moe/runner/moe_runner.py`
  - `vllm/model_executor/layers/fused_moe/shared_fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/bitsandbytes.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/experts_int8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gguf.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/moe_wna16.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR introduces an `MoERunner` abstraction to simplify the complex execution logic in `FusedMoE`. The existing forward logic is moved to a new `DefaultMoERunner` subclass, while the base `FusedMoE` layer is refactored to delegate execution to the runner. This is the first step in a larger refactor aimed at splitting the runner into specialized subclasses for different feature combinations.

**Technical impact**  
The changes decouple execution logic from the `FusedMoE` layer, improving modularity and maintainability. The runner now handles routing, shared experts, DP chunking, and parallel operations. This also updates the configuration to include sequence parallelism (`sp_size`) and logical expert counts, aligning with broader parallelization strategies. The refactor touches multiple quantization methods to pass a `shared_experts_input` parameter consistently.

**Potential risks**  
Moving execution logic to a new abstraction could introduce subtle behavioral differences, especially around shared expert overlap and stream synchronization. The runner initialization in `FusedMoE` may need careful handling when swapping quantization methods. Changes to parallel configs (e.g., `sp_size`) must be validated across all distributed scenarios.

**Key insights**  
This is a foundational change that sets the stage for more targeted optimizations. Developers should verify that all MoE features (shared experts, quantization, parallel modes) work correctly after the refactor. The runner pattern will eventually allow specialized implementations, but for now, the `DefaultMoERunner` consolidates the existing logic with minimal functional changes.

---

## 20. [[CI] Add pip caching to cleanup_pr_body workflow](https://github.com/vllm-project/vllm/pull/32979)


### Base Information

- **PR Number:** #32979
- **Author:** [sjhddh](https://github.com/sjhddh)
- **Merged By:** [russellb](https://github.com/russellb)
- **Merged time:** 2026-02-10 16:45:28
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32979/files) (1):**
  - `.github/workflows/cleanup_pr_body.yml`

### Summary

**What changed and why**  
A `cache: 'pip'` configuration was added to the `actions/setup-python` step in the `cleanup_pr_body` GitHub Actions workflow. This change enables caching of pip dependencies to reduce installation time on subsequent workflow runs when dependencies are unchanged.

**Technical impact**  
The workflow will now attempt to restore cached pip packages before installation, potentially speeding up execution. This follows a standard caching pattern already used in other workflows, so it integrates consistently with the existing CI/CD infrastructure.

**Potential risks**  
The risk is minimal as this is a well-established caching approach. However, if the cache becomes corrupted or outdated, it could theoretically cause dependency resolution issues, though the workflow would fall back to a fresh install.

**Key insights**  
This is a low-risk optimization that improves CI efficiency. Developers should monitor workflow logs for "Cache restored" messages to verify it's working. If any dependency-related issues arise, removing the `cache` line provides an immediate rollback path.

---

## 21. [[Misc] Add pre-commit hook to catch boolean ops in with-statements](https://github.com/vllm-project/vllm/pull/34271)


### Base Information

- **PR Number:** #34271
- **Author:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-10 15:13:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34271/files) (2):**
  - `.pre-commit-config.yaml`
  - `tools/pre_commit/check_boolean_context_manager.py`

### Summary

**What changed and why**  
Added a new pre-commit hook that uses AST parsing to detect boolean operators (`and`/`or`) in `with` and `async with` statements. This addresses a subtle bug where `with ctx_a() and ctx_b():` only enters the second context manager, which previously caused a significant memory offloading regression.

**Technical impact**  
The hook performs static analysis on Python files during pre-commit, catching a specific class of bugs that existing linters (ruff, pylint, flake8) don't detect. It adds minimal overhead (~60ms per file) and integrates seamlessly into the existing pre-commit workflow.

**Potential risks**  
The AST parsing may fail on files with syntax errors, but these are gracefully ignored. There's a theoretical risk of false positives if boolean operators appear in nested expressions within context managers, though the current implementation only checks the top-level context expression.

**Key insights**  
This is a targeted, effective solution for a real bug that caused production issues. The implementation is clean and focused, with clear error messages that educate developers about the correct patterns. Consider adding this check to CI pipelines as well for comprehensive coverage.

---

## 22. [[torch.compile] Disable recursive pre_grad_passes](https://github.com/vllm-project/vllm/pull/34092)


### Base Information

- **PR Number:** #34092
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-10 15:02:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34092/files) (2):**
  - `vllm/compilation/compiler_interface.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
The changes disable PyTorch Inductor's recursive pre-grad passes by default to improve vLLM cold compile times by approximately 1 second. This is implemented via a new environment variable `VLLM_ENABLE_PREGRAD_PASSES` (default: false) that, when disabled, patches the `_recursive_pre_grad_passes` function to return the graph unchanged. This optimization is temporary until PyTorch issue #174502 is resolved.

**Technical impact**  
When the environment variable is false, the pre-grad optimization passes are effectively skipped during compilation, reducing overhead. The generated output code remains identical, confirming these passes are unnecessary for vLLM's current workloads. This change only affects the compilation phase and does not alter runtime execution or model behavior.

**Potential risks**  
If PyTorch Inductor adds beneficial pre-grad optimizations before the upstream issue is fixed, vLLM might miss out on performance improvements. The patch relies on a private PyTorch API (`_recursive_pre_grad_passes`), which could change in future PyTorch versions, potentially breaking compatibility. There's also a risk if other parts of the codebase depend on side effects from these passes.

**Key insights**  
This is a targeted performance optimization that should be removed once the referenced PyTorch issue is resolved. Developers should monitor PyTorch releases and re-enable the passes via the environment variable if needed for testing. The use of an environment variable rather than a config option is appropriate given the temporary nature of this workaround.

---

## 23. [[Misc][Spec Decode] support different load config for draft model](https://github.com/vllm-project/vllm/pull/34022)


### Base Information

- **PR Number:** #34022
- **Author:** [ZhengkaiZ](https://github.com/ZhengkaiZ)
- **Merged By:** [luccafong](https://github.com/luccafong)
- **Merged time:** 2026-02-10 14:52:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34022/files) (3):**
  - `vllm/config/speculative.py`
  - `vllm/model_executor/model_loader/__init__.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
The changes add support for specifying a separate load configuration for draft models in speculative decoding. This allows customization of draft model loading (e.g., different checkpoint formats or performance optimizations) independent of the target model's load configuration.

**Technical impact**  
The system now accepts an optional `draft_load_config` parameter in `SpeculativeConfig`. When loading the draft model, this configuration overrides the default load config from the target model, providing flexibility for private or specially trained draft models while maintaining backward compatibility.

**Potential risks**  
If the draft load config specifies incompatible settings (e.g., different tensor parallelism than the target model), it could cause runtime errors or performance degradation. The changes don't include validation to ensure the draft load config aligns with the target model's architecture requirements.

**Key insights**  
Developers should carefully test draft load configurations in their deployment environments. Consider adding validation logic to catch configuration mismatches early. The implementation cleanly separates concerns by passing the config through the existing model loading pipeline without major architectural changes.

---

## 24. [[BugFix] Fix async EPLB hang with DeepEP LL all2all backend](https://github.com/vllm-project/vllm/pull/32860)


### Base Information

- **PR Number:** #32860
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-10 14:34:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32860/files) (2):**
  - `vllm/distributed/eplb/eplb_utils.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
Added a utility function `override_envs_for_eplb` that conditionally sets the `NCCL_MAX_CTAS` environment variable to 8. This prevents a deadlock when using async Expert Parallel Load Balancing (EPLB) with the DeepEP low-latency all2all backend. The deadlock occurs due to resource contention between NCCL kernels (for weight exchange) and DeepEP's cooperative launch kernels, which can block each other across different ranks.

**Technical impact**  
The change modifies NCCL's occupancy by limiting the maximum number of concurrent thread blocks (CTAs) it can use. This is applied only under specific conditions: when data parallelism, EPLB, async EPLB, and the DeepEP low-latency backend are all enabled. The override is applied early during worker initialization, ensuring it takes effect before any NCCL operations begin. Performance impact is minimal as NCCL is not on the hot path in DP/EP mode.

**Potential risks**  
If `NCCL_MAX_CTAS` is already set by the user, the function respects that value and does not override it, which could potentially leave the deadlock unresolved if the user's value is too high. The fixed value of 8 is a heuristic; while it resolves the documented issue, it may not be optimal for all hardware configurations or future versions of NCCL/DeepEP. There's also a risk of unintended side effects on other NCCL operations if this environment variable influences performance in unexpected ways.

**Key insights**  
This is a targeted workaround for a specific deadlock scenario, not a fundamental architectural fix. Developers should be aware that this environment variable modification is conditional and scoped. The fix highlights a deeper integration challenge between asynchronous scheduling, cooperative kernel launches, and collective communication libraries. Consider adding a configuration option for `NCCL_MAX_CTAS` to allow user tuning if performance regressions are observed in related scenarios.

---

## 25. [[Perf] Move eplb rebalance algo to async thread](https://github.com/vllm-project/vllm/pull/30888)


### Base Information

- **PR Number:** #30888
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-10 14:19:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30888/files) (5):**
  - `tests/distributed/test_eplb_execute.py`
  - `vllm/distributed/eplb/async_worker.py`
  - `vllm/distributed/eplb/eplb_state.py`
  - `vllm/distributed/eplb/rebalance_execute.py`
  - `vllm/distributed/parallel_state.py`

### Summary

**What changed and why**  
This PR moves the EPLB (Expert Parallel Load Balancing) rebalancing algorithm from the main execution thread to an async worker thread. The primary goal is to eliminate CPU bubbles caused by waiting for GPU computations during expert rebalancing. By offloading the rebalancing algorithm and CPU-GPU synchronization to a separate thread, the main execution thread can continue processing model inferences without interruption.

**Technical impact**  
The changes introduce a new EPLB-specific process group (`_EPLB`) separate from the EP group to prevent deadlocks when using torch.distributed concurrently. The async worker now handles the rebalancing algorithm execution and weight transfer coordination, while the main thread only performs minimal synchronization via CUDA events. This significantly reduces gaps between model execution steps (from 29ms to 1ms as shown in performance validation).

**Potential risks**  
The introduction of a separate EPLB process group adds complexity to group management and could lead to initialization issues if not properly synchronized. There's also a risk of race conditions between the async worker and main thread when accessing shared state like `new_physical_to_logical_map`. The non-blocking copy operations (`non_blocking=True`) could cause synchronization issues if not properly coordinated with CUDA events.

**Key insights**  
The PR successfully decouples computational heavy rebalancing from the critical path, yielding significant performance improvements. Developers should ensure proper CUDA event synchronization between threads and verify that the EPLB group initialization occurs correctly in all deployment scenarios. The changes to function signatures (removing layer parameters, changing tensor shapes) require careful attention when integrating with other components.

---

## 26. [[Feature] Warn about unrecognized environment variables](https://github.com/vllm-project/vllm/pull/33581)


### Base Information

- **PR Number:** #33581
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-10 13:45:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33581/files) (3):**
  - `tests/config/test_config_generation.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
Added environment variable validation for unrecognized VLLM_ prefixed variables. The implementation includes a new `fail_on_environ_validation` flag in EngineArgs that controls whether to issue warnings (default) or raise errors for unknown environment variables, addressing feature request #33096.

**Technical impact**  
The changes introduce a validation step during engine configuration creation that scans `os.environ` for VLLM_ prefixed variables and checks them against the known `environment_variables` set. This adds runtime validation that helps users detect typos or deprecated environment variables before they cause unexpected behavior.

**Potential risks**  
The validation occurs only during engine initialization, so environment variables set after engine creation won't be detected. There's also a performance consideration since scanning all environment variables occurs on every engine creation, though the impact should be minimal. The warning messages don't currently include guidance on how to resolve the issue (e.g., suggesting similar valid variables).

**Key insights**  
The implementation follows a good progressive rollout strategy with warnings first and errors opt-in via the new flag. Developers should ensure all legitimate VLLM_ environment variables are added to the `environment_variables` set in envs.py. Consider adding the validation to other initialization paths beyond `create_engine_config` if needed.

---

## 27. [[SM100] Resubmit FMHA FP8 prefill for MLA](https://github.com/vllm-project/vllm/pull/31195)


### Base Information

- **PR Number:** #31195
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-10 13:18:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31195/files) (3):**
  - `tests/v1/attention/test_mla_backends.py`
  - `vllm/config/attention.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
This PR resubmits FP8 FMHA (Fused Multi-Head Attention) support for MLA (Multi-Query Latent Attention) prefill, focusing on cleaning up how kernel backends opt into FP8 prefill. It introduces a new flag `--attention-config.use_prefill_query_quantization=true` to enable FP8 prefill, which is currently guarded due to slightly lower overall performance compared to BF16 prefill (despite kernel-level improvements). The changes refactor query quantization logic and ensure compatibility with supported backends (FlashInfer and TRT-LLM ragged prefill).

**Technical impact**  
The modifications centralize FP8 prefill query quantization logic in `MLACommonMetadataBuilder.determine_prefill_query_data_type()`, which checks cache dtype, the new flag, and backend compatibility. This enables FP8 quantization for queries during prefill when using FP8 KV cache, improving kernel performance by ~1.5x. The changes also update metadata structures to propagate query and output dtypes, adjust workspace allocations, and add FP8-specific cache gathering paths.

**Potential risks**  
1. The feature is limited to GB200 devices (device capability 100), restricting broader deployment.  
2. Slight performance degradation may occur due to extra casts and quantization overhead, despite kernel improvements.  
3. Backend support is incomplete (cuDNN prefill and FlashAttention are excluded), which could lead to inconsistent behavior across configurations.  
4. The FP8 path introduces conditional logic in cache gathering and projection, increasing code complexity and testing surface.

**Key insights**  
1. FP8 prefill is a performance optimization that requires careful backend and hardware validation—ensure compatibility before enabling.  
2. The guarded flag (`use_prefill_query_quantization`) allows incremental rollout; monitor performance metrics closely when activated.  
3. Developers should verify that FP8 prefill is only used with supported backends (FlashInfer/TRT-LLM) and FP8 KV cache to avoid silent fallbacks.  
4. Consider extending backend support and optimizing quantization overhead to realize the full kernel-level gains in end-to-end scenarios.

---

## 28. [[Bugfix] Fix mamba cache dtype for Qwen3.5](https://github.com/vllm-project/vllm/pull/34200)


### Base Information

- **PR Number:** #34200
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-10 13:12:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34200/files) (1):**
  - `vllm/model_executor/models/qwen3_5.py`

### Summary

**What changed and why**  
The change modifies the `get_mamba_state_dtype_from_config` method in the Qwen3.5 model implementation to use `mamba_ssm_dtype` from the Hugging Face model configuration instead of the user-provided `mamba_cache_dtype` from the cache config. This hardcodes the cache dtype to float32 for Qwen3.5, eliminating the need for users to manually specify it via command-line arguments.

**Technical impact**  
This change ensures Qwen3.5 consistently uses float32 for Mamba cache computations by directly reading the dtype from the model's internal configuration. It decouples the cache dtype from user input, simplifying deployment and reducing configuration errors, but makes the dtype setting implicit rather than configurable.

**Potential risks**  
If the `mamba_ssm_dtype` is missing or incorrectly set in the model config, it could lead to dtype mismatches or runtime errors. Additionally, hardcoding this value reduces flexibility for future model variants or optimizations that might benefit from different cache dtypes.

**Key insights**  
This fix improves user experience by removing a manual configuration step, but developers should verify that `mamba_ssm_dtype` is reliably populated in all Qwen3.5 model files. Consider adding validation or fallback logic to handle edge cases where the config value is absent.

---

## 29. [[Benchmarks] Fix attention benchmark smoke test](https://github.com/vllm-project/vllm/pull/34269)


### Base Information

- **PR Number:** #34269
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-10 13:04:07
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34269/files) (1):**
  - `.buildkite/test_areas/benchmarks.yaml`

### Summary

**What changed and why**  
The PR fixes a CI failure in the attention benchmark smoke test by adding a `working_dir` configuration and changing the Python command from `python` to `python3`. This ensures the test runs in the correct directory and uses the explicit Python 3 interpreter.

**Technical impact**  
These changes improve the reliability of the CI pipeline by preventing path-related issues and ensuring compatibility with systems where `python` may not point to Python 3. The benchmark execution environment is now more predictable.

**Potential risks**  
If the `working_dir` path (`/vllm-workspace/`) does not exist or lacks necessary permissions, the test could fail. Additionally, some environments might not have `python3` available, though this is less common in modern CI setups.

**Key insights**  
Always specify the working directory and use explicit Python version commands in CI configurations to avoid environment inconsistencies. Verify that the specified directory exists and is accessible in the CI environment to prevent further failures.

---

## 30. [[Bugfix] Fix weights offloading for sleep mode](https://github.com/vllm-project/vllm/pull/32947)


### Base Information

- **PR Number:** #32947
- **Author:** [jseppanen](https://github.com/jseppanen)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-10 12:38:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32947/files) (1):**
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The fix corrects a context manager syntax issue in the `load_model` method. The original code incorrectly used `and` to combine two context managers, which would only execute the second context manager. The new syntax properly creates a tuple of context managers using parentheses and commas.

**Technical impact**  
This ensures both context managers (`_maybe_get_memory_pool_context` and `set_current_vllm_config`) are properly activated when loading the model. This is critical for memory management during weight offloading in sleep mode, as evidenced by the significant memory recovery improvement (from 24.08 GiB to 54.51 GiB freed).

**Potential risks**  
The original bug could have caused inconsistent memory pool behavior or incorrect configuration settings during model loading. While the fix appears straightforward, there may be other similar context manager patterns in the codebase that need review. The memory reporting improvement suggests the weights memory pool wasn't being properly managed before.

**Key insights**  
Always use proper context manager syntax `with (cm1, cm2, ...):` instead of `with cm1 and cm2:`. This bug highlights the importance of testing memory management behaviors, especially for features like sleep mode and weight offloading. Consider adding a lint rule or test to catch similar context manager misuse patterns.

---

## 31. [Convert online APIs to use Renderer](https://github.com/vllm-project/vllm/pull/34084)


### Base Information

- **PR Number:** #34084
- **Author:** [reaganjlee](https://github.com/reaganjlee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 11:44:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34084/files) (2):**
  - `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`
  - `vllm/entrypoints/serve/disagg/serving.py`

### Summary

**What changed and why**  
The PR converts online APIs to use the Renderer architecture by replacing direct calls to `engine_client.generate()` with a two-step process: first calling `input_processor.process_inputs()` to create an `engine_request` object, then passing that request to `engine_client.generate()`. This aligns with the Renderer pattern for better request preprocessing and abstraction.

**Technical impact**  
These changes standardize request handling across different API endpoints (speech-to-text and disagg serving) by introducing a consistent preprocessing layer. The architecture now separates input processing from generation execution, improving modularity and enabling future enhancements like advanced request validation or transformation.

**Potential risks**  
The main risk is inconsistent parameter passing between `process_inputs()` and `generate()` calls, as seen in the disagg serving file where `tokenization_kwargs` appears in both calls but wasn't moved consistently. There's also a risk of breaking existing functionality if the `engine_request` object doesn't properly encapsulate all required parameters from the original direct calls.

**Key insights**  
Developers should verify that all necessary parameters are correctly forwarded through the new `engine_request` object, particularly noting that `tokenization_kwargs` appears in both preprocessing and generation steps in one file but not the other. This transition removes the TODO comment about EngineCoreRequest, indicating progress toward the Renderer architecture completion.

---

## 32. [[Misc] Introduce ec_both role EC (encoder cache) connector](https://github.com/vllm-project/vllm/pull/34182)


### Base Information

- **PR Number:** #34182
- **Author:** [furionw](https://github.com/furionw)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-10 10:55:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34182/files) (3):**
  - `vllm/config/ec_transfer.py`
  - `vllm/distributed/ec_transfer/ec_connector/base.py`
  - `vllm/v1/worker/ec_connector_model_runner_mixin.py`

### Summary

**What changed and why**  
This PR introduces a new `ec_both` role for the EC (encoder cache) connector to support aggregated EPD nodes that perform both embedding offloading and loading. Previously, an EC connector could only be configured as either a `producer` (store embeddings) or `consumer` (load embeddings), which prevented a single node from handling both operations.

**Technical impact**  
The changes extend the type definitions (`ECProducer` and `ECConsumer`) to include `"ec_both"`, add a new `is_consumer` property to the connector base class, and update the model runner logic to conditionally load caches based on the consumer role. This enables a single node to act as both producer and consumer, aligning with similar patterns like the KV connector.

**Potential risks**  
The main risk is ensuring that the `ec_both` role correctly handles concurrent or sequential producer/consumer operations without conflicts. Edge cases may arise if the node's behavior isn't properly synchronized between roles, potentially leading to data corruption or performance issues during cache transfers.

**Key insights**  
Developers should verify that the `ec_both` role is used only for aggregated EPD nodes as intended. The logic update in the model runner now correctly checks `is_consumer` instead of negating `is_producer`, which is more explicit. Ensure thorough testing of both producer and consumer functionalities when this role is active.

---

## 33. [[UX nit] Fix non-default api_server_count message](https://github.com/vllm-project/vllm/pull/34152)


### Base Information

- **PR Number:** #34152
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 10:35:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34152/files) (1):**
  - `vllm/entrypoints/cli/serve.py`

### Summary

**What changed and why**  
A single-line change was added to reset `args.api_server_count` to `None` when running a single API server. This prevents the parameter from appearing in non-default argument logs when users haven't explicitly specified it, addressing a minor UX issue where the default value was unnecessarily logged.

**Technical impact**  
The change ensures that the argument parsing logic correctly identifies user-provided versus default values. By setting `api_server_count` to `None` in the single-server path, the logging system will no longer treat the default value of `1` as a non-default argument, reducing noise in startup messages.

**Potential risks**  
If other parts of the codebase rely on `args.api_server_count` being an integer (e.g., `1`) during single-server execution, this change could introduce subtle bugs. Additionally, any downstream logic that checks for `None` versus a numeric value may need adjustment, though the impact appears limited to logging.

**Key insights**  
This is a clean, targeted fix for a logging issue, but developers should verify that no other code depends on the original integer value. Consider adding a comment explaining why the value is reset to `None` to prevent future misunderstandings. Also, ensure that the argument validation (`validate` method) still functions correctly with this change.

---

## 34. [Minor cleanup for Voxtral](https://github.com/vllm-project/vllm/pull/34247)


### Base Information

- **PR Number:** #34247
- **Author:** [andylolu2](https://github.com/andylolu2)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-10 10:18:31
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34247/files) (1):**
  - `vllm/model_executor/models/voxtral.py`

### Summary

**What changed and why**  
The change modifies how the Hann window is created for the Whisper mel-spectrogram computation in the Voxtral model. Instead of creating the window on CPU and then moving it to the target device, the window is now created directly on the target device by passing the `device` parameter to `torch.hann_window`.

**Technical impact**  
This improves performance by eliminating an unnecessary device transfer operation. The change ensures the Hann window tensor is allocated directly on the same device as the audio waveforms, reducing memory movement overhead and potentially improving execution speed during spectrogram computation.

**Potential risks**  
The risk is minimal since this is a straightforward optimization. However, developers should verify that the `audio_waveforms.device` attribute is always properly initialized and accessible. There's also a slight risk if different code paths expect the window to be created differently, but this seems unlikely.

**Key insights**  
This is a good practice optimization that follows PyTorch best practices for device-aware tensor creation. Developers should adopt this pattern throughout the codebase where possible to minimize unnecessary device transfers. The change demonstrates attention to performance details in audio processing pipelines.

---

## 35. [[Model Runner V2] Use pinned memory for write_contents](https://github.com/vllm-project/vllm/pull/34222)


### Base Information

- **PR Number:** #34222
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-10 08:55:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34222/files) (1):**
  - `vllm/v1/worker/gpu/buffer_utils.py`

### Summary

**What changed and why**  
The PR replaces a custom UVA buffer management system for `write_contents` with pinned memory and asynchronous host-to-device transfers. Instead of manually resizing GPU buffers using `next_power_of_2`, it now uses `async_tensor_h2d` with `pin_memory=True` to handle data transfer.

**Technical impact**  
This simplifies buffer management by eliminating manual reallocation logic and synchronization points. The change leverages existing `torch_utils` infrastructure for pinned memory transfers, which may improve performance by reducing GPU memory fragmentation and avoiding frequent reallocations.

**Potential risks**  
Removing the power-of-two buffer growth strategy could lead to more frequent allocations if `_staged_write_contents` size varies significantly. The pinned memory approach may increase host memory pressure, especially if large tensors are frequently transferred. There's also a dependency on `async_tensor_h2d` behaving correctly with pinned memory.

**Key insights**  
The simplification aligns with standard PyTorch memory practices, but monitor host memory usage and allocation frequency. Consider adding a fallback or warning if pinned memory allocation fails. The removal of explicit `torch.cuda.synchronize()` is positive but ensure no hidden race conditions exist in the async transfer.

---

## 36. [[Docs] Speed up build environment set-up](https://github.com/vllm-project/vllm/pull/34240)


### Base Information

- **PR Number:** #34240
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-10 08:34:44
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34240/files) (1):**
  - `.readthedocs.yaml`

### Summary

**What changed and why**  
The PR optimizes the ReadTheDocs build process by speeding up Git operations and switching to `uv` for Python environment management. The `post_checkout` fetch is now targeted and filtered to reduce fetch time from ~10-20s to ~3s, while `uv` replaces traditional pip/venv setup, cutting environment setup from ~35s to ~12s.

**Technical impact**  
These changes improve build performance by reducing network I/O and leveraging `uv`'s faster dependency resolution and installation. The build configuration is updated to use `uv` for environment creation and package installation, and the Git fetch is optimized by limiting scope and using blobless cloning.

**Potential risks**  
The `--filter=blob:none` may cause issues if the build process unexpectedly requires file blobs. Switching to `uv` introduces a new tool dependency; if `uv` has compatibility issues with certain packages or ReadTheDocs updates, it could break builds. The targeted Git fetch assumes only `origin main` is needed, which may fail if future documentation builds require other branches or tags.

**Key insights**  
These optimizations are low-risk and provide significant cumulative time savings. Developers should monitor build logs for any errors related to missing Git objects or `uv` installation. Consider documenting the rationale for these optimizations to prevent future reverts and ensure the team understands the trade-offs.

---

## 37. [[BUGFIX] Fix accuracy bugs in Qwen3-Next MTP](https://github.com/vllm-project/vllm/pull/34077)


### Base Information

- **PR Number:** #34077
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-10 07:57:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34077/files) (1):**
  - `vllm/v1/attention/backends/gdn_attn.py`

### Summary

**What changed and why**  
The changes fix accuracy bugs in Qwen3-Next MTP+CG (CUDAGraph) by correcting how padded sequences are handled during speculative decoding. Previously, zero-length padded sequences were incorrectly counted as prefill requests, and tensor indexing didn't properly exclude padded elements, leading to shape mismatches and hidden accuracy loss.

**Technical impact**  
These modifications ensure that padded sequences are excluded from prefill/decode counts and that tensor slicing (e.g., `spec_state_indices_tensor`, `spec_query_start_loc`) correctly filters out padded entries. This aligns tensor shapes with actual active sequences, preventing silent errors that only manifested under low-concurrency conditions where CUDAGraphs are enabled.

**Potential risks**  
The assertion preventing simultaneous non-speculative and speculative decodes (`assert not (num_decodes > 0 and num_spec_decodes > 0)`) could trigger in untested scenarios. Additionally, the assumption that "padded sequences are always at the back" may be fragile if batching logic changes. Edge cases with fully padded batches or varying padding positions need validation.

**Key insights**  
Always validate tensor indexing against actual active sequence counts, not just batch dimensions. The fix highlights that CUDAGraph correctness critically depends on precise padding handling. Developers should add explicit checks for padding invariants and consider adding unit tests for low-concurrency speculative decoding paths.

---

## 38. [[Core][BugFix] Fix PP KV cache sharding memory validation](https://github.com/vllm-project/vllm/pull/33698)


### Base Information

- **PR Number:** #33698
- **Author:** [junuxyz](https://github.com/junuxyz)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-10 07:46:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33698/files) (2):**
  - `tests/v1/core/test_kv_cache_utils.py`
  - `vllm/v1/core/kv_cache_utils.py`

### Summary

**What changed and why**  
The fix addresses a regression in Pipeline Parallelism (PP) where KV cache memory validation incorrectly used global layer specifications instead of per-worker subsets. This caused false out-of-memory errors because the validation assumed each worker held all layers, overestimating memory requirements. The solution projects global KV cache groups onto each worker's assigned layers and validates memory per worker.

**Technical impact**  
Memory validation now correctly accounts for PP sharding by projecting global KV cache groups to each worker's layer subset. The `_auto_fit_max_model_len` and memory check functions now iterate over per-worker projected groups, ensuring validation uses actual per-worker memory availability and layer assignments. This maintains the existing grouping strategy while fixing the validation logic.

**Potential risks**  
If the projection logic mishandles `UniformTypeKVCacheSpecs` or incorrectly filters layer names, it could lead to underestimation of memory needs. Asymmetric memory setups might still face issues if the `num_blocks` synchronization (step 5 in the docs) doesn't align with per-worker validation. Edge cases with empty layer assignments or hybrid models require careful testing.

**Key insights**  
The fix ensures PP memory validation reflects actual per-worker resource usage, preventing false OOM errors. Developers should verify that the projection function `_project_kv_cache_groups_to_worker` correctly handles all KV cache spec types, especially in hybrid or heterogeneous setups. The added unit tests provide coverage for PP sharding and projection logic, which should be extended to asymmetric memory scenarios.

---

## 39. [[Perf][Kernel] Add faster topKperRow decode kernel for DeepSeek-V3.2 sparse attention](https://github.com/vllm-project/vllm/pull/33680)


### Base Information

- **PR Number:** #33680
- **Author:** [LopezCastroRoberto](https://github.com/LopezCastroRoberto)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-10 07:29:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33680/files) (8):**
  - `CMakeLists.txt`
  - `csrc/ops.h`
  - `csrc/sampler.cu`
  - `csrc/topk.cu`
  - `csrc/torch_bindings.cpp`
  - `tests/kernels/test_top_k_per_row.py`
  - `vllm/model_executor/layers/sparse_attn_indexer.py`
  - `vllm/v1/attention/backends/mla/indexer.py`

### Summary

**What changed and why**  
This PR adds an optimized CUDA kernel `large_context_topk` specifically for DeepSeek-V3.2 sparse attention decode with K=2048. It targets long-context inference (seq_len > 8192) where the existing `top_k_per_row_decode` kernel becomes inefficient. The implementation uses a 5-pass radix selection algorithm adapted from SGLang/TileLang to handle large-K workloads more efficiently.

**Technical impact**  
The new kernel is conditionally integrated into the sparse attention indexer, selected only when batch size ≤ 128 and sequence length > 8192. This creates a hybrid approach where the original kernel handles prefill, short contexts, and large batches, while the optimized kernel accelerates long-context decode. The system now requires maintaining two top-k implementations with different performance characteristics.

**Potential risks**  
The kernel has a hardcoded K=2048, making it incompatible with other sparse attention configurations. The shared memory allocation (32KB) and thread block size (1024) may not be optimal across all GPU architectures. There's also a risk of incorrect kernel selection if the batch/sequence length thresholds aren't properly validated across different hardware.

**Key insights**  
This is a targeted optimization that provides 4-10% throughput improvements for specific DeepSeek-V3.2 workloads. Developers should note the strict applicability constraints and consider making K configurable for future sparse attention models. The implementation demonstrates how specialized kernels can complement general-purpose ones in production systems.

---

## 40. [[compile] Enable AOT compile with 2.10 in trunk.](https://github.com/vllm-project/vllm/pull/34155)


### Base Information

- **PR Number:** #34155
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 07:24:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34155/files) (1):**
  - `vllm/envs.py`

### Summary

**What changed and why**  
The change modifies the condition for enabling AOT (Ahead-Of-Time) compilation by lowering the required PyTorch version from `2.11.0.dev` to `2.10.0`. This aligns with an RFC plan to eventually disable AOT compilation by default in the next vLLM release, but for now keeps it enabled with PyTorch 2.10 to allow CI to catch regressions.

**Technical impact**  
This will expand AOT compilation usage in CI and for users with PyTorch 2.10, potentially improving performance for compatible setups. However, it may introduce new failures or performance variations if AOT compilation is less stable with PyTorch 2.10 compared to 2.11.dev.

**Potential risks**  
AOT compilation with PyTorch 2.10 might have unresolved bugs or compatibility issues that could cause runtime errors, increased memory usage, or degraded performance. The change also assumes PyTorch 2.10 is stable enough for AOT, which may need validation across different hardware and model configurations.

**Key insights**  
Developers should monitor CI results closely for any new failures related to compilation or execution. Consider adding version-specific fallbacks or warnings if issues arise. Ensure the follow-up plan to disable AOT by default in the next release is tracked and executed to avoid long-term reliance on this temporary enablement.

---

## 41. [[ROCm][Quantization] GPT_OSS in amd-quark format model loading and emulations](https://github.com/vllm-project/vllm/pull/29008)


### Base Information

- **PR Number:** #29008
- **Author:** [xuebwang-amd](https://github.com/xuebwang-amd)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-10 07:08:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29008/files) (13):**
  - `tests/kernels/moe/test_gpt_oss_triton_kernels.py`
  - `tests/models/quantization/test_gpt_oss.py`
  - `tests/models/quantization/test_gpt_oss_attn_quantization.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/utils.py`
  - `vllm/model_executor/layers/quantization/base_config.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/quark/quark.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`
  - `vllm/model_executor/layers/quantization/utils/ocp_mx_utils.py`
  - `vllm/model_executor/models/gpt_oss.py`

### Summary

**What changed and why**  
This PR extends GPT-OSS model support by unifying weight loading for both OpenAI (`mxfp4`) and AMD-Quark formats, and adds new quantization schemes for MoE layers. It introduces `QuarkOCP_MX_MoEMethod` to handle OCP-MX quantization (e.g., W4A16, W4AFP8), refactors fused MoE runtime to support FP8 activation emulation via QDQ, and adds bias support to attention projections. The changes enable loading and running quantized GPT-OSS models across different tensor-parallel configurations.

**Technical impact**  
The modifications create a unified loading path for GPT-OSS MoE weights, reducing code duplication and improving maintainability. New quantization configs (`mxfp4_w4a16_moe_quant_config`, `mxfp4_w4a8_moe_quant_config`) expand supported precision combinations. The fused MoE runtime now dynamically detects input quantization dtype and handles OCP-MX schemes generically, while the hidden-size rounding logic is refined to conditionally apply only for GPT-OSS with MXFP4. The refactored Quark MoE method centralizes backend selection (native/ROCm AIter/emulation) and improves scalability.

**Potential risks**  
- The conditional hidden-size rounding based on `model_type` and MXFP4 quantization could introduce subtle bugs if other model types later use similar quantization.  
- The expanded OCP-MX scheme enum may lead to unimplemented kernel paths (e.g., FP8-activation kernels are not yet available).  
- Bias addition to `qkv_proj` and `o_proj` may affect performance or compatibility with existing non-GPT-OSS models if not properly isolated.  
- The unified loading logic assumes consistent tensor layouts between OpenAI and Quark formats, which could break if formats diverge.

**Key insights**  
- Developers should ensure that any new quantization schemes added to `OCP_MX_Scheme` have corresponding kernel implementations or emulation paths.  
- The hidden-size rounding refactor moves the rounding earlier in layer initialization, which is more logical but requires careful validation of all model types.  
- The PR sets the stage for auto-mixed-precision quantization in GPT-OSS, but further refactoring of loading/inference methodologies will be needed as noted in the TODO.  
- Test coverage is robust with new Triton kernel equivalence and GSM8k accuracy tests, but edge cases in expert mapping and EP/TP slicing should be monitored.

---

## 42. [Support benchmarking of Geospatial models](https://github.com/vllm-project/vllm/pull/33922)


### Base Information

- **PR Number:** #33922
- **Author:** [mgazz](https://github.com/mgazz)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 07:04:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33922/files) (3):**
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/lib/endpoint_request_func.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
This PR adds support for benchmarking geospatial models like Prithvi by introducing two key features: a `--skip-tokenizer-init` flag to run benchmarks without tokenizer initialization (skipping token-related metrics), and a new `vllm-pooling` backend for sending requests to a `/pooling` endpoint. These changes enable benchmarking models that don't rely on traditional tokenization, such as those processing geospatial data via URLs or custom payloads.

**Technical impact**  
The modifications extend the benchmarking framework to handle non-tokenized workflows by conditionally skipping tokenizer usage in dataset sampling, metric calculation, and result printing. The new `async_request_vllm_pooling` function supports custom endpoints, allowing integration with specialized model serving pipelines. This maintains backward compatibility while adding flexibility for geospatial and other non-standard models.

**Potential risks**  
Skipping tokenizer initialization may lead to inaccurate token counts if downstream logic assumes tokenizer presence (e.g., prompt length defaults to 1). The new pooling backend assumes specific payload structures (`"truncate_prompt_tokens": -1`), which could cause errors if the endpoint expectations change. Edge cases like mixed tokenizer/non-tokenizer workflows are not explicitly handled.

**Key insights**  
Developers should ensure that custom datasets used with `--skip-tokenizer-init` provide appropriate prompt and output length metadata, as token-based validation is bypassed. The pooling backend is tailored for geospatial models; verify endpoint compatibility before use. Consider adding validation for required fields in pooling payloads to improve robustness.

---

## 43. [add --insecure arg to the vllm bench to skip TLS](https://github.com/vllm-project/vllm/pull/34026)


### Base Information

- **PR Number:** #34026
- **Author:** [fanyang-real](https://github.com/fanyang-real)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 06:23:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34026/files) (2):**
  - `tests/benchmarks/test_serve_cli.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
Added an `--insecure` flag to `vllm bench serve` to disable SSL certificate verification, enabling benchmarking against servers with self-signed certificates. The changes include a new CLI argument, SSL context handling in the benchmark logic, and a comprehensive test with a self-signed certificate.

**Technical impact**  
The benchmark client now conditionally disables SSL verification when `--insecure` is used, while maintaining default verification for HTTPS URLs. This modifies the aiohttp client session’s SSL configuration and propagates the setting through the request pipeline, including the model-fetching step.

**Potential risks**  
Disabling SSL verification exposes connections to man-in-the-middle attacks and should only be used in trusted testing environments. The `ssl_context` logic defaults to `True` for HTTPS URLs, which may cause confusion if `--insecure` is omitted but a self-signed certificate is used. The test relies on `openssl` being available in the environment.

**Key insights**  
The implementation cleanly integrates SSL context management across async HTTP calls. Developers should ensure `--insecure` is documented as a testing-only feature and not for production use. Consider adding a warning when the flag is used to emphasize security implications.

---

## 44. [Bump `mamba-ssm` version in CI for Transformers v5 compatibility](https://github.com/vllm-project/vllm/pull/34233)


### Base Information

- **PR Number:** #34233
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-10 05:46:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34233/files) (2):**
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/models_language.yaml`

### Summary

**What changed and why**  
The PR updates the `mamba-ssm` dependency from version `v2.2.5` to `v2.3.0` in two CI configuration files. This change ensures compatibility with Transformers v5 by incorporating a specific commit (`35e927b20fd674f0b30a799a6408b7aac6ffe642`) that updates imports removed in Transformers v5.

**Technical impact**  
This update modifies the CI pipeline to install a newer version of `mamba-ssm`, which includes necessary import adjustments. The change ensures that tests for language models (including hybrid models and Plamo2) run successfully against Transformers v5 without encountering import errors.

**Potential risks**  
Upgrading to `mamba-ssm v2.3.0` may introduce new dependencies or behavioral changes that could affect model inference or test stability. There is also a risk if the new version contains bugs or regressions not present in `v2.2.5`, which could cause CI failures beyond the import fixes.

**Key insights**  
The update is critical for maintaining compatibility with Transformers v5, but thorough testing of language model generation and hybrid model workflows is recommended. Developers should monitor CI results closely and consider pinning the dependency more precisely if future breaking changes occur in `mamba-ssm`.

---

## 45. [[V1][BugFix] Fix EAGLE3 encoder cache miss with disable_chunked_mm_input](https://github.com/vllm-project/vllm/pull/34220)


### Base Information

- **PR Number:** #34220
- **Author:** [KrxGu](https://github.com/KrxGu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 05:05:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34220/files) (2):**
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The fix addresses an encoder cache miss issue in EAGLE3 speculative decoding when multimodal inputs are used with `disable_chunked_mm_input=True`. The scheduler's rollback calculation in `_try_schedule_encoder_inputs` did not account for `shift_computed_tokens`, causing it to schedule tokens overlapping the multimodal range without scheduling encoder inputs, triggering assertions.

**Technical impact**  
This change ensures that when EAGLE3's speculative shift is active, the scheduler correctly rolls back token scheduling to stop before the multimodal input range, guaranteeing encoder inputs are scheduled when needed. This maintains cache consistency and prevents assertion failures during multimodal inference with speculative decoding.

**Potential risks**  
If the shift calculation is incorrect or negative values arise, the `max(0, ...)` guard prevents negative `num_new_tokens`, but improper shift values could still cause under-scheduling. Edge cases where `shift_computed_tokens` varies per request or changes dynamically might not be fully handled.

**Key insights**  
Developers should verify that `shift_computed_tokens` is accurately computed and consistent across the scheduling logic. The regression test effectively captures the boundary condition; similar tests should be added for other speculative decoding variants to ensure robustness. Always validate encoder scheduling when modifying token-shift mechanisms.

---

## 46. [Stop testing for slow tokenizers as they will not exist soon](https://github.com/vllm-project/vllm/pull/34235)


### Base Information

- **PR Number:** #34235
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-10 04:08:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34235/files) (1):**
  - `tests/tokenizers_/test_basic.py`

### Summary

**What changed and why**  
The PR removes test code that verifies slow tokenizer functionality, specifically the test case for `PreTrainedTokenizer` (slow tokenizer). This change anticipates Transformers v5 where slow tokenizers will no longer exist, making the test obsolete.

**Technical impact**  
This simplifies the test suite by eliminating deprecated functionality checks. The remaining test now focuses solely on fast tokenizers (`PreTrainedTokenizerFast`), aligning the codebase with the upcoming architectural shift in the Transformers library.

**Potential risks**  
If the library upgrade to Transformers v5 is delayed or rolled back, the test coverage gap could mask regressions in slow tokenizer behavior. Additionally, any downstream code still relying on slow tokenizers might encounter undetected compatibility issues during transition periods.

**Key insights**  
This is a forward-looking change that prepares the test suite for library evolution. Developers should ensure all tokenizer-related code exclusively uses fast tokenizers and monitor the Transformers v5 release timeline. Consider adding version-specific conditional testing if backward compatibility remains a concern during migration.

---

## 47. [[Misc] allow specify is_mm_prefix_lm in hf_config](https://github.com/vllm-project/vllm/pull/34215)


### Base Information

- **PR Number:** #34215
- **Author:** [lkhphuc](https://github.com/lkhphuc)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-10 03:16:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34215/files) (1):**
  - `vllm/config/model.py`

### Summary

**What changed and why**  
The PR adds a new mechanism to determine if a model should use PrefixLM (bidirectional attention) for multimodal positions. Previously, this was controlled solely by a hardcoded list (`MM_PREFIX_LM_MODELS`). Now, the model's Hugging Face configuration can optionally specify this behavior via a boolean field `is_mm_prefix_lm`, with a fallback to the existing hardcoded list if the field is absent.

**Technical impact**  
This change introduces a more flexible, configuration-driven approach for enabling PrefixLM in multimodal models. It decouples the logic from a static list, allowing new or custom models (especially those in specialized forks like vllm-omni) to opt into PrefixLM without requiring code changes in the main vllm repository.

**Potential risks**  
If the `is_mm_prefix_lm` field is incorrectly set in a model's config (e.g., a non-boolean value), the `bool()` conversion may lead to unexpected behavior. Additionally, the fallback logic relies on the hardcoded list, which could cause inconsistencies if models are added there later without updating configs. There is also a risk of silent misconfiguration if the field is missing or misspelled.

**Key insights**  
This is a sensible extension that improves configurability for multimodal models. Developers should ensure that any model using this new field explicitly documents it and validates the config value. Consider adding a warning log when falling back to the hardcoded list for better debuggability. Future work could include deprecating the hardcoded list entirely in favor of config-driven flags.

---

## 48. [Add flagos in MiniCPM-o](https://github.com/vllm-project/vllm/pull/34126)


### Base Information

- **PR Number:** #34126
- **Author:** [tc-mb](https://github.com/tc-mb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 02:51:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34126/files) (1):**
  - `vllm/model_executor/models/minicpmo.py`

### Summary

**What changed and why**  
Added conditional import and configuration of the `flag_gems` library when the `USE_FLAGOS=1` environment variable is set. This enables specific operator optimizations for the MiniCPM-O model inference.

**Technical impact**  
Introduces an optional dependency on the `flag_gems` library that activates a predefined set of operator optimizations. The change is backward compatible—existing functionality remains unchanged unless the environment variable is explicitly set, making it a non-breaking feature addition.

**Potential risks**  
The conditional import creates a hidden dependency; if `USE_FLAGOS=1` is set but `flag_gems` is not installed, the import will fail at runtime. The list of enabled operators is hardcoded and may require updates if the library interface changes or if different operators are needed.

**Key insights**  
Consider adding a try-except block around the import to provide a clearer error message if `flag_gems` is missing. Document the environment variable requirement and the purpose of the enabled operators for future maintainers. Validate that the selected operators align with the model's computational patterns.

---

## 49. [[Bugfix] Fix FI kernel`chunk_gated_delta_rule` output shape for Qwen3.5](https://github.com/vllm-project/vllm/pull/34219)


### Base Information

- **PR Number:** #34219
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-10 02:41:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34219/files) (1):**
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
The fix addresses a shape mismatch in the `fi_chunk_gated_delta_rule` function for Qwen3.5. Previously, the function returned the output directly from `chunk_gated_delta_rule_fi`, but now it correctly unpacks the tuple into `output` and `final_state`, then unsqueezes the output to restore a 4D shape (1, L, H, D) to match the expected format from the FLASHINFER implementation.

**Technical impact**  
This change ensures compatibility between the FLASHINFER kernel output and the model's expected tensor dimensions. The unsqueeze operation adds a batch dimension back, which is crucial for downstream operations that rely on consistent tensor shapes. The function now properly returns both the transformed output and the final state as a tuple.

**Potential risks**  
If other parts of the codebase assume the old output shape (without the batch dimension), this could introduce subtle bugs. There's also a risk if the unsqueeze dimension (0) doesn't align with the expected batch axis in all usage contexts. The change assumes `chunk_gated_delta_rule_fi` always returns a tuple, which could fail if the underlying kernel changes.

**Key insights**  
Developers should verify that all callers of this function handle the 4D output correctly. This fix highlights the importance of maintaining consistent tensor shape contracts between custom kernels and model layers. Consider adding a shape assertion or comment explaining why the unsqueeze is necessary for clarity.

---

## 50. [[Docs] Fix format error in KV load failure recovery doc](https://github.com/vllm-project/vllm/pull/34137)


### Base Information

- **PR Number:** #34137
- **Author:** [zzaebok](https://github.com/zzaebok)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 02:16:27
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34137/files) (1):**
  - `examples/offline_inference/kv_load_failure_recovery/README.md`

### Summary

**What changed and why**  
Added a missing closing backticks (` ``` `) to a code fence in the KV Load Failure Recovery documentation. This fixes a rendering issue where the code block was not properly terminated, ensuring the documentation displays correctly.

**Technical impact**  
The change only affects documentation rendering—specifically, the formatting of a code example in the Markdown file. It has no impact on code functionality, build processes, or runtime behavior.

**Potential risks**  
Minimal risk, as this is a documentation-only fix. The only potential issue is if the added backticks are misplaced or introduce new formatting errors, but the provided test confirms correct rendering.

**Key insights**  
Always validate documentation changes with a local preview (e.g., `mkdocs serve`) to catch formatting errors. While trivial, such fixes improve readability and user experience, and they should be reviewed to ensure they don’t inadvertently break other parts of the document.

---

## 51. [[Bugfix] Fix `--trust-remote-code` conflict](https://github.com/vllm-project/vllm/pull/34218)


### Base Information

- **PR Number:** #34218
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-10 00:29:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34218/files) (2):**
  - `examples/offline_inference/spec_decode.py`
  - `vllm/benchmarks/datasets.py`

### Summary

**What changed and why**  
Removed `--trust-remote-code` argument from the dataset parser in `datasets.py` to resolve a conflict introduced by PR #34188. Also removed import fallback logic in `spec_decode.py` to ensure consistent CLI parsing behavior and prevent silent errors.

**Technical impact**  
These changes simplify the codebase by removing redundant argument definitions and import fallback patterns. The CLI interface becomes more consistent, and the system now relies on a single source for argument definitions, reducing potential parsing conflicts.

**Potential risks**  
Removing the import fallback could break environments where `FlexibleArgumentParser` is not available, though this is likely intentional to enforce proper dependencies. The removal of `--trust-remote-code` from the dataset parser may affect users who relied on it for specific dataset loading scenarios, though it should be handled elsewhere.

**Key insights**  
Always centralize CLI argument definitions to avoid conflicts. Import fallbacks can mask dependency issues; explicit imports are preferred for clarity and error detection. Verify that argument removal doesn't break existing workflows, especially for less common use cases.

---

## 52. [[Bugfix] Fix memory inconsistency in cross-process shared memory](https://github.com/vllm-project/vllm/pull/32022)


### Base Information

- **PR Number:** #32022
- **Author:** [slippersss](https://github.com/slippersss)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-10 00:22:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32022/files) (1):**
  - `vllm/distributed/device_communicators/shm_broadcast.py`

### Summary

**What changed and why**  
A memory fence (`memory_fence()`) was added before setting `metadata_buffer[0] = 1` in the shared memory write acquisition logic. This ensures that all writes to the data buffer (`buf`) are fully visible to other processes before the flag indicating new data is ready is set. The fix addresses intermittent memory inconsistency issues on CPU architectures with weak memory ordering, as described in issue #27858.

**Technical impact**  
The change enforces strict write ordering between the shared data buffer and its metadata flag across processes. This prevents readers from observing `metadata_buffer[0] = 1` (indicating data is ready) while the associated `buf` content is still partially written or not yet visible due to CPU or compiler reordering. It strengthens cross-process synchronization guarantees in the shared memory communication layer.

**Potential risks**  
While the memory fence resolves the ordering issue, it may introduce minor performance overhead due to enforced memory barrier operations. The fix assumes `memory_fence()` is implemented correctly for all target platforms; if not, the inconsistency could persist. Additionally, the intermittent nature of the bug means thorough long-term testing is required to confirm the fix under high concurrency.

**Key insights**  
This fix highlights the importance of explicit memory ordering in low-level, cross-process synchronization—especially on weakly ordered architectures. Developers should audit similar shared memory or lock-free patterns for missing barriers. Consider adding architecture-specific memory ordering annotations (e.g., `std::atomic` in C++ or `atomic` in Python with appropriate fences) where applicable, and ensure the fence implementation is platform-robust.

---

