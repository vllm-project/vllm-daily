# vLLM Merged PR Report

**Report Date:** 2026-02-16 PST

**Total Merged PRs:** 27

---

## 1. [[CI] Fix bake config artifact path for AMI rebuild pipeline](https://github.com/vllm-project/vllm/pull/34656)


### Base Information

- **PR Number:** #34656
- **Author:** [amrmahdi](https://github.com/amrmahdi)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-16 22:39:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34656/files) (1):**
  - `.buildkite/image_build/image_build.sh`

### Summary

**What changed and why**  
The change modifies the artifact upload command to execute from within the temporary directory where the bake configuration file is stored. This ensures the artifact path uploaded to Buildkite is just the filename (e.g., `bake-config-build-NNN.json`) rather than including the full temporary directory path. The fix addresses a regression where the AMI rebuild pipeline could not locate the artifact due to mismatched path expectations.

**Technical impact**  
This adjustment corrects the artifact naming in Buildkite's storage, aligning it with the downstream pipeline's lookup logic. The change is minimal and localized to the upload step, preserving the existing workflow of generating the file in a temporary directory while ensuring reliable artifact retrieval.

**Potential risks**  
If the temporary directory is cleaned up prematurely or is inaccessible during the upload, the command may fail. Additionally, any scripts that previously relied on the full artifact path (though unlikely) would need updates. The subshell `cd` approach assumes `dirname` and `basename` are available and function as expected in the environment.

**Key insights**  
Always verify artifact paths match downstream consumption patterns when changing file locations. Using relative paths during upload is a clean solution to control artifact naming. Consider adding a comment in the script explaining why the `cd` is necessary to prevent future regressions.

---

## 2. [[Model Runner V2] Minor refactoring for penalties](https://github.com/vllm-project/vllm/pull/34662)


### Base Information

- **PR Number:** #34662
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-16 21:43:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34662/files) (4):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/bad_words.py`
  - `vllm/v1/worker/gpu/sample/penalties.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`

### Summary

**What changed and why**  
This PR refactors penalty and bad words processing to accept a unified `RequestState` object instead of individual tensor parameters. The primary motivation is to leverage GPU tensors like `prompt_len.gpu` to enable batched `bincount` operations across multiple requests with a single kernel launch, improving performance. Additionally, it aligns the initialization style of `PenaltiesState` and `BadWordsState` with `BadWordsState` for consistency.

**Technical impact**  
The changes centralize state management through `RequestState`, reducing parameter passing and enabling more efficient GPU operations. The `bincount` kernel is now vectorized to process multiple requests simultaneously, which should reduce kernel launch overhead and improve throughput. The architecture becomes cleaner with less duplicated tensor passing between components.

**Potential risks**  
The batched `bincount` operation introduces a dependency on `max_prefill_len` across requests, which could cause performance degradation if requests have highly variable prefill lengths. The `idx_mapping` tensor creation adds minor CPU-GPU transfer overhead. There's also a risk of incorrect tensor indexing in the refactored `_bincount_kernel` due to the added stride calculations.

**Key insights**  
The refactoring successfully reduces code duplication and enables more efficient GPU utilization. Developers should verify that the batched `bincount` kernel correctly handles edge cases where `max_prefill_len` is significantly larger than individual request lengths. The consistent use of `RequestState` across components establishes a good pattern for future state management improvements.

---

## 3. [[Model Runner V2] Minor simplification for BadWordsState](https://github.com/vllm-project/vllm/pull/34669)


### Base Information

- **PR Number:** #34669
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-16 21:27:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34669/files) (1):**
  - `vllm/v1/worker/gpu/sample/bad_words.py`

### Summary

**What changed and why**  
The `use_bad_words` boolean array was removed from `BadWordsState` and its related logic was simplified. Instead of tracking whether each request uses bad words separately, the code now checks if the maximum number of bad words across requests is zero to determine if the bad words kernel should be skipped.

**Technical impact**  
This change reduces memory usage by eliminating the `use_bad_words` array and simplifies the state management. The logic for determining whether to apply bad words filtering now relies solely on the `num_bad_words` tensor, making the code more straightforward and reducing redundant state tracking.

**Potential risks**  
The main risk is that the condition `max_num_bad_words == 0` might be slightly less efficient than the previous `np.any(self.use_bad_words[idx_mapping_np])` check when only a small subset of requests have bad words, as it requires computing the maximum across all mapped requests rather than a simple boolean check. However, this is likely negligible in practice.

**Key insights**  
This is a clean simplification that removes redundant state. Developers should note that the bad words kernel skipping logic now depends entirely on the `num_bad_words` values, ensuring consistency between the two previously separate indicators. The change demonstrates good refactoring by eliminating unnecessary complexity while maintaining the same functional behavior.

---

## 4. [[Model Runner V2] Minor cleanup for PP](https://github.com/vllm-project/vllm/pull/34666)


### Base Information

- **PR Number:** #34666
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-16 19:15:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34666/files) (2):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/pp_handler.py`

### Summary

**What changed and why**  
The changes refactor pipeline parallelism (PP) handling by removing the `get_pp_handler` factory function and embedding PP rank checks directly into the ModelRunner. Instead of dynamically querying PP group status, the runner now stores `is_first_pp_rank` and `is_last_pp_rank` flags during initialization, and the PPHandler is instantiated with a device parameter.

**Technical impact**  
This reduces runtime overhead by replacing repeated `get_pp_group()` calls with cached boolean flags, simplifying conditional logic throughout the model execution flow. The PPHandler now manages its own device state, eliminating the need to pass the device repeatedly during token reception. The changes also consolidate speculative decoding execution to only occur on the last PP rank.

**Potential risks**  
If PP group topology changes dynamically after initialization, the cached rank flags could become stale. The removal of the `get_pp_handler` factory weakens encapsulation and may complicate testing. Additionally, the `assert received is not None` in `sample_tokens` assumes non-last ranks always receive data, which could fail if broadcast synchronization is disrupted.

**Key insights**  
The refactor improves performance by minimizing distributed communication checks, but developers should ensure PP group configuration is static per ModelRunner instance. Consider adding validation for PP rank flag consistency in long-running processes. The simplified PPHandler initialization enhances code clarity but reduces modularity—weigh this trade-off if future extensions are anticipated.

---

## 5. [[Model Runner V2] Fix unintended CPU-GPU sync in make_dummy](https://github.com/vllm-project/vllm/pull/34667)


### Base Information

- **PR Number:** #34667
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-16 19:00:30
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34667/files) (1):**
  - `vllm/v1/worker/gpu/input_batch.py`

### Summary

**What changed and why**  
The change fixes an unintended CPU-GPU synchronization issue by replacing a scalar assignment to a tensor element (`input_buffers.query_start_loc[0] = 0`) with a slice assignment (`input_buffers.query_start_loc[:1] = 0`). This prevents a potential performance penalty caused by synchronizing the GPU to update a single element.

**Technical impact**  
This modification maintains the same logical behavior (setting the first element to zero) while avoiding a host-device synchronization point. The slice assignment allows PyTorch to handle the operation more efficiently, potentially keeping the computation entirely on the GPU without interrupting the execution pipeline.

**Potential risks**  
The risk is minimal since the change preserves the original intent. However, developers should verify that `input_buffers.query_start_loc` is indeed a GPU tensor and that the slice assignment behaves identically in all execution contexts. There's a slight risk if the tensor is unexpectedly on CPU or has unexpected dimensions.

**Key insights**  
This is a performance optimization that demonstrates awareness of PyTorch's synchronization behavior. Developers should prefer slice operations over single-element assignments when working with GPU tensors to avoid unnecessary synchronization. The pattern should be reviewed for similar issues elsewhere in the codebase.

---

## 6. [[Bugfix] Fix fused MoE int32 overflow in stride*offset without perf regression](https://github.com/vllm-project/vllm/pull/34507)


### Base Information

- **PR Number:** #34507
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 17:58:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34507/files) (2):**
  - `tests/kernels/moe/test_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The fix addresses an int32 overflow issue in fused MoE kernels where `stride_cm * offs_token` could exceed 32-bit limits when chunking is disabled and token counts are large. Instead of widening all stride parameters to int64 (which caused significant performance regression), the solution casts only `offs_token` to int64 after loading, leveraging Triton's type promotion to prevent overflow while minimizing register pressure.

**Technical impact**  
This change ensures numerical correctness for large-scale MoE operations without sacrificing performance on small GPUs. The casting approach follows existing patterns in the codebase (e.g., `off_experts` and `offs_bn` are already int64) and maintains compatibility with Triton's type promotion rules. The regression test validates the fix by explicitly disabling chunking and using parameters that trigger the overflow condition.

**Potential risks**  
While the overflow is mitigated, the cast occurs after `offs_token` is loaded, which assumes the pointer arithmetic `sorted_token_ids_ptr + offs_token_id` remains within int32 bounds during memory access. If `offs_token_id` itself could overflow int32 during computation, this might still cause issues. Additionally, the test requires substantial GPU memory (~12 GB), which could limit its execution in some CI environments.

**Key insights**  
The fix demonstrates a targeted approach to overflow prevention by widening only the offset variable rather than all strides, balancing correctness and performance. Developers should audit similar stride-offset multiplications in other kernels, especially when dimensions scale beyond typical ranges. Ensure that any future changes to token indexing or chunking logic consider 64-bit safety for large `M * topk` products.

---

## 7. [[CI] Enable mypy import following for vllm/v1/kv_offload](https://github.com/vllm-project/vllm/pull/34639)


### Base Information

- **PR Number:** #34639
- **Author:** [aneeshkp](https://github.com/aneeshkp)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-16 17:58:15
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34639/files) (1):**
  - `tools/pre_commit/mypy.py`

### Summary

**What changed and why**  
The PR removes `vllm/v1/kv_offload` from the `SEPARATE_GROUPS` list in the mypy pre-commit configuration. This change ensures mypy uses `follow_imports = "silent"` (like the main group) instead of `--follow-imports skip` for this module, enabling proper import-following type checking.

**Technical impact**  
This resolves inconsistencies where local pre-commit checks on individual files could produce false type errors due to skipped import analysis. The module will now be type-checked with full import tracking, aligning it with the rest of the codebase and improving type safety.

**Potential risks**  
If `vllm/v1/kv_offload` has unresolved type issues that were previously masked by `--follow-imports skip`, this change could expose new mypy errors. However, the test plan shows zero errors, indicating the module is already type-clean.

**Key insights**  
This is a low-risk configuration fix that enhances type consistency. Developers should verify that future changes to `vllm/v1/kv_offload` maintain type correctness, as mypy will now perform stricter import-based checks. The change also demonstrates the importance of aligning mypy settings across modules to avoid false negatives in local development.

---

## 8. [[Core] Pipeline Parallel support for Model Runner V2](https://github.com/vllm-project/vllm/pull/33960)


### Base Information

- **PR Number:** #33960
- **Author:** [ZhanqiuHu](https://github.com/ZhanqiuHu)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-16 17:48:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33960/files) (2):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/pp_handler.py`

### Summary

**What changed and why**  
This PR adds Pipeline Parallel (PP) support to Model Runner V2 by introducing a new `PPHandler` class that encapsulates all PP communication logic. The changes modify `execute_model` to handle intermediate tensor passing between PP stages and `sample_tokens` to broadcast sampled tokens from the last rank. The goal is to enable PP in V2 while maintaining clean separation of concerns and matching V1's accuracy.

**Technical impact**  
The implementation introduces a modular PP handler that centralizes communication, reducing conditional logic in the model runner. It changes the flow: first/middle ranks forward intermediate tensors, while only the last rank performs sampling and broadcasts results. The model runner now returns `IntermediateTensors` for non-last ranks and `ModelRunnerOutput` only from the last rank. CUDA graph capture is disabled when PP is enabled (eager-only for now).

**Potential risks**  
Disabling CUDA graphs for PP may impact performance until future support is added. The synchronous broadcast of sampled tokens could become a bottleneck, especially with speculative decoding. Edge cases like variable `max_sample_len` for speculative decoding need careful handling. There's also a risk of state desynchronization if broadcast/receive logic fails on any rank.

**Key insights**  
The PP handler's no-op design simplifies caller code by eliminating rank checks. Developers should note that PP currently runs in eager mode only—CUDA graph support is a known TODO. Future optimizations should focus on asynchronous communication for intermediate tensors and sampled tokens. The deterministic request sorting (`req_id` tie-breaker) is critical for consistent batch ordering across PP ranks.

---

## 9. [[Model Runner V2] support bad_words sampling param](https://github.com/vllm-project/vllm/pull/33433)


### Base Information

- **PR Number:** #33433
- **Author:** [izhuhaoran](https://github.com/izhuhaoran)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-16 16:36:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33433/files) (7):**
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/bad_words.py`
  - `vllm/v1/worker/gpu/sample/penalties.py`
  - `vllm/v1/worker/gpu/sample/prompt_logprob.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`
  - `vllm/v1/worker/gpu/states.py`

### Summary

**What changed and why**  
This PR adds support for the `bad_words` sampling parameter in Model Runner V2. The implementation introduces a new `BadWordsState` class that tracks forbidden token sequences and modifies the sampling process to mask logits of tokens that would complete any bad word sequence. Key changes include renaming `prefill_token_ids` to `all_token_ids` to better reflect its expanded purpose and adding `total_len` tracking for request progression.

**Technical impact**  
The changes extend the sampling pipeline with a new bad words filtering stage that runs after logit computation but before sampling. This requires maintaining additional per-request metadata (bad word token sequences, offsets, and counts) and modifying several components to use the renamed `all_token_ids` tensor, which now holds both prompt and generated tokens. The architecture now supports dynamic bad word lists with configurable limits (max 128 bad words, 1024 total tokens).

**Potential risks**  
The Triton kernel for bad word matching performs linear scans over bad word lists for each token position, which could impact performance for requests with many bad words. Edge cases include handling speculative decoding tokens correctly and ensuring the kernel correctly skips requests without bad words. The fixed-size buffers (`MAX_BAD_WORDS_TOTAL_TOKENS`, `MAX_NUM_BAD_WORDS`) may cause errors for extreme configurations.

**Key insights**  
Developers should note the semantic rename from `prefill_token_ids` to `all_token_ids`—this tensor now contains the full token history. The bad words implementation efficiently reuses existing token tracking infrastructure but adds non-trivial per-token computation. Consider performance profiling for workloads with extensive bad word lists, and ensure error handling for buffer limit violations is clearly communicated to users.

---

## 10. [[NemotronH] Do not force router to run in fp32](https://github.com/vllm-project/vllm/pull/34582)


### Base Information

- **PR Number:** #34582
- **Author:** [roikoren755](https://github.com/roikoren755)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 10:15:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34582/files) (2):**
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
The PR removes forced FP32 computation for the MoE router in NemotronH models. Previously, router logits were computed in FP32 regardless of checkpoint precision (BF16), consuming ~40% of forward pass time without accuracy benefits. The changes allow router computations to use the model's native data type.

**Technical impact**  
Router computations now use the model's input data type (likely BF16), reducing memory bandwidth and computational overhead. The DeepSeekV3 routing method retains explicit FP32 conversion in the fused MoE implementation, while NemotronH's gate layer and forward pass operate in the native precision.

**Potential risks**  
DeepSeekV3 routing might have precision requirements that justify keeping FP32 conversion. Removing FP32 forcing could expose numerical instability in other routing methods if they were relying on FP32's wider dynamic range. Mixed precision behavior should be validated across all supported routing types.

**Key insights**  
This optimization provides significant performance gains (40% of forward pass) without accuracy degradation, as shown by GSM8K benchmarks. Developers should ensure routing method-specific precision requirements are documented and tested. The selective FP32 retention for DeepSeekV3 suggests routing algorithms may have different numerical sensitivity.

---

## 11. [Targeting the MI355 agent pool with all existing tests](https://github.com/vllm-project/vllm/pull/34629)


### Base Information

- **PR Number:** #34629
- **Author:** [Alexei-V-Ivanov-AMD](https://github.com/Alexei-V-Ivanov-AMD)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-16 09:02:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34629/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
A new test configuration section targeting the MI355 agent pool has been added to the CI/CD pipeline. This duplicates the existing test suite to run on AMD MI355 hardware, as indicated by the comment stating the test set is currently mirrored while determining which tests should ultimately be routed there.

**Technical impact**  
This change extends CI coverage to AMD MI355 hardware by creating parallel test jobs that mirror existing tests. It introduces new agent pool configurations (`mi355_1` for single-GPU tests and `mi355_4` for 4-GPU distributed tests) while maintaining the same test commands, dependencies, and structure as existing configurations.

**Potential risks**  
- Duplicate test execution could double CI resource consumption and runtime if not properly managed during the transition period.
- Hardware-specific issues may arise since the tests were originally designed for different hardware platforms.
- The comment indicates this is temporary ("TBD which tests are to be routed there ultimately"), suggesting potential future breaking changes to this configuration.

**Key insights**  
- This is an infrastructure expansion to support AMD MI355 hardware validation.
- Developers should monitor test results on the new pool for hardware-specific failures.
- The mirrored approach suggests a future refactoring where tests will be selectively routed rather than duplicated.

---

## 12. [[Bugfix][CI] Fix flaky `entrypoints/openai/test_response_api_with_harmony.py::test_function_calling[openai/gpt-oss-20b]`](https://github.com/vllm-project/vllm/pull/34624)


### Base Information

- **PR Number:** #34624
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-16 08:11:07
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34624/files) (1):**
  - `tests/entrypoints/openai/responses/test_harmony.py`

### Summary

**What changed and why**  
The test function `get_weather` was modified to return a static temperature value (15.0) instead of making an actual HTTP request to the external Open‑Meteo API. This change eliminates flaky test failures caused by SSL/network errors in CI environments, ensuring consistent test execution.

**Technical impact**  
The test no longer depends on external network connectivity or third‑party API availability, making it fully deterministic and isolated. This improves CI reliability and execution speed, but removes the integration‑testing aspect of the external API call.

**Potential risks**  
If the test’s purpose was to validate real API interactions or response parsing, mocking the call removes that coverage. Additionally, hard‑coding the return value could mask issues in how the function’s output is used elsewhere in the test suite.

**Key insights**  
Mocking external dependencies is a best practice for unit tests to ensure stability, but consider whether this test should remain an integration test. If the API interaction is critical, move it to a separate integration test suite or use a more sophisticated mock that validates request/response shapes.

---

## 13. [[Bugfix] Treat generation_config max_tokens as default not ceiling](https://github.com/vllm-project/vllm/pull/34063)


### Base Information

- **PR Number:** #34063
- **Author:** [almogtavor](https://github.com/almogtavor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 07:58:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34063/files) (7):**
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/entrypoints/test_utils.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/utils.py`

### Summary

**What changed and why**  
The fix addresses issue #34005 where `generation_config.max_tokens` was incorrectly treated as an unconditional ceiling, overriding explicit user requests. The core change modifies `get_max_tokens()` to treat the generation config value as a fallback default only when the user does not specify `max_tokens`. A new `override_max_tokens` parameter is introduced to support explicit server-side limits when configured via `--override-generation-config`.

**Technical impact**  
The logic in `get_max_tokens()` now prioritizes values in this order: `max_model_len - input_length` (hard ceiling), user-provided `max_tokens` (if set), `override_max_tokens` (if configured), and finally the generation config default. This ensures user requests are respected while maintaining safety via model and platform limits. The change is propagated across all OpenAI‑compatible serving endpoints (chat, completion, responses, and engine tools).

**Potential risks**  
If `override_max_tokens` is incorrectly derived (e.g., due to misconfiguration of `generation_config` or `override_generation_config`), it could unintentionally cap user requests. The conditional logic for determining `override_max_tokens` across different serving classes adds complexity and must remain consistent. Edge cases where `max_tokens` is zero or negative are not explicitly handled in the updated function.

**Key insights**  
The fix correctly distinguishes between model‑author defaults (fallback) and user‑intended overrides (ceiling). Developers should verify that `override_generation_config` is used only when explicit server‑side limits are needed. The comprehensive test suite ensures both the bug fix and backward compatibility, but integration tests should confirm behavior with real model configs.

---

## 14. [[CI] Enable mypy coverage for individual excluded files](https://github.com/vllm-project/vllm/pull/34292)


### Base Information

- **PR Number:** #34292
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 07:34:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34292/files) (9):**
  - `tools/pre_commit/mypy.py`
  - `vllm/config/cache.py`
  - `vllm/config/parallel.py`
  - `vllm/config/scheduler.py`
  - `vllm/config/utils.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/logger.py`
  - `vllm/outputs.py`
  - `vllm/v1/cudagraph_dispatcher.py`

### Summary

**What changed and why**  
This PR enables mypy type checking for four previously excluded files (`vllm/engine/arg_utils.py`, `vllm/v1/cudagraph_dispatcher.py`, `vllm/outputs.py`, `vllm/logger.py`) by removing them from the exclusion list. The changes include adding type hints, defensive assertions, and fixing type annotations to resolve mypy errors, moving towards eliminating the need for a custom mypy check.

**Technical impact**  
The modifications improve type safety across configuration handling, logging, output aggregation, and CUDA graph dispatch logic. Key changes include stricter type definitions for configuration fields (e.g., using `DataParallelBackend` instead of `str`), adding assertions to guard against `None` values at runtime, and fixing type casting issues in data structures.

**Potential risks**  
The added assertions could introduce runtime crashes if invariants are violated (e.g., `assert self.enable_prefix_caching is not None`). Some type ignores (`# type: ignore[...]`) may mask underlying issues if the annotations are incorrect. Changes to public configuration APIs (like `data_parallel_backend` type) could affect downstream code relying on string values.

**Key insights**  
The PR demonstrates a systematic approach to enabling incremental type checking by addressing mypy errors file-by-file. Developers should review the assertions for correctness and ensure they align with the intended API contracts. Future work should aim to reduce the use of type ignores and further refine type annotations for full mypy compliance.

---

## 15. [(bugfix): Fixed encode in LLM entrypoint for IOProcessr plugin prompts](https://github.com/vllm-project/vllm/pull/34618)


### Base Information

- **PR Number:** #34618
- **Author:** [christian-pinto](https://github.com/christian-pinto)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 07:33:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34618/files) (3):**
  - `examples/pooling/plugin/prithvi_geospatial_mae_io_processor.py`
  - `tests/plugins_tests/test_io_processor_plugins.py`
  - `vllm/entrypoints/llm.py`

### Summary

**What changed and why**  
The fix addresses a bug in the LLM entrypoint's `encode` method when using IO Processor plugins. Previously, the entire prompt dictionary was passed to the plugin's `parse_data` method, but only the `prompt.data` field should be passed, aligning with online serving behavior. This ensures consistency and correct data handling for plugin processing.

**Technical impact**  
This change modifies the data flow to IO Processor plugins, ensuring they receive only the structured data from the `prompt.data` field rather than the full prompt object. It enforces validation that the `data` field exists and is non-null, improving error handling and maintaining parity between offline and online serving modes.

**Potential risks**  
If existing code passes prompts without a `data` field, it will now raise a `ValueError`, potentially breaking backward compatibility. Additionally, any plugins relying on the full prompt structure in `parse_data` may need updates, though this aligns with intended plugin contracts.

**Key insights**  
Developers must ensure prompts include a `data` field when using IO Processor plugins. The fix standardizes plugin input across serving modes, reducing inconsistency. Review plugin implementations to confirm they only expect the `data` field, and update any affected integration tests or client code accordingly.

---

## 16. [[ROCm][CI] Fix plugins test group; updating terratorch and dependencies](https://github.com/vllm-project/vllm/pull/34589)


### Base Information

- **PR Number:** #34589
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 07:33:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34589/files) (2):**
  - `requirements/rocm-test.txt`
  - `vllm/model_executor/models/terratorch.py`

### Summary

**What changed and why**  
This PR stabilizes TerraTorch plugin tests by replacing a git commit pin with the stable `terratorch==1.2.2` release and adding compatible version pins for `segmentation-models-pytorch` and `imagehash`. A minor code patch in `terratorch.py` replaces `torch.tensor(v)` with `torch.as_tensor(v).clone()` to avoid a `UserWarning` when constructing tensors from existing data.

**Technical impact**  
The dependency changes ensure deterministic builds and prevent runtime errors caused by API incompatibilities (e.g., `use_batchnorm` vs. `use_norm`). The tensor construction update eliminates a warning but maintains functional equivalence, as `torch.as_tensor` shares data unless cloning is explicitly called.

**Potential risks**  
Pinning `segmentation-models-pytorch==0.5.0` may conflict with other dependencies that require newer versions. The `torch.as_tensor` change could inadvertently share memory if `.clone()` is omitted in the future, though it’s currently safe due to the `.unsqueeze(0)` operation creating a new view.

**Key insights**  
Always pin transitive dependencies when API stability is critical. The tensor warning fix is a good practice, but ensure `clone()` is retained if downstream modifications mutate the tensor. Verify that the new pins don’t break other ROCm test dependencies.

---

## 17. [[Bugfix] Fix ARC touch KeyError for non-ready T1 blocks in kv offload](https://github.com/vllm-project/vllm/pull/34576)


### Base Information

- **PR Number:** #34576
- **Author:** [Vivo50E](https://github.com/Vivo50E)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 07:33:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34576/files) (1):**
  - `vllm/v1/kv_offload/arc_manager.py`

### Summary

**What changed and why**  
The fix addresses a `KeyError` in `ARCOffloadingManager.touch()` by modifying how non-ready T1 blocks are handled. Previously, a block was popped from `self.t1` before checking readiness, causing a `KeyError` when attempting to move it to the MRU position. Now, for non-ready blocks, the code retains the block in `self.t1` by reassigning it instead of popping and re-adding.

**Technical impact**  
This change preserves the ARC caching algorithm's intended behavior: non-ready blocks remain in the T1 list while being marked as most recently used, and ready blocks are correctly promoted to T2. The fix ensures the method works correctly in deferred offloading paths without disrupting the cache's state management.

**Potential risks**  
If `block_hash` is not present in `self.t1` when `touch()` is called, the assignment `self.t1[block_hash] = block` could still fail, though this scenario is likely precluded by earlier logic. Additionally, the reassignment may subtly affect reference counts or ordering if `self.t1` is not a standard `OrderedDict` with move semantics.

**Key insights**  
The fix is minimal and targeted, directly resolving the root cause. Developers should verify that `block_hash` always exists in `self.t1` when `touch()` is invoked and consider adding a defensive check or assertion. This change reinforces the importance of maintaining data structure consistency during state transitions.

---

## 18. [Fix call to moe_mk in modelopt MoE modules (required for LoRA)](https://github.com/vllm-project/vllm/pull/34575)


### Base Information

- **PR Number:** #34575
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 07:33:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34575/files) (1):**
  - `vllm/model_executor/layers/quantization/modelopt.py`

### Summary

**What changed and why**  
The PR changes two calls to `moe_mk()` in the modelopt quantization module from positional arguments to keyword arguments. This fixes a KeyError that occurs when running Nemotron Nano with LoRA adapters, where the LoRA wrapper expects keyword arguments but receives positional ones.

**Technical impact**  
This change ensures compatibility between the modelopt quantization layer and the LoRA-injected fused MoE layer. The LoRA wrapper's forward decorator specifically looks for the 'hidden_states' keyword argument, which wasn't being passed when using positional arguments.

**Potential risks**  
The change assumes all other callers of `moe_mk()` also accept keyword arguments. If there are other code paths that call this method with positional arguments only, they might break. Additionally, the parameter names ('hidden_states', 'w1', 'w2') must exactly match what the underlying `FusedMoEModularKernel.forward` method expects.

**Key insights**  
When integrating LoRA with fused operations, careful attention must be paid to argument passing conventions. This fix highlights the importance of consistent API design between different components (quantization layers and LoRA wrappers). Developers should verify that all call sites for modified methods use the same argument passing style.

---

## 19. [[Models] Fuse Qwen3.5 GDN's qkvz_proj and ba_proj](https://github.com/vllm-project/vllm/pull/34492)


### Base Information

- **PR Number:** #34492
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 07:32:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34492/files) (3):**
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/models/qwen3_5.py`
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
This PR fuses Qwen3.5's Gated Delta Network (GDN) qkvz_proj and ba_proj layers into merged linear layers to improve performance. The changes modify the linear layer weight loading logic to support multi-index shard IDs and update both Qwen3.5 and Qwen3-Next model implementations to use merged projections instead of separate linear layers.

**Technical impact**  
The fusion reduces the number of separate linear operations in the GDN module from four (qkv, z, b, a) to two merged layers (qkvz and ba), decreasing kernel launch overhead and improving memory access patterns. This is reflected in the benchmark showing ~2.5% improvement in output token throughput (1551.78 → 1591.00 tok/s) and reduced latency metrics across TTFT, TPOT, and ITL.

**Potential risks**  
The weight loading changes introduce new logic for handling tuple-based shard IDs that could break compatibility with existing model checkpoints if not properly handled. The NotImplementedError for multiple-index shards in weight_loader may cause issues for certain quantization schemes or distributed configurations. The changes assume specific tensor partitioning patterns that must align with the model architecture.

**Key insights**  
The performance gains demonstrate the value of operator fusion for attention-like components. Developers should ensure the new weight loading logic properly handles all quantization and sharding scenarios, particularly for BlockQuantScaleParameter. The stacked_params_mapping updates show how to map checkpoint weights to the new fused parameter structure, which is critical for backward compatibility.

---

## 20. [[CI] Disable precompiled wheel path in CI image builds](https://github.com/vllm-project/vllm/pull/34606)


### Base Information

- **PR Number:** #34606
- **Author:** [amrmahdi](https://github.com/amrmahdi)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-16 07:14:43
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34606/files) (1):**
  - `.buildkite/image_build/image_build.sh`

### Summary

**What changed and why**  
The script arguments for `VLLM_USE_PRECOMPILED` and `VLLM_MERGE_BASE_COMMIT` have been hardcoded to `0` and an empty string, respectively. This change removes the pipeline generator's ability to pass dynamic values, ensuring deterministic Docker layer cache keys and eliminating unnecessary cache invalidation during CI builds.

**Technical impact**  
By hardcoding these values, the Docker build cache is no longer invalidated by varying commit hashes or precompilation flags from the pipeline. This improves build performance and reliability, as Docker's layer caching and sccache now handle incremental compilation without interference from these previously dynamic arguments.

**Potential risks**  
Local builds that rely on passing custom values for these arguments via the script will no longer work as intended, though the Dockerfile retains the ARGs for manual `--build-arg` usage. There is a minor risk if any external scripts or processes depend on the positional arguments being mutable, but this is mitigated by the planned cleanup in subsequent PRs.

**Key insights**  
This change simplifies CI infrastructure by removing redundant optimization logic and aligning with modern caching mechanisms. Developers should note that the positional arguments are now fixed, and any local build customization must use Docker's `--build-arg` directly. The phased cleanup approach ensures a smooth transition across repositories.

---

## 21. [Bump `lm-eval` version for Transformers v5 compatibility](https://github.com/vllm-project/vllm/pull/33994)


### Base Information

- **PR Number:** #33994
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 05:24:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33994/files) (14):**
  - `.buildkite/lm-eval-harness/run-lm-eval-chartqa-vllm-vlm-baseline.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh`
  - `.buildkite/lm-eval-harness/run-lm-eval-mmlupro-vllm-baseline.sh`
  - `.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh`
  - `.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh`
  - `docs/features/quantization/fp8.md`
  - `docs/features/quantization/int4.md`
  - `docs/features/quantization/int8.md`
  - `docs/features/quantization/quark.md`
  - `requirements/nightly_torch_test.txt`
  - `requirements/rocm-test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`

### Summary

**What changed and why**  
The PR updates the pinned version of `lm-eval` from `>=0.4.9.2` to `>=0.4.11` across multiple files. This change ensures compatibility with Transformers v5 by including a specific commit (EleutherAI/lm-evaluation-harness@384118d) that removes dependencies on a class no longer present in the newer Transformers version.

**Technical impact**  
This version bump resolves a breaking import issue when using Transformers v5, ensuring evaluation scripts and tests continue to function correctly. The changes are consistent across build scripts, documentation, and dependency files, maintaining alignment between development, testing, and deployment environments.

**Potential risks**  
While the update addresses compatibility, there is a risk that `lm-eval` version `0.4.11` may introduce its own behavioral changes or new dependencies that could affect existing evaluation workflows. Additionally, the transitive dependency updates (e.g., removal of `numexpr`, `pybind11`, `tqdm-multiprocess`) might impact performance or functionality in edge cases.

**Key insights**  
Developers should verify that all evaluation pipelines run as expected after this update, particularly for quantization and benchmarking tasks. It's also advisable to monitor for any regressions in CI/CD jobs, as the dependency changes could subtly alter the runtime environment.

---

## 22. [[Fix] Fix tracing test race condition by adding server readiness check](https://github.com/vllm-project/vllm/pull/34364)


### Base Information

- **PR Number:** #34364
- **Author:** [emricksini-h](https://github.com/emricksini-h)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-02-16 04:57:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34364/files) (1):**
  - `tests/tracing/conftest.py`

### Summary

**What changed and why**  
Added a server readiness check function `_wait_for_server_ready` and integrated it into the `trace_service` fixture. This change addresses a race condition where tests could attempt to connect to the gRPC server before it was fully ready, causing segmentation faults.

**Technical impact**  
The fixture now ensures the gRPC server is accepting TCP connections before yielding control to tests. This improves test reliability by eliminating timing-dependent failures, particularly in multi-GPU environments where server startup might have variable latency.

**Potential risks**  
The 5-second timeout may be insufficient in heavily loaded CI environments or slower systems. The socket check only verifies TCP connectivity, not gRPC service readiness, which could still lead to race conditions if the gRPC server needs additional initialization after binding the port.

**Key insights**  
Always validate service availability in test fixtures that depend on network services. Consider adding exponential backoff instead of fixed 0.1-second sleeps for better performance under variable conditions. For production-grade solutions, implement proper health checks using the actual protocol (gRPC health checks) rather than just TCP connectivity.

---

## 23. [[Scheduler][ASR] Fix CrossAttn blocks per-request for Variable length encoder inputs](https://github.com/vllm-project/vllm/pull/31058)


### Base Information

- **PR Number:** #31058
- **Author:** [ekagra-ranjan](https://github.com/ekagra-ranjan)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-16 03:08:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31058/files) (2):**
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The PR generalizes cross-attention block allocation for encoder-decoder models with variable-length encoder inputs. Previously, the scheduler allocated blocks based on a fixed maximum encoder length, which could waste memory. Now, it calculates the exact number of blocks needed per request by summing the actual encoder token counts from scheduled encoder inputs.

**Technical impact**  
This change enables efficient memory usage for models like Whisper that process variable-length audio inputs. The scheduler now dynamically allocates cross-attention KV blocks proportional to each request's actual encoder sequence length rather than a static maximum, reducing memory overhead and improving scalability for diverse input sizes.

**Potential risks**  
If `encoder_inputs_to_schedule` is empty or `get_num_encoder_embeds` returns incorrect values, block allocation could be zero or insufficient, causing under-allocation. The change assumes encoder inputs are fully scheduled in one pass; partial scheduling might lead to inaccurate token counts. Edge cases around block boundaries (e.g., exact multiples of block size) are tested but could still fail if the block calculation logic has off-by-one errors.

**Key insights**  
Developers should ensure `encoder_inputs_to_schedule` accurately reflects all encoder inputs needing cross-attention blocks during scheduling. The fix is critical for memory efficiency in production workloads with highly variable encoder lengths. The comprehensive unit tests validate correctness but should be extended to cover concurrent requests and mixed encoder/decoder-only scenarios.

---

## 24. [[CI][Metrics] Stabilize tests with polling and subprocess guards](https://github.com/vllm-project/vllm/pull/34566)


### Base Information

- **PR Number:** #34566
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-02-16 02:52:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34566/files) (1):**
  - `tests/entrypoints/instrumentator/test_metrics.py`

### Summary

**What changed and why**  
This PR replaces hardware-dependent fixed sleeps with polling mechanisms in two flaky tests. `test_abort_metrics_reset` now uses an async `_poll_until` helper to detect running requests and metric resets, while `test_metrics_exist_run_batch` adds subprocess lifecycle guards, dynamic port allocation, and proper cleanup to prevent false positives from early engine crashes.

**Technical impact**  
The changes improve test reliability across different hardware speeds by eliminating timing assumptions. The new polling approach makes tests more deterministic, and the subprocess guards ensure failures are properly detected and reported. The addition of `get_open_port()` reduces port collision risks in CI environments.

**Potential risks**  
The polling timeouts (10s and 120s) may still be insufficient under extreme load or resource contention, potentially causing false negatives. The increased `max_tokens` to 500 could slightly prolong test execution time. There's a minor risk that the `_poll_until` helper might not handle predicate exceptions gracefully.

**Key insights**  
Always prefer polling over fixed sleeps for concurrency-dependent assertions. Subprocess-based tests must include startup timeouts, exit code monitoring, and guaranteed cleanup. These patterns should be adopted for similar tests to improve overall CI stability. Consider extracting `_poll_until` to a shared test utility if used elsewhere.

---

## 25. [[Bugfix] Fix Dynamo unexpected keyword argument](https://github.com/vllm-project/vllm/pull/34320)


### Base Information

- **PR Number:** #34320
- **Author:** [samutamm](https://github.com/samutamm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-16 01:32:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34320/files) (1):**
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`

### Summary

**What changed and why**  
The PR fixes a Dynamo compilation error on ROCm when the `quant_fp8` custom operator is disabled. The error occurred because `use_triton` was passed as a keyword argument via `**kwargs` in three quantization forward methods, which Dynamo could not properly trace. The change moves `use_triton` from `**kwargs` to an explicit positional argument with a default value.

**Technical impact**  
This change improves compatibility with PyTorch's Dynamo compiler by eliminating a graph break caused by variable keyword arguments. It ensures consistent function signatures across CUDA, HIP, and native implementations, making the code more predictable for compilation and reducing runtime errors during `torch.compile` on ROCm.

**Potential risks**  
If any existing code calls these methods with additional keyword arguments beyond `use_triton`, it will now raise a `TypeError` due to the removal of `**kwargs`. There is also a risk of silent behavior changes if `use_triton` was previously being passed with a different key or if other kwargs were unintentionally being ignored.

**Key insights**  
Always prefer explicit arguments over `**kwargs` in performance‑critical or compiled code paths to avoid Dynamo tracing issues. Review other uses of `**kwargs` in the codebase that might cause similar problems. Ensure all callers of these methods are updated to pass `use_triton` as a positional argument.

---

## 26. [Revert "[Misc] fix qwen3.5 config"](https://github.com/vllm-project/vllm/pull/34610)


### Base Information

- **PR Number:** #34610
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-16 01:06:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34610/files) (2):**
  - `vllm/transformers_utils/configs/qwen3_5.py`
  - `vllm/transformers_utils/configs/qwen3_5_moe.py`

### Summary

**What changed and why**  
This PR reverts a previous change that modified the `ignore_keys_at_rope_validation` parameter from a list to a dictionary. The reversion restores the original list format, as the prior change was intended for a future transformers version (v5) and is not currently compatible.

**Technical impact**  
The change ensures compatibility with the current transformers library version by maintaining `ignore_keys_at_rope_validation` as a list. This affects how rope validation handles the keys "mrope_section" and "mrope_interleaved" during model initialization, aligning with existing validation logic.

**Potential risks**  
If the codebase is upgraded to transformers v5 without updating this configuration, rope validation may fail due to incorrect data type expectations. Additionally, any dependent code expecting a dictionary could break, though this seems unlikely given the revert.

**Key insights**  
Developers should note that this configuration is version-sensitive and must be updated when migrating to transformers v5. The revert highlights the importance of aligning library dependencies with internal configurations to avoid runtime validation errors.

---

## 27. [[Misc] fix qwen3.5 config](https://github.com/vllm-project/vllm/pull/34604)


### Base Information

- **PR Number:** #34604
- **Author:** [JJJYmmm](https://github.com/JJJYmmm)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-16 00:25:38
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34604/files) (2):**
  - `vllm/transformers_utils/configs/qwen3_5.py`
  - `vllm/transformers_utils/configs/qwen3_5_moe.py`

### Summary

**What changed and why**  
The PR fixes a TypeError in Qwen3.5 configuration loading by changing `ignore_keys_at_rope_validation` from a list to a set. The error occurred because the code attempted to perform a set union operation (`\|`) between a list and a set, which is not supported in Python.

**Technical impact**  
This change ensures compatibility with the underlying transformers library's rope validation logic, which expects `ignore_keys_at_rope_validation` to be a set for proper set operations. Both Qwen3.5 and Qwen3.5 MoE configurations are updated consistently.

**Potential risks**  
Minimal risk since this is a straightforward type correction. However, if other parts of the codebase rely on `ignore_keys_at_rope_validation` being a list (e.g., for iteration order), this could cause subtle issues, though sets are generally suitable for membership checks.

**Key insights**  
Always verify the expected data types when integrating with external libraries. The fix is correct because sets are the appropriate data structure for collections of unique keys where order doesn't matter. Developers should ensure similar configurations across model variants are updated uniformly.

---

