# vLLM Merged PR Report

**Report Date:** 2026-02-27 PST

**Total Merged PRs:** 46

---

## 1. [[Benchmark] Rename SLA Finder to Workload Explorer](https://github.com/vllm-project/vllm/pull/35586)


### Base Information

- **PR Number:** #35586
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 23:31:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35586/files) (6):**
  - `docs/benchmarking/sweeps.md`
  - `docs/cli/bench/sweep/serve_sla.md`
  - `docs/cli/bench/sweep/serve_workload.md`
  - `docs/mkdocs/hooks/generate_argparse.py`
  - `vllm/benchmarks/sweep/cli.py`
  - `vllm/benchmarks/sweep/serve_workload.py`

### Summary

**What changed and why**  
This PR renames "SLA Finder" to "Workload Explorer" across the codebase, including CLI commands, documentation, and internal variables. The change aims to better reflect the script's actual functionality—exploring latency-throughput tradeoffs under different workload levels—rather than just finding Service Level Agreements (SLAs).

**Technical impact**  
The rename affects CLI entry points (`vllm bench sweep serve_sla` → `vllm bench sweep serve_workload`), argument names (`--sla-variable` → `--workload-var`, `--sla-iters` → `--workload-iters`), and internal classes/functions (e.g., `SweepServeSLAArgs` → `SweepServeWorkloadArgs`). Documentation is updated accordingly, and the underlying algorithm remains unchanged.

**Potential risks**  
Breaking changes for users or scripts relying on the old CLI command or argument names. The PR does not include backward compatibility aliases, which could disrupt existing workflows. Additionally, any external references to the old terminology in other documentation or integrations may become inconsistent.

**Key insights**  
The rename improves clarity but introduces a breaking change. Consider adding deprecation warnings or aliases for the old command to ease migration. Ensure all team members and dependent projects are aware of the change, and update any automation or CI/CD scripts that invoke the old command.

---

## 2. [[Misc] Change logging level from info to debug for tool parser import](https://github.com/vllm-project/vllm/pull/35575)


### Base Information

- **PR Number:** #35575
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 22:51:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35575/files) (1):**
  - `vllm/tool_parsers/qwen3coder_tool_parser.py`

### Summary

**What changed and why**  
The change modifies a logging statement from `logger.info()` to `logger.debug()` in the Qwen3Coder tool parser's initialization. This reduces log verbosity by moving a successful import confirmation message from the standard info level to the more granular debug level.

**Technical impact**  
This adjustment decreases noise in production logs, as debug-level messages are typically hidden unless explicitly enabled. It maintains the same logging functionality but shifts it to a more appropriate severity level for operational monitoring.

**Potential risks**  
The main risk is reduced visibility during debugging if debug logging is disabled by default. Developers troubleshooting tool parser imports may need to explicitly enable debug logging to see this confirmation message, which could slightly increase investigation time.

**Key insights**  
This is a standard logging optimization that follows best practices—reserving info level for operational events and using debug for detailed development/troubleshooting information. Ensure debug logging is accessible in test/staging environments to maintain diagnostic capability.

---

## 3. [[CI] add trainer_send_weights for MockWeightTransferEngine](https://github.com/vllm-project/vllm/pull/35589)


### Base Information

- **PR Number:** #35589
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 22:47:43
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35589/files) (1):**
  - `tests/entrypoints/weight_transfer/test_weight_transfer_llm.py`

### Summary

**What changed and why**  
Added a `trainer_send_weights` method to the `MockWeightTransferEngine` class to fix a test failure. The change ensures the mock engine properly implements an expected interface, resolving a CI build error and addressing issue #35585.

**Technical impact**  
This change maintains test compatibility by ensuring the mock engine matches the real engine's API. It prevents test failures due to missing method calls, allowing the test `test_init_weight_transfer_engine_calls_engine` to pass without affecting production code.

**Potential risks**  
Minimal risk since it's a test-only change. However, if the real `trainer_send_weights` method's signature evolves, this mock may become outdated and cause false test passes. The mock's `pass` implementation also doesn't simulate any behavior, which could mask issues if the test later relies on side effects.

**Key insights**  
Always ensure mock objects fully implement the interfaces they simulate. Consider adding a minimal implementation (e.g., logging or setting a flag) to verify the method is called as expected. Regularly sync mock signatures with production code to avoid drift.

---

## 4. [[ROCm][CI] Expose tests to AMD production CI and fix amdsmi heap corruption](https://github.com/vllm-project/vllm/pull/35071)


### Base Information

- **PR Number:** #35071
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 21:57:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35071/files) (2):**
  - `.buildkite/test-amd.yaml`
  - `tests/utils.py`

### Summary

**What changed and why**  
This PR expands ROCm CI coverage by adding `amdproduction` to the `mirror_hardwares` list across multiple test steps in `test-amd.yaml`, allowing AMD's production CI pipeline to optionally run a broader set of tests. Additionally, it fixes a heap corruption bug in GPU memory queries by making `amdsmi` initialization thread-safe via a lock and moving initialization to module import with `atexit` cleanup.

**Technical impact**  
The CI changes enable parallel test execution on AMD's production hardware, increasing validation coverage without affecting existing `amdexperimental` runs. The `amdsmi` fix prevents race conditions during concurrent memory queries in distributed tests, eliminating intermittent crashes and improving test stability on MI325 hardware.

**Potential risks**  
Adding `amdproduction` could increase CI resource consumption if both hardware signals trigger simultaneously, though the `optional: true` flag mitigates this. The thread-safety fix introduces a global lock that may cause minor contention during high-concurrency memory queries, but this is preferable to heap corruption.

**Key insights**  
The fix correctly addresses a subtle threading issue in library initialization—a pattern that should be reviewed in other external library integrations. Developers should note that `amdsmi` and `pynvml` now use singleton initialization patterns, ensuring thread safety for GPU monitoring utilities across the test suite.

---

## 5. [[ROCm] Derive device capability from GCN arch string without CUDA init](https://github.com/vllm-project/vllm/pull/35069)


### Base Information

- **PR Number:** #35069
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 21:55:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35069/files) (2):**
  - `vllm/platforms/rocm.py`
  - `vllm/utils/system_utils.py`

### Summary

**What changed and why**  
This PR modifies ROCm platform support to derive device capability from GCN architecture strings without requiring CUDA initialization. The primary motivation is to fix a failure when vLLM runs inside Ray actors with `num_gpus=0`, where `torch.cuda.get_device_capability()` would initialize CUDA/HIP and fail with "No HIP GPUs are available" due to unset visibility environment variables.

**Technical impact**  
The changes introduce a parsing-based approach for GCN architecture strings that mirrors HIP's internal derivation of device properties, eliminating the need for a static lookup table. This makes the system more maintainable and future-proof. Additionally, bidirectional synchronization of `HIP_VISIBLE_DEVICES` and `CUDA_VISIBLE_DEVICES` environment variables ensures consistency across the process lifecycle and prevents silent conflicts.

**Potential risks**  
The GCN arch string parsing assumes specific digit layouts (2-3 digits for gfx9, 4 digits for gfx10/11/12). Future AMD architectures with different digit patterns could break the parser. The validation logic rejects major versions <9 or >12, which might incorrectly block legitimate future hardware. Environment variable synchronization at import time could raise errors for pre-existing misconfigurations that were previously tolerated.

**Key insights**  
The parsing approach is more robust than a static lookup table but requires careful monitoring of AMD's future architecture naming conventions. Developers should be aware that environment variable conflicts now raise explicit errors rather than silently overriding values. The fallback to `torch.cuda.get_device_capability()` with a warning ensures backward compatibility but should be monitored for unintended CUDA initialization in edge cases.

---

## 6. [[ROCm][CI] Adding infiniband mappings for moriio tests](https://github.com/vllm-project/vllm/pull/35170)


### Base Information

- **PR Number:** #35170
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 21:53:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35170/files) (1):**
  - `.buildkite/scripts/hardware_ci/run-amd-test.sh`

### Summary

**What changed and why**  
This PR adds RDMA (InfiniBand) device passthrough support to the AMD test runner script, enabling MORI/UCX-based tests that require RDMA hardware. It also significantly refactors the command parsing logic to handle pytest marker quoting issues more robustly and introduces a new environment variable (`VLLM_TEST_COMMANDS`) for better command preservation.

**Technical impact**  
The changes enable RDMA-dependent tests (like `test_moriio_handshake_returns_metadata`) to run on hosts with RDMA hardware by passing through `/dev/infiniband` devices and adding `IPC_LOCK` capability. The enhanced `re_quote_pytest_markers` function now properly handles complex pytest expressions with better boundary detection, while the new `VLLM_TEST_COMMANDS` environment variable provides a reliable way to preserve command quoting that was previously lost through shell expansion.

**Potential risks**  
The RDMA passthrough logic assumes tests will gracefully skip when hardware is unavailable, but there's a risk of false negatives if system RDMA libraries (`ibverbs-providers`, `librdmacm1`) are missing even when devices exist. The complex tokenization logic in `re_quote_pytest_markers` could potentially misinterpret edge cases in command syntax, though the added debug output helps with troubleshooting. The dual command sourcing (environment variable vs positional args) creates maintenance overhead.

**Key insights**  
Developers should migrate to using `VLLM_TEST_COMMANDS` for all new test invocations to avoid quoting issues. The RDMA dependencies (`ibverbs-providers`, `librdmacm1`) must be explicitly installed in test environments, as they're not always transitive dependencies. The enhanced debugging output (`echo "After re-quoting: $commands"`) will help diagnose command parsing issues during CI failures.

---

## 7. [[EPLB] Enforce sync eplb for NCCL-based all2all backend](https://github.com/vllm-project/vllm/pull/35212)


### Base Information

- **PR Number:** #35212
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-27 21:47:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35212/files) (1):**
  - `vllm/config/parallel.py`

### Summary

**What changed and why**  
Added validation logic to force synchronous EPLB when using NCCL-based all2all backends (`allgather_reducescatter` or `naive`) with async EPLB enabled. This prevents hangs caused by overlapping NCCL kernels in the main thread with communication in the async EPLB thread, addressing a known PyTorch issue.

**Technical impact**  
The change automatically overrides `eplb_config.use_async` to `False` for affected configurations, ensuring compatibility between NCCL all2all operations and EPLB. This enforces a safer execution mode without requiring manual user intervention, maintaining system stability for expert-parallel workloads.

**Potential risks**  
If users depend on async EPLB for performance gains with these backends, they may experience unexpected behavior or reduced throughput. The warning log may be missed in noisy environments, leading to confusion. Edge cases where `eplb_config` is modified post-initialization could bypass this safeguard.

**Key insights**  
Developers should treat this as a critical stability fix; async EPLB remains unsafe with NCCL all2all backends. Consider documenting this limitation in user-facing guides. Future improvements could include a more granular control mechanism or runtime checks to re-validate configuration consistency.

---

## 8. [[Bugfix] Move chat completion response_format validation to Pydantic model_validator](https://github.com/vllm-project/vllm/pull/35510)


### Base Information

- **PR Number:** #35510
- **Author:** [umut-polat](https://github.com/umut-polat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 21:26:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35510/files) (2):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`

### Summary

**What changed and why**  
The PR adds a Pydantic `model_validator` to `ChatCompletionRequest` to validate that when `response_format.type` is `"json_schema"`, the `json_schema` field is present. This prevents an `assert` failure in `to_sampling_params()` that would cause a 500 Internal Server Error, ensuring the API returns a proper 400 Bad Request for invalid requests.

**Technical impact**  
This change aligns the chat completions endpoint with the validation pattern already applied to the completions endpoint (#35456), promoting consistency. It moves validation logic from runtime assertion to request construction, improving error handling and user experience by providing clearer, earlier feedback.

**Potential risks**  
The validator handles both `dict` and object-style `response_format` inputs, but complex nested structures or unexpected data types could still cause issues. There's a minor risk of the validator interfering with other model validators if the order of execution isn't carefully considered, though `mode="before"` mitigates this.

**Key insights**  
This fix is a direct application of reviewer feedback, demonstrating good pattern adoption. Developers should ensure similar validation is applied consistently across all endpoints. The use of `VLLMValidationError` with a `parameter` argument is crucial for generating user-friendly error messages.

---

## 9. [[Bugfix] Propagate compilation_time from workers to main process for TP>1](https://github.com/vllm-project/vllm/pull/35503)


### Base Information

- **PR Number:** #35503
- **Author:** [huydhn](https://github.com/huydhn)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-27 21:03:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35503/files) (4):**
  - `vllm/v1/executor/abstract.py`
  - `vllm/v1/worker/cpu_worker.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/worker_base.py`

### Summary

**What changed and why**  
The changes fix a bug where `vllm bench startup` reported zero compilation time when tensor_parallel_size > 1. Previously, compilation time was tracked only in worker processes' local configs, leaving the main process's compilation time at zero. Now, `compile_or_warm_up_model` returns the compilation time from each worker, and the executor aggregates these values (using max across workers) into the main process config.

**Technical impact**  
This modification ensures accurate compilation time reporting in distributed tensor parallelism scenarios. The main process now receives the maximum compilation time from all workers, reflecting the parallel compilation overhead correctly. The change affects the abstract executor and all worker implementations (CPU, GPU, base), unifying their return signatures.

**Potential risks**  
If workers have significantly different compilation times due to hardware or load imbalances, using `max` may overestimate the effective compilation latency. There is also a risk if `compilation_times` is empty (though guarded), but the collective RPC should ensure all workers respond. The change assumes compilation happens concurrently across workers, which may not hold for all execution modes.

**Key insights**  
The fix correctly addresses the symptom but consider whether `max` is the appropriate aggregation—alternative metrics like average or sum could provide different insights. Ensure all future worker implementations adhere to the new return type. The change is minimal and focused, maintaining backward compatibility for single-worker cases.

---

## 10. [[1/N] Elastic EP Milestone 2](https://github.com/vllm-project/vllm/pull/34861)


### Base Information

- **PR Number:** #34861
- **Author:** [itayalroy](https://github.com/itayalroy)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-27 20:46:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34861/files) (53):**
  - `.buildkite/test_areas/expert_parallelism.yaml`
  - `tests/compile/passes/distributed/test_async_tp.py`
  - `tests/compile/passes/distributed/test_fusion_all_reduce.py`
  - `tests/compile/passes/distributed/test_sequence_parallelism.py`
  - `tests/conftest.py`
  - `tests/distributed/eplb_utils.py`
  - `tests/distributed/test_elastic_ep.py`
  - `tests/distributed/test_eplb_execute.py`
  - `tests/distributed/test_nccl_symm_mem_allreduce.py`
  - `tests/distributed/test_pynccl.py`
  - `tests/kernels/mamba/test_mamba_mixer2.py`
  - `tests/lora/conftest.py`
  - `tests/lora/test_fused_moe_lora_kernel.py`
  - `tests/lora/test_worker.py`
  - `tests/models/test_vision.py`
  - `tests/utils.py`
  - `tests/v1/worker/test_gpu_model_runner.py`
  - `tests/v1/worker/test_worker_memory_snapshot.py`
  - `vllm/compilation/wrapper.py`
  - `vllm/config/parallel.py`
  - `vllm/distributed/device_communicators/all2all.py`
  - `vllm/distributed/device_communicators/base_device_communicator.py`
  - `vllm/distributed/device_communicators/cuda_communicator.py`
  - `vllm/distributed/device_communicators/pynccl.py`
  - `vllm/distributed/elastic_ep/__init__.py`
  - `vllm/distributed/elastic_ep/elastic_execute.py`
  - `vllm/distributed/elastic_ep/elastic_state.py`
  - `vllm/distributed/elastic_ep/standby_state.py`
  - `vllm/distributed/eplb/async_worker.py`
  - `vllm/distributed/eplb/eplb_state.py`
  - `vllm/distributed/eplb/rebalance_execute.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/distributed/stateless_coordinator.py`
  - `vllm/distributed/utils.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/entrypoints/cli/serve.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/rocm.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/coordinator.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/utils.py`
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/executor/ray_executor.py`
  - `vllm/v1/executor/uniproc_executor.py`
  - `vllm/v1/worker/cpu_model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/workspace.py`

### Summary

**What changed and why**  
This PR introduces Elastic Expert Parallelism (Elastic EP) milestone 2, enabling dynamic scaling of expert-parallel workloads. The changes allow scaling up/down the data-parallel size while serving requests, using stateless NCCL groups to avoid reinitializing the entire distributed environment. The core architecture was originally designed by @libertyeagle and rebased onto the latest vLLM main.

**Technical impact**  
The PR adds a new elastic scaling mechanism that works alongside existing expert parallelism (EP) and EPLB (Expert Parallel Load Balancing). It introduces stateless process groups (`StatelessGroupCoordinator`) that can be created/destroyed independently of the main PyTorch world group, enabling dynamic addition/removal of GPUs. The compilation wrapper is extended to reset CUDA graphs and compiled artifacts during scaling. The changes affect distributed initialization, communication backends, and model loading to support dummy weight initialization for new workers.

**Potential risks**  
- The complexity of stateless group coordination increases the risk of deadlocks or race conditions during scaling events.  
- Resetting compilation artifacts (CUDA graphs, compiled kernels) may cause temporary performance degradation after scaling.  
- Edge cases with uneven scaling (e.g., 2→3 GPUs) require careful sender-receiver pairing logic.  
- Compatibility issues may arise with pipeline parallelism or external load balancers, which are explicitly unsupported.

**Key insights**  
- Elastic EP requires `enable_eplb=True` and does not support pipeline parallelism or external/hybrid load balancing.  
- The scaling process involves multiple synchronization barriers and weight transfers; thorough testing of edge cases is critical.  
- Developers should ensure that all distributed tests use the new `ensure_current_vllm_config()` context manager to avoid missing config errors.  
- The PR includes comprehensive integration tests (`test_elastic_ep.py`) that validate scaling accuracy and uneven distributions.

---

## 11. [[CI/Build] CPU release supports both of AVX2 and AVX512](https://github.com/vllm-project/vllm/pull/35466)


### Base Information

- **PR Number:** #35466
- **Author:** [majian4work](https://github.com/majian4work)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-27 20:35:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35466/files) (6):**
  - `cmake/cpu_extension.cmake`
  - `csrc/cpu/torch_bindings.cpp`
  - `setup.py`
  - `vllm/_custom_ops.py`
  - `vllm/platforms/cpu.py`
  - `vllm/v1/worker/cpu_worker.py`

### Summary

**What changed and why**  
The changes consolidate multiple x86 ISA variants (AVX2, AVX512, AVX512-BF16, AVX512-VNNI, AMX-BF16) into a single unified x86 backend. Previously, each ISA required separate environment variables and conditional compilation. Now, a single `VLLM_CPU_X86` environment variable enables all x86 optimizations, and the build system creates two separate libraries: one with AVX512 extensions and another with AVX2 support. This enables a single wheel to support multiple x86 instruction sets via runtime detection.

**Technical impact**  
The build system now generates two x86 libraries (`_C` for AVX512 and `_C_AVX2` for AVX2) instead of conditionally compiling based on CPU detection. Runtime ISA detection is moved to `vllm/platforms/cpu.py`, which imports the appropriate library. This simplifies the CMake configuration by removing per-ISA environment variables and conditional logic, but introduces complexity in library loading and symbol management (note the `TORCH_EXTENSION_NAME` override in `torch_bindings.cpp` to handle naming conflicts).

**Potential risks**  
The `TORCH_EXTENSION_NAME` override could cause symbol collisions if other extensions use the same macro. The fallback logic in `import_kernels()` catches and logs import errors, which may mask genuine loading failures. The requirement for GCC ≥12.3 for x86 builds is now enforced globally, which could break existing build environments. Additionally, the oneDNN configuration now enables JIT profiling and verbose output by default, which may impact performance and log verbosity.

**Key insights**  
This refactor significantly simplifies x86 build configuration and enables multi-ISA support in a single distribution. Developers should ensure their build environment meets the GCC 12.3+ requirement for x86 targets. The runtime library selection is transparent to most users, but the override of `TORCH_EXTENSION_NAME` is a hack that should be addressed in future refactoring. Verify that the oneDNN verbose and profiling flags do not negatively impact production performance.

---

## 12. [[MTP] Validate that MTP weights are actually loaded](https://github.com/vllm-project/vllm/pull/35548)


### Base Information

- **PR Number:** #35548
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 20:27:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35548/files) (1):**
  - `vllm/model_executor/models/deepseek_mtp.py`

### Summary

**What changed and why**  
Added validation in the DeepSeek MTP model's weight loading logic to ensure all expected MTP layer weights are present in the checkpoint. This prevents loading uninitialized memory into MTP layers when using quantized models that lack these weights, which previously caused zero acceptance rates in speculative decoding.

**Technical impact**  
The change introduces a post-loading verification step that checks each MTP layer index against loaded weight names. If any MTP layer is missing weights, it raises a clear error, preventing silent failures and ensuring models used with MTP speculative decoding are properly configured.

**Potential risks**  
The validation assumes MTP layers are contiguous and relies on correct `mtp_start_layer_idx` and `num_mtp_layers` attributes. If these are misconfigured or the checkpoint uses non-standard naming, false positives could occur. Additionally, the check adds a small overhead during model loading but only runs once.

**Key insights**  
This is a critical safety check that addresses a subtle but severe bug where invalid weights lead to broken speculative decoding. Developers should ensure MTP-compatible checkpoints include the required layers or disable speculative decoding. The error message is user-friendly and actionable, guiding users toward resolution.

---

## 13. [[Bugfix] Fixes for SLA finder](https://github.com/vllm-project/vllm/pull/35537)


### Base Information

- **PR Number:** #35537
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 20:20:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35537/files) (5):**
  - `docs/benchmarking/sweeps.md`
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/sweep/plot.py`
  - `vllm/benchmarks/sweep/serve.py`
  - `vllm/benchmarks/sweep/serve_sla.py`

### Summary

**What changed and why**  
This PR fixes several issues in the SLA finder benchmarking tool: setting proper concurrency limits for initial iterations, extracting dataset size from benchmark commands, correcting `_benchmark_name` handling, improving plotting defaults/logging, and updating documentation. The changes ensure accurate SLA scanning and better result visualization.

**Technical impact**  
The SLA scanning algorithm now correctly uses `max_concurrency=1` for serial inference and `max_concurrency=dataset_size` for batch inference, replacing the previous approach that used `sla_variable` values directly. Plotting defaults shift from `request_throughput`/`p99_ttft_ms` to `total_token_throughput`/`median_ttft_ms`, aligning metrics with typical benchmarking needs. Documentation updates clarify the algorithm and provide corrected examples.

**Potential risks**  
If `dataset_size` extraction fails (e.g., malformed `bench_cmd`), it may fall back to `DEFAULT_NUM_PROMPTS` (1000), potentially causing mismatched concurrency limits. The new `extra_parts` parameter in `_get_comb_base_path` could introduce path length issues on some filesystems. Changes to plotting defaults might affect existing visualization scripts that rely on previous metric names.

**Key insights**  
Always validate `dataset_size` extraction in test plans to ensure concurrency limits match the actual dataset. The shift to `max_concurrency` as the preferred SLA variable (over `request_rate`) should be emphasized in user guidance. Developers should update any automation relying on old metric names (`request_throughput`, `p99_ttft_ms`) to use the new defaults (`total_token_throughput`, `median_ttft_ms`).

---

## 14. [[ROCm] Add `stablelm` Head Size 80 To Supported Head Sizes For ROCM_ATTN](https://github.com/vllm-project/vllm/pull/35527)


### Base Information

- **PR Number:** #35527
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-27 20:16:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35527/files) (2):**
  - `docs/design/attention_backends.md`
  - `vllm/v1/attention/backends/rocm_attn.py`

### Summary

**What changed and why**  
Added head size 80 to the list of supported head sizes for the ROCM_ATTN attention backend. This change resolves test failures when using ROCM_ATTN with models like `stabilityai/stablelm-3b-4e1t`, which require this head size.

**Technical impact**  
Enables ROCM_ATTN to correctly support models with a head size of 80, aligning its compatibility with other backends like TRITON_ATTN and FLEX_ATTENTION. The change is minimal and only affects the validation logic, allowing existing tests to pass without altering core attention mechanisms.

**Potential risks**  
The addition assumes the underlying ROCm kernels can efficiently handle head size 80; performance or correctness issues may arise if the kernel implementation lacks optimization for this specific size. Edge cases involving mixed head sizes within a single batch or unusual model configurations should be validated.

**Key insights**  
This update highlights the need to keep backend support lists synchronized with model requirements. Developers should verify that any new head size additions are thoroughly tested across diverse model architectures and consider adding automated checks for unsupported configurations during model loading.

---

## 15. [[ROCm][Quantization] Add Composable Kernel (CK) backend support for M…](https://github.com/vllm-project/vllm/pull/34301)


### Base Information

- **PR Number:** #34301
- **Author:** [dllehr-amd](https://github.com/dllehr-amd)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-27 19:37:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34301/files) (2):**
  - `vllm/_aiter_ops.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`

### Summary

**What changed and why**  
This PR adds Composable Kernel (CK) backend support for MXFP4-quantized Mixture-of-Experts (MoE) models on ROCm via the Aiter library. It introduces new parameters to the fused MoE implementation for handling padded dimensions and biases, adds a fused top-k operation for expert selection, and implements weight/shuffle transformations to match CK's A16W4 memory layout. The CK backend is automatically selected on supported ROCm hardware (gfx950) when Aiter is available, providing an optimized inference path for A16W4 MoE workloads.

**Technical impact**  
The changes extend the MXFP4 quantization backend options on ROCm, introducing a new `Mxfp4Backend.CK` variant. This adds a performance-optimized path for MoE inference on AMD GPUs, leveraging CK kernels for improved efficiency. The backend selection logic prioritizes CK over Triton when conditions are met, and the implementation includes specialized weight preparation (interleaving, shuffling) and forward pass routing through new Aiter ops (`fused_topk`, `fused_moe`). This enhances the system's capability to handle padded tensor dimensions and bias terms required by CK.

**Potential risks**  
The CK backend is conditionally enabled only on gfx950 GPUs, limiting its applicability across all ROCm devices. The weight and scale shuffling logic introduces additional tensor transformations that could affect memory usage or correctness if not aligned with CK kernel expectations. There is a risk of regression if the backend selection logic incorrectly chooses CK over Triton in unsupported environments. The per_1x32 quantization method and padding calculations (`hidden_pad // 128 * 128`) may not generalize to all tensor shapes.

**Key insights**  
Developers should ensure the Aiter library is properly installed on target ROCm systems to enable this backend. The padding parameters (`hidden_pad`, `intermediate_pad`) and bias handling are critical for CK kernel compatibility; any changes to tensor dimensions must align with these padding requirements. The backend selection logic is hardware-specific, so testing on non-gfx950 ROCm GPUs may fall back to Triton. The weight shuffling utilities (`shuffle_weight_a16w4`, `shuffle_scale_a16w4`) are essential for correct memory layout and should be validated during model loading.

---

## 16. [[Model Runner V2] Move MM encoder to Model States [3/N]](https://github.com/vllm-project/vllm/pull/35564)


### Base Information

- **PR Number:** #35564
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-27 18:32:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35564/files) (7):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/mm/encoder_cache.py`
  - `vllm/v1/worker/gpu/mm/encoder_runner.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/model_states.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/speculator.py`

### Summary

**What changed and why**  
The changes move multimodal (MM) encoder management from `ModelRunner` to `ModelState` by extracting the encoder cache into a separate `EncoderCache` class and relocating encoder execution logic. This refactoring centralizes MM-related state management within `ModelState` to improve modularity and align with the ongoing Model Runner V2 architecture updates.

**Technical impact**  
`ModelRunner` no longer directly handles MM embeddings; instead, `ModelState` now owns the `EncoderRunner` and coordinates with the shared `EncoderCache`. The `inputs_embeds` parameter is removed from several function signatures (e.g., `capture_graph`), and dummy inputs for graph capture are now prepared by `ModelState.prepare_dummy_inputs()`. This shifts responsibility for MM input preparation to the model state layer, simplifying the runner’s execution flow.

**Potential risks**  
- The `EncoderCache` is only initialized on the first pipeline-parallel (PP) rank, but `ModelState` is created on all ranks. This could cause inconsistencies if MM-related methods are called on non-first PP ranks.  
- The `reset_mm_cache` and `reset_encoder_cache` methods in `EncoderCache` are partially implemented (with TODOs), which may lead to incomplete cache management during profiling or weight updates.  
- Removing `inputs_embeds` from `InputBatch` and related APIs might break compatibility with any external code that still expects this field.

**Key insights**  
- This refactoring enhances separation of concerns by decoupling MM processing from the core runner logic. Developers should ensure MM operations are only invoked on the first PP rank.  
- The `EncoderCache` introduces a clear lifecycle for MM features and encoder outputs, but its cache eviction and budget mechanisms need future implementation.  
- When updating related code, verify that all MM embedding accesses now go through `ModelState.get_mm_embeddings()` and that dummy inputs for graph capture are correctly provided via `ModelState`.

---

## 17. [[Model Runner V2] Support pooling models](https://github.com/vllm-project/vllm/pull/35120)


### Base Information

- **PR Number:** #35120
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-27 18:03:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35120/files) (6):**
  - `vllm/v1/worker/gpu/async_utils.py`
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/pool/__init__.py`
  - `vllm/v1/worker/gpu/pool/pooling_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR adds initial support for decoder-only last-token pooling models (e.g., embedding models) to Model Runner V2. The changes establish the foundational structure by introducing a new `PoolingRunner` class, asynchronous output handling for pooling results, and kernel updates for token accounting, without aiming for comprehensive model support.

**Technical impact**  
The modifications extend the model runner to distinguish between generation and pooling tasks, adding a new execution path for pooling that bypasses the sampler. The architecture now supports asynchronous copying of pooling outputs to CPU and updates token computation tracking specifically for pooling operations. This introduces a new runner type (`"pooling"`) and adjusts task reporting via `get_supported_tasks()`.

**Potential risks**  
- The pooling implementation currently hardcodes L2 normalization and only supports the `"embed"` task, limiting flexibility.  
- The `is_valid` tensor logic assumes sequence lengths equal to prompt lengths, which may not hold for all pooling scenarios.  
- Conditional execution in `gpu_worker.py` could lead to subtle bugs if `is_pooling_model` is set but `pool()` returns `None` unexpectedly.

**Key insights**  
- This is a structural foundation; developers should expect future extensions for additional pooling tasks and model types.  
- The `PoolingRunner` should be enhanced to support configurable normalization and broader pooling strategies.  
- Ensure thorough testing of the asynchronous output flow and token accounting updates to prevent regressions in generation tasks.

---

## 18. [[Misc] Clean up ResponsesRequest model validators](https://github.com/vllm-project/vllm/pull/35531)


### Base Information

- **PR Number:** #35531
- **Author:** [umut-polat](https://github.com/umut-polat)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 17:19:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35531/files) (1):**
  - `vllm/entrypoints/openai/responses/protocol.py`

### Summary

**What changed and why**  
Two cleanups were made to the `ResponsesRequest` model validators: adding missing `@classmethod` decorators to four `@model_validator(mode="before")` methods to align with other validators in the codebase, and replacing bare `ValueError` exceptions with `VLLMValidationError` in three validation points to ensure consistent error responses that include parameter names.

**Technical impact**  
These changes improve consistency across the validation logic, ensuring that all before-mode validators are properly defined as class methods and that error handling uniformly provides structured error details (including parameter names) to clients. This aligns the responses protocol with the patterns used in completion and chat_completion protocols.

**Potential risks**  
If any downstream code explicitly catches `ValueError` from these validators, it may break unless updated to handle `VLLMValidationError`. Additionally, the `@classmethod` decorator addition could affect inheritance or subclass behavior if validators are overridden, though this is unlikely given the current usage.

**Key insights**  
Maintaining consistency in decorator usage and error types across protocols reduces technical debt and improves client error handling. Developers should ensure that all model validators with `mode="before"` include `@classmethod`, and use `VLLMValidationError` for validation errors to provide better error context.

---

## 19. [[misc] cleanup one level of error stack when nixl fails to initialize](https://github.com/vllm-project/vllm/pull/35517)


### Base Information

- **PR Number:** #35517
- **Author:** [youkaichao](https://github.com/youkaichao)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-02-27 16:42:38
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35517/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
Added a guard condition in the `shutdown()` method to check for the existence of `_handshake_initiation_executor` before attempting to shut it down. This prevents an `AttributeError` from being raised during cleanup when NIXL initialization fails, ensuring the error log only shows the root cause ("RuntimeError: NIXL is not available").

**Technical impact**  
The change makes the shutdown process more robust by gracefully handling partial initialization states. It avoids masking the original initialization error with a secondary attribute error, improving debuggability and log clarity for users when NIXL is unavailable.

**Potential risks**  
If other attributes relied upon in `shutdown()` are also missing due to initialization failure, similar issues could arise (e.g., `_recving_transfers`). The fix addresses only one symptom; a broader approach might be needed if the class has multiple dependencies that require cleanup.

**Key insights**  
This is a defensive coding practice that prevents secondary errors during teardown. Developers should consider whether other cleanup steps in `shutdown()` could fail under partial initialization and apply similar guards if needed. The fix aligns with the principle that cleanup methods should be safe to call even if initialization was incomplete.

---

## 20. [[Refactor][Kernel] Add global helper to deduplicate vectorized memory ops](https://github.com/vllm-project/vllm/pull/35105)


### Base Information

- **PR Number:** #35105
- **Author:** [LopezCastroRoberto](https://github.com/LopezCastroRoberto)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 16:28:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35105/files) (6):**
  - `csrc/activation_kernels.cu`
  - `csrc/cuda_vec_utils.cuh`
  - `csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu`
  - `csrc/quantization/fp4/nvfp4_experts_quant.cu`
  - `csrc/quantization/fp4/nvfp4_quant_kernels.cu`
  - `csrc/quantization/fp4/nvfp4_utils.cuh`

### Summary

**What changed and why**  
This refactoring extracts duplicated 256-bit PTX load/store primitives, type converters, and vector traits into a shared `csrc/cuda_vec_utils.cuh` header. It removes dead code like `PackedTraits` and wrapper functions, centralizing vectorized memory operations to improve maintainability and reduce duplication across the codebase. The activation kernel also now includes a compile-time guard (`CUDA_VERSION >= 12090`) to ensure 256-bit paths are only used when supported.

**Technical impact**  
The changes introduce a reusable utility library for CUDA vector operations, enabling consistent handling of 128-bit and 256-bit memory accesses across kernels. This simplifies future implementations (e.g., for new PRs #32957, #34917) and ensures proper fallback mechanisms for older CUDA toolkits. The refactoring also streamlines kernel logic by replacing custom type traits with standardized helpers like `CUDATypeConverter` and `PackedVec`.

**Potential risks**  
The removal of inline fallback code in `ld256`/`st256` (replaced with `assert(false)`) could cause runtime failures if the `VLLM_256B_PTX_ENABLED` macro is misconfigured or unsupported hardware is targeted. Additionally, the reliance on a new header increases coupling; any bugs in `cuda_vec_utils.cuh` could affect multiple kernels. Care must be taken to ensure the CUDA version guard aligns with actual hardware capabilities.

**Key insights**  
Developers should adopt `cuda_vec_utils.cuh` for all new vectorized memory operations to avoid code duplication. The refactoring highlights the importance of compile-time feature detection (CUDA version and architecture) for performance-critical paths. Testing should verify that both 128-bit and 256-bit paths function correctly across supported CUDA versions and GPU architectures.

---

## 21. [[ROCm]: fix aiter rope functionalization](https://github.com/vllm-project/vllm/pull/35533)


### Base Information

- **PR Number:** #35533
- **Author:** [Rohan138](https://github.com/Rohan138)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-27 14:42:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35533/files) (1):**
  - `vllm/compilation/passes/utility/fix_functionalization.py`

### Summary

**What changed and why**  
The fix addresses a functionalization issue with the ROCm-specific AITER RoPE operation by adding it to the list of rotary embedding targets that need defunctionalization. Previously, only the standard `torch.ops._C.rotary_embedding.default` was handled, causing the AITER variant to be overlooked.

**Technical impact**  
This change ensures that both standard and ROCm-specific RoPE operations are properly defunctionalized during graph compilation, maintaining consistency in how rotary embeddings are processed across different hardware backends. It prevents potential graph execution failures on ROCm platforms.

**Potential risks**  
If other RoPE variants (like FI RoPE) are introduced in the future, they may require similar updates. The current implementation checks for the existence of the ROCm operation at runtime, which could fail silently if the attribute name changes or is removed.

**Key insights**  
Consider centralizing RoPE operation definitions in `matcher_utils.py` (similar to `QUANT_OPS`) to improve maintainability. This would make it easier to add new variants without modifying multiple files. The conditional import pattern used here is appropriate for handling backend-specific operations.

---

## 22. [[ROCm] Enabling encoder and encoder-decoder on ROCm and AITER unified backends](https://github.com/vllm-project/vllm/pull/35334)


### Base Information

- **PR Number:** #35334
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-27 13:32:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35334/files) (3):**
  - `docs/design/attention_backends.md`
  - `vllm/v1/attention/backends/rocm_aiter_unified_attn.py`
  - `vllm/v1/attention/backends/rocm_attn.py`

### Summary

**What changed and why**  
This PR enables encoder and encoder-decoder attention support for ROCm backends (`ROCM_ATTN` and `ROCM_AITER_UNIFIED_ATTN`). The changes update documentation and implement encoder attention handling by adding `supports_attn_type` methods and forward logic for encoder attention without KV caching, using direct Q/K/V tensors.

**Technical impact**  
The ROCm attention backends now support all attention types (decoder, encoder, encoder-only, encoder-decoder), aligning them with other backends like `TRITON_ATTN`. Encoder attention bypasses KV cache updates and uses a direct flash attention kernel (`context_attention_fwd`), which may affect performance and memory usage for encoder workloads on ROCm platforms.

**Potential risks**  
- FP8 quantization is explicitly unsupported for encoder attention (raises `NotImplementedError`), which could limit performance or compatibility for quantized encoder models.  
- The encoder path uses a Triton kernel (`context_attention_fwd`) in the ROCm backend, which may introduce cross-platform dependencies or require validation on ROCm hardware.  
- Edge cases with mixed attention types or sliding window configurations in encoder mode may not be fully tested.

**Key insights**  
- Developers should verify that encoder workloads perform correctly on ROCm hardware, especially with sliding window or non-causal attention.  
- The lack of FP8 support for encoder attention may necessitate fallback to other backends for quantized models.  
- Ensure that the Triton kernel used (`context_attention_fwd`) is compatible with ROCm’s execution environment and does not introduce regressions.

---

## 23. [[Feat][RL][2/2] Native Weight Syncing API: IPC](https://github.com/vllm-project/vllm/pull/34171)


### Base Information

- **PR Number:** #34171
- **Author:** [hao-aaron](https://github.com/hao-aaron)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-27 12:45:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34171/files) (14):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/distributed.yaml`
  - `examples/offline_inference/new_weight_syncing/rlhf_async_new_apis.py`
  - `examples/offline_inference/new_weight_syncing/rlhf_ipc.py`
  - `examples/offline_inference/new_weight_syncing/rlhf_nccl.py`
  - `examples/online_serving/new_weight_syncing/rlhf_http_ipc.py`
  - `examples/online_serving/new_weight_syncing/rlhf_http_nccl.py`
  - `tests/distributed/test_weight_transfer.py`
  - `tools/pre_commit/check_forbidden_imports.py`
  - `vllm/config/weight_transfer.py`
  - `vllm/distributed/weight_transfer/base.py`
  - `vllm/distributed/weight_transfer/factory.py`
  - `vllm/distributed/weight_transfer/ipc_engine.py`
  - `vllm/distributed/weight_transfer/nccl_engine.py`

### Summary

**What changed and why**  
This PR introduces an IPC-based weight transfer engine that enables trainer and inference processes to share model weights on the same GPU via CUDA IPC handles. It extends the existing weight syncing API by adding a new `ipc` backend alongside the existing `nccl` backend, formalizes the `trainer_send_weights` pattern with dedicated argument dataclasses, and provides examples for both Ray and HTTP transport modes.

**Technical impact**  
The changes add a new weight transfer backend (`ipc`) to the system, which allows zero-copy weight sharing between colocated processes on the same GPU. This reduces communication overhead compared to NCCL for single-node scenarios. The architecture now supports two distinct transport modes (Ray and HTTP) for IPC handle serialization, and the `trainer_send_weights` API is standardized across backends with type-safe argument classes.

**Potential risks**  
IPC requires processes to share the same GPU, which introduces strict colocation constraints and memory management challenges. The use of pickle for IPC handle serialization over HTTP (`VLLM_ALLOW_INSECURE_SERIALIZATION`) poses security risks if exposed externally. Additionally, mismatched GPU UUIDs or device indices between sender and receiver could lead to runtime errors.

**Key insights**  
Developers should ensure proper GPU memory allocation when using IPC, as both trainer and inference models must fit on the same device. The HTTP mode should only be used in trusted environments due to insecure serialization. The new argument dataclasses (`NCCLTrainerSendWeightsArgs`, `IPCTrainerSendWeightsArgs`) improve API clarity and should be adopted for all new integrations.

---

## 24. [[Bugfix][Model] Fix gpt-oss batch invariance](https://github.com/vllm-project/vllm/pull/35404)


### Base Information

- **PR Number:** #35404
- **Author:** [jzakrzew](https://github.com/jzakrzew)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-02-27 12:41:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35404/files) (2):**
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/models/gpt_oss.py`

### Summary

**What changed and why**  
This PR fixes batch invariance failures for GPT-OSS on H100 GPUs by addressing two issues: replacing the router's `nn.Linear` with `ReplicatedLinear` to ensure deterministic CUDA behavior, and modifying `UnquantizedLinearMethod` to unconditionally use batch-invariant linear layers on CUDA when batch invariance is enabled, rather than restricting it to MoE router gates.

**Technical impact**  
The changes ensure that all linear operations (including QKV and output projections) use deterministic batch-invariant kernels on CUDA, correcting previously incorrect assumptions about cuBLAS behavior. This aligns the GPT-OSS implementation with vLLM's batch invariance guarantees and affects how linear layers are dispatched in batch-invariant mode across CUDA platforms.

**Potential risks**  
Expanding batch-invariant linear usage to all layers on CUDA may introduce performance overhead or compatibility issues with other hardware or quantization methods. The removal of the `is_layer_moe_router_gate` check could inadvertently affect other models if they rely on specific dispatch behavior, though the conditional guard (`vllm_is_batch_invariant()`) limits this to batch-invariant contexts.

**Key insights**  
Developers should verify that batch invariance tests pass for other supported models, as the change in `UnquantizedLinearMethod` applies globally. The fix highlights the importance of ensuring consistent deterministic kernel usage across all linear layers in batch-invariant mode, not just router layers. Future work should address the noted bad assumptions about cuBLAS on Hopper architectures in `batch_invariance.py`.

---

## 25. [[DP] Only use DP padding when cudagraphs are actually used](https://github.com/vllm-project/vllm/pull/34102)


### Base Information

- **PR Number:** #34102
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged time:** 2026-02-27 12:14:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34102/files) (7):**
  - `tests/v1/cudagraph/test_cudagraph_dispatch.py`
  - `vllm/config/compilation.py`
  - `vllm/forward_context.py`
  - `vllm/v1/cudagraph_dispatcher.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/worker/dp_utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR refines when DP padding is applied during CUDA graph execution. Previously, DP padding was enabled whenever CUDA graphs were globally enabled. Now, it's only applied when a specific batch/step actually uses CUDA graphs, particularly targeting `FusedMoE.forward_impl_chunked` which requires DP padding only when inside a CUDA graph. This prevents unnecessary padding in scenarios like DeepEP HT prefilling where CUDA graphs are disabled.

**Technical impact**  
The changes introduce a more granular control mechanism for DP padding by modifying the `coordinate_batch_across_dp` function to conditionally enable padding based on the synchronized CUDA graph mode across DP ranks, rather than the global compilation config. This affects batch coordination in distributed settings, ensuring padding only occurs when CUDA graphs are actively used for a given step, improving efficiency in mixed-mode workloads.

**Potential risks**  
There's a risk of inconsistency if the CUDA graph mode synchronization across DP ranks fails or if the logic for determining when to pad is incorrect. Edge cases may arise in hybrid parallelism setups where some ranks use CUDA graphs while others do not. Additionally, the removal of `allow_dp_padding` parameter could affect other features relying on explicit DP padding control.

**Key insights**  
Developers should verify that DP padding is correctly synchronized with CUDA graph usage in all distributed scenarios. The PR also suggests a future enhancement: checking if any layers have `FusedMoE.use_dp_chunking==True` before enabling DP padding, which could further optimize performance. Ensure thorough testing in DP environments to confirm no regressions in CUDA graph capture or execution.

---

## 26. [[Bugfix] Add monkeypatch to prevent race condition from writing](https://github.com/vllm-project/vllm/pull/35420)


### Base Information

- **PR Number:** #35420
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-27 11:51:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35420/files) (1):**
  - `vllm/compilation/compiler_interface.py`

### Summary

**What changed and why**  
This PR adds a monkeypatch to `CompiledArtifact.save()` method from PyTorch's Inductor compiler to use atomic writes when saving binary cache files. This backports a fix from PyTorch 2.10+ to prevent race conditions where multiple processes compiling concurrently could corrupt cache files. The patch is conditionally applied only for PyTorch versions below 2.10.0.

**Technical impact**  
The change ensures cache file integrity during concurrent compilation processes in vLLM when using PyTorch <2.10. This prevents silent corruption of compiled artifacts that could lead to runtime crashes or incorrect behavior. The patch is version-gated and idempotent (checks `_vllm_patched` flag), ensuring it only applies once per process.

**Potential risks**  
Monkeypatching internal PyTorch classes (`CompiledArtifact`) creates a dependency on implementation details that could change in future PyTorch releases. The patch only handles the "binary" format case, potentially leaving "unpacked" format vulnerable if used. There's also a risk of incompatibility if the original `save` method signature or behavior changes in PyTorch versions between the current and 2.10.

**Key insights**  
This is a necessary workaround for a known race condition that will be resolved when vLLM upgrades to PyTorch 2.10+. Developers should be aware that this patch modifies PyTorch internal behavior and monitor for any compilation-related issues. The version gating and idempotency check are good defensive practices that should be maintained if similar patches are needed elsewhere.

---

## 27. [[Transformers backend] Ignore MTP weights when num_nextn_predict_layers=0](https://github.com/vllm-project/vllm/pull/34888)


### Base Information

- **PR Number:** #34888
- **Author:** [SteadfastAsArt](https://github.com/SteadfastAsArt)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-27 11:39:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34888/files) (2):**
  - `vllm/model_executor/models/transformers/base.py`
  - `vllm/model_executor/models/utils.py`

### Summary

**What changed and why**  
The PR fixes a loading issue for models (like `zai-org/GLM-OCR`) that have `num_nextn_predict_layers=0` in their config but still contain MTP (Multi-Token Prediction) layer weights in the checkpoint. These weights are intentionally kept for optional speculative decoding. The change modifies the Transformers backend to ignore unexpected MTP weight prefixes when `num_nextn_predict_layers=0`, preventing a crash during model loading.

**Technical impact**  
This update enhances the robustness of the Transformers backend by allowing it to tolerate checkpoint weights that are not part of the base model architecture. It dynamically detects the transformer layer module list and adds the appropriate MTP layer prefix to `ignore_unexpected_prefixes`, ensuring compatibility with models that support speculative decoding without affecting standard inference.

**Potential risks**  
The fix relies on correctly identifying the transformer layer `nn.ModuleList` and assumes the MTP layers are contiguous after the base layers. If a model’s architecture deviates from this pattern (e.g., non-contiguous MTP layers or a different module structure), the prefix detection may fail or incorrectly ignore valid weights. Additionally, the condition `num_nextn_predict_layers=0` might not cover all config variants (e.g., `mtp_num_hidden_layers`), though the code attempts to handle multiple attribute names.

**Key insights**  
The solution is model-agnostic and avoids hardcoding paths, making it adaptable to various architectures. Developers should ensure that any future changes to speculative decoding or layer indexing align with this prefix detection logic. Testing with diverse model configurations is recommended to validate the robustness of the ignore mechanism.

---

## 28. [[compile] Fix caching error over pytree slice node.](https://github.com/vllm-project/vllm/pull/35308)


### Base Information

- **PR Number:** #35308
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-27 11:34:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35308/files) (2):**
  - `tests/compile/test_aot_compile.py`
  - `vllm/compilation/caching.py`

### Summary

**What changed and why**  
This PR fixes a caching error that occurs when serializing graph modules containing `slice()` nodes during compilation. The issue arises because PyTorch's pytree treats `slice()` as a leaf node, causing pickling failures. The fix temporarily patches pytree to handle `slice()` as a non-leaf node during serialization/deserialization, with a workaround that will be removed once Torch 2.12 addresses the root cause.

**Technical impact**  
The changes modify the `VllmSerializableFunction` serialization process to temporarily register `slice` as a pytree node, enabling proper tree mapping during caching. This ensures graph modules with dynamic slicing operations (e.g., `x[slice(0, symint_0)]`) can be cached and reloaded without pickling errors, maintaining compilation caching functionality for models using symbolic shapes.

**Potential risks**  
The patch uses private PyTorch APIs (`_private_register_pytree_node` and `_deregister_pytree_node`), which may change in future PyTorch versions. There's a risk of interference if other code concurrently modifies pytree node registrations. The workaround should be removed once Torch 2.12 fixes the issue to avoid technical debt.

**Key insights**  
This is a targeted workaround for a PyTorch limitation, with a clear migration path. Developers should monitor PyTorch updates and remove the patch once Torch 2.12 is adopted. The added test validates the fix for slice-inclusive graph modules, ensuring regression coverage. Consider isolating the patch to minimize side effects on other pytree operations.

---

## 29. [[Model Runner V2] Warmup kernels](https://github.com/vllm-project/vllm/pull/35172)


### Base Information

- **PR Number:** #35172
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-27 10:43:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35172/files) (3):**
  - `vllm/sampling_params.py`
  - `vllm/v1/worker/gpu/warmup.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR adds kernel warmup functionality for the Model Runner V2 architecture to JIT compile Triton kernels before serving requests. It introduces a `warmup_kernels()` function that simulates both prefill and decode steps using specially crafted `SamplingParams` that exercise all sampling features, ensuring comprehensive kernel compilation.

**Technical impact**  
The warmup logic is integrated into the worker initialization pipeline, specifically in `compile_or_warm_up_model()`. For V2 runners, it executes simulated inference cycles to trigger JIT compilation of Triton kernels, while V1 runners retain their existing warmup approach. This improves first-request latency by moving compilation overhead to startup time.

**Potential risks**  
The warmup creates artificial requests with specific IDs (`_warmup_{i}_`) that could potentially conflict with real request IDs if not properly namespaced. The temporary disabling of the KV connector during warmup (`set_disabled(True)`) assumes no side effects on subsequent operations. Edge cases with extremely small batch sizes or token limits may not be fully exercised.

**Key insights**  
The `SamplingParams.for_sampler_warmup()` factory method provides a maintainable way to define comprehensive sampling parameters for testing. Developers should ensure warmup request IDs are truly unique and consider adding validation for edge configurations. The alternative external warmup approach mentioned in the PR description warrants evaluation for architectural separation concerns.

---

## 30. [[BugFix] Fix 3D rope in transformers backend](https://github.com/vllm-project/vllm/pull/35097)


### Base Information

- **PR Number:** #35097
- **Author:** [zucchini-nlp](https://github.com/zucchini-nlp)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-27 10:34:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35097/files) (1):**
  - `vllm/model_executor/models/transformers/multimodal.py`

### Summary

**What changed and why**  
This PR addresses two issues: ensuring compatibility with transformers' recent changes to 3D position ID preparation for Qwen-VL models, and fixing GLM video input handling to match transformers' float timestamp convention. The changes modify multimodal processing to handle `mm_token_type_ids` correctly and clean up unused parameters.

**Technical impact**  
The modifications maintain forward/backward compatibility with transformers' API changes, particularly for multimodal models using rotary position embeddings. The code now properly passes `mm_token_type_ids` to the rope index computation while removing it from other processing paths. This ensures 3D rope calculations work correctly for image/video models.

**Potential risks**  
The conditional logic around `mm_token_type_ids` (line 473) could cause issues if the tensor contains zeros but should still be passed. The error checking in `get_mrope_input_positions` may be too restrictive since it only allows `image_grid_thw` and `mm_token_type_ids`, potentially blocking future multimodal features.

**Key insights**  
Developers should verify that all multimodal models pass the correct token type IDs through the processing pipeline. The cleanup of unused parameters (`token_type_ids`, `mm_token_type_ids`) from `embed_multimodal` is good practice. Consider revising the error checking to be more flexible or adding proper documentation about supported features.

---

## 31. [Support parakeet as audio encoder for nemotron-nano-vl](https://github.com/vllm-project/vllm/pull/35100)


### Base Information

- **PR Number:** #35100
- **Author:** [netanel-haber](https://github.com/netanel-haber)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-27 10:07:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35100/files) (3):**
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/parakeet.py`
  - `vllm/transformers_utils/configs/parakeet.py`

### Summary

**What changed and why**  
Added audio encoding support to the Nemotron-Nano-VL model using Parakeet as the audio encoder. This enables multimodal processing of audio inputs alongside existing image and video capabilities. The changes include a new ParakeetExtractor for feature extraction, integration into the processor, and updates to the model's multimodal data handling.

**Technical impact**  
The model now supports audio inputs through a dedicated audio encoder pipeline. Audio is processed into features via ParakeetExtractor, projected to match the LLM's hidden size, and integrated into the multimodal input flow. This extends the model's multimodal capabilities without breaking existing image/video functionality, maintaining backward compatibility.

**Potential risks**  
- Audio processing may increase memory usage and latency, especially for long audio clips (up to 10 minutes).  
- The `_normalize_audio_length` logic could introduce padding inconsistencies if audio lengths are near threshold boundaries.  
- Error handling for mismatched audio tokens in text is present but may need validation for edge cases like empty audio arrays.

**Key insights**  
- The implementation follows a modular pattern similar to existing video/image support, ensuring consistency.  
- Developers should verify audio sampling rate and channel requirements when integrating audio data.  
- Consider profiling audio processing performance, as feature extraction and projection add computational overhead.

---

## 32. [[Doc] Fix link to Llama chat template for usability](https://github.com/vllm-project/vllm/pull/35525)


### Base Information

- **PR Number:** #35525
- **Author:** [hickeyma](https://github.com/hickeyma)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 09:51:10
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35525/files) (1):**
  - `docs/serving/openai_compatible_server.md`

### Summary

**What changed and why**  
Updated a broken hyperlink in the documentation from a deprecated GitHub reference to the official Meta Llama 3 documentation page. This ensures users can access the correct chat template example when following the guide.

**Technical impact**  
No functional impact on the codebase—this is a documentation-only change that improves usability by directing users to an authoritative, up-to-date resource for Llama 3 chat template specifications.

**Potential risks**  
Minimal risk; the new link is from the official Meta domain and should be stable. However, future changes to the external documentation structure could break the link again, so periodic validation is recommended.

**Key insights**  
Maintaining accurate external links in documentation is critical for user experience. Consider implementing automated link checking in CI/CD to catch broken references early, especially for frequently updated external resources.

---

## 33. [[perf] Use pinned memory for async H2D transfer in do_mamba_copy_block](https://github.com/vllm-project/vllm/pull/35480)


### Base Information

- **PR Number:** #35480
- **Author:** [hl475](https://github.com/hl475)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 09:50:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35480/files) (4):**
  - `tests/v1/e2e/test_mamba_prefix_cache.py`
  - `tests/v1/worker/test_mamba_utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/mamba_utils.py`

### Summary

**What changed and why**  
The PR optimizes Mamba state copying by replacing dynamic list-based parameter collection with pre-allocated pinned memory buffers (`MambaCopyBuffers`). This eliminates expensive CPU-to-GPU synchronization (`aten::to` stalls) observed during async H2D transfers in `do_mamba_copy_block`, reducing the worst-case latency from ~20ms to 0.13ms.

**Technical impact**  
The changes introduce a reusable buffer abstraction (`MambaCopyBuffers`) that stores copy metadata (source/destination pointers, sizes) in pinned CPU memory. This enables asynchronous H2D transfers via `copy_to_gpu()`, reducing synchronization overhead. The buffer is lazily initialized and reset per inference step, maintaining compatibility with existing Mamba state management logic.

**Potential risks**  
- Buffer size calculation depends on `max_num_reqs` and Mamba group configuration; insufficient size could lead to overflow.  
- The `offset` reset logic assumes proper sequencing between pre/post-processing phases; concurrent or misordered calls could corrupt metadata.  
- Changes to function signatures (e.g., `do_mamba_copy_block`) require updates to all callers, including test mocks.

**Key insights**  
- Pinned memory for async transfers is critical for performance in memory-bound operations.  
- Ensure `max_num_reqs` accurately reflects runtime scaling to prevent buffer overallocation or underflow.  
- Consider adding bounds checking in `collect_mamba_copy_meta` to safeguard against offset exceeding buffer capacity.

---

## 34. [[Misc] Fill in some v1 CODEOWNERS gaps](https://github.com/vllm-project/vllm/pull/35524)


### Base Information

- **PR Number:** #35524
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-27 09:34:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35524/files) (1):**
  - `.github/CODEOWNERS`

### Summary

**What changed and why**  
This PR updates the CODEOWNERS file to fill coverage gaps in the v1 codebase, specifically adding ownership for the engine, executor, and worker directories while adjusting ownership for GPU worker components. The changes ensure proper code review assignment for recently added or previously uncovered v1 modules.

**Technical impact**  
These changes improve the code review process by ensuring relevant experts are automatically assigned to review changes in critical v1 components. The engine, executor, and worker directories now have designated owners, while GPU worker components gain additional ownership coverage for better collaboration.

**Potential risks**  
The main risk is potential review assignment conflicts if multiple owners have overlapping responsibilities or if the specified owners lack context for certain changes within these directories. Additionally, if the newly assigned owners have limited availability, it could slow down the review process for these components.

**Key insights**  
The PR effectively addresses ownership gaps but should be complemented with clear documentation about each owner's specific areas of expertise within these directories. Consider establishing backup reviewers for critical paths like the engine directory to ensure timely reviews during peak development periods.

---

## 35. [[Model] Add huggingface skt/A.X-K1 model](https://github.com/vllm-project/vllm/pull/32407)


### Base Information

- **PR Number:** #32407
- **Author:** [fort726](https://github.com/fort726)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 09:26:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32407/files) (7):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/AXK1.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/configs/AXK1.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`

### Summary

**What changed and why**  
This PR adds support for the SKT A.X-K1 model to vLLM. It introduces a new model architecture implementation (`AXK1ForCausalLM`) with a corresponding configuration class (`AXK1Config`), registers the model in the codebase, and updates documentation and tests. The model is a MoE (Mixture of Experts) architecture with DeepSeek-v2-like components, supporting features like MLA (Multi-Head Latent Attention) and expert parallelism.

**Technical impact**  
The changes extend vLLM's model zoo with a new large-scale MoE model, requiring integration of specialized components like `AXK1MoE` (which leverages `SharedFusedMoE` for expert routing) and `AXK1MLP`. The implementation reuses and adapts existing DeepSeek-v2 infrastructure (e.g., attention and MLP layers), ensuring consistency with vLLM's optimized kernels and distributed execution backends (e.g., tensor parallelism, expert parallelism). The model supports advanced features like rope scaling, LoRA, and sequence parallelism.

**Potential risks**  
- The model relies on several experimental or complex features (e.g., `noaux_tc` top-k method, FP16 overflow workarounds, fused MoE kernels) that may have edge cases or performance variations across hardware.  
- Configuration parameters like `routed_scaling_factor` and expert-balancing settings (e.g., `enable_eplb`) require careful tuning to avoid numerical instability or load imbalance.  
- The integration assumes compatibility with existing DeepSeek-v2 components; deviations in architecture (e.g., attention head dimensions, scaling logic) could lead to subtle bugs.

**Key insights**  
- The implementation efficiently reuses vLLM's MoE and attention abstractions, minimizing code duplication. Developers should verify the model's behavior with the provided test command, especially for distributed setups (TP32).  
- Pay close attention to the FP16 overflow fix in `AXK1MoE.forward` and the scaling logic in `_get_llama_4_scaling` to ensure numerical correctness.  
- Update any downstream documentation (e.g., model cards, deployment guides) to reflect the new model's requirements and limitations.

---

## 36. [[Kernel] [Helion] [7/N] Use HOP to represent Helion Kernel call to enable fx tracing and pattern matching](https://github.com/vllm-project/vllm/pull/34390)


### Base Information

- **PR Number:** #34390
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-27 09:21:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34390/files) (3):**
  - `tests/kernels/helion/test_pattern_matching.py`
  - `tests/kernels/helion/test_register.py`
  - `vllm/kernels/helion/register.py`

### Summary

**What changed and why**  
This PR introduces a HigherOrderOp (HOP) `helion_kernel_wrapper_mutation` to represent Helion kernel calls in PyTorch graphs, enabling FX tracing and pattern matching. The changes add a new HOP-based execution path in `HelionKernelWrapper.__call__` that records kernel calls as HOP nodes during tracing, while maintaining backward compatibility with a CustomOp fallback for older PyTorch versions (<2.11).

**Technical impact**  
The integration allows Helion kernels to participate in FX graph transformations and inductor optimizations, such as pattern matching and fusion. The `HelionKernelWrapper` now dynamically selects between HOP tracing, eager execution, and CustomOp fallback based on PyTorch version and tracing context. This lays the groundwork for future lowering into specialized Triton code.

**Potential risks**  
The dual-path logic (HOP vs. CustomOp) increases complexity and may lead to subtle behavioral differences between tracing and eager modes. The HOP path relies on internal Helion APIs (`helion_kernel_side_table`, `infer_output_spec`) which could change. There is also a risk of incorrect tensor/constant argument partitioning in `_partition_args` if kernel signatures are mismatched.

**Key insights**  
Developers should ensure kernel signatures are well-defined to avoid argument partitioning errors. The HOP path is only active under `get_proxy_mode()`, so tests must use `make_fx` or similar tracing to validate. The PR prepares for future fusion opportunities but currently only replaces patterns without optimization—subsequent PRs will need to implement actual fusion logic.

---

## 37. [[Core] Fix `gpu_worker.py` pre-commit errors](https://github.com/vllm-project/vllm/pull/35312)


### Base Information

- **PR Number:** #35312
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 07:54:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35312/files) (1):**
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
The changes fix pre-commit errors by refactoring profiler type checking in `gpu_worker.py`. Instead of repeatedly accessing `self.profiler_config.profiler`, the value is stored in a local variable `profiler_type`. Additionally, a fallback case was added to log a warning and return early for unrecognized profiler types, preventing potential runtime errors.

**Technical impact**  
This improves code maintainability by reducing repeated attribute access and centralizing the profiler type logic. The new warning and early return enhance robustness by gracefully handling unsupported profiler configurations, though it may silently skip profiling if an invalid type is provided.

**Potential risks**  
If an unrecognized profiler type is configured, profiling will be skipped without raising an error, which could lead to unnoticed missing performance data. The change assumes `profiler_type` is always a string; non-string values might cause unexpected behavior in the logging or comparisons.

**Key insights**  
Always validate configuration inputs early to avoid silent failures. Consider raising a more explicit error (e.g., `ValueError`) for invalid profiler types to ensure misconfigurations are caught during development. Using a local variable for repeated config access is a good practice for readability and performance.

---

## 38. [Add @BoyuanFeng to CODEOWNERS](https://github.com/vllm-project/vllm/pull/35317)


### Base Information

- **PR Number:** #35317
- **Author:** [BoyuanFeng](https://github.com/BoyuanFeng)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-27 07:53:47
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35317/files) (2):**
  - `.github/CODEOWNERS`
  - `docs/governance/committers.md`

### Summary

**What changed and why**  
The changes add @BoyuanFeng as a code owner for the `vllm/compilation` directory and as a committer responsible for the torch.compile integration. This reflects their new role in maintaining and reviewing PRs related to vLLM's compilation features, as indicated in the PR description.

**Technical impact**  
These updates formalize @BoyuanFeng's responsibilities within the project governance. The CODEOWNERS change ensures they are automatically requested for reviews on changes to compilation-related code, while the committers.md update publicly acknowledges their expertise in compilation and CUDAGraph.

**Potential risks**  
Minimal risk, as this is a straightforward update to project metadata. However, ensuring all listed code owners remain active and aligned on review standards is important to avoid review bottlenecks or inconsistencies.

**Key insights**  
This change strengthens the maintenance team for a critical performance component (torch.compile integration). Developers should note the expanded review coverage for compilation-related PRs and engage the updated code owner list for faster feedback.

---

## 39. [[Bugfix] Handle case when kimi ends reasoning with a tool call](https://github.com/vllm-project/vllm/pull/33646)


### Base Information

- **PR Number:** #33646
- **Author:** [koush](https://github.com/koush)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-27 06:58:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33646/files) (2):**
  - `vllm/reasoning/__init__.py`
  - `vllm/reasoning/kimi_k2_reasoning_parser.py`

### Summary

**What changed and why**  
A new reasoning parser (`KimiK2ReasoningParser`) was added to handle Kimi K2 model's unique behavior where reasoning can end implicitly with a tool-call start token (`<\|tool_calls_section_begin\|>`) instead of an explicit `</think>` token. The parser replaces the previous DeepSeek-based parser for the `kimi_k2` model to correctly extract and execute tool calls that were previously missed.

**Technical impact**  
The change introduces a model-specific parser that properly detects reasoning boundaries and tool-call initiation, ensuring tool calls are parsed and executed in both streaming and non-streaming scenarios. It maintains backward compatibility by falling back to an identity parser when thinking is disabled via `chat_template_kwargs`.

**Potential risks**  
If the tokenizer lacks the required special tokens (`<think>`, `</think>`, `<\|tool_calls_section_begin\|>`), the parser will raise a `RuntimeError`. Edge cases where reasoning contains partial or overlapping tokens may lead to incorrect boundary detection. The identity mode fallback relies on correct `chat_template_kwargs` configuration.

**Key insights**  
Developers should ensure tokenizers include the necessary special tokens for Kimi K2. The parser’s design cleanly separates reasoning and content extraction, but thorough testing is needed for streaming scenarios with fragmented token sequences. This approach avoids over-reliance on inherited parsers that don’t match the model’s behavior.

---

## 40. [[Bugfix] Fix check_interleaved_audio_video false positive for batched non-interleaved requests](https://github.com/vllm-project/vllm/pull/35487)


### Base Information

- **PR Number:** #35487
- **Author:** [linyueqian](https://github.com/linyueqian)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 06:48:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35487/files) (2):**
  - `tests/models/multimodal/processing/test_qwen2_5_omni_embed.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`

### Summary

**What changed and why**  
The fix addresses a false positive in `check_interleaved_audio_video` where batched non-interleaved requests were incorrectly detected as interleaved. Previously, the function used only a range-overlap check, which failed when audio tokens from one request fell between video tokens of consecutive requests in the batch. The solution adds a density check to ensure that every position in the combined video/audio span is occupied by either a video or audio token, eliminating gaps from text or image tokens at batch boundaries.

**Technical impact**  
This change refines the detection logic for the `use_audio_in_video=True` case, ensuring that only truly interleaved sequences are routed to `merge_interleaved_embeddings`. It prevents shape mismatch crashes that occurred when non-interleaved batched requests were incorrectly processed as interleaved. The fix is backward-compatible and applies to both Qwen2.5-Omni and Qwen3-Omni models due to shared imports.

**Potential risks**  
The density check assumes that true interleaved sequences have no gaps, which may not hold if future tokenization schemes introduce sparse multimodal patterns. Edge cases with exactly adjacent but non-interleaved video/audio blocks could still pass the density check if no other tokens separate them, though this is likely intentional. Performance impact is minimal, but the added O(1) span calculation slightly increases computational overhead.

**Key insights**  
Developers should note that interleaving detection now requires both range overlap and token density, making the logic more robust to batching artifacts. The regression test added provides a clear example of the batched scenario to prevent future regressions. When modifying multimodal token handling, ensure that any new token types or arrangements are considered in the density check to avoid false negatives.

---

## 41. [[Bugfix] Fix DCP + FA3 crash due to missing num_splits in _forward_with_dcp](https://github.com/vllm-project/vllm/pull/35082)


### Base Information

- **PR Number:** #35082
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 06:27:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35082/files) (1):**
  - `vllm/v1/attention/backends/flash_attn.py`

### Summary

**What changed and why**  
Two lines were added to pass the `num_splits` parameter (set to `attn_metadata.max_num_splits`) to both `flash_attn_varlen_func` calls within the `_forward_with_dcp` function. This fixes a crash when using decode-context-parallel-size (DCP) with FlashAttention 3, where a mismatch in `num_splits` values between graph capture and execution caused a shape error in scheduler metadata.

**Technical impact**  
The change ensures consistency between the DCP and non-DCP forward paths in FlashAttention 3, aligning the `num_splits` parameter used during CUDA graph capture and execution. This prevents a `RuntimeError` due to mismatched `metadata_size` calculations and enables stable operation of large models (like Qwen3.5) with DCP on H200 GPUs.

**Potential risks**  
If `attn_metadata.max_num_splits` is incorrectly set or varies unexpectedly, it could still lead to metadata size mismatches. Additionally, the fix assumes that the non-DCP path's handling of `num_splits` is correct; any latent issues there might now propagate to the DCP path.

**Key insights**  
Always ensure that all code paths using FlashAttention 3's CUDA graph features pass identical parameters during capture and execution. Developers should verify that `attn_metadata.max_num_splits` is reliably derived and consider adding validation for parameter consistency across related functions to prevent similar bugs.

---

## 42. [Revert "Add GlmOcrConfig for GLM-OCR model type recognition"](https://github.com/vllm-project/vllm/pull/35512)


### Base Information

- **PR Number:** #35512
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-27 06:13:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35512/files) (3):**
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/glm_ocr.py`

### Summary

**What changed and why**  
This PR reverts commit #34982, which added GLM-OCR model configuration support. The revert removes the `GlmOcrConfig` and `GlmOcrVisionConfig` classes, their imports, and the model type mapping. The reason stated is that the original change broke GLM-OCR functionality instead of fixing it.

**Technical impact**  
The removal eliminates GLM-OCR model recognition capability from the vLLM codebase. Models of type `glm_ocr` will no longer be recognized or properly configured, potentially causing failures when loading such models. This reverts the system to a state before the problematic configuration was introduced.

**Potential risks**  
If GLM-OCR models are still in use or expected to be supported, this revert will cause them to fail to load or initialize correctly. There is also a risk that other dependent code or configurations might still reference the removed classes, leading to import errors or runtime failures.

**Key insights**  
This revert is a corrective action to address a regression. Developers should verify that GLM-OCR models are not required in current workflows. If support is needed, a corrected implementation must be developed and tested to ensure it properly handles GLM-OCR model initialization without breaking existing functionality.

---

## 43. [[compile] Cleanup: Remove unnecessary +rms_norm forcing for sequence parallelism](https://github.com/vllm-project/vllm/pull/35410)


### Base Information

- **PR Number:** #35410
- **Author:** [jasonlizhengjian](https://github.com/jasonlizhengjian)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-27 05:36:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35410/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
Removed code that forced `+rms_norm` custom ops when sequence parallelism (SP) is enabled. This forcing is no longer required because PR #27126 introduced `MatcherRMSNorm`, which can handle both custom and native RMS norm implementations, making the workaround obsolete.

**Technical impact**  
This change allows the native `rms_norm` implementation to be traced through the inductor compiler when SP is enabled, which should improve runtime performance. The removal simplifies the configuration logic and reduces unnecessary coupling between SP and specific operator implementations.

**Potential risks**  
If the `MatcherRMSNorm` matcher has any undiscovered edge cases or compatibility issues with certain model architectures, SP could potentially break in scenarios where it previously worked due to the forced custom op. The forcing is kept for pipeline parallelism (PP>1) and dynamo partitioning as a workaround for bug #27894, so those configurations remain unaffected.

**Key insights**  
This is a cleanup change that aligns the code with improved compiler capabilities. Developers should verify that SP continues to work correctly with models using RMS normalization after this change. The performance improvement from native tracing is a key benefit, but thorough testing is recommended to ensure no regression occurs.

---

## 44. [[Bugfix] Add missing activation attr to RMSNormGated](https://github.com/vllm-project/vllm/pull/35423)


### Base Information

- **PR Number:** #35423
- **Author:** [Tib-Gridello](https://github.com/Tib-Gridello)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-27 04:53:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35423/files) (1):**
  - `vllm/model_executor/layers/layernorm.py`

### Summary

**What changed and why**  
The fix adds a missing `activation` parameter and corresponding `self.activation` attribute to the `RMSNormGated.__init__()` method. This resolves an `AttributeError` that occurred when `forward_cuda()` tried to access `self.activation`, which was referenced in a previous change (#35102) but never properly initialized.

**Technical impact**  
This change ensures consistency between the `RMSNormGated` class in `layernorm.py` and its sibling implementation in `fla/ops/layernorm_guard.py`. Models using `RMSNormGated` (specifically Qwen3.5 GatedDeltaNet layers) will now load and execute without errors, restoring functionality that was broken by the incomplete implementation.

**Potential risks**  
The default value `"swish"` should be verified to match the expected behavior in `forward_cuda()`. If other activation types are used elsewhere in the codebase, there's a risk of inconsistency. Additionally, any existing code that instantiated `RMSNormGated` without the activation parameter might now have different default behavior than before the bug was introduced.

**Key insights**  
This is a straightforward initialization fix that aligns two parallel implementations. Developers should ensure that both `layernorm.py` and `layernorm_guard.py` remain synchronized for future changes. The fix highlights the importance of updating all related files when modifying shared interfaces, especially when dealing with CUDA and Python implementations of the same layer.

---

## 45. [Flashinfer cuDNN backend for Qwen3 VL ViT attention](https://github.com/vllm-project/vllm/pull/34580)


### Base Information

- **PR Number:** #34580
- **Author:** [maxyanghu](https://github.com/maxyanghu)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-27 04:20:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34580/files) (6):**
  - `tests/kernels/attention/test_mha_attn.py`
  - `vllm/model_executor/layers/attention/mm_encoder_attention.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/platforms/cuda.py`
  - `vllm/v1/attention/ops/vit_attn_wrappers.py`

### Summary

**What changed and why**  
Added FlashInfer as a new backend for Qwen3 VL ViT attention in the multimodal encoder, enabled via `--mm-encoder-attn-backend=FLASHINFER`. This includes support for cuDNN's requirements by computing and padding `cu_seqlens`, `sequence_lengths`, and `max_seqlen` to avoid graph recompilation. The changes are limited to Qwen3 VL ViT; Qwen2.5 VL ViT is not supported.

**Technical impact**  
Introduces a new attention backend that improves encoder performance by 19.3% (as shown in benchmarks) for supported models. The implementation adds bucketing for batch sizes and sequence lengths to reduce cuDNN graph recompilation overhead and includes a workspace buffer for FlashInfer operations. The attention layer interface is extended to pass `sequence_lengths` for the new backend.

**Potential risks**  
The backend is only supported for Qwen3 VL ViT, and using it with Qwen2.5 VL ViT may lead to incorrect behavior or errors. Padding and bucketing logic adds complexity and could introduce subtle bugs if sequence lengths or batch sizes exceed bucket boundaries. The workspace buffer is a global singleton, which may cause issues in multi-GPU or multi-process scenarios if not properly managed.

**Key insights**  
Developers should ensure `--mm-encoder-attn-backend=FLASHINFER` is only used with Qwen3 VL models. The performance gain is significant, but the bucketing mechanism is a temporary workaround until CUDA graph support is added (referenced issue #34763). Reviewers should verify that the padding logic correctly handles edge cases and that the workspace buffer is safely accessed across different execution contexts.

---

## 46. [[Bugfix] Replace assert with ValueError for response_format validation in completions endpoint](https://github.com/vllm-project/vllm/pull/35456)


### Base Information

- **PR Number:** #35456
- **Author:** [umut-polat](https://github.com/umut-polat)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-27 00:01:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35456/files) (2):**
  - `tests/entrypoints/openai/test_completion_error.py`
  - `vllm/entrypoints/openai/completion/protocol.py`

### Summary

**What changed and why**  
The changes replace `assert` statements with explicit `ValueError` raises and add a Pydantic model validator for `response_format` validation in the completions endpoint. This ensures missing `json_schema` fields result in a proper 400 Bad Request error instead of a 500 Internal Server Error, aligning with a previous fix for the chat completions endpoint.

**Technical impact**  
These modifications improve error handling by ensuring validation failures are caught by `create_error_response` and returned as client errors (400). The addition of a model validator centralizes validation logic for `response_format`, making the code more maintainable and consistent across endpoints.

**Potential risks**  
The validator assumes `response_format` may be a dictionary or an object with attributes; if other data structures are introduced, it could fail. Additionally, the removal of the `assert structural_tag is not None` check might hide null values if `structural_tag` is unexpectedly `None` elsewhere.

**Key insights**  
Always use explicit validation over `assert` in production code to avoid internal server errors. Consider extending the validator to handle `structural_tag` similarly for consistency. The test addition is crucial for preventing regressions and should be run as part of the CI pipeline.

---

