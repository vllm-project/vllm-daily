# vLLM Merged PR Report

**Report Date:** 2026-02-25 PST

**Total Merged PRs:** 35

---

## 1. [[XPU][8/N] Fix kernel bugs in XPU LoRA and MOE LORA](https://github.com/vllm-project/vllm/pull/34115)


### Base Information

- **PR Number:** #34115
- **Author:** [chaojun-zhang](https://github.com/chaojun-zhang)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-25 23:46:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34115/files) (5):**
  - `tests/lora/test_fused_moe_lora_kernel.py`
  - `tests/lora/test_punica_xpu_ops.py`
  - `vllm/lora/ops/xpu_ops/__init__.py`
  - `vllm/lora/ops/xpu_ops/lora_ops.py`
  - `vllm/lora/punica_wrapper/punica_xpu.py`

### Summary

**What changed and why**  
This PR fixes kernel bugs in XPU LoRA and MOE LoRA implementations by aligning MOE LoRA kernels with CUDA counterparts and correcting XPU LoRA operations. Changes include adding comprehensive XPU-specific test coverage, updating kernel implementations to use `torch.ops._xpu_C` instead of IPEX, and enhancing the Punica wrapper to support fused MoE LoRA operations.

**Technical impact**  
The modifications improve XPU platform compatibility by replacing IPEX dependencies with direct XPU kernel calls, ensuring consistent behavior with CUDA implementations. The addition of fused MoE LoRA support enables efficient mixture-of-experts execution on XPU devices, while updated tests validate correctness across various configurations.

**Potential risks**  
Direct use of `torch.ops._xpu_C` may introduce platform-specific dependencies, potentially affecting portability. The fused MoE LoRA implementation relies on Triton, which could limit support on systems without Triton compatibility. Edge cases in tensor padding and slicing (e.g., in `bgmv_expand_slice`) require careful validation to avoid out-of-bounds errors.

**Key insights**  
Developers should verify XPU backend availability when deploying these changes. The test suite now robustly covers XPU LoRA operations, but integration testing on actual XPU hardware is recommended. The shift from IPEX to direct XPU ops simplifies the codebase but necessitates monitoring for future PyTorch/XPU API changes.

---

## 2. [[BugFix][XPU] Fix speculative decoding on Intel XPU due to bug with `IGC_ForceOCLSIMDWidth=16`](https://github.com/vllm-project/vllm/pull/35298)


### Base Information

- **PR Number:** #35298
- **Author:** [ofirzaf](https://github.com/ofirzaf)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-25 23:15:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35298/files) (1):**
  - `vllm/platforms/xpu.py`

### Summary

**What changed and why**  
Removed the environment variable `IGC_ForceOCLSIMDWidth=16` that was previously set when speculative decoding was enabled on Intel XPU. This variable was originally introduced to reduce memory usage during kernel compilation but was found to trigger a bug in the IGC compiler that broke speculative decoding.

**Technical impact**  
This change resolves multiple speculative decoding failures on XPU, including issues with draft models, sampling (temperature > 0), and Qwen3 models. The underlying OOM issue that the environment variable aimed to mitigate has been solved independently by implementing tiled computation in the `sample_recovered_tokens_kernel` (PR #34974).

**Potential risks**  
Removing this workaround could theoretically reintroduce high memory usage during kernel compilation if the tiled implementation has unhandled edge cases. However, the fix in #34974 appears robust. There is also a risk if the IGC compiler bug is later fixed, making the `IGC_ForceOCLSIMDWidth=16` setting potentially beneficial again, but this is unlikely and would require re-evaluation.

**Key insights**  
The root cause was a compiler bug, not a logic error in the code. This highlights the importance of validating compiler-level workarounds across all affected kernels. Developers should consider adding a comment in the code referencing the related PRs (#30538, #32887, #34974) to document this issue and its resolution for future maintenance.

---

## 3. [[Benchmark] Simplify SLA scan](https://github.com/vllm-project/vllm/pull/35306)


### Base Information

- **PR Number:** #35306
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 22:35:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35306/files) (8):**
  - `docs/benchmarking/cli.md`
  - `docs/benchmarking/sweeps.md`
  - `tests/benchmarks/sweep/test_serve_sla.py`
  - `vllm/benchmarks/sweep/plot.py`
  - `vllm/benchmarks/sweep/serve.py`
  - `vllm/benchmarks/sweep/serve_sla.py`
  - `vllm/benchmarks/sweep/sla_sweep.py`
  - `vllm/benchmarks/sweep/startup.py`

### Summary

**What changed and why**  
The PR simplifies the SLA scan functionality by replacing the complex SLA constraint matching system with a uniform sweep through latency vs. throughput space. It removes `--sla-params` and introduces `--sla-iters` to control the number of sampling points, aligning with GuideLLM's approach. Documentation is updated to promote GuideLLM for production benchmarking and clarify the new SLA scanning behavior.

**Technical impact**  
The SLA scanning logic shifts from an iterative root-finding algorithm (using spline interpolation) to a straightforward uniform sampling across the SLA variable range. This reduces code complexity by removing the `sla_sweep.py` module and associated tests, while maintaining the ability to explore latency-throughput tradeoffs. The `serve.py` module is refactored to better handle server context and parameter linking.

**Potential risks**  
Removing the SLA constraint solver may reduce precision in finding exact SLA boundaries, as the new approach relies on uniform sampling and post-hoc analysis. The deletion of comprehensive unit tests (`test_serve_sla.py`) could impact regression safety. Users accustomed to the old `--sla-params` syntax will need to adapt to the new workflow, which may cause confusion.

**Key insights**  
This change prioritizes simplicity and alignment with GuideLLM over precise SLA boundary detection. Developers should note that SLA evaluation is now decoupled from benchmarking—users must analyze results visually or programmatically to determine feasible SLAs. Ensure thorough testing of the new uniform sampling logic, especially edge cases like low `sla_iters` values. The documentation updates effectively guide users toward GuideLLM for production scenarios.

---

## 4. [[Misc][Harmony] Move Responses API only harmony utils to responses/harmony.py](https://github.com/vllm-project/vllm/pull/35339)


### Base Information

- **PR Number:** #35339
- **Author:** [sfeng33](https://github.com/sfeng33)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-25 22:35:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35339/files) (6):**
  - `tests/entrypoints/openai/parser/test_harmony_utils.py`
  - `tests/entrypoints/openai/responses/test_harmony_utils.py`
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`
  - `vllm/entrypoints/openai/responses/harmony.py`
  - `vllm/entrypoints/openai/responses/serving.py`

### Summary

**What changed and why**  
This PR extracts Responses API-specific Harmony conversion logic from `parser/harmony_utils.py` into a new dedicated module `responses/harmony.py`. The change addresses architectural concerns by separating shared Chat Completion parsing logic from Responses API-specific code, eliminating improper dependencies on Responses API types in the parser module. Functions are renamed to clarify conversion direction (e.g., `parse_response_input` → `response_input_to_harmony`).

**Technical impact**  
The refactoring improves modularity and separation of concerns. The parser module now focuses on generic Harmony conversions for Chat Completion, while the new module handles bidirectional conversion between Harmony and Responses API formats. This reduces coupling and makes the codebase easier to maintain and test independently. Import paths in dependent files (like `serving.py`) are updated accordingly.

**Potential risks**  
While the PR claims no behavioral changes, there is a risk of subtle regressions due to the large-scale code movement and renaming. The test suite has been restructured, but thorough integration testing is essential to ensure all edge cases (e.g., tool call parsing, MCP tool handling) remain functional. Additionally, the rename of `_BUILTIN_TOOL_TO_MCP_SERVER_LABEL` to `BUILTIN_TOOL_TO_MCP_SERVER_LABEL` (making it public) could affect encapsulation.

**Key insights**  
This is a well-executed architectural cleanup that enhances code organization. Developers should verify that all import updates are consistent across the codebase and run the full test suite to confirm no regressions. The new module structure provides a clear pattern for future API-specific conversions, making the system more extensible.

---

## 5. [[BugFix] anthropic/serving_messages: fix tool call arguments streaming](https://github.com/vllm-project/vllm/pull/34887)


### Base Information

- **PR Number:** #34887
- **Author:** [dtrifiro](https://github.com/dtrifiro)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-25 21:39:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34887/files) (1):**
  - `vllm/entrypoints/anthropic/serving.py`

### Summary

**What changed and why**  
The fix addresses a bug where the `/v1/messages` endpoint returns invalid JSON for tool call arguments when streaming with Mistral models. The issue occurs because the first delta from `create_chat_completion` includes both the tool call ID and initial JSON arguments, but the converter incorrectly assumed the first delta only contained metadata and discarded the arguments.

**Technical impact**  
This change modifies the `message_stream_converter` to immediately emit a `content_block_delta` event after the `content_block_start` event when tool call arguments are present in the initial delta. This ensures that partial JSON arguments are streamed correctly, maintaining compatibility with the Anthropic streaming format and preventing JSON parsing failures in downstream clients.

**Potential risks**  
If the initial arguments are empty or malformed, the converter may still emit unnecessary delta events. Additionally, the fix assumes that the first delta always contains valid partial JSON; edge cases where the arguments field is present but non‑string could cause serialization errors. The change is specific to Mistral models, but other models with similar delta structures might also be affected.

**Key insights**  
The fix is minimal and targeted, but developers should verify that the `tool_call.function.arguments` field is always a string before streaming it. Consider adding a guard clause or validation to prevent unexpected data types. Also, ensure that the streaming logic remains consistent across all model providers to avoid similar issues in the future.

---

## 6. [[torch.compile] Sequence Parallelism threshold compile ranges](https://github.com/vllm-project/vllm/pull/28672)


### Base Information

- **PR Number:** #28672
- **Author:** [jasonlizhengjian](https://github.com/jasonlizhengjian)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-25 21:00:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28672/files) (8):**
  - `tests/compile/conftest.py`
  - `tests/compile/fusions_e2e/conftest.py`
  - `tests/compile/fusions_e2e/test_tp2_async_tp.py`
  - `tests/compile/test_config.py`
  - `tests/compile/test_sequence_parallelism_threshold.py`
  - `vllm/compilation/passes/fusion/sequence_parallelism.py`
  - `vllm/config/compilation.py`
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
This PR introduces dynamic threshold-based sequence parallelism (SP) for torch.compile. It calculates a minimum token threshold based on model size (hidden_size, TP size, element size) and device capability (currently H100). SP is only applied when batch sizes exceed this threshold, preventing overhead on small batches. Default thresholds are derived from performance data in issue #27700.

**Technical impact**  
The changes modify compilation behavior by splitting compile ranges: small token counts use standard compilation, while larger batches activate SP optimizations. This improves performance for large models (hidden_size ≥ 8192 on H100) as shown by the ~2.7x latency reduction in benchmarks. The system automatically disables SP for small models or unsupported hardware.

**Potential risks**  
- Threshold calculations depend on device capability detection; unsupported or misdetected GPUs may incorrectly enable/disable SP.  
- Edge cases with non-concrete tensor shapes in piecewise compilation could cause incorrect splitting.  
- Manual overrides of `sp_min_token_num` might bypass safety checks, potentially degrading performance on small batches.

**Key insights**  
- SP now activates only when beneficial, avoiding overhead for small batches. Developers should verify hidden_size and TP configuration match SP requirements.  
- Test coverage is robust but focuses on H100; validate behavior on other GPU architectures.  
- Performance gains are significant (50.3s → 18.9s latency), but ensure model dimensions meet the hidden_size ≥ 8192 threshold for H100.

---

## 7. [[CPU][Feat]  Enable KleidiAI INT8_W4A8 for all input dtypes](https://github.com/vllm-project/vllm/pull/34890)


### Base Information

- **PR Number:** #34890
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-25 21:00:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34890/files) (1):**
  - `vllm/model_executor/kernels/linear/mixed_precision/dynamic_4bit.py`

### Summary

**What changed and why**  
This PR extends KleidiAI INT8_W4A8 kernel support to handle float16 activations and groupwise quantization with bfloat16/float16 activations. Previously, the kernel only natively supported float32/bfloat16 for channelwise and float32 for groupwise configurations. The changes enable upcasting to float32 before matrix multiplication and downcasting back to the original dtype when necessary.

**Technical impact**  
The kernel now supports all common activation dtypes (float32, bfloat16, float16) for both channelwise and groupwise quantization schemes. This is achieved by conditionally upcasting activations to float32 before the quantized matmul operation and downcasting the result, ensuring compatibility with PyTorch/KleidiAI's underlying kernel requirements while maintaining the original activation precision.

**Potential risks**  
The added dtype conversions introduce extra memory movement and potential precision loss during upcast/downcast cycles, which could impact performance. There's also a risk of incorrect dtype handling if the conditional logic doesn't cover all edge cases, particularly with mixed quantization configurations or unusual group sizes.

**Key insights**  
Developers should verify that performance benchmarks account for the overhead of dtype conversions, especially for float16 workloads. The logic correctly distinguishes between channelwise and groupwise quantization, but testing should ensure all supported dtype combinations produce numerically stable results. This change significantly broadens the kernel's applicability to more models and hardware configurations.

---

## 8. [[Model Runner V2] Add coding style guide](https://github.com/vllm-project/vllm/pull/35325)


### Base Information

- **PR Number:** #35325
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-25 20:42:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35325/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
A coding style guide docstring was added to the model runner file to establish clear architectural principles. The guide emphasizes that this file must only contain code common to all models (text, multimodal, generative, embedding) and warns against adding model-specific logic.

**Technical impact**  
This change doesn't affect runtime behavior but sets important architectural guardrails. It formally defines this file as a stable, minimal core component and encourages developers to keep specialized logic in model-specific modules rather than expanding this shared file.

**Potential risks**  
The guidelines could be ignored if not enforced during code reviews. There's also a risk of over-abstraction if developers misinterpret "keeping complexity out of this path" and create unnecessary utility functions instead of appropriate model-specific implementations.

**Key insights**  
This is a proactive architectural documentation change that helps maintain codebase scalability. Reviewers should strictly enforce these principles during PR reviews. Consider extending similar documentation to other critical shared components to ensure consistent architectural understanding across the team.

---

## 9. [[Kernel] Refactor FlashInfer allreduce for mnnvl backend](https://github.com/vllm-project/vllm/pull/34109)


### Base Information

- **PR Number:** #34109
- **Author:** [hjjq](https://github.com/hjjq)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-25 19:17:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34109/files) (7):**
  - `benchmarks/kernels/benchmark_device_communicators.py`
  - `benchmarks/kernels/benchmark_fused_collective.py`
  - `tests/compile/passes/distributed/test_fusion_all_reduce.py`
  - `vllm/compilation/passes/fusion/allreduce_rms_fusion.py`
  - `vllm/distributed/device_communicators/cuda_communicator.py`
  - `vllm/distributed/device_communicators/flashinfer_all_reduce.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
This PR introduces support for FlashInfer's new `mnnvl` allreduce backend, which is optimized for multi-node NVLink topologies. It adds environment variables to select between `mnnvl` and `trtllm` backends (`VLLM_FLASHINFER_ALLREDUCE_BACKEND`) and enables standalone FlashInfer allreduce usage (`VLLM_ALLREDUCE_USE_FLASHINFER`) for scenarios like MoE+TP where fused RMSNorm may not apply. The changes update fusion passes, benchmarks, and device communicators to support both backends.

**Technical impact**  
The codebase now supports two FlashInfer allreduce backends with distinct capabilities: `mnnvl` excels in multi-node NVLink environments but lacks quantization support, while `trtllm` retains quantization (FP8/FP4) fusion. The architecture separates workspaces for quant and non-quant patterns, ensuring compatibility. Standalone allreduce integration allows broader use beyond fused RMSNorm patterns, improving performance in distributed setups like MoE.

**Potential risks**  
- The `mnnvl` backend does not support quantization fusion; enabling it may silently disable FP8/FP4 optimizations if not properly guarded.  
- Workspace initialization failures in non-NVSwitch topologies could disable fusion entirely, though fallbacks exist.  
- Multi-node `trtllm` backend errors noted in benchmarks may indicate unresolved compatibility issues.  
- Environment variable management in benchmarks could lead to cross-test contamination if not carefully restored.

**Key insights**  
- Developers should choose `mnnvl` for multi-node NVLink performance but avoid it if quantization fusion is required.  
- The standalone allreduce option (`VLLM_ALLREDUCE_USE_FLASHINFER=1`) is valuable for MoE and other non-fusion cases.  
- Workspace lifecycle is now centralized via `destroy_fi_ar_workspace()`, improving resource management.  
- Benchmark updates provide comprehensive performance comparisons, but ensure environment variables are reset after each test to avoid side effects.

---

## 10. [openpangu-vl support video input](https://github.com/vllm-project/vllm/pull/34134)


### Base Information

- **PR Number:** #34134
- **Author:** [hujiaxin0](https://github.com/hujiaxin0)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-25 19:08:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34134/files) (1):**
  - `vllm/multimodal/video.py`

### Summary

**What changed and why**  
A new video loader backend `OpenCVDynamicOpenPanguVideoBackend` was added to support video input for the OpenPangu model. The implementation introduces a dynamic frame‑sampling method that corrects timestamp calculations by aligning frame 0 with time 0.0 seconds, ensuring accurate frame‑to‑time mapping.

**Technical impact**  
This change extends the video‑loading infrastructure by registering a new backend under the `"openpangu"` key. It modifies the sampling logic to compute timestamps based on `(total_frames_num – 1) / original_fps` for the last frame, which affects how frames are selected and may influence downstream processing that relies on frame timing metadata.

**Potential risks**  
The logic assumes `fps > 0` or `fps == -1`; invalid `fps` values could raise exceptions. Edge cases like very short videos (e.g., `total_frames_num < 1` or `original_fps <= 0`) may lead to zero duration and empty sampling. The `round()` operation in frame‑index calculation could cause off‑by‑one errors near video boundaries.

**Key insights**  
Developers should verify that the timestamp‑correction approach aligns with OpenPangu’s expectations. Consider adding validation for `total_frames_num` and `original_fps` early to handle degenerate videos gracefully. Ensure the `fps` parameter is clearly documented, especially the special `-1` case, to prevent misuse.

---

## 11. [[BugFix] Fix fp4 quant kernel on CUDA 12.8](https://github.com/vllm-project/vllm/pull/35210)


### Base Information

- **PR Number:** #35210
- **Author:** [LopezCastroRoberto](https://github.com/LopezCastroRoberto)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 18:32:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35210/files) (2):**
  - `csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu`
  - `csrc/quantization/fp4/nvfp4_quant_kernels.cu`

### Summary

**What changed and why**  
The fix addresses a boundary calculation bug in FP4 quantization kernels for CUDA 12.8 where `CVT_FP4_ELTS_PER_THREAD == 8`. Previously, the kernel used `sf_n_unpadded` (scale-factor column count) to bound thread-column indices, but with 8 elements per thread, the correct bound is `num_packed_cols` (number of packed columns). This mismatch caused test failures in non-swizzled quantization paths.

**Technical impact**  
These changes ensure correct thread indexing and grid dimension calculations when `CVT_FP4_ELTS_PER_THREAD == 8`, aligning kernel behavior with the actual data layout. The CUDA 12.9+ path remains unchanged, maintaining performance for both code paths while fixing the specific CUDA 12.8 issue.

**Potential risks**  
The fix assumes `CVT_FP4_ELTS_PER_THREAD` is consistently defined across compilation units and correctly reflects the CUDA version. Any future changes to this macro or thread-element mapping could reintroduce similar bugs. Additionally, the manual testing on SM120/CUDA 12.8 isn't covered by CI, leaving a gap in automated regression detection.

**Key insights**  
Always validate kernel launch parameters against the actual data partitioning scheme, especially when constants like `ELTS_PER_THREAD` vary by environment. Consider adding CI coverage for CUDA 12.8 configurations to catch such issues earlier. The fix is minimal and focused, avoiding unnecessary changes to the working CUDA 12.9+ path.

---

## 12. [[Bugfix] Fix CUDA compatibility path setting for both datacenter and consumer NVIDIA GPUs](https://github.com/vllm-project/vllm/pull/33992)


### Base Information

- **PR Number:** #33992
- **Author:** [ehfd](https://github.com/ehfd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 18:15:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33992/files) (6):**
  - `docker/Dockerfile`
  - `docs/getting_started/installation/gpu.cuda.inc.md`
  - `docs/usage/troubleshooting.md`
  - `tests/cuda/test_cuda_compatibility_path.py`
  - `vllm/env_override.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
This PR fixes CUDA compatibility library handling by making it opt-in via environment variables instead of automatically enabled in Docker images. It addresses issues where consumer GPUs (GeForce, RTX) fail with CUDA compatibility libraries, while allowing datacenter/professional GPUs to use them when needed. The changes introduce `VLLM_ENABLE_CUDA_COMPATIBILITY` and `VLLM_CUDA_COMPATIBILITY_PATH` environment variables for controlled activation.

**Technical impact**  
The Docker images no longer pre-configure CUDA compatibility libraries via `ldconfig`, shifting responsibility to runtime environment variables. A new function `_maybe_set_cuda_compatibility_path()` in `env_override.py` dynamically adjusts `LD_LIBRARY_PATH` before PyTorch imports, ensuring compatibility libraries are loaded only when explicitly enabled. This prevents CUDA initialization errors on unsupported GPU types.

**Potential risks**  
Users with datacenter GPUs and older drivers may now experience failures if they forget to set `VLLM_ENABLE_CUDA_COMPATIBILITY=1`. The path detection logic relies on multiple fallbacks (custom path, Conda, default CUDA path), which could fail silently if none exist. There is also a risk of environment variable parsing inconsistencies across different shells or container orchestration tools.

**Key insights**  
Always set `VLLM_ENABLE_CUDA_COMPATIBILITY=0` for consumer GPUs to avoid CUDA Error 803. For datacenter/professional GPUs with driver version mismatches, enable it and optionally specify the compatibility library path. The comprehensive test suite ensures robust parsing and path handling, but developers should verify GPU compatibility before deployment.

---

## 13. [[Bugfix] Fix AttributeError in SMControlContextManager](https://github.com/vllm-project/vllm/pull/35338)


### Base Information

- **PR Number:** #35338
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 18:01:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35338/files) (1):**
  - `vllm/v1/worker/gpu_ubatch_wrapper.py`

### Summary

**What changed and why**  
The fix removes an erroneous `.index` attribute access on `torch.cuda.current_device()` in `SMControlContextManager.__init__()`. This resolves an `AttributeError` because `torch.cuda.current_device()` returns an integer device index directly, not a device object with an `.index` property.

**Technical impact**  
This change ensures compatibility with PyTorch's CUDA device API and corrects a regression introduced in PR #35042. The fix restores proper functionality when using data parallel and expert parallel configurations, allowing `num_compute_units()` to receive the correct integer device index.

**Potential risks**  
The risk is minimal as the change aligns with PyTorch's documented behavior. However, if `num_compute_units()` expects a different input type in the future, this might need revisiting. Additionally, the fix assumes `torch.cuda.current_device()` always returns an integer, which is standard but could vary in edge cases like non-CUDA environments.

**Key insights**  
Always verify the return type of PyTorch CUDA functions, as some return primitives rather than objects. This bug highlights the importance of testing distributed configurations (like data parallel + expert parallel) to catch regressions early. Ensure the test plan includes the specific failing CI tests to validate the fix.

---

## 14. [[UX] Add `--moe-backend` arg for explicit kernel selection](https://github.com/vllm-project/vllm/pull/33807)


### Base Information

- **PR Number:** #33807
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 17:44:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33807/files) (37):**
  - `tests/evals/gsm8k/configs/Qwen3-Next-80B-A3B-NVFP4-EP2.yaml`
  - `tests/evals/gsm8k/configs/Qwen3-Next-FP8-EP2.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Llama-4-Scout-Fp8-ModelOpt-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutedsl-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutedsl-deepep-ll.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-Fp8-AutoFp8-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Nemotron-Nano-30B-Fp8-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Nemotron-Nano-30B-NvFp4-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-BF16-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-vllm-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-vllm-cutlass.yaml`
  - `tests/quantization/test_blackwell_moe.py`
  - `vllm/config/kernel.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`

### Summary

**What changed and why**  
This PR adds a new `--moe-backend` command-line argument to allow explicit selection of MoE (Mixture of Experts) kernel backends, replacing the previous environment variable-based approach. The change supports all three quantization formats (unquantized, FP8, and NVFP4) and provides validation when users specify incompatible backends for their configuration.

**Technical impact**  
The implementation centralizes MoE backend selection through a unified configuration system, replacing scattered environment variable checks with a structured approach. It introduces a `MoEBackend` enum in the kernel configuration, updates the argument parsing system, and modifies backend selection logic across all three quantization paths to respect the user's explicit choice when provided.

**Potential risks**  
There's a risk of breaking existing workflows that rely on environment variables, though the PR maintains backward compatibility by keeping environment variable support as a fallback. The validation logic could potentially reject valid configurations if the mapping between user-facing backend names and internal backend enums is incomplete or incorrect. The changes touch multiple critical paths in the MoE execution pipeline.

**Key insights**  
This change significantly improves user experience by providing a clean, documented interface for MoE backend selection. Developers should update their test configurations and automation scripts to use the new `--moe-backend` argument instead of environment variables. The PR demonstrates good architectural practice by centralizing configuration logic while maintaining backward compatibility during the transition period.

---

## 15. [[MoE Refactor] MXFP4 Cutlass Experts to MK](https://github.com/vllm-project/vllm/pull/34542)


### Base Information

- **PR Number:** #34542
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 17:32:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34542/files) (19):**
  - `.buildkite/test_areas/lm_eval.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `tests/evals/gpt_oss/README.md`
  - `tests/evals/gpt_oss/configs/gpt-oss-20b-baseline.yaml`
  - `tests/evals/gpt_oss/configs/gpt-oss-20b-flashinfer-mxfp4-bf16.yaml`
  - `tests/evals/gpt_oss/configs/gpt-oss-20b-flashinfer-mxfp4-mxfp8-cutlass.yaml`
  - `tests/evals/gpt_oss/configs/gpt-oss-20b-marlin.yaml`
  - `tests/evals/gpt_oss/configs/gpt-oss-20b-sm100-fi-mxfp4-mxfp8-trtllm.yaml`
  - `tests/evals/gpt_oss/configs/models-b200.txt`
  - `tests/evals/gpt_oss/configs/models-h100.txt`
  - `tests/evals/gpt_oss/conftest.py`
  - `tests/evals/gpt_oss/test_gpqa_correctness.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/utils.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`

### Summary

**What changed and why**  
This PR refactors the MXFP4 Cutlass backend for the ongoing MoE (Mixture of Experts) refactor, moving from direct FlashInfer kernel calls to a modular kernel (MK) interface. It also introduces comprehensive testing infrastructure for GPQA benchmarks across different hardware (H100, B200) and quantization backends (MXFP4-BF16, MXFP4-MXFP8-CUTLASS, Marlin, etc.).

**Technical impact**  
The changes centralize MoE kernel selection and execution through the modular kernel abstraction, improving code maintainability and enabling consistent handling of quantization schemes (MXFP4, NVFP4, FP8). The testing framework now uses YAML-based configuration files to define model variants and environment variables, making it easier to add new test cases and ensure correctness across different hardware and quantization backends.

**Potential risks**  
- The refactor introduces new dependencies on the modular kernel interface, which may have subtle behavioral differences from the previous direct FlashInfer calls.  
- Hardcoded constants (e.g., `gemm1_alpha=1.702`) are specific to GPT-OSS and may not generalize to other models.  
- The removal of direct kernel calls in `mxfp4.py` could affect performance or correctness if the modular kernel implementation has undiscovered edge cases.

**Key insights**  
- The modular kernel approach standardizes MoE execution and simplifies future backend additions.  
- Developers should verify that all supported quantization schemes (MXFP4, NVFP4, FP8) are correctly routed through the new interface.  
- The testing infrastructure is now more scalable; when adding new models, ensure configuration files follow the YAML schema and are referenced in the appropriate `models-*.txt` list.

---

## 16. [[UX] Add `--performance-mode {balanced,interactivity,throughput}`](https://github.com/vllm-project/vllm/pull/34936)


### Base Information

- **PR Number:** #34936
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 17:28:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34936/files) (2):**
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
Added a new `--performance-mode` flag with options `balanced`, `interactivity`, and `throughput` to allow users to specify workload intent. The default `balanced` preserves existing behavior, while `interactivity` enables fine-grained CUDA graphs for small batch sizes (up to 32) and `throughput` doubles default batch size limits. This provides a foundation for future runtime optimizations tailored to latency or throughput goals.

**Technical impact**  
The changes introduce a new configuration parameter that influences CUDA graph capture strategies and scheduler limits. In `interactivity` mode, CUDA graphs are captured for batch sizes 1–32 to reduce padding overhead, while `throughput` mode automatically doubles `max_num_batched_tokens` and `max_num_seqs` when not explicitly set. This allows the system to adapt batching and kernel selection based on user intent.

**Potential risks**  
Automatically doubling batch limits in `throughput` mode could increase memory usage or cause OOM errors if not carefully monitored. The fine-grained CUDA graphs in `interactivity` mode may increase graph compilation overhead for small batch sizes. Future expansions of this feature must ensure backward compatibility and avoid unintended side effects when combined with other optimization flags.

**Key insights**  
This PR establishes a flexible framework for performance tuning, but developers should validate memory impacts when using `throughput` mode in production. Consider adding validation to prevent excessive memory allocation and documenting the trade-offs between modes. The feature is well-scoped for initial rollout, with clear extension points for future enhancements like scheduler adjustments and kernel selection.

---

## 17. [[offloader] v2: Hide weight onloading latency via prefetching](https://github.com/vllm-project/vllm/pull/29941)


### Base Information

- **PR Number:** #29941
- **Author:** [minosfuture](https://github.com/minosfuture)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 17:20:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29941/files) (20):**
  - `.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_prefetch_offload.sh`
  - `.buildkite/test_areas/e2e_integration.yaml`
  - `tests/basic_correctness/test_prefetch_offload.py`
  - `vllm/compilation/cuda_graph.py`
  - `vllm/config/__init__.py`
  - `vllm/config/cache.py`
  - `vllm/config/offload.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/model_executor/offloader/__init__.py`
  - `vllm/model_executor/offloader/base.py`
  - `vllm/model_executor/offloader/prefetch.py`
  - `vllm/model_executor/offloader/prefetch_ops.py`
  - `vllm/model_executor/offloader/uva.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle/cudagraph.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_ubatch_wrapper.py`

### Summary

**What changed and why**  
This PR introduces a prefetch-based CPU weight offloader that hides onloading latency by asynchronously prefetching weights ahead of time. It refactors the existing offloading infrastructure into a modular system with three backends: UVA (zero-copy), prefetch (async H2D with grouping), and no-op. The changes enable torch.compile and CUDA graph compatibility by using static buffers, custom ops (`wait_prefetch`/`start_prefetch`), and stream synchronization.

**Technical impact**  
The architecture shifts from ad-hoc CPU offloading in `utils.py` to a pluggable offloader system. Prefetch offloading groups layers (e.g., offload last N layers per group) and uses a dedicated copy stream for overlapping transfers. Custom ops with `mutates_args` create dependencies that preserve ordering under torch.compile. CUDA graph integration requires explicit stream synchronization before capture/replay to avoid race conditions with prefetches.

**Potential risks**  
- Incorrect stream synchronization could lead to data races or deadlocks, especially when switching between eager and graph modes.  
- The static buffer pool assumes uniform parameter shapes across layers; mismatched strides or names may cause allocation errors.  
- The prefetch step must be carefully tuned—too high increases GPU memory, too low exposes latency.  
- Deprecation of `CacheConfig` CPU offload fields may break existing configurations if not migrated.

**Key insights**  
- The prefetch offloader is designed for models with regular layer patterns (e.g., MoE experts) and benefits from fast CPU-GPU interconnects.  
- Developers must ensure `sync_prev_onload()` and `join_after_forward()` are called at appropriate points in CUDA graph capture/replay.  
- Selective offloading via `offload_params` allows targeting specific weight types (e.g., MoE expert weights) to balance memory and performance.  
- The new configuration hierarchy (`OffloadConfig` > `PrefetchOffloadConfig`/`UVAOffloadConfig`) provides clearer separation of concerns than the previous flat structure.

---

## 18. [[ROCm][CI] Amending deletion of AMD mirror](https://github.com/vllm-project/vllm/pull/35322)


### Base Information

- **PR Number:** #35322
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-25 14:17:56
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35322/files) (1):**
  - `.buildkite/test_areas/entrypoints.yaml`

### Summary

**What changed and why**  
This change adds a new `mirror` configuration block for AMD devices to the Buildkite CI pipeline. The addition restores a mirror configuration that was accidentally deleted in a previous PR (#34923), ensuring AMD-specific tests can properly execute with the required `mi325_1` device and its dependency on the `image-build-amd` step.

**Technical impact**  
The change reintroduces a necessary CI configuration for AMD (ROCm) hardware testing. It ensures the `entrypoints` test job has a properly defined mirror agent targeting AMD hardware (`device: mi325_1`) and correctly depends on the AMD-specific Docker image build step, restoring the intended pipeline structure for ROCm platform validation.

**Potential risks**  
The main risk is that the restored configuration might be outdated if the `image-build-amd` step or the `mi325_1` device specification has changed since the original deletion. There is also a minor risk of indentation or YAML syntax errors if the restoration wasn't copied precisely from the original state.

**Key insights**  
This is a straightforward revert of an accidental deletion. Developers should verify that the `image-build-amd` step still exists and functions correctly in the current pipeline. When modifying CI configuration files, extra caution is needed as deletions can break platform-specific testing silently.

---

## 19. [[ROCm][CI] Extending attention backend coverage for Eagle spec decode tests](https://github.com/vllm-project/vllm/pull/35265)


### Base Information

- **PR Number:** #35265
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-25 14:16:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35265/files) (4):**
  - `.buildkite/test_areas/engine.yaml`
  - `tests/utils.py`
  - `tests/v1/e2e/test_async_scheduling.py`
  - `tests/v1/e2e/test_spec_decode.py`

### Summary

**What changed and why**  
This PR extends ROCm CI coverage for Eagle speculative decode tests by replacing unconditional skips with functional fallback configurations. Specifically, it modifies attention backend handling for Llama-4-Scout (falling back to `FLEX_ATTENTION` when `FLASH_ATTN` is unsupported) and DeepSeek models (enabling `ROCM_AITER_FA` with `TRITON_MLA` backend). Additionally, new GPU-tier decorators (`single_gpu_only`, `multi_gpu_only`) are introduced to restrict tests to appropriate hardware scales.

**Technical impact**  
The changes improve test coverage on ROCm platforms by enabling previously skipped configurations. The refactoring splits the monolithic `test_eagle_correctness` into lighter and heavier variants (`test_eagle_correctness_light` and parameterized heavy tests) for better test granularity. The GPU-tier decorators provide a reusable mechanism for hardware-dependent test filtering.

**Potential risks**  
Fallback logic may introduce platform-specific behavior that could diverge from CUDA implementations. The `TRITON_MLA` backend for DeepSeek on ROCm requires validation of performance and correctness. Splitting tests increases maintenance overhead but improves selectivity.

**Key insights**  
Developers should ensure fallback backends are functionally equivalent to their intended counterparts. The new GPU-tier decorators offer a clean pattern for hardware-aware test filtering—consider adopting them elsewhere. Monitor ROCm-specific attention backend behavior for regressions in speculative decoding accuracy.

---

## 20. [fix(mxfp4): Disable monolithic path for TRITON backend with EP](https://github.com/vllm-project/vllm/pull/34270)


### Base Information

- **PR Number:** #34270
- **Author:** [elizabetht](https://github.com/elizabetht)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 13:33:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34270/files) (2):**
  - `tests/kernels/quantization/test_mxfp4_triton_ep.py`
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`

### Summary

**What changed and why**  
The fix addresses a crash when running MXFP4 models with TRITON backend and expert parallelism (EP). Previously, `triton_kernel_moe_forward` passed `expert_map` to `triton_kernel_fused_experts` without remapping global expert IDs to local indices, causing illegal memory access. The solution splits routing: when `expert_map` is provided, it performs top‑k selection, remaps IDs via `expert_map`, and builds routing data with `make_routing_data`; otherwise, it falls back to `legacy_routing`.

**Technical impact**  
This change ensures correct routing for EP by aligning global expert IDs with local weight indices, maintaining monolithic kernel execution for TRITON backend. The non‑EP path remains unchanged, preserving backward compatibility. The fix avoids the need to disable monolithic kernels, which would have broken MXFP4 due to missing modular kernel support.

**Potential risks**  
If `expert_map` is incorrectly constructed (e.g., containing invalid indices), it could lead to out‑of‑bounds accesses. The remapping logic assumes `expert_map` maps global to local IDs consistently across ranks; any mismatch may cause silent errors. Additionally, the change introduces a dependency on `triton_kernels.topk`, which must be available at runtime.

**Key insights**  
The fix elegantly separates EP and non‑EP routing while keeping the monolithic kernel path active—critical for MXFP4 compatibility. Developers should verify that `expert_map` is correctly populated and that all ranks use consistent mapping. Future work should consider unifying routing approaches across backends to reduce complexity.

---

## 21. [[CI][AMD][BugFix] Add  torch.cuda.set_device to test_punica_ops so punica kernels execute on same device as tensor](https://github.com/vllm-project/vllm/pull/34985)


### Base Information

- **PR Number:** #34985
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-25 11:18:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34985/files) (1):**
  - `tests/lora/test_punica_ops.py`

### Summary

**What changed and why**  
Added `torch.cuda.set_device(device)` calls in two test functions (`test_kernels` and `test_kernels_hidden_size`) to ensure that CUDA operations and Triton kernels execute on the same device as the tensors. This prevents test failures caused by previous tests that may have changed the active CUDA device.

**Technical impact**  
This change ensures device consistency between PyTorch tensor operations and Triton kernel execution. The `torch.set_default_device(device)` call sets the default device for tensor creation, while `torch.cuda.set_device(device)` explicitly sets the current CUDA device for kernel launches, preventing device mismatches that could lead to incorrect kernel execution or system instability.

**Potential risks**  
The fix assumes the device parameter is a CUDA device (e.g., "cuda:0"). If tests are run on non-CUDA devices, this could cause errors. Additionally, while this resolves the immediate issue, there may be other tests in the codebase with similar device synchronization problems that could still cause intermittent failures.

**Key insights**  
Always ensure device synchronization between PyTorch tensor operations and kernel launches when using multiple GPUs or device-changing operations. Consider adding a broader review of test files that use `torch.cuda.set_device` to identify similar patterns. The fix is minimal and targeted, but developers should be aware that device management is critical when mixing high-level PyTorch operations with low-level kernel execution.

---

## 22. [Revert "[Misc] Enable weights loading tracking for quantized models"](https://github.com/vllm-project/vllm/pull/35309)


### Base Information

- **PR Number:** #35309
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 09:20:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35309/files) (1):**
  - `vllm/model_executor/model_loader/default_loader.py`

### Summary

**What changed and why**  
This PR reverts commit #35074, which introduced weight loading tracking for quantized models. The reversion was necessary because the original change broke weight loading for many quantized models in nightly builds, as evidenced by CI failures. The fix restores the previous behavior where strict weight loading checks are only applied to non-quantized models.

**Technical impact**  
The revert removes the `track_weights_loading` method and associated logic that tracked loaded weights for quantized layers, particularly KV cache scaling parameters. This simplifies the weight loading process for quantized models by disabling strict checks, ensuring compatibility with existing quantized model checkpoints that may not include all expected parameters.

**Potential risks**  
Disabling strict weight loading checks for quantized models could allow silent failures if future quantized models have missing weights that should be validated. Additionally, the revert may reintroduce inconsistencies in weight loading behavior between quantized and non-quantized models, potentially hiding configuration or checkpoint issues.

**Key insights**  
The issue highlights the sensitivity of quantized model weight loading to strict validation logic. Before re-landing this feature, ensure comprehensive testing across all supported quantized model types using `ready-run-all-tests`. Consider implementing a more robust tracking mechanism that accommodates optional parameters (like KV cache scales) without breaking existing models.

---

## 23. [[ROCm][CI] Disable skinny GEMMs in multimodal tests to fix non-deterministic results](https://github.com/vllm-project/vllm/pull/35049)


### Base Information

- **PR Number:** #35049
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-25 08:48:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35049/files) (1):**
  - `tests/models/multimodal/conftest.py`

### Summary

**What changed and why**  
A `pytest_configure` function was added to `tests/models/multimodal/conftest.py` to set the environment variable `VLLM_ROCM_USE_SKINNY_GEMM` to `"0"` specifically for ROCm platforms. This disables the "skinny GEMM" kernel during multimodal tests to eliminate non-deterministic results caused by floating-point non-associativity in atomic operations.

**Technical impact**  
The change ensures that the environment variable is set before any test modules are imported, guaranteeing that the skinny GEMM kernel is disabled for all ROCm multimodal tests. This resolves intermittent test failures in accuracy-sensitive tests by enforcing deterministic floating-point behavior across runs.

**Potential risks**  
If other test suites rely on the performance benefits of the skinny GEMM kernel, they may experience performance regressions unless similarly configured. Additionally, the warning issued may be overlooked in test logs, potentially masking the underlying issue in non-test environments where determinism is also required.

**Key insights**  
This fix is a targeted workaround for test stability; developers should consider whether the kernel's atomic reduction design needs a deterministic alternative for production use. Ensure that any future tests requiring skinny GEMM are explicitly configured to re-enable it, and monitor for similar non-determinism in other GPU kernels using atomic operations.

---

## 24. [[Bugfix] Fix Harmony preamble visibility in Responses API](https://github.com/vllm-project/vllm/pull/32114)


### Base Information

- **PR Number:** #32114
- **Author:** [thepushkarp](https://github.com/thepushkarp)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 08:08:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32114/files) (10):**
  - `tests/entrypoints/openai/parser/test_harmony_utils.py`
  - `tests/entrypoints/openai/responses/test_harmony.py`
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`
  - `tests/entrypoints/openai/test_serving_chat_stream_harmony.py`
  - `tests/entrypoints/openai/test_serving_responses.py`
  - `tests/entrypoints/test_context.py`
  - `vllm/entrypoints/openai/chat_completion/stream_harmony.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`
  - `vllm/entrypoints/openai/responses/context.py`
  - `vllm/entrypoints/openai/responses/streaming_events.py`

### Summary

**What changed and why**  
This PR fixes a bug where Harmony preambles (commentary-channel messages without a recipient) were incorrectly treated as hidden reasoning instead of user-visible output. The changes align vLLM with the Harmony spec by making preambles visible in the Responses API, streaming events, and token counts, and ensuring the commentary channel is always available.

**Technical impact**  
Preambles now appear as `ResponseOutputMessage` items instead of `ResponseReasoningItem`, emit proper streaming delta/done events, and are excluded from reasoning token counts. The parser logic distinguishes between preambles (visible) and tool-directed commentary (hidden), and the system message always includes the commentary channel to allow preamble generation.

**Potential risks**  
If the distinction between preambles and tool calls is misapplied (e.g., incorrectly identifying `recipient`), visible output could leak tool-call JSON or hide user-facing text. Changes to token counting could affect billing or logging metrics if reasoning tokens are under/over-counted. The updated channel filtering in `parse_chat_output` must correctly exclude tool-call payloads.

**Key insights**  
Developers should note that commentary-channel handling now depends on the presence of a `recipient`: `None` indicates a user-visible preamble, while a non-`None` value indicates a hidden tool interaction. All three Harmony channels are now always enabled in the system message, ensuring spec compliance. Test updates validate the new behavior across parsing, streaming, and token counting.

---

## 25. [[Bugfix] Gracefully disable AllReduceFusionPass on GPUs without multicast support](https://github.com/vllm-project/vllm/pull/35085)


### Base Information

- **PR Number:** #35085
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 07:31:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35085/files) (1):**
  - `vllm/compilation/passes/fusion/allreduce_rms_fusion.py`

### Summary

**What changed and why**  
The change wraps the `create_allreduce_fusion_workspace()` call in a try-except block to catch `RuntimeError` when the underlying hardware lacks multicast support (e.g., H200/H100 without NVSwitch). On such GPUs, the pass now logs a warning and returns early, leaving itself disabled, which allows the model to fall back to non-fused allreduce operations.

**Technical impact**  
This modification ensures the `AllReduceFusionPass` degrades gracefully on incompatible hardware, preventing a hard crash during initialization. The system behavior remains unchanged for supported GPUs but now allows unsupported configurations to proceed with standard allreduce operations, maintaining functional parity without the fusion optimization.

**Potential risks**  
The error handling only catches `RuntimeError` containing "multicast" (case-insensitive), which could miss other related errors or allow unrelated `RuntimeError` instances to be incorrectly suppressed. There is also a risk that the warning log may be overlooked in production environments, leading to unnoticed performance degradation.

**Key insights**  
This pattern mirrors existing graceful degradation in `symm_mem.py`, promoting consistency. Developers should ensure similar hardware capability checks are applied elsewhere in the codebase. Consider enhancing the error matching logic to be more robust, possibly by checking specific error codes or types from the underlying library.

---

## 26. [[XPU]Fix for Qwen-OMNI crash](https://github.com/vllm-project/vllm/pull/35249)


### Base Information

- **PR Number:** #35249
- **Author:** [xuechendi](https://github.com/xuechendi)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 07:31:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35249/files) (1):**
  - `vllm/_xpu_ops.py`

### Summary

**What changed and why**  
The PR fixes two XPU-related crashes in the vLLM-omni project. First, it hard-configures `max_cudagraph_capture_size=0` to avoid a comparison crash when XPU doesn't support CUDA graphs. Second, it ensures the `k` tensor is contiguous in the XPU flash attention kernel for encode attention, addressing a "k must be contiguous" runtime error.

**Technical impact**  
These changes enable XPU compatibility by preventing unsupported CUDA graph operations and ensuring tensor memory layout requirements are met for attention kernels. The modifications are minimal and targeted, avoiding broader architectural changes while resolving specific runtime failures.

**Potential risks**  
Hard-coding `max_cudagraph_capture_size=0` might affect performance if CUDA graph support is added later without revisiting this configuration. The additional `contiguous()` call could introduce minor overhead, though deemed insignificant. There's a risk of similar non-contiguous tensor issues arising in other XPU kernels.

**Key insights**  
Developers should verify that all XPU kernels handle tensor contiguity consistently, especially for encode attention paths. Consider adding a runtime check or abstraction for CUDA graph compatibility to avoid hard-coded values. Monitor performance impacts of the `contiguous()` call in production workloads.

---

## 27. [[Misc][LoRA] Increase max vocab size limit to 258048 in logits processor](https://github.com/vllm-project/vllm/pull/34773)


### Base Information

- **PR Number:** #34773
- **Author:** [bhoomit](https://github.com/bhoomit)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-25 07:30:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34773/files) (3):**
  - `tests/lora/conftest.py`
  - `tests/lora/test_layers.py`
  - `vllm/lora/layers/logits_processor.py`

### Summary

**What changed and why**  
The changes increase the LoRA logits processor's maximum vocabulary size limit from 257,024 to 258,048 to support models with larger vocabularies (e.g., 258,048 tokens). Test coverage is added to validate the new upper bound and ensure invalid sizes are properly rejected.

**Technical impact**  
This update allows the system to accommodate models with vocabularies up to 258,048 tokens, expanding compatibility with newer or larger language models. The validation logic is adjusted to enforce the new limit, and tests are updated to verify both valid and invalid vocabulary sizes.

**Potential risks**  
If the underlying tensor allocations or memory layouts assume the previous limit, increasing it could lead to out-of-bounds errors or memory issues. The condition change from `32000 < vocab_size > 257024` to `vocab_size <= 32000 or vocab_size > 258048` also tightens the lower bound check, which may inadvertently reject valid sizes exactly equal to 32,000.

**Key insights**  
Developers should verify that all dependent components (e.g., embedding layers, GPU memory buffers) can handle the increased vocabulary size. The updated error message clearly communicates the new bounds, but ensure the lower bound exclusion of exactly 32,000 is intentional for the use case.

---

## 28. [[Bugfix] Fix step3p5 reasoning with interleaved thinking](https://github.com/vllm-project/vllm/pull/34211)


### Base Information

- **PR Number:** #34211
- **Author:** [mariohong128](https://github.com/mariohong128)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-25 07:13:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34211/files) (2):**
  - `tests/reasoning/test_step3p5_reasoning_parser.py`
  - `vllm/reasoning/step3p5_reasoning_parser.py`

### Summary

**What changed and why**  
The fix addresses a bug in the step3p5 reasoning parser where multiple conversation rounds caused incorrect detection of reasoning boundaries. Previously, the parser used a simple offset counter (`end_offset`) that failed when prompts contained residual `</think>` tokens from prior turns. The new implementation tracks the last special token (`<think>` or `</think>`) and uses a pending state (`_end_token_pending`) to accurately determine when reasoning ends, even with interleaved thinking tags.

**Technical impact**  
The changes replace a naive offset-based approach with a stateful parser that scans input IDs to identify the most recent special token. This improves robustness in multi-turn scenarios and ensures correct reasoning extraction regardless of previous conversation context. The parser now properly handles edge cases like newlines after `</think>` and maintains compatibility with both streaming and non-streaming modes.

**Potential risks**  
The stateful nature of `_end_token_pending` could introduce subtle bugs if the parser is reused across unrelated sequences without proper reset. Additionally, the backward scanning logic assumes special tokens appear in expected patterns; malformed inputs (e.g., multiple unpaired `</think>` tokens) might lead to ambiguous behavior. The fix also modifies the internal API (`_is_reasoning_end_from_ids`), which could affect subclasses or integrations.

**Key insights**  
Developers should ensure the parser instance is reset or reinitialized for each new sequence to avoid state leakage. The comprehensive test suite added covers many edge cases, but further validation with real multi-turn dialogues is recommended. When extending this parser, maintain the invariant that `_end_token_pending` is cleared appropriately after reasoning ends.

---

## 29. [[Tests] Add GSM8k check to SpecDec E2E tests](https://github.com/vllm-project/vllm/pull/34772)


### Base Information

- **PR Number:** #34772
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-02-25 04:51:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34772/files) (2):**
  - `tests/evals/gsm8k/gsm8k_eval.py`
  - `tests/v1/e2e/test_spec_decode.py`

### Summary

**What changed and why**  
This PR adds GSM8k evaluation to speculative decoding E2E tests to replace flaky sentence-based prompts. It refactors the GSM8k evaluation module to support both HTTP and offline evaluation, and updates test cases to use GSM8k prompts with accuracy thresholds instead of unreliable token-matching checks.

**Technical impact**  
The changes shift speculative decoding validation from heuristic token matching to quantitative accuracy testing using a standardized benchmark (GSM8k). This improves test reliability and provides a more meaningful measure of model correctness. The refactored GSM8k module now supports both server-based and direct LLM object evaluation, increasing flexibility for different testing scenarios.

**Potential risks**  
The GSM8k test set may be too computationally expensive for regular CI runs if run fully. Accuracy thresholds are model-specific and may need adjustment as models evolve. The removal of reference model comparison in some tests could mask subtle correctness issues that GSM8k accuracy alone might not catch.

**Key insights**  
Developers should verify that GSM8k accuracy thresholds are appropriate for their target models and consider adding lightweight sanity checks alongside GSM8k. The refactored evaluation functions enable reuse across other test suites. Future work should unify speculative decoding test configurations as noted in the PR description.

---

## 30. [Doc link typo](https://github.com/vllm-project/vllm/pull/35281)


### Base Information

- **PR Number:** #35281
- **Author:** [gante](https://github.com/gante)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 03:00:32
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35281/files) (1):**
  - `docs/design/moe_kernel_features.md`

### Summary

**What changed and why**  
Fixed a broken documentation link for the `FusedMoE` class by adding a missing closing bracket. Also corrected swapped class names for DeepEP backends (`DeepEPHTPrepareAndFinalize` and `DeepEPLLPrepareAndFinalize`) to match their intended high-throughput and low-latency roles.

**Technical impact**  
These changes only affect documentation readability and accuracy. The corrections ensure that hyperlinks in the rendered documentation work properly and that backend class names are correctly associated with their respective functionalities, reducing potential confusion for developers.

**Potential risks**  
No functional risks exist since this is purely a documentation update. However, if the class name corrections were previously accurate in code but mislabeled in docs, developers relying on the documentation might have been misled—though this is now resolved.

**Key insights**  
Always validate documentation links and cross-reference class names with the actual codebase. Even minor typos can break user experience and lead to misunderstandings. Consider adding automated link validation in the documentation build process to catch similar issues early.

---

## 31. [Fix custom processors that use deleted behaviour for Transformers v5](https://github.com/vllm-project/vllm/pull/35107)


### Base Information

- **PR Number:** #35107
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-25 02:36:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35107/files) (1):**
  - `vllm/transformers_utils/processor.py`

### Summary

**What changed and why**  
Added a backward compatibility patch (`_transformers_v4_compatibility_init`) to handle custom processors that pass arbitrary attributes via `ProcessorMixin.__init__`. This is necessary because Transformers v5 no longer allows such attributes, but some remote code (e.g., `Molmo2ForConditionalGeneration`) still relies on this behavior.

**Technical impact**  
The patch intercepts `optional_attributes` defined in processor subclasses and sets them on the instance before calling the original `__init__`. This ensures compatibility with existing custom processors without modifying their code, while maintaining support for Transformers v5.

**Potential risks**  
- The patch may inadvertently affect other processor subclasses that don’t define `optional_attributes`, though the guard conditions minimize this risk.  
- If `optional_attributes` are not properly validated, setting arbitrary attributes could lead to unexpected behavior or conflicts with future Transformers updates.  
- The patch relies on internal Transformers implementation details, which may change in future versions.

**Key insights**  
- This is a temporary workaround; the long-term solution is upstreaming `Molmo2ForConditionalGeneration` to Transformers.  
- Developers should be aware that the patch is conditional and only applies when `optional_attributes` exist and the init hasn’t been patched already.  
- Consider adding logging or warnings to notify users of deprecated usage to encourage migration.

---

## 32. [[Bugfix][CPU] Fix basic unit tests failing in CPU platforms](https://github.com/vllm-project/vllm/pull/34677)


### Base Information

- **PR Number:** #34677
- **Author:** [jasonyanwenl](https://github.com/jasonyanwenl)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-25 00:36:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34677/files) (1):**
  - `tests/test_config.py`

### Summary

**What changed and why**  
This PR fixes CPU platform test failures by making cudagraph mode defaults platform-aware. The `test_vllm_config_defaults` test now expects `CUDAGraphMode.NONE` on platforms without static graph support, and `test_vllm_config_explicit_overrides` is skipped entirely on such platforms to avoid asserting GPU-specific values.

**Technical impact**  
The changes ensure that CPU platforms correctly default to `CUDAGraphMode.NONE` instead of GPU-specific modes like `PIECEWISE`. GPU platforms remain unaffected as `support_static_graph_mode()` returns `True` for CUDA/ROCm, preserving existing behavior. This improves platform compatibility and test reliability.

**Potential risks**  
If `support_static_graph_mode()` is incorrectly implemented or changes, tests may produce false positives/negatives. The skip condition in `test_vllm_config_explicit_overrides` could mask future issues on CPU platforms if the test logic evolves. Edge cases involving hybrid platforms or new backends may require additional validation.

**Key insights**  
Always validate platform-specific defaults in tests to avoid hardcoding GPU assumptions. Consider centralizing platform-aware default logic in the codebase rather than only in tests. For future changes, ensure any new compilation features are tested across all supported platforms to prevent similar issues.

---

## 33. [[Doc] Suggest "--managed-python" flag when installing python using uv](https://github.com/vllm-project/vllm/pull/33069)


### Base Information

- **PR Number:** #33069
- **Author:** [jasonyanwenl](https://github.com/jasonyanwenl)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-25 00:19:43
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33069/files) (1):**
  - `docs/getting_started/installation/python_env_setup.inc.md`

### Summary

**What changed and why**  
The PR adds the `--managed-python` flag to the `uv venv` command in the documentation. This change addresses a build failure that occurs when installing vllm from source with CPU-only support, where CMake cannot locate Python development headers. The flag ensures uv installs a managed Python distribution that includes necessary development files.

**Technical impact**  
This documentation update guides users to install a complete Python distribution via uv, which includes development headers required by CMake during the native extension build. Without this flag, users relying on system Python may encounter missing Python development packages, causing the build to fail when compiling CPU-only targets.

**Potential risks**  
The change assumes all users need the managed Python distribution, which may not be necessary if system Python already has development headers installed. Additionally, the managed Python installation could increase environment setup time and disk usage compared to using the system Python. There is also a risk that the flag might not be supported in older versions of uv.

**Key insights**  
Developers should ensure they use uv version 0.4+ which supports the `--managed-python` flag. For existing environments, manually installing `python-dev` (or equivalent) remains a valid workaround. This update highlights the importance of consistent documentation across all installation guides to prevent recurring issues.

---

## 34. [[DOC][BugFix] Specfiy build dependency installation](https://github.com/vllm-project/vllm/pull/34513)


### Base Information

- **PR Number:** #34513
- **Author:** [jonoillar](https://github.com/jonoillar)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-25 00:04:06
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34513/files) (1):**
  - `docs/contributing/README.md`

### Summary

**What changed and why**  
The PR updates the development environment setup documentation to explicitly include installation of build dependencies (cmake, setuptools, etc.) before installing vLLM in editable mode with the `--no-build-isolation` flag. This addresses a missing step that previously required manual dependency installation.

**Technical impact**  
This change ensures developers can successfully build vLLM from source without encountering build failures due to missing dependencies. It clarifies the workflow by separating PyTorch installation, build dependency installation, and the final vLLM installation into distinct, ordered steps.

**Potential risks**  
The `grep -v '^torch=='` command assumes `torch` appears exactly as `torch==` in `requirements/build.txt`; if the format differs (e.g., extra spaces or comments), it may incorrectly filter or include lines. Additionally, developers using different package managers (e.g., `pip` instead of `uv`) might need adjusted commands.

**Key insights**  
Always verify the format of `requirements/build.txt` to ensure the `grep` command works reliably. Consider adding a note for alternative package managers or providing a more robust method (e.g., using `sed` or a script) to exclude `torch`. This documentation update is crucial for a smooth setup process and should be kept in sync with any changes to the build requirements file.

---

## 35. [[Docs]Fix documentation formatting in architecture overview](https://github.com/vllm-project/vllm/pull/34679)


### Base Information

- **PR Number:** #34679
- **Author:** [lichuang](https://github.com/lichuang)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-25 00:00:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34679/files) (1):**
  - `docs/design/arch_overview.md`

### Summary

**What changed and why**  
This PR fixes documentation formatting in the architecture overview by replacing verbose HTML `<figure>` tags with standard Markdown image syntax and removing incorrect code block annotations. These changes ensure consistent rendering across different Markdown parsers and improve readability.

**Technical impact**  
The modifications standardize the documentation markup, making it more portable and easier to maintain. Using native Markdown syntax reduces parsing complexity and aligns with common documentation practices, enhancing compatibility with various documentation generators.

**Potential risks**  
There is minimal risk as these are purely cosmetic changes to documentation formatting. However, if the original HTML tags were used for specific styling or alignment purposes, switching to plain Markdown might affect visual presentation in some renderers.

**Key insights**  
Always prefer standard Markdown over custom HTML for documentation to ensure broad compatibility. While this change is low-risk, verify that the image rendering and alignment remain consistent after deployment, especially if the documentation is processed by multiple tools.

---

