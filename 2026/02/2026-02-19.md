# vLLM Merged PR Report

**Report Date:** 2026-02-19 PST

**Total Merged PRs:** 28

---

## 1. [[Bugfix][Hardware][AMD] Fix ROCM_AITER_FA speculative decoding support](https://github.com/vllm-project/vllm/pull/32877)


### Base Information

- **PR Number:** #32877
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [zhuohan123](https://github.com/zhuohan123)
- **Merged time:** 2026-02-19 22:25:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32877/files) (1):**
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
The fix addresses a bug in the ROCM_AITER_FA attention backend where speculative decoding (multi-token decode) was incorrectly handled. Previously, the decode path hardcoded `max_seqlen_q=1`, which broke speculative decoding. The changes now extract the actual `max_query_len` from decode metadata and route multi-token decode queries to `unified_attention` instead of `paged_attention_v1`.

**Technical impact**  
These changes enable proper support for speculative decoding in the ROCM_AITER_FA backend by dynamically handling variable query lengths. The backend now correctly distinguishes between single-token and multi-token decode scenarios, ensuring accurate attention computation when using n-gram or other speculative methods. This aligns the ROCM backend behavior with other attention backends.

**Potential risks**  
The updated assertion for shuffle KV cache layout now covers both sliding window and speculative decoding, which could mask specific unsupported configurations. There's a risk if `decode_max_query_len` is incorrectly set or if the `unified_attention` kernel has unhandled edge cases for multi-token decodes. Additionally, the descale shape calculation assumes consistent tensor dimensions, which may fail with irregular input shapes.

**Key insights**  
Developers should verify that `decode_max_query_len` is reliably populated across all speculative decoding methods. The fix correctly centralizes multi-token decode logic but introduces a dependency on the `unified_attention` kernel; ensure this kernel is thoroughly tested for all supported AMD hardware. Consider adding validation for `descale_shape` to prevent runtime errors.

---

## 2. [[Minor] Add logging when using MXFP4 MXFP8 TRTLLM backend](https://github.com/vllm-project/vllm/pull/34916)


### Base Information

- **PR Number:** #34916
- **Author:** [frankwang28](https://github.com/frankwang28)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 22:07:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34916/files) (1):**
  - `vllm/model_executor/layers/quantization/mxfp4.py`

### Summary

**What changed and why**  
Added a single logging statement to indicate when the FlashInfer MXFP4 MXFP8 TRTLLM backend is selected. This change ensures consistency with other backends that already log their activation, improving observability during model initialization.

**Technical impact**  
The modification adds minimal runtime overhead—only a one-time log message when the specific backend condition is met. It does not alter the backend selection logic or affect performance, maintaining the existing architecture and behavior.

**Potential risks**  
No functional risks are introduced, as the change is purely additive logging. However, if the logger or its configuration is altered in the future, this line could potentially fail silently without affecting backend functionality.

**Key insights**  
This is a straightforward improvement to logging consistency that aids debugging and system monitoring. Developers should ensure similar logging is present for all backend selection paths to maintain uniform observability across the codebase.

---

## 3. [[Models] LFM2: Support LoRA](https://github.com/vllm-project/vllm/pull/34921)


### Base Information

- **PR Number:** #34921
- **Author:** [tianshu-Michael-yu](https://github.com/tianshu-Michael-yu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 22:07:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34921/files) (2):**
  - `vllm/model_executor/models/lfm2.py`
  - `vllm/model_executor/models/lfm2_moe.py`

### Summary

**What changed and why**  
This PR enables LoRA support for LFM2 and LFM2-MoE models by fixing weight mapping issues. The changes rename the fused MLP projection module from `w1` to `w13`, update the `packed_modules_mapping` accordingly, add a `WeightsMapper` to handle HF-to-vLLM naming differences for convolutional layers, and refine weight-name matching to use segment-boundary matching (trailing dot) to prevent incorrect matches (e.g., `.w1` inside `.w13`).

**Technical impact**  
These modifications ensure that LoRA adapters are correctly targeted to the appropriate modules, particularly for fused MLP layers (`w1` and `w3`) and short-convolution paths. The updates maintain compatibility with existing weight loading logic while preventing misalignment between Hugging Face and vLLM module naming conventions, which is critical for proper LoRA integration.

**Potential risks**  
If the segment-boundary matching logic fails in edge cases (e.g., weight names without trailing dots or unusual naming patterns), it could lead to missed or incorrect LoRA targeting. Additionally, the reliance on string replacement for weight mapping may be fragile if future model variations introduce new naming schemes. The lack of pytest validation in this environment means potential regressions could go undetected.

**Key insights**  
Developers should verify that the segment-boundary matching works correctly for all expected weight names and consider adding unit tests for LoRA targeting once testing infrastructure is available. The addition of `hf_to_vllm_mapper` is a clean solution to naming collisions, but care must be taken to ensure it covers all relevant module renames. These changes are essential for reliable LoRA support in LFM2 models.

---

## 4. [[ROCm][CI] Loosen RemoteOpenAIServer Startup Timeout](https://github.com/vllm-project/vllm/pull/34922)


### Base Information

- **PR Number:** #34922
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-19 21:37:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34922/files) (2):**
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/utils.py`

### Summary

**What changed and why**  
The changes increase timeout values in test infrastructure to address flaky CI failures on AMD platforms. The global `RemoteOpenAIServer` timeout is raised from 240 to 360 seconds, and a specific test (`TestGPTOSSSpeculativeChat`) receives a custom 480-second timeout to accommodate longer JIT compilation times for AITER and Triton kernels.

**Technical impact**  
These adjustments reduce test flakiness by allowing more time for server initialization, particularly when GPU kernels require extended compilation. The changes apply universally across both AMD and NVIDIA CI environments, potentially increasing overall test suite duration but improving reliability.

**Potential risks**  
Overly generous timeouts may mask underlying performance regressions or resource contention issues. If compilation times continue to grow, even these increased limits may eventually be insufficient. The uniform application across platforms might unnecessarily slow NVIDIA CI runs if the issue is AMD-specific.

**Key insights**  
Consider isolating timeout increases to AMD platforms initially, as suggested in the PR description, to avoid impacting NVIDIA CI performance. Monitor compilation times to determine if optimizations are needed rather than continually extending timeouts. Adding logging for initialization breakdowns could help identify specific bottlenecks.

---

## 5. [[Misc] Add deprecated environment variable utilities](https://github.com/vllm-project/vllm/pull/33677)


### Base Information

- **PR Number:** #33677
- **Author:** [carlory](https://github.com/carlory)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 21:33:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33677/files) (1):**
  - `vllm/config/utils.py`

### Summary

**What changed and why**  
Two new utility functions, `get_from_deprecated_env_if_set` and `set_from_deprecated_env_if_set`, were added to `vllm/config/utils.py` to provide a standardized way to handle deprecated environment variables. This addresses a previous suggestion to create reusable utilities for deprecating environment variables in favor of CLI arguments or config options, ensuring consistent warning messages and behavior across the codebase.

**Technical impact**  
These functions centralize the logic for reading deprecated environment variables, issuing deprecation warnings via `logger.warning_once`, and optionally converting values to boolean or integer types. This promotes code reuse, reduces duplication, and ensures that all future environment variable deprecations follow a consistent pattern, improving maintainability.

**Potential risks**  
The `set_from_deprecated_env_if_set` function raises a `ValueError` if both `to_bool` and `to_int` are set, but it does not validate the environment variable value before conversion (e.g., non-numeric strings for `to_int` could cause a `ValueError`). Additionally, there is no handling for default values or validation of the `field_name` against the config object's structure, which could lead to runtime errors.

**Key insights**  
Developers should use these utilities for all future environment variable deprecations to ensure consistency. Consider adding input validation for type conversions and verifying that the target field exists on the config object. The `logger.warning_once` is a good choice to avoid spam, but ensure the deprecation messages are clear and include the removal version as specified.

---

## 6. [[CI][AMD][BugFix][P/D] Add default_vllm_config to test_moriio_connector.py so tests pass](https://github.com/vllm-project/vllm/pull/33739)


### Base Information

- **PR Number:** #33739
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 21:32:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33739/files) (1):**
  - `tests/v1/kv_connector/unit/test_moriio_connector.py`

### Summary

**What changed and why**  
Two test functions (`test_register_kv_caches` and `test_moriio_handshake_returns_metadata`) were modified to wrap the instantiation of `MoRIIOConnector` within a `set_current_vllm_config` context manager. This ensures that `get_current_vllm_config()` can be called successfully during the tests, fixing an `AssertionError` that occurred because the current vLLM config was not set.

**Technical impact**  
The changes align the tests with the expected pattern for testing custom operations/modules in the codebase. By using the context manager, the tests now properly set the thread-local vLLM configuration, which is required by components that rely on `get_current_vllm_config()`. This ensures the tests run without configuration-related errors and maintain consistency with other tests that use the `default_vllm_config` fixture.

**Potential risks**  
If the context manager is not used in other similar test cases, those tests may also fail with the same `AssertionError`. Additionally, there is a risk that the context manager might not be properly nested or exited in more complex test scenarios, potentially leading to configuration leaks or incorrect state between tests.

**Key insights**  
Developers should ensure that any test directly interacting with custom ops or modules uses the appropriate vLLM config context. Consider adding a linter or test rule to catch missing `set_current_vllm_config` usage. For consistency, evaluate whether the `default_vllm_config` fixture could be used instead of manually managing the context in these tests.

---

## 7. [Add validation to reject non-text content in system messages](https://github.com/vllm-project/vllm/pull/34072)


### Base Information

- **PR Number:** #34072
- **Author:** [veeceey](https://github.com/veeceey)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 21:30:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34072/files) (2):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`

### Summary

**What changed and why**  
Added a Pydantic model validator `check_system_message_content_type` to the `ChatCompletionRequest` class. This validator logs a warning when system messages contain non-text content types (images, audio, video), aligning with the OpenAI API specification. The change addresses a deviation where vLLM previously accepted multimodal content in system messages without indication.

**Technical impact**  
The validator runs during request parsing and logs a one-time warning via `logger.warning_once` to avoid log spam. It handles both explicit `type` fields and inferred types from content keys (e.g., `image_url`). User messages and other roles remain unaffected, preserving full multimodal support for non-system roles.

**Potential risks**  
The warning-only approach may not prevent users from inadvertently relying on unsupported behavior, as requests still proceed. Edge cases include complex nested content structures or future content types not covered by the inference logic. Additionally, the validator only checks list-format content; string-format system messages are implicitly allowed as text.

**Key insights**  
This change improves API compliance while maintaining backward compatibility. Developers should ensure system messages use only text content to avoid warnings. Consider extending validation to reject non-text content in future versions if strict compliance is required. The comprehensive test suite effectively validates both warnings and accepted cases.

---

## 8. [[Model Bash]: Improve FP8 Oracle for Config Specific Kernel Selection](https://github.com/vllm-project/vllm/pull/34260)


### Base Information

- **PR Number:** #34260
- **Author:** [elizabetht](https://github.com/elizabetht)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 21:29:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34260/files) (1):**
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`

### Summary

**What changed and why**  
The PR modifies the FP8 MoE backend selection logic to be architecture-aware. Previously, FlashInfer backends had higher priority than DeepGEMM across all platforms, causing suboptimal kernel selection on Hopper (SM90) where FlashInfer kernels are not well-optimized. The change introduces a priority function that reorders backends based on GPU architecture (Hopper vs. Blackwell) and specific quantization configurations.

**Technical impact**  
This change improves performance on Hopper GPUs by ensuring DeepGEMM is preferred over FlashInfer backends, while maintaining FlashInfer priority on Blackwell (SM100+). Additionally, for Hopper with block FP8 quantization, it now prioritizes Triton for tensor parallelism and FlashInfer CUTLASS for expert parallelism, optimizing kernel selection for specific parallel configurations.

**Potential risks**  
The architecture-specific logic may become complex as more GPU generations are added. There's a risk of missing edge cases for other quantization schemes or parallel configurations not covered by the current condition. The hardcoded backend list could become outdated if new backends are introduced without updates to the priority function.

**Key insights**  
Developers should extend the `_get_priority_backends` function for future architecture support rather than modifying the static list directly. Consider adding unit tests that validate backend selection across different GPU architectures and quantization configurations. Monitor performance regressions on Blackwell to ensure the priority order remains optimal.

---

## 9. [[Bugfix] Add regression test for MoE quant_config under torch.compile](https://github.com/vllm-project/vllm/pull/34335)


### Base Information

- **PR Number:** #34335
- **Author:** [mgehre-amd](https://github.com/mgehre-amd)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-19 21:27:27
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34335/files) (1):**
  - `tests/quantization/test_compressed_tensors.py`

### Summary

**What changed and why**  
This PR adds a regression test for a bug where W4A16 quantized Mixture-of-Experts (MoE) models fail under `torch.compile`. The bug occurred because `ensure_moe_quant_config_init()`, which sets a critical quantization attribute, was called in a compiled function where its attribute mutation side effect was not captured at runtime. The fix was already applied separately; this PR ensures the issue does not reoccur.

**Technical impact**  
The test validates that the fix, which moves the `ensure_moe_quant_config_init()` call into the `moe_forward` custom op's implementation, works correctly. This ensures the quantization configuration is properly initialized during runtime execution, not just during tracing, which is essential for the correct weight dimension handling in 4-bit quantized MoE models.

**Potential risks**  
The test depends on an external model (`nm-testing/tinysmokeqwen3moe-W4A16-first-only-CTstable`), which could become unavailable or change, causing test failures unrelated to the code. Additionally, the test uses a specific compilation configuration (`"cudagraph_mode": "NONE"`); changes to default compilation behavior could affect test reliability.

**Key insights**  
This is a classic example of a side-effect mutation issue under torch.compile, where runtime behavior diverges from traced behavior. Developers should be cautious about performing critical initialization or state mutations inside functions that will be compiled by Dynamo. The fix's location inside the custom op is correct because it executes outside the compiled graph.

---

## 10. [[Quark] Fix MoE fp8 activation scale handling on mi300](https://github.com/vllm-project/vllm/pull/34386)


### Base Information

- **PR Number:** #34386
- **Author:** [BowenBao](https://github.com/BowenBao)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 21:27:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34386/files) (1):**
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`

### Summary

**What changed and why**  
The fix addresses fp8 activation scale handling for Mi300 GPUs by adding a dtype check (`self.input_dtype == "fp8"`) before processing fp8 static input scales, and updates tensor dtype arguments in normalization functions from `torch.float8_e4m3fnuz` to `torch.float8_e4m3fn` to match the expected input format.

**Technical impact**  
This ensures fp8 scale conversion only occurs when activations are actually quantized to fp8, preventing unnecessary operations and potential errors. The dtype corrections align the normalization function calls with the actual tensor types being processed, maintaining consistency in fp8 quantization workflows.

**Potential risks**  
If other hardware platforms use different fp8 formats, similar dtype mismatches could occur. The fix assumes `torch.float8_e4m3fn` is always the correct input dtype for the normalization function, which may not hold for all quantization configurations or future fp8 variants.

**Key insights**  
Always validate quantization dtype conditions before performing format-specific operations. When working with multiple fp8 formats, ensure tensor dtype arguments match the actual data types being processed. Consider adding platform-specific dtype mappings to handle fp8 format variations more robustly.

---

## 11. [[ci] Use the right tag for CPU arm64 image](https://github.com/vllm-project/vllm/pull/34915)


### Base Information

- **PR Number:** #34915
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 19:59:15
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34915/files) (1):**
  - `.buildkite/image_build/image_build_cpu_arm64.sh`

### Summary

**What changed and why**  
The changes fix a Docker image tag naming inconsistency in the ARM64 CPU image build script. During a previous migration, the tag suffix was incorrectly set to `-cpu` instead of `-arm64-cpu`, which this PR corrects to maintain consistent naming conventions across the CI pipeline.

**Technical impact**  
This ensures the ARM64 CPU images are properly tagged and identified in the container registry, preventing potential conflicts with x86 CPU images. The fix maintains build skipping logic accuracy and ensures downstream processes can correctly reference the appropriate architecture-specific images.

**Potential risks**  
If other CI scripts or deployment processes still reference the old `-cpu` tag pattern for ARM64 images, they may fail to locate the correct images. There's also a risk that existing `-cpu` tagged ARM64 images in the registry could become orphaned if cleanup processes aren't updated accordingly.

**Key insights**  
This is a straightforward correction that highlights the importance of consistent tagging across multi-architecture CI pipelines. Developers should verify that all references to these image tags are updated consistently and consider implementing automated tag validation in future migrations to prevent similar issues.

---

## 12. [[Refactor] Implement output type check in LLM](https://github.com/vllm-project/vllm/pull/34794)


### Base Information

- **PR Number:** #34794
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 19:57:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34794/files) (2):**
  - `vllm/entrypoints/llm.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
The PR moves output type validation from the `LLMEngine.validate_outputs` method (which was a no-op) into the `LLM` class. Instead of calling the engine's validation after generating outputs, the `LLM` methods now pass an `output_type` parameter to internal methods (`_run_completion`, `_run_chat`, `_run_engine`, etc.) and perform type checking directly in `_run_engine` using `isinstance` assertions.

**Technical impact**  
This refactor centralizes output type validation within the `LLM` class, making the engine class simpler by removing the empty `validate_outputs` method. The changes ensure that output types (`RequestOutput`, `PoolingRequestOutput`, etc.) are validated closer to where they are produced, improving type safety and consistency across different generation methods (generate, chat, encode, score). The addition of overloads for `wait_for_completion` also enhances type hinting flexibility.

**Potential risks**  
The `isinstance` assertion in `_run_engine` (line 1973) appears truncated in the diff, which could lead to runtime errors if incomplete. There is also a risk of regression if the type checking logic does not correctly handle all output variants (e.g., mixed types in `wait_for_completion`). Additionally, removing the engine's validation method might affect downstream code that relied on it, though it was previously a no-op.

**Key insights**  
Developers should verify that the truncated `isinstance` check is complete and correctly validates against the provided `output_type`. This change improves code clarity by eliminating a redundant method, but care must be taken to ensure all code paths (including edge cases like pooling and scoring) correctly propagate and validate the output type. The overloads for `wait_for_completion` are a positive addition for better type hints.

---

## 13. [[Core] Fix state names in pause_scheduler()](https://github.com/vllm-project/vllm/pull/34840)


### Base Information

- **PR Number:** #34840
- **Author:** [markmc](https://github.com/markmc)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-19 17:21:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34840/files) (1):**
  - `vllm/v1/engine/core.py`

### Summary

**What changed and why**  
The changes update documentation strings in the `pause_scheduler()` method to correct state name references that became out of sync during previous iterations (specifically #34125). The documentation now aligns the pause mode descriptions with the actual state names used in the code (e.g., `PAUSED_NEW` and `PAUSED_ALL` instead of `PAUSE_ABORT`, `PAUSE_KEEP`, and `PAUSE_WAIT`).

**Technical impact**  
This is a documentation-only change that improves clarity and accuracy for developers using or maintaining the scheduler. It ensures that the documented behavior matches the implementation, reducing confusion about how different pause modes (`abort`, `wait`, `keep`) interact with internal states and caching behavior.

**Potential risks**  
Since no functional code is modified, there is minimal risk of introducing bugs. However, if the actual state names in the code (e.g., `PAUSED_NEW`, `PAUSED_ALL`) are still incorrect or inconsistent elsewhere, this documentation update might mask deeper inconsistencies that could affect future development or debugging.

**Key insights**  
Always verify that documentation reflects the current implementation after refactoring or iterative changes. Consider adding a consistency check (e.g., via static analysis or unit tests) to catch mismatches between documented state names and actual constants in the codebase.

---

## 14. [[CI] Add GPT-OSS Eval job for H100](https://github.com/vllm-project/vllm/pull/34359)


### Base Information

- **PR Number:** #34359
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 17:14:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34359/files) (1):**
  - `.buildkite/test_areas/misc.yaml`

### Summary

**What changed and why**  
A new CI job "GPT-OSS Eval (H100)" was added to run GPT-OSS evaluation tests on H100 GPUs. This addresses a gap where previously evaluations only ran on B200 (Blackwell) GPUs, which use a different default kernel path (triton_kernels) than H100 (Hopper) GPUs.

**Technical impact**  
This change expands test coverage by ensuring the GPT-OSS model is evaluated on both Blackwell and Hopper architectures. It will trigger the default triton_kernels execution path on H100, providing validation for kernel configurations that weren't previously tested in CI.

**Potential risks**  
The job is marked as optional (`optional: true`), which could lead to it being skipped in some CI runs. There's also a risk of increased CI resource consumption and runtime due to adding another GPU-intensive evaluation job.

**Key insights**  
This is a valuable addition for catching architecture-specific issues, particularly around kernel selection. Developers should monitor whether this optional job runs consistently enough to provide meaningful coverage. Consider making it non-optional once stability is confirmed to ensure continuous validation of both kernel paths.

---

## 15. [[Model Runner V2] Minor CPU optimizations](https://github.com/vllm-project/vllm/pull/34856)


### Base Information

- **PR Number:** #34856
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-19 16:05:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34856/files) (4):**
  - `vllm/distributed/device_communicators/shm_broadcast.py`
  - `vllm/v1/worker/gpu/async_utils.py`
  - `vllm/v1/worker/gpu/buffer_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The changes optimize CPU overhead in low-latency scenarios by reducing expensive CUDA API calls. Specifically, the PR addresses overhead from `Tensor.is_pinned()` checks and repeated `torch.cuda.current_stream()` lookups by caching the default stream and removing unnecessary tensor pinning assertions.

**Technical impact**  
These optimizations reduce Python-CUDA interaction overhead in critical paths. The stream caching eliminates repeated device/context lookups, while the `stream()` context manager avoids redundant current stream queries. The removal of `is_pinned()` checks and assertion adjustments streamline memory operations without changing functional behavior.

**Potential risks**  
The `assert tmp is not x` replacement for `assert not x.is_pinned()` assumes `pin_memory()` always returns a new tensor when unpinned, which may not hold in all PyTorch versions or configurations. The stream caching assumes the default stream remains valid throughout model execution, which is generally safe but could be problematic if device contexts change.

**Key insights**  
The optimizations demonstrate good low-latency engineering by targeting profiling-identified bottlenecks. Developers should verify the `pin_memory()` behavior assumption across different PyTorch versions. The stream caching pattern could be extended to other CUDA resource lookups, but careful consideration is needed for multi-device or context-changing scenarios.

---

## 16. [[Bugfix] Fix benchmark_fused_collective crash on CustomOp init](https://github.com/vllm-project/vllm/pull/34665)


### Base Information

- **PR Number:** #34665
- **Author:** [mayank-ketkar-sf](https://github.com/mayank-ketkar-sf)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-19 16:01:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34665/files) (1):**
  - `benchmarks/kernels/benchmark_fused_collective.py`

### Summary

**What changed and why**  
The fix moves the instantiation of `VllmFusedAllreduce` inside each `set_current_vllm_config()` context block. This ensures that `CustomOp` subclasses (like `RMSNorm` and `QuantFP8`) are initialized with the correct vLLM configuration, preventing a crash due to missing context and ensuring proper dispatch between native and custom kernel paths during benchmarking.

**Technical impact**  
These changes ensure the benchmark accurately measures performance differences between native and custom kernel implementations by binding the correct forward method at initialization. The fix is confined to the benchmark script and does not affect production code, maintaining separation between testing and runtime behavior.

**Potential risks**  
Repeated instantiation of `VllmFusedAllreduce` could introduce minor overhead, but this is acceptable for benchmarking. If future modifications extend this pattern to production code, careful consideration of initialization costs and context management will be necessary to avoid performance degradation.

**Key insights**  
Always instantiate context-dependent objects within the appropriate context to ensure correct initialization and dispatch. For benchmarks comparing multiple configurations, avoid reusing objects across different settings to prevent misleading results. This fix highlights the importance of isolating configuration-dependent initialization in performance testing.

---

## 17. [[UX] More descriptive reasons in is_supported_config for MoE](https://github.com/vllm-project/vllm/pull/34908)


### Base Information

- **PR Number:** #34908
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 15:20:53
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34908/files) (3):**
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
The PR enhances error messages in MoE (Mixture of Experts) kernel support checks by adding specific values to rejection reasons. Previously, messages were generic (e.g., "kernel does not support current device"); now they include details like device names, quantization schemes, and configuration values to aid debugging.

**Technical impact**  
These changes improve debuggability for users and developers when MoE kernels fail to load, by providing actionable context in error messages. The core logic remains unchanged—only the informational content of rejection reasons is enriched, which does not affect runtime behavior or kernel selection.

**Potential risks**  
Minimal risk, as the changes are purely additive and do not alter control flow. However, ensure that string formatting does not introduce runtime errors (e.g., if `current_platform.device_name` is unexpectedly `None`). Also, verify that the added details do not expose sensitive internal state.

**Key insights**  
This is a UX-focused improvement that significantly enhances troubleshooting. Developers should adopt similar descriptive patterns in other kernel support checks. Consider adding unit tests to validate error message formats and ensure all formatted values are safely serializable.

---

## 18. [[Bugfix] Fix Basic Models Test](https://github.com/vllm-project/vllm/pull/34818)


### Base Information

- **PR Number:** #34818
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 14:49:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34818/files) (14):**
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/utils.py`
  - `tests/v1/spec_decode/test_eagle.py`
  - `vllm/config/cache.py`
  - `vllm/config/vllm.py`
  - `vllm/model_executor/layers/attention/chunked_local_attention.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/interface.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/executor/ray_executor.py`
  - `vllm/v1/executor/uniproc_executor.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR fixes multiple test failures by addressing block size initialization issues. The main change moves block size validation and backend-aware selection from platform initialization to engine startup, ensuring block size is properly set based on the actual attention backend used.

**Technical impact**  
The architecture shifts block size determination from `Platform.check_and_update_config()` to a new `update_block_size_for_backend()` method called during executor initialization. This ensures block size is finalized after the model is loaded and attention backends are instantiated, enabling proper validation against DCP and Mamba constraints via `validate_block_size()`.

**Potential risks**  
If `update_block_size_for_backend()` is called before attention layers are initialized (e.g., in tests), it may default to block size 16 without backend-specific optimization. The lazy initialization of `chunked_prefill_workspace_size` in MLA could cause thread-safety issues if accessed concurrently before initialization.

**Key insights**  
Always call `update_block_size_for_backend()` after model loading in executors. Tests must explicitly set `block_size=16` in `CacheConfig` to avoid backend selection logic. The refactor improves separation of concerns but requires careful ordering of initialization steps.

---

## 19. [[Bugfix] Fix Qwen3.5 Cutlass fp8 kernel on hopper - clamp block scales](https://github.com/vllm-project/vllm/pull/34914)


### Base Information

- **PR Number:** #34914
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-19 13:34:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34914/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`

### Summary

**What changed and why**  
Added clamping of FP8 block scales to a minimum value (1e-10) in the FlashInfer CUTLASS MoE kernel preparation. This prevents NaN outputs when models like Qwen3.5-397B-A17B-FP8 contain experts with near-zero block scales (~1e-23), which the kernel on Hopper GPUs cannot handle correctly.

**Technical impact**  
The change ensures numerical stability for FP8 MoE models with "dead" experts by forcing extremely small scales to a safe minimum. This allows the FlashInfer CUTLASS kernel to produce near-zero outputs instead of NaN, maintaining model accuracy while enabling the kernel's use without environment variable workarounds.

**Potential risks**  
Clamping could theoretically affect models where such low scales are intentional, though the description indicates these experts are effectively dead. The chosen threshold (1e-10) is arbitrary and may need adjustment if other models exhibit different scale distributions. There is also a risk if block_quant is misapplied or scales are incorrectly identified.

**Key insights**  
This is a critical fix for FP8 MoE support on Hopper GPUs, addressing a hardware-specific kernel limitation. Developers should verify that clamping does not impact other FP8 models and consider making the minimum scale configurable. The fix highlights the importance of robust handling of edge cases in quantization kernels, especially for large-scale models with sparse expert usage.

---

## 20. [Change targets for AMD build in the "CI" pipeline](https://github.com/vllm-project/vllm/pull/34918)


### Base Information

- **PR Number:** #34918
- **Author:** [Alexei-V-Ivanov-AMD](https://github.com/Alexei-V-Ivanov-AMD)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-19 13:26:53
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34918/files) (1):**
  - `.buildkite/hardware_tests/amd.yaml`

### Summary

**What changed and why**  
The change modifies the `ARG_PYTORCH_ROCM_ARCH` build argument in the AMD CI pipeline configuration, updating the target GPU architectures from `gfx90a;gfx942` to `gfx942;gfx950`. This likely aligns the build with newer or different AMD GPU hardware for testing or deployment.

**Technical impact**  
This adjustment will cause the CI pipeline to compile PyTorch's ROCm support for the `gfx942` and `gfx950` architectures instead of `gfx90a` and `gfx942`. It may affect binary compatibility and performance for specific AMD GPUs, potentially excluding older hardware (gfx90a) and adding support for newer hardware (gfx950).

**Potential risks**  
If any part of the codebase or dependencies rely on features specific to `gfx90a`, this change could lead to runtime errors or performance regressions on that architecture. Additionally, if the new `gfx950` architecture isn't fully supported in the current ROCm or PyTorch versions, the build might fail or produce unstable binaries.

**Key insights**  
Ensure that the ROCm and PyTorch versions used in the Docker build explicitly support `gfx950`. Consider maintaining backward compatibility if `gfx90a` is still in use, possibly through conditional builds or multi-architecture targets. Validate that the CI pipeline tests cover both new architectures to catch any issues early.

---

## 21. [[Refactor] Deprecate `head_first` for `chunk_gated_delta_rule`](https://github.com/vllm-project/vllm/pull/34263)


### Base Information

- **PR Number:** #34263
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-19 10:23:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34263/files) (3):**
  - `vllm/model_executor/layers/fla/ops/chunk.py`
  - `vllm/model_executor/models/llava_onevision.py`
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
The `head_first` parameter has been removed from the `chunk_gated_delta_rule` function and related implementations. This follows through on a deprecation warning that was introduced five months ago, standardizing the function to only accept inputs in the `[B, T, H, ...]` (batch, sequence, head) format instead of supporting both `[B, T, H, ...]` and `[B, H, T, ...]` formats.

**Technical impact**  
This change simplifies the function's interface and internal logic by removing format conversion code (including `rearrange` operations and conditional branches). The function now consistently expects and returns tensors in the `[B, T, H, ...]` format, reducing complexity and potential for format-related bugs. All callers have been updated to remove the `head_first=False` argument.

**Potential risks**  
The main risk is that any downstream code still passing data in the `[B, H, T, ...]` format will now fail or produce incorrect results. The warning check for shape mismatch (`seq_len < num_heads`) remains but no longer references the removed parameter. There's also a risk if any external users were relying on the deprecated `head_first=True` behavior despite the warning.

**Key insights**  
This is a clean-up change that reduces maintenance burden and eliminates a deprecated code path. Developers should ensure all input tensors to `chunk_gated_delta_rule` and related functions are in the `[B, T, H, ...]` format. The shape mismatch warning provides helpful guidance for debugging format issues. The removal of `einops` import reduces dependencies.

---

## 22. [Revert "[NemotronH] Do not force router to run in fp32 (#34582)"](https://github.com/vllm-project/vllm/pull/34808)


### Base Information

- **PR Number:** #34808
- **Author:** [roikoren755](https://github.com/roikoren755)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 09:47:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34808/files) (1):**
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
This PR reverts a performance optimization that allowed router logits to use mixed precision, instead forcing them back to float32. The revert was necessary because the original change (#34582) caused accuracy degradation in the NemotronH model. The team is working on a better implementation of the performance improvement in a separate PR (#34302).

**Technical impact**  
The router's linear layer (`self.gate`) and the subsequent `SharedFusedMoE` layer are now explicitly configured to use `torch.float32` for router logits. Additionally, the forward pass explicitly casts hidden states to float32 before computing router logits. This ensures numerical stability at the cost of potential performance overhead from maintaining float32 precision in the router computation path.

**Potential risks**  
The main risk is performance regression, as forcing float32 precision may reduce throughput, especially on hardware optimized for lower precision (like FP16/BF16). There's also a risk that future attempts to re-enable mixed precision (#34302) could reintroduce accuracy issues if not carefully validated with comprehensive accuracy testing across different workloads.

**Key insights**  
Accuracy takes precedence over performance when numerical precision affects model outputs. Developers should treat router/logit computations as sensitive operations that may require higher precision. Any future optimization in this area must include rigorous accuracy validation across diverse inputs before deployment. The explicit dtype parameterization in `ReplicatedLinear` and `SharedFusedMoE` provides a cleaner pattern for controlling precision than relying solely on casting in the forward pass.

---

## 23. [[Model Bash][DeepSeekR1] Remove Shared Expert Clone](https://github.com/vllm-project/vllm/pull/34344)


### Base Information

- **PR Number:** #34344
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 07:56:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34344/files) (2):**
  - `vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`
  - `vllm/model_executor/models/minicpm.py`

### Summary

**What changed and why**  
The PR removes the `.clone()` operation on shared expert inputs in the MoE (Mixture of Experts) layer. Previously, cloning was required to prevent in-place modifications from interfering with parallel execution on separate CUDA streams. Now, in-place operations are systematically disabled when shared experts are used (via `moe_config.disable_inplace`), making the clone unnecessary.

**Technical impact**  
This change simplifies the MoE execution flow by eliminating redundant tensor cloning, reducing memory overhead and improving performance. The code now directly passes the original input tensor (`shared_experts_input`) to the shared experts stream, relying on the enforced `disable_inplace` flag to ensure data integrity.

**Potential risks**  
If `disable_inplace` is not correctly enforced or if in-place operations are inadvertently reintroduced in downstream code, data corruption could occur due to race conditions between CUDA streams. Additionally, the change assumes all models using shared experts have been updated to disable in-place operations, which may not hold for custom or third-party implementations.

**Key insights**  
Developers should verify that `moe_config.disable_inplace` is consistently set for any model using shared experts. The removal of cloning is a performance optimization but requires careful validation to avoid silent data corruption. Review related model configurations (like the updated `minicpm.py`) to ensure in-place is disabled where needed.

---

## 24. [[Llama4,Quantization] Simplify and generalize logic for Q/K permutations in quantized self-attn layers](https://github.com/vllm-project/vllm/pull/34471)


### Base Information

- **PR Number:** #34471
- **Author:** [eldarkurtic](https://github.com/eldarkurtic)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 07:55:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34471/files) (1):**
  - `vllm/model_executor/models/llama4.py`

### Summary

**What changed and why**  
The PR simplifies and generalizes the logic for permuting Q/K weights and scales in quantized self-attention layers for Llama-4 models. It replaces framework-specific, hardcoded checks for NVFP4 and compressed-tensors INT8/FP8 formats with a unified approach that supports per-tensor, per-channel, and per-group quantization schemes. This enables loading of previously unsupported quantized checkpoints, such as compressed-tensors INT4.

**Technical impact**  
The changes remove dependencies on specific quantization frameworks (like `compressed_tensors`) and complex conditional logic. The new implementation uses a more generic shape-based permutation that works across different quantization granularities by inspecting tensor dimensions and names. This improves code maintainability and extends compatibility to a broader range of quantized models without altering the core permutation operation.

**Potential risks**  
The logic assumes that permutations are feasible for all supported schemes, but explicitly excludes per-block quantization (e.g., DeepSeek's 128x128 blocks) where scales are grouped across matrix rows, making permutation infeasible. There is a risk if the tensor's `f_out` dimension is not evenly divisible by `n_heads * 2`, which could cause reshaping errors. Additionally, the removal of framework-specific checks might inadvertently affect edge cases not covered in testing.

**Key insights**  
The refactoring significantly enhances the robustness and extensibility of quantized model loading. Developers should note that per-block quantization remains unsupported for Q/K projections, and alternative strategies may be needed for those cases. The simplified code is easier to test and maintain, but thorough validation with diverse quantization formats is recommended to ensure no regression.

---

## 25. [[Bugfix] Qwen3.5 kv-scale weight remapping](https://github.com/vllm-project/vllm/pull/34719)


### Base Information

- **PR Number:** #34719
- **Author:** [Linda-Stadter](https://github.com/Linda-Stadter)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 04:13:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34719/files) (1):**
  - `vllm/model_executor/models/qwen3_5.py`

### Summary

**What changed and why**  
This PR adds KV cache scaling factor name remapping for the Qwen3.5 model, similar to what was previously implemented for Qwen3 Next. The change ensures proper loading of FP8 KV cache scaling weights by remapping parameter names during weight loading, fixing a compatibility issue with certain weight formats.

**Technical impact**  
The modification integrates the `maybe_remap_kv_scale_name` utility into Qwen3.5's weight loading logic, aligning its behavior with other models in the codebase. This ensures consistent handling of FP8 quantization scaling factors across different Qwen model variants and prevents potential loading failures when FP8 KV cache is enabled.

**Potential risks**  
The conditional check `if name.endswith("scale"):` might be too broad and could inadvertently remap unrelated parameters that happen to end with "scale". Additionally, if `maybe_remap_kv_scale_name` returns `None`, the weight is silently skipped without logging, which could make debugging weight loading issues more difficult.

**Key insights**  
This is a targeted fix that follows established patterns in the codebase, but consider adding logging when weights are skipped to improve debuggability. Verify that no other model parameters ending with "scale" are affected by this change, and ensure the remapping logic correctly handles all expected weight naming conventions for Qwen3.5 variants.

---

## 26. [[CI/Build] Try to make beam search test less flaky](https://github.com/vllm-project/vllm/pull/34885)


### Base Information

- **PR Number:** #34885
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-19 03:16:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34885/files) (1):**
  - `tests/samplers/test_beam_search.py`

### Summary

**What changed and why**  
The change modifies a test configuration dictionary to apply `async_scheduling=False` and `max_num_seqs=1` settings to non-ROCm platforms (likely CUDA), not just ROCm. This aims to reduce test flakiness in beam search tests by ensuring consistent execution behavior across different hardware backends.

**Technical impact**  
This standardizes test execution parameters between ROCm and CUDA backends, potentially reducing non-determinism in beam search operations. The settings likely limit concurrency and sequence parallelism, making test outcomes more predictable and reproducible.

**Potential risks**  
The change might mask underlying concurrency issues rather than fixing them, potentially allowing race conditions to persist in production code. There's also a risk that overly restrictive settings could diverge from real-world usage patterns, reducing test coverage for actual deployment scenarios.

**Key insights**  
Developers should verify that these settings don't hide actual bugs in the beam search implementation. Consider whether similar constraints should be applied to production code or if the test should be refactored to handle concurrency more robustly. Monitor test stability after this change to confirm it addresses the flakiness issue.

---

## 27. [[Bugfix] Fix edge case in UUID data parsing](https://github.com/vllm-project/vllm/pull/34884)


### Base Information

- **PR Number:** #34884
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-19 02:24:30
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34884/files) (2):**
  - `tests/renderers/test_process_multi_modal_uuids.py`
  - `vllm/renderers/base.py`

### Summary

**What changed and why**  
The PR fixes an edge case in UUID data parsing for multi-modal inputs where empty UUID lists were incorrectly validated. Previously, the validation logic only checked length mismatches when UUID lists had items, but now it properly validates all cases by comparing lengths directly.

**Technical impact**  
The change ensures consistent validation between multi-modal data items and their corresponding UUIDs, fixing a bug where empty UUID lists would bypass validation even when data items existed. This improves data integrity and prevents potential runtime errors in multi-modal processing pipelines.

**Potential risks**  
The stricter validation could potentially break existing code that relied on the previous lenient behavior with empty UUID lists. Edge cases involving None vs empty list semantics for different modalities (image, video, audio) need careful handling as shown in the updated test.

**Key insights**  
Developers should ensure their multi-modal UUID lists always match the length of corresponding data items, including empty lists. The test additions provide clear examples of valid and invalid cases, particularly demonstrating the distinction between None (cached input) and empty lists (no input).

---

## 28. [[ROCm][Test] Fix beam search determinism failures from batch-size-dependent FP divergence and removed wrong marker](https://github.com/vllm-project/vllm/pull/34878)


### Base Information

- **PR Number:** #34878
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-19 00:25:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34878/files) (3):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/samplers.yaml`
  - `tests/samplers/test_beam_search.py`

### Summary

**What changed and why**  
The changes fix beam search determinism failures on ROCm by addressing batch-size-dependent floating-point divergence in attention and GEMM kernels. Platform-specific engine settings are added to enforce deterministic execution, and the "skinny GEMM" kernel is disabled via environment variable. Buildkite configurations are also updated to remove incorrect test markers.

**Technical impact**  
On ROCm, the fix ensures beam search produces identical outputs across runs by eliminating non-associative floating-point reductions that vary with batch geometry. CUDA behavior remains unchanged due to platform gating. The changes enforce deterministic batch composition, disable CUDA graph padding, and fix batch sizes to prevent numerical divergence.

**Potential risks**  
Platform-specific logic increases maintenance complexity and may mask underlying kernel determinism issues. Disabling optimizations like skinny GEMM and prefix caching could impact ROCm performance. Future kernel changes or new platforms may require similar conditional handling.

**Key insights**  
This highlights a fundamental difference in ROCm's floating-point reduction semantics compared to CUDA. Developers should be aware that batch-size-dependent numerical divergence can cascade in beam search. Consider documenting platform-specific determinism requirements and evaluating whether kernel fixes could address the root cause more broadly.

---

