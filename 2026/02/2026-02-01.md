# vLLM Merged PR Report

**Report Date:** 2026-02-01 PST

**Total Merged PRs:** 18

---

## 1. [[CPU][IBM Z][Dockerfile] Fix IBM Z builds](https://github.com/vllm-project/vllm/pull/33243)


### Base Information

- **PR Number:** #33243
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-01 23:41:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33243/files) (1):**
  - `docker/Dockerfile.s390x`

### Summary

**What changed and why**  
The Dockerfile for IBM Z (s390x) architecture was updated to fix build issues. Key changes include upgrading the base UBI image to version 9.6, updating PyTorch to 2.10.0 and torchvision to v0.25.0, adding OpenCV compilation from source, and adjusting build dependencies and environment variables to ensure compatibility.

**Technical impact**  
These modifications ensure that the Docker image can successfully compile and run vLLM on s390x CPUs. The updates to library versions and build tools resolve dependency conflicts, while the addition of OpenCV and adjusted paths support the full software stack required for model inference on this architecture.

**Potential risks**  
The switch to a newer UBI base image could introduce unforeseen compatibility issues with existing systems. Additionally, building OpenCV from source may increase image build time and complexity, and the reliance on specific toolchain versions (gcc-toolset-14) could create maintenance challenges if those packages are deprecated or updated.

**Key insights**  
Developers should verify that all custom patches (like the numba header fix) remain necessary with the updated dependencies. The build process now uses `requirements/cpu-build.txt` instead of `requirements/build.txt`, which may affect future dependency management. Ensure that the OpenCV build does not introduce unnecessary bloat or licensing issues in the final image.

---

## 2. [[Model] Support DeepSeek-OCR-2](https://github.com/vllm-project/vllm/pull/33165)


### Base Information

- **PR Number:** #33165
- **Author:** [LiuLi1998](https://github.com/LiuLi1998)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-01 22:24:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33165/files) (9):**
  - `docs/models/supported_models.md`
  - `examples/offline_inference/vision_language.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/deepencoder.py`
  - `vllm/model_executor/models/deepencoder2.py`
  - `vllm/model_executor/models/deepseek_ocr2.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/chat_templates/registry.py`
  - `vllm/transformers_utils/processors/deepseek_ocr2.py`

### Summary

**What changed and why**  
This PR adds support for the DeepSeek-OCR-2 model to the vLLM codebase. The changes include a new model implementation (`DeepseekOCR2ForCausalLM`), a custom visual encoder (`deepencoder2.py`), a processor for image-text preprocessing, and updates to documentation, examples, and registries. The purpose is to enable inference with this advanced OCR model, which uses a hybrid attention mechanism (non-causal for image tokens, causal for text) and tiled image processing.

**Technical impact**  
The integration extends vLLM's multimodal capabilities with a model that requires specialized handling of image tokens and attention masks. It introduces a new visual encoder based on a modified Qwen2 decoder architecture and reuses components from existing DeepSeek models (e.g., `MlpProjector`). The model supports tiled image processing for high-resolution inputs and uses a custom logits processor (`NGramPerReqLogitsProcessor`) for improved OCR performance.

**Potential risks**  
- The custom attention mask logic in `deepencoder2.py` (non-causal/causal separation via `token_type_ids`) may have edge cases in batched inference or with varying sequence lengths.  
- Image tiling logic could produce inconsistent token counts if input dimensions exceed expected thresholds.  
- Reusing the `NGramPerReqLogitsProcessor` from DeepSeek-OCR requires careful configuration (e.g., `skip_special_tokens=False`) to avoid breaking OCR output formatting.

**Key insights**  
- Developers must configure sampling parameters correctly: use `skip_special_tokens=False` and include the `NGramPerReqLogitsProcessor` for optimal OCR results.  
- The model relies on a fixed image token (`<image>`), so prompt templates must align with this placeholder.  
- The visual encoder's dual architecture (ViT + Qwen2 decoder) is a significant departure from standard vision encoders and may affect performance profiling.

---

## 3. [Fix mistral sliding window parsing](https://github.com/vllm-project/vllm/pull/33521)


### Base Information

- **PR Number:** #33521
- **Author:** [andylolu2](https://github.com/andylolu2)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-01 21:08:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33521/files) (2):**
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/mistral.py`

### Summary

**What changed and why**  
The fix addresses incorrect parsing of Mistral's sliding window configuration. The issue occurred because the `sliding_window` attribute was nested under `config["text_config"]["sliding_window"]` in Voxtral models, causing the original logic in `config.py` to be bypassed. The parsing logic has been moved from `config.py` to the Mistral-specific adapter function `adapt_config_dict` to ensure proper handling.

**Technical impact**  
This change centralizes sliding window configuration parsing within the Mistral adapter, making it more robust for nested configurations like Voxtral. It ensures that sliding window attention is correctly applied instead of falling back to full attention, improving performance for streaming scenarios. The refactor also removes duplicate logic and clarifies the handling of integer vs. list-based sliding window definitions.

**Potential risks**  
If the `sliding_window` value is a list containing multiple non-`None` values, the assertion `len(set(sliding_window) - {None}) <= 1` could fail. The removal of the `ragged_attention` mapping in `_remap_mistral_audio_args` might affect older configurations if they rely on that field. Additionally, the new `ValueError` for unsupported `sliding_window` types could introduce new failure modes.

**Key insights**  
Always validate configuration nesting paths when adapting models with multi-modal or composite architectures. The fix demonstrates the importance of placing model-specific logic in dedicated adapters rather than generic parsing functions. Developers should ensure that any existing configurations using `ragged_attention` are updated to use `sliding_window` directly.

---

## 4. [[Doc]: update paths for Offline/Online/Others example sections](https://github.com/vllm-project/vllm/pull/33494)


### Base Information

- **PR Number:** #33494
- **Author:** [soyr-redhat](https://github.com/soyr-redhat)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-01 19:56:53
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33494/files) (1):**
  - `docs/examples/README.md`

### Summary

**What changed and why**  
The PR updates three bullet points in `docs/examples/README.md` from plain text to Markdown hyperlinks, restoring navigation to the correct `examples/` subdirectories. This fixes broken links that previously caused 404 errors, ensuring users can directly access example categories.

**Technical impact**  
This change improves documentation usability by enabling clickable navigation from the examples overview to specific example directories. It aligns the documentation structure with the actual repository layout without altering any functional code or build processes.

**Potential risks**  
The relative paths (`../../examples/...`) assume the documentation is viewed from the correct base URL in the rendered site; if the docs are viewed in a different context (e.g., raw GitHub view), links may still break. There's also a risk of future directory restructuring breaking these links again.

**Key insights**  
Always verify relative paths from the documentation file's location to the target directories. Consider adding automated link validation in CI/CD to prevent regressions. The fix is minimal and focused, directly addressing the reported issue without unnecessary changes.

---

## 5. [[Doc] add missing model entries in supported_models.md](https://github.com/vllm-project/vllm/pull/33220)


### Base Information

- **PR Number:** #33220
- **Author:** [pacoxu](https://github.com/pacoxu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-01 19:37:25
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33220/files) (1):**
  - `docs/models/supported_models.md`

### Summary

**What changed and why**  
This PR updates the `supported_models.md` documentation to add nine missing model architectures (e.g., `CwmForCausalLM`, `Glm4MoeLiteForCausalLM`) and corrects a typo (`ExaoneMoeCausalLM` → `ExaoneMoEForCausalLM`). The changes synchronize the documentation with the model registry in `registry.py`, ensuring accuracy and completeness.

**Technical impact**  
The update improves documentation accuracy, helping users identify supported models and their capabilities (e.g., continuous batching, prefix caching). No functional code changes are made, so system behavior remains unaffected. However, it ensures developers and users have a reliable reference for model compatibility.

**Potential risks**  
If the documentation now includes models that are not fully implemented or tested in the codebase, users may encounter unexpected errors when attempting to use them. Additionally, inconsistencies between the documented support (e.g., checkmarks for features) and actual implementation could lead to confusion.

**Key insights**  
Always verify documentation updates against the source code (like `registry.py`) to prevent discrepancies. Consider adding automated checks to keep documentation in sync with the registry. For future PRs, include a brief note on whether the added models have been tested end-to-end to ensure reliability.

---

## 6. [[Bugfix] GLM-4 tool parser: incremental string streaming](https://github.com/vllm-project/vllm/pull/33218)


### Base Information

- **PR Number:** #33218
- **Author:** [QwertyJack](https://github.com/QwertyJack)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-01 19:13:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33218/files) (2):**
  - `tests/tool_parsers/test_glm4_moe_tool_parser.py`
  - `vllm/tool_parsers/glm4_moe_tool_parser.py`

### Summary

**What changed and why**  
The PR fixes a streaming issue where long string parameters in GLM-4 tool calls were fully buffered before emission, causing multi-second delays. The solution rewrites `extract_tool_calls_streaming()` with a state machine that incrementally streams string-type values as they arrive, while buffering short non-string types (integers, booleans) until complete.

**Technical impact**  
This changes the parser from a batch-oriented approach to a streaming-first architecture. String values are now emitted character-by-character, and tool names are sent immediately upon parsing. The state machine handles partial XML tags at buffer boundaries and ensures proper JSON escaping of streamed content, improving real-time responsiveness for long content like code files.

**Potential risks**  
The stateful parser introduces complexity that could lead to edge-case bugs, such as incomplete state resets between tool calls or malformed XML fragments. JSON escaping during incremental streaming must be rigorously validated to avoid corruption. Additionally, the reliance on tool schemas for type detection means string streaming may fail if schemas are unavailable or incorrect.

**Key insights**  
Developers should verify that all tool schemas are correctly defined to enable string-type detection. The new state machine requires thorough testing with varied input patterns, including parallel tool calls and mixed data types. Monitor performance to ensure the incremental overhead doesn’t negate streaming benefits for shorter content.

---

## 7. [[Nightly CI] Remove CT Model](https://github.com/vllm-project/vllm/pull/33530)


### Base Information

- **PR Number:** #33530
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-01 19:09:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33530/files) (1):**
  - `tests/weight_loading/models.txt`

### Summary

**What changed and why**  
Removed the `nm-testing/TinyLlama-1.1B-Chat-v1.0-actorder-group` model entry from the weight loading test configuration file. This change was made because the model was removed from the Hugging Face Hub, which was causing test failures in the nightly CI pipeline.

**Technical impact**  
The update eliminates a failing test case that depended on a now-unavailable external resource. This ensures the nightly CI runs successfully without being blocked by missing model weights, maintaining pipeline reliability.

**Potential risks**  
If the model is restored to the Hub or if other tests depend on this specific model configuration, the removal could mask underlying issues. Additionally, there is a risk that similar external dependencies in the test suite may cause future intermittent failures.

**Key insights**  
Regularly audit test dependencies on external resources to prevent CI instability. Consider adding validation or fallback mechanisms for tests that rely on third-party assets, and document the rationale for removing test entries to aid future debugging.

---

## 8. [[Models] Step-3.5-Flash](https://github.com/vllm-project/vllm/pull/33523)


### Base Information

- **PR Number:** #33523
- **Author:** [csy0225](https://github.com/csy0225)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-02-01 18:21:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33523/files) (18):**
  - `docs/models/supported_models.md`
  - `tests/kernels/core/test_activation.py`
  - `tests/models/registry.py`
  - `vllm/config/speculative.py`
  - `vllm/model_executor/layers/activation.py`
  - `vllm/model_executor/layers/fused_moe/deep_gemm_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/utils.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/step3p5.py`
  - `vllm/model_executor/models/step3p5_mtp.py`
  - `vllm/reasoning/__init__.py`
  - `vllm/reasoning/step3p5_reasoning_parser.py`
  - `vllm/tool_parsers/__init__.py`
  - `vllm/tool_parsers/step3p5_tool_parser.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/step3p5.py`

### Summary

**What changed and why**  
This PR adds support for the Step-3.5-Flash model to the vLLM codebase. The changes include a new model implementation (`Step3p5ForCausalLM`), a corresponding Multi-Token Predictor (MTP) variant, a custom SwiGLU activation with clamping, a reasoning parser, a tool parser, and configuration updates. The purpose is to enable inference and speculative decoding for this new model architecture.

**Technical impact**  
The integration introduces a new model family with specialized components: a Triton kernel for the SwiGLU activation with limit clamping, support for mixed sliding-window and full attention layers, and MTP-based speculative decoding. The model leverages existing fused MoE and attention layers but adds new activation functions and configuration handling. The changes extend the model registry, config system, and reasoning/tool parsing pipelines.

**Potential risks**  
The custom Triton kernel for `swiglustep_and_mul` may have device compatibility or performance issues on non-NVIDIA platforms. The model’s complex layer types (sliding vs. full attention) and conditional rope usage increase configuration complexity and risk of misalignment. The MTP integration assumes specific weight naming and stacking conventions that could break if the upstream model changes. The reasoning parser’s newline trimming logic is fragile and may incorrectly handle edge cases in streaming.

**Key insights**  
Developers should verify that the SwiGLU limit clamping is correctly applied per layer via `config.swiglu_limits_shared`. The MTP weight loading logic relies on expert parameter mapping; ensure any future MoE changes preserve these mappings. The reasoning parser’s stateful handling of newlines requires careful testing in streaming scenarios. Consider adding unit tests for the new activation kernel across different dtypes and hardware.

---

## 9. [[Fix] prefix cache hit rate == 0 bug with gpt-oss style models](https://github.com/vllm-project/vllm/pull/33524)


### Base Information

- **PR Number:** #33524
- **Author:** [ivanium](https://github.com/ivanium)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-02-01 17:59:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33524/files) (2):**
  - `tests/v1/core/test_prefix_caching.py`
  - `vllm/v1/core/kv_cache_coordinator.py`

### Summary

**What changed and why**  
This PR fixes a cache hit rate bug for GPT-OSS style hybrid models (1 Full Attention group + 1 Sliding Window Attention group) by adding a special case to avoid unnecessary while-loop iterations. The fix prevents EAGLE spiral block dropping in these simple hybrid models and improves efficiency by eliminating redundant convergence checks.

**Technical impact**  
The changes modify the cache coordination logic to detect simple hybrid models and exit the convergence loop after one iteration. This ensures consistent cache hit length across attention groups and prevents the EAGLE system from incorrectly dropping blocks. The test suite is expanded with comprehensive test cases for both regular and EAGLE-enabled scenarios.

**Potential risks**  
The fix only addresses simple hybrid models with exactly two attention groups where the first is Full Attention. More complex hybrid models with multiple attention groups still experience the EAGLE spiral block drop issue. The implementation assumes Full Attention is always first when present, which could break if model architectures change.

**Key insights**  
This is a targeted workaround rather than a complete solution. Developers should note that the fundamental issue with non-downward-closed attention types (SWA, Mamba) remains unresolved. The expanded test coverage is valuable for preventing regressions, but the code contains a FIXME comment indicating the need for more fundamental changes when complex hybrid models are introduced.

---

## 10. [Add unpermute-aware fused MoE LoRA path](https://github.com/vllm-project/vllm/pull/32655)


### Base Information

- **PR Number:** #32655
- **Author:** [RunkaiTao](https://github.com/RunkaiTao)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-01 17:46:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32655/files) (6):**
  - `benchmarks/kernels/benchmark_lora.py`
  - `tests/lora/test_fused_moe_lora_kernel.py`
  - `vllm/lora/layers/fused_moe.py`
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`
  - `vllm/lora/punica_wrapper/punica_base.py`
  - `vllm/lora/punica_wrapper/punica_gpu.py`

### Summary

**What changed and why**  
This PR enhances the fused MoE LoRA implementation by adding an unpermute-aware execution path that skips the permutation kernel for small-batch scenarios with many experts. It introduces a fallback path using naive token-wise block assignment when `num_tokens * top_k * SPARSITY_FACTOR <= num_experts * max_loras`, improving performance for models with large expert counts.

**Technical impact**  
The changes modify the fused MoE LoRA kernel to conditionally bypass the `moe_lora_align_block_size` kernel and its associated tensor permutations. Instead, a simplified data flow uses flattened `expert_ids` and optional `None` for `sorted_token_ids` and `num_tokens_post_padded`. This reduces overhead for sparse expert activation patterns, leading to measurable throughput gains (15.2% benchmark duration speedup, 18% OTPS improvement).

**Potential risks**  
The heuristic (`SPARSITY_FACTOR = 8`) for selecting the naive path may not generalize across all workloads, potentially causing suboptimal kernel selection. The introduction of conditional `None` tensors increases code complexity and requires careful handling in downstream functions (e.g., stride calculations). Edge cases with mixed tensor states (some `None`, some not) could lead to runtime errors if not uniformly validated.

**Key insights**  
The optimization effectively trades permutation overhead for direct indexing in sparse regimes, demonstrating significant performance gains. Developers should ensure the heuristic threshold is calibrated for target deployment scenarios. The changes necessitate robust null-checking in all dependent functions and clear documentation of the two execution modes to maintain code maintainability.

---

## 11. [[ModelRunner V2] Support spec decode with structured outputs](https://github.com/vllm-project/vllm/pull/33374)


### Base Information

- **PR Number:** #33374
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-01 16:20:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33374/files) (3):**
  - `vllm/v1/worker/gpu/input_batch.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/utils.py`

### Summary

**What changed and why**  
This PR adds support for structured outputs during speculative decoding. It introduces a `DraftTokensHandler` to manage draft token propagation from the model runner back to the scheduler when structured outputs (e.g., grammar constraints) are active, ensuring draft tokens are validated against output specifications.

**Technical impact**  
The changes extend the speculative decoding pipeline to handle structured output requests by adding a flag (`has_structured_output_reqs`) to `InputBatch` and a new handler that asynchronously transfers draft tokens to the scheduler for validation. This maintains compatibility with existing workflows while enabling grammar-aware speculative decoding.

**Potential risks**  
If `has_structured_output_reqs` is incorrectly set, draft tokens may not be validated when needed, leading to invalid outputs. The asynchronous copy relies on CUDA stream synchronization; improper event handling could cause race conditions or stale token data. The handler’s state management assumes batch-level consistency—partial updates could leak tokens across batches.

**Key insights**  
The implementation cleanly separates concerns by delegating draft token management to a dedicated handler. Ensure `has_structured_output_reqs` is accurately populated from `scheduler_output`. Verify the async copy stream synchronization in `get_draft_tokens()` to prevent data races, especially under high load or frequent batch changes.

---

## 12. [[ModelRunner V2] Misc minor simplifications and optimizations](https://github.com/vllm-project/vllm/pull/33467)


### Base Information

- **PR Number:** #33467
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-01 14:17:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33467/files) (21):**
  - `vllm/v1/worker/gpu/async_utils.py`
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/block_table.py`
  - `vllm/v1/worker/gpu/buffer_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/dp_utils.py`
  - `vllm/v1/worker/gpu/kv_connector.py`
  - `vllm/v1/worker/gpu/lora_utils.py`
  - `vllm/v1/worker/gpu/mm/encoder_runner.py`
  - `vllm/v1/worker/gpu/mm/mrope_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/sample/logit_bias.py`
  - `vllm/v1/worker/gpu/sample/logprob.py`
  - `vllm/v1/worker/gpu/sample/min_p.py`
  - `vllm/v1/worker/gpu/sample/prompt_logprob.py`
  - `vllm/v1/worker/gpu/sample/sampler.py`
  - `vllm/v1/worker/gpu/sample/states.py`
  - `vllm/v1/worker/gpu/spec_decode/__init__.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py`
  - `vllm/v1/worker/gpu/structured_outputs.py`

### Summary

**What changed and why**  
This PR introduces minor simplifications and optimizations across the ModelRunner V2 codebase. Changes include removing unnecessary line breaks in function signatures, using more efficient Python constructs (like `zip`, `partial`, and `fromiter`), simplifying conditional logic, and consolidating imports. The goal is to improve code readability and maintainability while reducing line count.

**Technical impact**  
The modifications are primarily syntactic and do not alter core functionality. They streamline code by eliminating redundant parentheses, optimizing data structure creation, and using Python idioms more effectively. Some changes may offer marginal performance improvements (e.g., avoiding intermediate list creation with `fromiter`), but the overall system behavior remains unchanged.

**Potential risks**  
- Over-condensing code (e.g., removing line breaks in multi-parameter functions) could reduce readability for some developers.  
- The use of `partial` in `buffer_utils.py` assumes consistent `max_concurrency` values, which could be problematic if future changes require per-buffer configuration.  
- Simplified conditionals (e.g., in `sampler.py` and `states.py`) might obscure edge cases if not thoroughly tested.

**Key insights**  
- The PR demonstrates a focus on code hygiene and minor performance tuning.  
- Developers should ensure that condensed code remains clear and that any logic simplifications are covered by existing tests.  
- Future changes should maintain the balance between conciseness and explicitness, especially in critical paths like sampling and attention metadata handling.

---

## 13. [[Misc] skip target model mm emb in draft proposal step when draft is text-only](https://github.com/vllm-project/vllm/pull/33437)


### Base Information

- **PR Number:** #33437
- **Author:** [kkt-cohere](https://github.com/kkt-cohere)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-01 13:13:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33437/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The change modifies a conditional check in the draft proposal step to skip gathering multimodal embeddings when the draft model doesn't support multimodal inputs. Previously, embeddings were gathered based solely on the target model's capabilities, but they were unused when the draft model was text-only, causing unnecessary computation and potential crashes.

**Technical impact**  
This optimization reduces computational overhead by avoiding the collection of unused multimodal embeddings in text-only draft scenarios. It ensures the system only performs necessary operations, aligning resource usage with actual functional requirements and preventing cache misses that could crash the model in production.

**Potential risks**  
If the `supports_mm_inputs` flag is incorrectly configured for either model, it could lead to missing embeddings when needed or unnecessary overhead. The change assumes the flag accurately reflects model capabilities, so any misalignment might cause functional regressions in multimodal workflows.

**Key insights**  
Developers should verify that both target and draft models have correct `supports_mm_inputs` configurations. This change highlights the importance of conditional logic based on both models' capabilities in multi-model systems, and similar patterns should be reviewed elsewhere in the codebase to ensure consistency.

---

## 14. [Fix DeepSeek V2 RoPE initialization error](https://github.com/vllm-project/vllm/pull/33501)


### Base Information

- **PR Number:** #33501
- **Author:** [catswe](https://github.com/catswe)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-01 13:00:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33501/files) (1):**
  - `vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py`

### Summary

**What changed and why**  
The PR removes explicit device specification (`device=current_platform.device_type`) from two `torch.arange` calls in the DeepSeek V2 RoPE initialization code. This fixes a TPU-specific error where PyTorch/XLA does not support the `aten::empty.memory_format` operator when a non-XLA device is requested during tensor creation.

**Technical impact**  
By omitting the device argument, PyTorch will default to the current device context (likely CPU during initialization), avoiding the XLA backend error. This change ensures RoPE cache computation works correctly on TPUs while maintaining compatibility with other hardware platforms where device specification is handled implicitly.

**Potential risks**  
If the tensor creation occurs on an unintended device (e.g., CPU instead of the target accelerator), it could lead to performance overhead from device transfers or incorrect behavior if the tensor is expected to reside on a specific device. However, since this is initialization code and the tensors are likely moved to the correct device later, the risk is minimal.

**Key insights**  
The fix highlights a common PyTorch/XLA compatibility issue: explicit device assignment can break on TPUs. Developers should rely on PyTorch's default device placement in such contexts or use XLA-specific utilities. This change is a minimal, targeted fix that resolves the immediate error without broader refactoring.

---

## 15. [Add MoE config for Super B200 TP2](https://github.com/vllm-project/vllm/pull/33510)


### Base Information

- **PR Number:** #33510
- **Author:** [shaharmor98](https://github.com/shaharmor98)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-01 10:48:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33510/files) (1):**
  - `vllm/model_executor/layers/fused_moe/configs/E=512,N=1344,device_name=NVIDIA_B200.json`

### Summary

**What changed and why**  
A new JSON configuration file (`E=512,N=1344,device_name=NVIDIA_B200.json`) was added to provide optimized kernel parameters for Mixture-of-Experts (MoE) operations on the NVIDIA B200 GPU with tensor parallelism size 2. This eliminates a performance warning and improves inference throughput by replacing default MoE configurations with tuned parameters for specific batch sizes.

**Technical impact**  
The configuration enables vLLM to use hardware-specific optimizations for fused MoE kernels on B200, directly impacting kernel selection and execution efficiency. This change is additive and only activates when the exact model configuration (E=512, N=1344) and hardware are detected, otherwise falling back to defaults.

**Potential risks**  
The configuration is tightly coupled to a specific GPU architecture (B200) and model parameters; using it on other hardware or model configurations may cause performance degradation or errors. Additionally, the tuning was performed for discrete batch sizes—intermediate batch sizes may use suboptimal configurations from the nearest match.

**Key insights**  
This is a performance-tuning change that demonstrates significant throughput gains (up to 24.9% at higher batch sizes). Developers should ensure similar configurations are added for other GPU architectures or model variants to avoid performance warnings. Consider automating configuration generation for new hardware/model combinations to maintain scalability.

---

## 16. [[BUGFIX] Fix hipErrorIllegalState in Qwen3-Omni during startup profiling allow inference Omni on ROCM](https://github.com/vllm-project/vllm/pull/33077)


### Base Information

- **PR Number:** #33077
- **Author:** [JartX](https://github.com/JartX)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-01 05:36:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33077/files) (1):**
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
The PR addresses a critical crash on AMD (ROCm) hardware during Qwen3-Omni model startup profiling. Specifically, a `hipErrorIllegalState` occurs when `torch.repeat_interleave` is executed on the GPU during the `cu_seqlens` calculation in `Qwen3Omni_VisionTransformer.forward`. The fix implements a fallback mechanism: it attempts the original GPU operation but catches any `RuntimeError` and switches to a CPU-based vectorized approach using `searchsorted` and `cumsum` for the same computation.

**Technical impact**  
This change introduces a conditional execution path that moves the problematic tensor operations to the CPU only when the GPU operation fails. Since the tensors involved (`grid_thw`) are small, performance impact is negligible. The solution maintains backward compatibility for environments where the original GPU operation works correctly (e.g., NVIDIA CUDA) while providing a stable workaround for ROCm.

**Potential risks**  
The fallback relies on catching a generic `RuntimeError`, which could inadvertently mask other unrelated runtime issues. Additionally, the CPU-based path may introduce slight latency if triggered frequently, though this is mitigated by the small tensor size. There is also a risk that the `searchsorted` approach could behave differently in edge cases (e.g., with empty or malformed `grid_thw` tensors).

**Key insights**  
This is a targeted workaround for a ROCm driver/backend instability, not a fundamental algorithm change. Developers should monitor for similar issues with other dynamic shape operations on AMD hardware. Consider adding more specific error handling or environment detection (e.g., checking for ROCm) to avoid unnecessary try-catch overhead. The fix highlights the importance of hardware-specific testing for cross-platform ML frameworks.

---

## 17. [[W8A8 Block Linear Refactor][1/N] Keep all quantization types into `QuantFP8` class.](https://github.com/vllm-project/vllm/pull/33047)


### Base Information

- **PR Number:** #33047
- **Author:** [maralbahari](https://github.com/maralbahari)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-01 01:28:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33047/files) (3):**
  - `vllm/_aiter_ops.py`
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`

### Summary

**What changed and why**  
This PR refactors FP8 quantization logic by consolidating group quantization methods into the `QuantFP8` class. The changes ensure that all quantization types (per-tensor, per-token, per-group) are handled through a unified interface, particularly focusing on ROCm/AIT backend integration and DeepGemm support.

**Technical impact**  
The refactor centralizes quantization dispatch logic, making `QuantFP8` the single entry point for all FP8 quantization operations. This improves code organization and enables consistent handling across different backends (CUDA, HIP, Triton). The changes also introduce better integration with DeepGemm's UE8M0 format and ensure proper fallback mechanisms.

**Potential risks**  
The consolidation of quantization paths could introduce subtle behavioral differences if not all edge cases are properly handled. The conditional logic for DeepGemm support and UE8M0 format adds complexity that may cause issues on certain hardware configurations. The removal of direct function calls in favor of `input_quant_op` dispatch could mask errors if the quantization operator isn't properly initialized.

**Key insights**  
Developers should verify that all quantization paths (per-tensor, per-token, per-group) work correctly across different hardware platforms. The `use_ue8m0` parameter now defaults to `is_deep_gemm_e8m0_used()` when `None`, which changes the default behavior. The HIP backend now includes explicit Triton support for group quantization and better AIT integration.

---

## 18. [[Redo] #33110 with threading limit](https://github.com/vllm-project/vllm/pull/33502)


### Base Information

- **PR Number:** #33502
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-01 01:18:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33502/files) (2):**
  - `vllm/utils/torch_utils.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This PR reimplements threading limit handling to fix a hanging startup issue. The main changes are: 1) Enhanced `set_default_torch_num_threads` to read `OMP_NUM_THREADS` environment variable with proper error handling, and 2) Moved multimodal encoder cache size calculation inside the threading context during initialization to match inference behavior.

**Technical impact**  
The threading context manager now properly handles default thread configuration via environment variables and ensures cleanup with a try-finally block. The multimodal encoder cache validation is now performed during input processing, preventing oversized multimodal inputs from exceeding pre-allocated cache limits.

**Potential risks**  
The `OMP_NUM_THREADS` environment variable parsing could still fail if set to non-integer values, though this is now caught with a warning. The encoder cache validation occurs late in the processing pipeline, which might cause wasted computation before validation failures. The threading changes assume `OMP_NUM_THREADS` is the appropriate source for thread limits across all deployment scenarios.

**Key insights**  
The fix ensures consistent threading behavior between startup and inference phases. Developers should ensure `OMP_NUM_THREADS` is properly set in deployment environments. The encoder cache validation provides important guardrails for multimodal inputs but could be moved earlier in the pipeline for better efficiency. The try-finally pattern in the threading context manager improves robustness.

---

