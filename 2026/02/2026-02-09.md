# vLLM Merged PR Report

**Report Date:** 2026-02-09 PST

**Total Merged PRs:** 36

---

## 1. [Revert #34208](https://github.com/vllm-project/vllm/pull/34216)


### Base Information

- **PR Number:** #34216
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-09 23:59:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34216/files) (2):**
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
This PR reverts commit f69b903b4c70716224b3936cb8503e562e25388e, which had moved the `--trust-remote-code` argument from `vllm/benchmarks/serve.py` to `vllm/benchmarks/datasets.py`. The revert is necessary because the original change broke the `vllm serve` command functionality.

**Technical impact**  
The `--trust-remote-code` argument is restored to its original location in the serve module's CLI argument parser, removing it from the datasets module. This ensures the argument is properly available when running the serve command, maintaining compatibility with the existing command-line interface.

**Potential risks**  
The PR description lacks specific details about how the original commit broke `vllm serve`, making it difficult to assess the full impact. Without a test plan or results, there's risk that other dependencies on the moved argument might be overlooked. The revert might also reintroduce any issues that the original commit was attempting to fix.

**Key insights**  
This appears to be a corrective action for a regression introduced in a previous change. Developers should ensure CLI argument placement aligns with actual usage patterns in the codebase. The missing test documentation reduces confidence in the change; future PRs should include proper test plans and results to validate functionality.

---

## 2. [[Perf] Optimize detokenizer python logic](https://github.com/vllm-project/vllm/pull/32975)


### Base Information

- **PR Number:** #32975
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-09 23:54:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32975/files) (2):**
  - `vllm/v1/engine/detokenizer.py`
  - `vllm/v1/engine/output_processor.py`

### Summary

**What changed and why**  
The changes optimize detokenizer performance by adding a `num_output_tokens()` method to avoid repeated slicing operations when accessing `len(self.output_token_ids)`. This addresses inefficiencies in `SlowIncrementalDetokenizer` where the `output_token_ids` property performs slicing on each access. The optimization is applied consistently across both detokenizer implementations and their usage in output processing.

**Technical impact**  
This change improves performance by eliminating unnecessary list slicing operations in hot code paths, particularly during token-by-token processing and stop condition evaluation. The new method provides direct access to token counts without creating intermediate list slices, reducing memory overhead and computational cost in iterative detokenization loops.

**Potential risks**  
The main risk is potential off-by-one errors in the `SlowIncrementalDetokenizer.num_output_tokens()` implementation where `prompt_len` subtraction could produce negative values if `prompt_len > len(self.token_ids)`. Additionally, the change assumes `prompt_len` is properly initialized and maintained throughout the detokenizer's lifecycle.

**Key insights**  
The optimization demonstrates good practice by replacing expensive property access with efficient method calls. Developers should ensure `prompt_len` is always valid in `SlowIncrementalDetokenizer` and consider similar patterns elsewhere in the codebase where property computations might be expensive. The removal of the TODO comment suggests this addresses a known performance concern.

---

## 3. [[BugFix] Avoid prefix cache hit in the same schedule step for mamba layers](https://github.com/vllm-project/vllm/pull/29387)


### Base Information

- **PR Number:** #29387
- **Author:** [heheda12345](https://github.com/heheda12345)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-02-09 23:41:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29387/files) (6):**
  - `tests/models/language/generation/test_hybrid.py`
  - `tests/v1/core/test_prefix_caching.py`
  - `vllm/v1/core/kv_cache_coordinator.py`
  - `vllm/v1/core/kv_cache_manager.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/core/single_type_kv_cache_manager.py`

### Summary

**What changed and why**  
The fix addresses a prefix caching issue in Mamba layers where blocks computed within the same scheduling step were incorrectly treated as cache hits. This caused state dependency errors because later requests would depend on states that hadn't been computed yet. The solution delays such requests to the next scheduling step rather than treating them as cache misses to avoid redundant computation.

**Technical impact**  
The changes introduce step-aware caching for Mamba layers by tracking blocks cached within the current step (`cached_blocks_this_step`). When a request needs a block computed in the same step, it's deferred to the next step by making allocation appear impossible. The scheduler now calls `new_step_starts()` to reset this tracking between steps, ensuring correct state dependencies.

**Potential risks**  
Deferring requests to the next step could impact throughput in workloads with many similar prefixes scheduled together. The current implementation stops scheduling additional waiting requests once a deferred request is encountered, which might be suboptimal for P/D prefill. Edge cases around speculative decoding modes ("align" vs others) need careful validation.

**Key insights**  
This fix is essential for Mamba's correctness due to its stateful nature. Developers should note that the solution prioritizes correctness over latency for edge cases. The TODO indicates future work needed for P/D prefill optimization. The added test validates the fix using a real Mamba model with controlled scheduling.

---

## 4. [[Frontend][CI]  Consolidate instrumentator entrypoints](https://github.com/vllm-project/vllm/pull/34123)


### Base Information

- **PR Number:** #34123
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-09 23:30:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34123/files) (16):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/entrypoints.yaml`
  - `tests/entrypoints/instrumentator/test_basic.py`
  - `tests/entrypoints/instrumentator/test_optional_middleware.py`
  - `tests/entrypoints/instrumentator/test_orca_metrics.py`
  - `tests/entrypoints/instrumentator/test_sleep.py`
  - `tests/entrypoints/sleep/__init__.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/basic/__init__.py`
  - `vllm/entrypoints/sagemaker/api_router.py`
  - `vllm/entrypoints/serve/__init__.py`
  - `vllm/entrypoints/serve/instrumentator/__init__.py`
  - `vllm/entrypoints/serve/instrumentator/basic.py`
  - `vllm/entrypoints/serve/instrumentator/health.py`
  - `vllm/entrypoints/serve/instrumentator/server_info.py`

### Summary

**What changed and why**  
This PR consolidates instrumentator-related entrypoints by moving basic API routers from `vllm/entrypoints/openai/basic/` to `vllm/entrypoints/serve/instrumentator/`. It attaches disaggregation, RLHF, and elastic endpoint routers specifically for generative models, and aggregates instrumentator tests. The changes address code organization feedback from a previous PR review.

**Technical impact**  
The refactoring centralizes instrumentator functionality under a unified module, improving code organization and maintainability. Router registration logic is now more modular, with generative models receiving additional specialized routes. CI test configurations are updated to reflect the new structure, removing references to the old `sleep` test directory.

**Potential risks**  
The removal of the `sleep` test directory from CI configurations could inadvertently drop test coverage if those tests weren't migrated. Conditional router attachment based on model type (generative vs. non-generative) introduces potential inconsistency in available endpoints across different serving scenarios. The refactoring touches multiple router registration points, increasing the risk of import errors or missing dependencies.

**Key insights**  
Developers should verify that all instrumentator functionality remains accessible after these changes, particularly for non-generative models. The CI pipeline updates should be cross-checked to ensure no test coverage was lost during the migration. Future router additions should follow the new consolidated pattern established in `vllm/entrypoints/serve/instrumentator/__init__.py`.

---

## 5. [[Bugfix] Sort hf_weights_files in fastsafetensors_weights_iterator to match #33491](https://github.com/vllm-project/vllm/pull/34190)


### Base Information

- **PR Number:** #34190
- **Author:** [jaim12005](https://github.com/jaim12005)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-09 23:06:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34190/files) (1):**
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
Added natural sorting to `hf_weights_files` in `fastsafetensors_weights_iterator` to ensure deterministic file ordering before splitting across ranks. This matches the fix previously applied to `safetensors_weights_iterator` in PR #33491, preventing NCCL crashes in multi-node tensor parallel deployments.

**Technical impact**  
Ensures consistent shard assignment across distributed nodes by guaranteeing identical file list ordering regardless of filesystem enumeration differences. This eliminates collective size mismatches during NCCL operations when loading model weights.

**Potential risks**  
Minimal risk as this replicates an already-proven fix from the non-fast path. However, any existing deployments relying on unsorted behavior for custom sharding schemes could theoretically be affected, though such usage would be undocumented and unsupported.

**Key insights**  
Filesystem enumeration order is non-deterministic across nodes—always sort distributed file lists before partitioning. This bug highlights the importance of maintaining consistency between parallel implementations (fast vs standard safetensors paths). Consider adding a shared helper function to avoid future divergence.

---

## 6. [[responsesAPI] fix simpleContext streaming output_messages](https://github.com/vllm-project/vllm/pull/34188)


### Base Information

- **PR Number:** #34188
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-09 22:53:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34188/files) (3):**
  - `tests/entrypoints/test_context.py`
  - `vllm/benchmarks/datasets.py`
  - `vllm/entrypoints/openai/responses/context.py`

### Summary

**What changed and why**  
The PR fixes a bug in SimpleContext where streaming responses produced an array of output_messages (one per token delta) instead of consolidating them into a single message. This made debugging harder and caused inconsistent behavior between streaming and non-streaming modes. The fix changes the `output_messages` property to accumulate text and tokens across streaming deltas and return them as a single consolidated message.

**Technical impact**  
The `output_messages` property is now computed dynamically instead of stored as a mutable list. This ensures streaming responses produce the same output structure as non-streaming responses, improving consistency across API modes. The internal state management shifts from appending to a list to accumulating text and token IDs in `_accumulated_text` and `_accumulated_token_ids` attributes.

**Potential risks**  
If `_accumulated_text` or `_accumulated_token_ids` attributes are not properly initialized or maintained, the property could return incorrect data or raise AttributeErrors. The change assumes these accumulation fields exist and are updated in `append_output`, which isn't shown in the diff but is critical for correctness. There's also a risk of breaking existing code that might have relied on the per-delta array structure.

**Key insights**  
The fix elegantly solves the consistency problem by making `output_messages` a computed property. Developers should verify that `_accumulated_text` and `_accumulated_token_ids` are properly initialized and updated in the `append_output` method. The comprehensive test suite added provides good coverage for both streaming and non-streaming scenarios, which should be maintained as the code evolves.

---

## 7. [[Bugfix] Add `--trust-remote-code` to dataset bench args](https://github.com/vllm-project/vllm/pull/34208)


### Base Information

- **PR Number:** #34208
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-09 22:37:50
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34208/files) (2):**
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
The PR moves the `--trust-remote-code` command-line argument from the `serve.py` module's argument parser to the `datasets.py` module's `add_dataset_parser` function. This change was made to fix failing example tests, as a previous change (#32300) now requires the `trust_remote_code` argument when loading datasets, but the spec decode examples were not passing it.

**Technical impact**  
This refactor centralizes the dataset-related argument (`--trust-remote-code`) within the dedicated dataset argument parser. It ensures that any benchmark or example script using the dataset utilities will automatically have access to this flag, preventing the specific loading failures encountered. The system behavior for serving arguments is unchanged for non-dataset use cases.

**Potential risks**  
If other parts of the codebase (outside the dataset utilities) were relying on `--trust-remote-code` being defined in the serve parser, they may now lack access to this flag, potentially causing errors. There is also a minor risk of argument namespace conflicts if the flag is defined elsewhere, but this is unlikely given the structured parser groups.

**Key insights**  
The fix correctly addresses the root cause by ensuring the required argument is available where it's needed. Developers should verify that no other benchmark or test scripts implicitly depended on the flag being in the serve parser. This change promotes better separation of concerns by grouping dataset-specific arguments together.

---

## 8. [[Bugfix] Fix DP Attention Padding in Dummy Run](https://github.com/vllm-project/vllm/pull/34187)


### Base Information

- **PR Number:** #34187
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-09 21:29:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34187/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The change adds a `num_tokens_padded` parameter when calling `_build_attention_metadata` during dummy runs in data parallel (DP) mode. This ensures the attention metadata builder receives padded token counts when attention padding is enabled, fixing a mismatch that caused crashes when `num_decode_tokens` was not divisible by `num_requests`.

**Technical impact**  
This resolves a specific crash scenario in DP where dummy runs with padded attention metadata incorrectly used unpadded token counts. The fix aligns token and request counts in attention metadata construction, maintaining synchronization across DP workers without altering core attention logic or model outputs.

**Potential risks**  
If `num_reqs > num_decode_tokens` is not supported by all attention backends (e.g., TRTLLM, FlashInfer), broader issues may persist. The fix assumes padded token counts are safe for metadata building, but edge cases in backend handling of imbalanced request/token ratios could still cause instability.

**Key insights**  
Always pass padded token counts when attention padding is active to prevent metadata mismatches. Developers should verify that all supported attention backends can handle cases where request counts exceed decode tokens, as this may require additional validation or padding logic in the future.

---

## 9. [[CI/Build] Relax `test_mcp_tool_call`](https://github.com/vllm-project/vllm/pull/34204)


### Base Information

- **PR Number:** #34204
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-09 21:18:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34204/files) (1):**
  - `tests/entrypoints/openai/responses/test_parsable_context.py`

### Summary

**What changed and why**  
The PR relaxes a flaky test assertion in `test_mcp_tool_call` by modifying string matching to accept two possible number formats ("56088" or "56,088") instead of only the former. This addresses a test failure where the output format varied, though the numerical result remained correct.

**Technical impact**  
The change makes the test more robust to formatting differences in model outputs, specifically regarding number formatting with or without thousands separators. This reduces false test failures without altering the core validation logic, as both formats represent the same numerical value.

**Potential risks**  
The relaxation could mask actual regressions if the model starts producing completely different numbers that coincidentally contain these substrings. There's also a risk that other formatting variations (like "56.088" or "56088.0") could still cause failures, potentially requiring further test adjustments.

**Key insights**  
When testing numerical outputs from LLMs, consider validating the semantic value rather than exact string formatting. For future similar cases, using regex patterns or numerical extraction would provide more robust validation. The fix appropriately addresses the immediate flakiness while maintaining test intent.

---

## 10. [[Doc] Update usage of `--limit-mm-per-prompt`](https://github.com/vllm-project/vllm/pull/34148)


### Base Information

- **PR Number:** #34148
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-09 21:12:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34148/files) (6):**
  - `docs/features/multimodal_inputs.md`
  - `docs/models/supported_models.md`
  - `examples/offline_inference/mistral-small.py`
  - `examples/online_serving/openai_chat_completion_client_for_multimodal.py`
  - `examples/pooling/classify/vision_classification_online.py`
  - `vllm/config/multimodal.py`

### Summary

**What changed and why**  
This PR updates documentation and examples to use the new shorthand format for the `--limit-mm-per-prompt` command-line argument. The changes replace JSON-style syntax (`'{"image":2}'`) with dot notation (`--limit-mm-per-prompt.image 2`) across multiple files, and clarify that `--language-model-only` is the preferred way to disable multimodal modules.

**Technical impact**  
The changes are purely documentation and example updates that reflect an existing CLI interface improvement. They ensure consistency between documentation and actual usage, making the command-line interface more intuitive by using standard CLI flag conventions instead of JSON strings.

**Potential risks**  
There's a minor risk that users following old documentation might continue using the JSON format, which may still be supported but could cause confusion. The PR doesn't update all possible documentation references, so some inconsistencies might remain if other files still use the old format.

**Key insights**  
Developers should adopt the new shorthand format in all new code and documentation. The `--language-model-only` flag is now the recommended approach for disabling multimodal modules instead of manually setting modality limits to zero. These changes improve CLI usability and align with standard command-line parsing conventions.

---

## 11. [[Bugfix][Core] Fix CPU memory leak from Request reference cycle in prefix caching](https://github.com/vllm-project/vllm/pull/34183)


### Base Information

- **PR Number:** #34183
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-09 21:03:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34183/files) (3):**
  - `tests/v1/core/test_async_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/request.py`

### Summary

**What changed and why**  
The fix addresses a CPU memory leak in multimodal inference by eliminating a reference cycle between a `Request` object and a `partial` function. Previously, `block_hasher` was bound to `self` via `functools.partial`, creating a circular reference (`Request` → `partial` → `Request`) that prevented immediate garbage collection. This was problematic for large multimodal requests containing `mm_features`. The solution stores the unbound `block_hasher` and introduces an `update_block_hashes()` method that explicitly passes `self` when needed.

**Technical impact**  
This change breaks the reference cycle, allowing the Python garbage collector to reclaim `Request` objects (and their potentially large `mm_features`) promptly after inference completes. It maintains the same functional behavior for prefix caching block hash computation. The `update_block_hashes()` method is now called consistently during request initialization, output token appending, and session updates.

**Potential risks**  
The refactored `_block_hasher` callable now requires the `Request` instance as an explicit argument. If any external code directly calls `_block_hasher`, it must be updated to pass the request. The test updates indicate existing internal usage patterns are already corrected. There's a minor risk if the `block_hasher` was `None` and `update_block_hashes()` is called, but the `if` check properly guards against this.

**Key insights**  
The primary insight is that using `partial` to bind methods can create subtle memory leaks in long-lived or high-memory objects. Explicitly passing `self` when needed is safer for garbage collection. Developers should be cautious of creating reference cycles in performance-critical paths, especially when objects hold large data. The fix is minimal and focused, demonstrating an effective pattern for breaking cycles while preserving API consistency.

---

## 12. [[ROCm][Bugfix] Resolve Dynamo tracing crash from amdsmi calls in on_gfx* arch detection](https://github.com/vllm-project/vllm/pull/34108)


### Base Information

- **PR Number:** #34108
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-09 20:50:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34108/files) (1):**
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
The fix replaces dynamic `amdsmi` queries in architecture detection functions with pre-computed module-level constants. This resolves a Dynamo tracing crash that occurred when `torch.compile` attempted to trace through FFI calls to `amdsmi_init()`, while preserving the original goal of avoiding early CUDA initialization for Ray worker compatibility.

**Technical impact**  
Architecture detection now occurs once at module import, storing results in plain Python booleans (`_ON_GFX9`, etc.). This eliminates tracing issues because Dynamo can natively handle constant returns, and it maintains the no‑early‑CUDA‑init behavior required by Ray. The change also centralizes GCN arch queries, reducing redundant calls and simplifying the codebase.

**Potential risks**  
If the GPU architecture changes at runtime (e.g., via hot‑swapping or dynamic device reconfiguration), the cached constants will become stale. Additionally, any fallback to `torch.cuda.get_device_properties()` in `_get_gcn_arch()` could still trigger CUDA initialization in edge cases where `amdsmi` fails, though this is unlikely.

**Key insights**  
Pre‑computing immutable values at import is an effective pattern for Dynamo compatibility. Developers should ensure that any similar detection logic used inside compiled regions also returns traceable types (e.g., constants, tensors). Verify that the fallback path does not inadvertently introduce CUDA initialization in target deployment environments.

---

## 13. [[Bugfix] Adopt `ChunkGatedDeltaRule` for Qwen3.5](https://github.com/vllm-project/vllm/pull/34198)


### Base Information

- **PR Number:** #34198
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-09 19:47:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34198/files) (1):**
  - `vllm/model_executor/models/qwen3_5.py`

### Summary

**What changed and why**  
The change adds `ChunkGatedDeltaRule` initialization to Qwen3.5's decoder layer. This fixes a bug where Qwen3.5 missed adopting a recently added component (`ChunkGatedDeltaRule` from PR #32846), causing errors because its `_forward_core` method inherits from Qwen3-Next which expects this component.

**Technical impact**  
This ensures Qwen3.5 models function correctly by providing the required `chunk_gated_delta_rule` attribute that the inherited forward method depends on. The change maintains consistency with the Qwen3-Next architecture and prevents runtime AttributeErrors during model execution.

**Potential risks**  
The initialization uses default parameters without configuration options, which could cause issues if Qwen3.5 requires different settings than Qwen3-Next. There's also a risk of duplicate layer name conflicts if the same prefix is used elsewhere in the compilation configuration.

**Key insights**  
This is a critical bugfix that prevents Qwen3.5 from failing at runtime. Developers should verify that the default `ChunkGatedDeltaRule()` configuration aligns with Qwen3.5's requirements and ensure no naming conflicts exist in the compilation config. Consider adding a test case to catch similar inheritance issues in the future.

---

## 14. [[LMCache] Token Base IPC API](https://github.com/vllm-project/vllm/pull/34175)


### Base Information

- **PR Number:** #34175
- **Author:** [Oasis-Git](https://github.com/Oasis-Git)
- **Merged By:** [ApostaC](https://github.com/ApostaC)
- **Merged time:** 2026-02-09 17:18:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34175/files) (2):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py`

### Summary

**What changed and why**  
This PR switches LMCache's multiprocess connector from hash-based to token-based mode for all operations (lookup, store, retrieve). It removes the `convert_block_hashes_to_bytes` helper and modifies `LoadStoreOp` to accept full `token_ids` with `start/end` ranges instead of block hashes. An `end_session` call is added to clean up request state on completion.

**Technical impact**  
The changes fundamentally alter how cache keys are constructed—from block-hash-derived keys to token-ID-based keys with positional ranges. This enables more granular prefix matching and aligns with token-based vLLM's semantics. The adapter now supports both modes via conditional logic, but hash mode is effectively deprecated as token mode becomes the default.

**Potential risks**  
- Mixed-mode usage could cause confusion if both `block_hashes` and `token_ids` are provided inadvertently.  
- The `end_session` cleanup must be robust to prevent memory leaks in long-running servers.  
- Token alignment logic (`aligned_end`) may truncate tokens incorrectly if chunk boundaries are misaligned.

**Key insights**  
- Developers must ensure token-based vLLM is used exclusively; hash mode is now legacy.  
- The `LoadStoreOp` dataclass changes require updates to any downstream code that instantiates it.  
- Testing should verify that `end_session` reliably releases cache resources to avoid accumulation.

---

## 15. [[structured output] validate unsupported json features first](https://github.com/vllm-project/vllm/pull/33233)


### Base Information

- **PR Number:** #33233
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [russellb](https://github.com/russellb)
- **Merged time:** 2026-02-09 15:49:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33233/files) (1):**
  - `vllm/v1/structured_output/backend_xgrammar.py`

### Summary

**What changed and why**  
The change reorders validation in `validate_xgrammar_grammar` to check for unsupported JSON features *before* attempting to compile the schema into a grammar. Previously, the xgrammar compilation attempt happened first, followed by the unsupported feature check.

**Technical impact**  
This improves error handling efficiency and clarity. By validating unsupported features first, the function fails fast with a more specific error message when the schema contains known incompatible features, rather than potentially allowing a generic compilation error from `xgr.Grammar.from_json_schema` to mask the root cause.

**Potential risks**  
If the `has_xgrammar_unsupported_json_features` function has false negatives (fails to detect some truly unsupported features), those cases will still reach the compilation step and could produce less informative errors. There's also a minor risk if the unsupported feature check itself has side effects or performance issues, though this is unlikely.

**Key insights**  
This is a defensive programming improvement that prioritizes user-friendly, specific error messages. Developers should ensure the unsupported feature detection function is comprehensive. The change is logically sound and follows the principle of early validation.

---

## 16. [[Bugfix][ROCm][GPT-OSS] Use old triton_kernels implementation on ROCm if the new API is not available](https://github.com/vllm-project/vllm/pull/34153)


### Base Information

- **PR Number:** #34153
- **Author:** [gshtras](https://github.com/gshtras)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-09 15:38:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34153/files) (1):**
  - `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`

### Summary

**What changed and why**  
This change adds fallback logic to use legacy triton_kernels implementations on ROCm platforms when the updated API (specifically `SparseMatrix` and `make_ragged_tensor_metadata`) is unavailable. The fix ensures GPT-OSS model support on ROCm by temporarily reverting to the older kernel interfaces until the ROCm triton_kernels are updated.

**Technical impact**  
The modifications introduce a platform-specific conditional import path (`use_legacy_triton_kernels` flag) and adjust three core functions (`legacy_routing_from_bitmatrix`, `legacy_routing`, `triton_kernel_fused_experts`, and `make_routing_data`) to switch between old and new API usage. This maintains backward compatibility without disrupting non-ROCm platforms, which continue to use the updated API.

**Potential risks**  
The fallback may introduce subtle behavioral differences between ROCm and other platforms if the legacy and new implementations are not functionally equivalent. Additionally, the warning log for ROCm could be missed in production, and the conditional logic increases code complexity, making future maintenance harder. There is also a risk that the legacy code path might not receive updates or optimizations.

**Key insights**  
Developers should treat this as a temporary workaround; the long-term solution requires updating the ROCm triton_kernels to match the new API. Ensure thorough testing on both ROCm and non-ROCm platforms to verify consistent behavior. Consider adding integration tests that explicitly validate the fallback path to prevent regression.

---

## 17. [[Doc] Add DCP support to attention backend doc](https://github.com/vllm-project/vllm/pull/33936)


### Base Information

- **PR Number:** #33936
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-09 15:33:44
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33936/files) (2):**
  - `docs/design/attention_backends.md`
  - `tools/pre_commit/generate_attention_backend_docs.py`

### Summary

**What changed and why**  
This PR adds Decode Context Parallelism (DCP) support documentation to the attention backend table and refactors the documentation generation script. The changes introduce a new "DCP" column to indicate which backends support the `--decode-context-parallel-size` feature, while also cleaning up code duplication and improving maintainability.

**Technical impact**  
The documentation now accurately reflects DCP support across various attention backends, helping users select appropriate backends for parallel decoding scenarios. The script refactoring centralizes table rendering logic, extracts FA/FI variant expansion, and improves compute capability parsing—making future updates easier and reducing redundancy.

**Potential risks**  
If the refactored parsing logic incorrectly interprets backend class attributes or method overrides, the generated documentation could become inaccurate. Additionally, the new DCP column values must be verified against actual backend implementations to ensure they match runtime behavior.

**Key insights**  
Developers should validate that the DCP support flags in the documentation align with each backend's actual capabilities. The refactored script provides a more maintainable foundation, but any changes to backend class structures may require updates to the parsing utilities.

---

## 18. [[ModelRunner V2][BugFix] Fix `max_query_len` calculation](https://github.com/vllm-project/vllm/pull/34167)


### Base Information

- **PR Number:** #34167
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-09 13:47:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34167/files) (4):**
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`

### Summary

**What changed and why**  
The fix addresses a bug in `max_query_len` calculation where it was incorrectly computed as the maximum cumulative query length instead of the maximum individual query length. This caused the attention decode-only path to never be used for batch sizes greater than 1, potentially impacting performance.

**Technical impact**  
The changes modify the `build_attn_metadata` function to accept `max_query_len` as an explicit parameter instead of calculating it internally. This ensures correct routing between prefill and decode attention paths, improving performance for multi-query scenarios by enabling the decode-only optimization path when appropriate.

**Potential risks**  
The main risk is incorrect `max_query_len` values being passed from call sites, which could lead to improper attention path selection. The eagle.py change hardcodes `max_query_len=1`, which assumes single-token proposals but should be validated against the actual proposal mechanism.

**Key insights**  
Always pass explicit parameters rather than calculating derived values inside shared functions when those values have semantic importance for routing decisions. Developers should verify that all call sites provide accurate `max_query_len` values, particularly for specialized components like speculative decoding.

---

## 19. [[torch.compile][Fusion] Fix attention fusion pass removing kv_udpate op.](https://github.com/vllm-project/vllm/pull/33945)


### Base Information

- **PR Number:** #33945
- **Author:** [charlifu](https://github.com/charlifu)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-09 13:15:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33945/files) (2):**
  - `tests/compile/passes/test_fusion_attn.py`
  - `vllm/compilation/passes/fusion/attn_quant_fusion.py`

### Summary

**What changed and why**  
The changes add a `kv_cache_dummy_dep` parameter to the attention fusion pass patterns and replacements, ensuring this parameter is preserved during fusion. This prevents the `kv_cache_update` operation from being incorrectly removed by subsequent cleanup passes, which was occurring because the dependency was being dropped.

**Technical impact**  
These modifications ensure that key-value cache dependencies are maintained in the intermediate representation (IR) after fusion. The fusion pass now correctly propagates the `kv_cache_dummy_dep` parameter through both the pattern matching and replacement steps, preserving critical dependency information needed for subsequent optimization passes and execution.

**Potential risks**  
If the `kv_cache_dummy_dep` parameter is not consistently handled across all fusion patterns or backends, it could lead to incomplete graph dependencies. Additionally, the test only validates consistency for the FP8 quantization path; other quantization modes or backends might require similar verification to ensure robustness.

**Key insights**  
Developers should verify that all attention fusion variants (including non-quantized and other backends) properly handle `kv_cache_dummy_dep`. The test addition is a good start but should be extended to cover more patterns. Ensure that any future modifications to fusion passes consider dependency preservation to avoid similar cleanup pass issues.

---

## 20. [[ROCm] update triton branch to support gpt-oss models for gfx11xx devices](https://github.com/vllm-project/vllm/pull/34032)


### Base Information

- **PR Number:** #34032
- **Author:** [hongxiayang](https://github.com/hongxiayang)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-09 11:36:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34032/files) (1):**
  - `docker/Dockerfile.rocm_base`

### Summary

**What changed and why**  
The PR updates the Triton branch hash in the ROCm base Dockerfile from `57c693b6` to `f332c492`. This change incorporates upstream Triton fixes (specifically from ROCm/triton#923) to enable support for GPT-OSS models (e.g., 20B) on gfx11xx AMD GPUs, addressing two reported issues (#33906 and #33143).

**Technical impact**  
Switching to a newer Triton branch introduces necessary kernel or backend optimizations for gfx11xx architectures, allowing vLLM to serve GPT-OSS models on these devices. This aligns the Docker environment with upstream Triton improvements, ensuring compatibility and performance for specific model architectures.

**Potential risks**  
The new Triton branch may introduce unintended side effects or regressions for other GPU architectures (e.g., gfx9xx or gfx10xx) if the changes are not fully backward compatible. Additionally, any unresolved bugs in the updated Triton branch could affect model serving stability or performance across all supported ROCm devices.

**Key insights**  
Developers should verify that the updated Triton branch passes existing CI tests for all ROCm GPU targets to ensure broad compatibility. Consider documenting this change in release notes if it enables new hardware support. Monitor for any performance deviations or new issues when serving other models on gfx11xx devices post-update.

---

## 21. [[Bugfix] Voxtral prompt/audio placeholder alignment](https://github.com/vllm-project/vllm/pull/34140)


### Base Information

- **PR Number:** #34140
- **Author:** [artuskg](https://github.com/artuskg)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-09 11:30:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34140/files) (1):**
  - `vllm/model_executor/models/voxtral.py`

### Summary

**What changed and why**  
This PR fixes alignment issues in Voxtral's audio preprocessing pipeline by ensuring consistent mono audio normalization, accurate audio token counting from processed arrays, and preserving all audio chunks during generation. The changes address correctness problems where audio placeholder token counts were miscalculated and audio data was being truncated.

**Technical impact**  
The modifications ensure audio processing consistency by enforcing mono channel output (`target_channels=1`) in the data parser. More importantly, audio token counts are now computed from the actual processed/padded `audio_arrays` rather than raw pre-pad lengths, preventing tokenization mismatches. The generation prompt now preserves all audio chunks instead of just the first one.

**Potential risks**  
The fallback path in `get_replacement()` could be triggered if `out_audio_items` doesn't contain expected data, potentially leading to inconsistent behavior. The type checking for `audio_arr` assumes torch.Tensor or np.ndarray but doesn't handle other sequence types. There's also a risk that the mono normalization could affect audio quality for stereo inputs if not properly handled upstream.

**Key insights**  
The core issue was a mismatch between audio processing and token counting logic. Developers should verify that all audio processing paths consistently produce mono outputs and that `out_audio_items` always contains the expected processed data. Consider adding validation to ensure the fallback path is never actually needed in production scenarios.

---

## 22. [[Bugfix] Avoid duplicate k-proj weight emission in helper](https://github.com/vllm-project/vllm/pull/34142)


### Base Information

- **PR Number:** #34142
- **Author:** [artuskg](https://github.com/artuskg)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-09 11:17:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34142/files) (1):**
  - `vllm/model_executor/models/whisper.py`

### Summary

**What changed and why**  
The fix reorders the yield statements in `_create_fake_bias_for_k_proj` to prevent duplicate emission of `k_proj.weight` entries. Previously, when a weight name matched `fake_bias_key_name`, both the weight and a synthetic bias were yielded, followed by an extra weight yield, causing duplication. Now, each weight is yielded exactly once, with a synthetic bias appended only for matching keys.

**Technical impact**  
This change ensures the helper function yields a clean sequence where each original weight appears once, accompanied by a zero-initialized bias for `k_proj` keys. It maintains compatibility with downstream weight loading logic that expects unique entries, preventing potential initialization errors or incorrect parameter counts in models using this helper.

**Potential risks**  
If downstream code implicitly relied on the duplicate weight emission (e.g., for specific weight processing order), this could introduce subtle bugs. Additionally, the fix assumes `fake_bias_key_name` always matches `".k_proj.weight"`; changes to this key name without corresponding updates might affect behavior.

**Key insights**  
The fix is minimal and corrects a clear logical error. Developers should verify that all callers of this function handle the new output sequence correctly. Consider adding a unit test to explicitly validate the uniqueness of yielded weight names for various input patterns to prevent regression.

---

## 23. [[Kernel] use flashinfer for gdn prefill](https://github.com/vllm-project/vllm/pull/32846)


### Base Information

- **PR Number:** #32846
- **Author:** [ZJY0516](https://github.com/ZJY0516)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-09 09:17:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32846/files) (1):**
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
This PR integrates FlashInfer's optimized prefill kernel for Gated Delta Rule (GDR) attention in Qwen3-Next models. It adds a custom operator that conditionally uses FlashInfer's kernel on CUDA compute capability 9.0+ devices, falling back to the existing FLA implementation otherwise. The change aims to improve prefill performance, as shown in the benchmark where TTFT decreased by ~7.7 seconds on average.

**Technical impact**  
The modification introduces a new `ChunkGatedDeltaRule` custom op that dynamically selects between FlashInfer and native FLA kernels based on GPU capability. This adds a hardware-specific optimization path while maintaining backward compatibility. The attention computation in `Qwen3NextAttention._forward_core` now uses this custom op instead of directly calling `chunk_gated_delta_rule`.

**Potential risks**  
1. The FlashInfer kernel requires tensor dtype conversions (to float32 for `g`, `beta`, `initial_state`), which may introduce precision differences or overhead.
2. The conditional logic depends on CUDA compute capability 90 (Hopper), limiting acceleration to newer GPUs.
3. The `squeeze(0)` operations assume specific tensor dimensions that could break with different input shapes.

**Key insights**  
1. The PR demonstrates a clear performance gain in prefill (TTFT reduction) with maintained accuracy, making it a valuable optimization.
2. Developers should ensure the FlashInfer dependency is properly versioned and available in the deployment environment.
3. Consider extending the conditional logic to support more GPU architectures if FlashInfer kernels become available for them.

---

## 24. [[Bugfix] Fix shared expert input for latent MoE in EP+DP (Nemotron-H)](https://github.com/vllm-project/vllm/pull/34087)


### Base Information

- **PR Number:** #34087
- **Author:** [TomerBN-Nvidia](https://github.com/TomerBN-Nvidia)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-09 08:44:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34087/files) (6):**
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`

### Summary

**What changed and why**  
This PR fixes a critical bug in latent MoE (Mixture of Experts) models like Nemotron-H when using Expert Parallelism (EP) with Data Parallelism (DP). In latent MoE, routed experts use a projected latent tensor, but shared experts require the original hidden states. The bug occurred because the `FusedMoEModularKernel` was incorrectly passing the latent-projected tensor to shared experts during EP+DP execution. The fix adds a `shared_experts_input` parameter to propagate the correct original hidden states through all MoE method implementations.

**Technical impact**  
The changes modify the core MoE kernel interface and its implementations across multiple quantization and execution methods. By adding the `shared_experts_input` parameter, the architecture now properly supports latent MoE's dual-input requirement. Additionally, enabling FlashInfer CUTLASS MoE for EP+DP configurations by removing the sequence-parallel restriction improves performance compatibility.

**Potential risks**  
The parameter addition changes the API signature of `FusedMoEModularKernel.forward()` and `_finalize()`, which could affect any custom implementations or downstream code. The FlashInfer change assumes the kernel now fully supports EP+DP; if underlying limitations remain, this could introduce correctness issues. There's also a risk if `layer._get_shared_experts_input(x)` returns incorrect tensors for non-latent MoE models.

**Key insights**  
This is a critical correctness fix for latent MoE models in EP+DP configurations. Developers must ensure all MoE method implementations propagate the new parameter. The FlashInfer change should be validated with thorough testing in EP+DP setups. The fix demonstrates the importance of proper input routing in heterogeneous expert architectures.

---

## 25. [[Kernel] FlashInfer: switch allreduce fusion to unified API](https://github.com/vllm-project/vllm/pull/33985)


### Base Information

- **PR Number:** #33985
- **Author:** [mmangkad](https://github.com/mmangkad)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-09 07:43:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33985/files) (3):**
  - `benchmarks/kernels/benchmark_fused_collective.py`
  - `tests/compile/passes/distributed/test_fusion_all_reduce.py`
  - `vllm/compilation/passes/fusion/allreduce_rms_fusion.py`

### Summary

**What changed and why**  
This PR migrates vLLM's FlashInfer allreduce fusion implementation from the deprecated `trtllm_allreduce_fusion` API to the new unified `flashinfer.comm.allreduce_fusion` API. The changes update workspace creation, function calls, and cleanup logic across the benchmark, test, and kernel fusion pass to align with FlashInfer version 0.6.3+.

**Technical impact**  
The migration centralizes workspace management through a new `create_allreduce_fusion_workspace` factory method that handles backend selection ("trtllm") and dtype-aware workspace allocation. This simplifies the codebase by removing manual IPC handle management and reducing parameter passing complexity in fused operations.

**Potential risks**  
The dependency on `flashinfer-python >= 0.6.3` introduces a version constraint that could break existing installations. Workspace dtype calculation in the benchmark may need verification for mixed precision scenarios. The `contextlib.suppress(Exception)` in cleanup could mask legitimate errors during workspace destruction.

**Key insights**  
Developers must ensure FlashInfer 0.6.3+ is installed before deploying these changes. The unified API reduces code complexity but requires careful attention to dtype propagation for workspace creation. All tests pass, indicating successful migration, but monitoring performance regressions in production workloads is recommended.

---

## 26. [Add NUMA Core binding in nixl_connector for CPU xPyD](https://github.com/vllm-project/vllm/pull/32365)


### Base Information

- **PR Number:** #32365
- **Author:** [ZhengHongming888](https://github.com/ZhengHongming888)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-09 07:39:13
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32365/files) (3):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`
  - `vllm/platforms/cpu.py`
  - `vllm/v1/worker/cpu_worker.py`

### Summary

**What changed and why**  
This PR introduces NUMA-aware core binding for the `nixl_connector` in CPU environments. It automatically detects NUMA topology and reserves the last physical core from each NUMA node exclusively for the `start_load_kv()` operation. This aims to reduce core competition between model inference (`model_forward()`) and KV cache transfer, improving TTFT and TPOT performance.

**Technical impact**  
The changes add NUMA topology discovery to the CPU platform and implement automatic core affinity setting in the connector initialization. This modifies process scheduling behavior by pinning specific cores to KV transfer tasks, which could affect overall CPU utilization and thread placement strategies in multi-NUMA systems.

**Potential risks**  
The core selection logic assumes the last core in each NUMA node is available and optimal, which may not hold true in all hardware configurations or when other processes are pinned. The `os.sched_setaffinity()` call affects the entire process, which could interfere with other threading models or libraries not designed for explicit core pinning. The implementation relies on Linux-specific sysfs paths.

**Key insights**  
This optimization is specifically valuable for CPU-based inference with KV transfer enabled. Developers should validate that reserved cores don't conflict with other pinned workloads. Consider making the core selection strategy configurable rather than hardcoded to the last core. Ensure compatibility testing with different NUMA architectures and kernel versions.

---

## 27. [[CI][torch.compile] Fix incorrect filtering for E2E fusion tests on B200](https://github.com/vllm-project/vllm/pull/34031)


### Base Information

- **PR Number:** #34031
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-09 07:05:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34031/files) (1):**
  - `.buildkite/test_areas/compile.yaml`

### Summary

**What changed and why**  
The PR fixes incorrect test filtering logic for E2E fusion tests on B200 GPUs that was inadvertently removing valid tests. It consolidates multiple `-k` arguments into single, more precise pytest expressions that properly combine conditions using parentheses and logical operators.

**Technical impact**  
These changes restore comprehensive test coverage for FLASHINFER attention backends, Inductor partitioning, and specific quantization configurations (particularly for Qwen models with FP8 quantization). The unified filtering expressions ensure all intended test combinations run correctly on B200 hardware.

**Potential risks**  
The complex boolean logic in the new pytest expressions could still have subtle edge cases or precedence issues. There's also a risk that the consolidated expressions might inadvertently exclude some test cases that were previously covered by separate `-k` arguments.

**Key insights**  
The fix demonstrates the importance of careful boolean logic construction when filtering tests with multiple conditions. Developers should verify that the new expressions match the intended test matrix, particularly for the Qwen FP8 quantization edge case and llama-3 configurations. Consider adding comments explaining the logic structure for future maintainability.

---

## 28. [[UX] Add `--language-model-only` for hybrid models](https://github.com/vllm-project/vllm/pull/34120)


### Base Information

- **PR Number:** #34120
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-09 06:57:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34120/files) (3):**
  - `vllm/config/model.py`
  - `vllm/config/multimodal.py`
  - `vllm/engine/arg_utils.py`

### Summary

**What changed and why**  
This PR introduces a `--language-model-only` flag to simplify disabling multimodal inputs for hybrid models. The flag sets all modality limits to zero, equivalent to manually setting `--limit-mm-per-prompt.modality 0` for each modality, making it easier for users to run hybrid models in text-only mode.

**Technical impact**  
The changes add a `language_model_only` boolean field to both `ModelConfig` and `MultiModalConfig`. When enabled, `get_limit_per_prompt()` returns 0 for any modality, effectively disabling multimodal processing. The flag is exposed via CLI arguments and integrated into the configuration initialization flow.

**Potential risks**  
If `language_model_only` is set alongside explicit `limit_mm_per_prompt` values, the flag will override those limits (setting them to 0), which may cause confusion if users expect both settings to apply. Additionally, the flag's effect on model loading or inference for purely multimodal models (non-hybrid) is untested and could lead to unexpected behavior.

**Key insights**  
This addition provides a user-friendly shortcut for a common use case, but developers should ensure the flag is clearly documented to avoid conflicts with manual limit settings. Consider adding validation or warnings if both `language_model_only` and `limit_mm_per_prompt` are specified.

---

## 29. [[Misc] Fix up attention benchmarks](https://github.com/vllm-project/vllm/pull/33810)


### Base Information

- **PR Number:** #33810
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-09 06:42:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33810/files) (5):**
  - `.buildkite/test_areas/benchmarks.yaml`
  - `benchmarks/attention_benchmarks/batch_spec.py`
  - `benchmarks/attention_benchmarks/common.py`
  - `benchmarks/attention_benchmarks/configs/standard_attention.yaml`
  - `benchmarks/attention_benchmarks/runner.py`

### Summary

**What changed and why**  
This PR fixes attention benchmarks that had issues on Blackwell GPUs, particularly for FlashInfer backend support. The changes update benchmark configurations, add batch type classification, improve result display, and refactor KV cache creation to use backend-specific layouts. A smoke test for B200 GPUs was also added to CI.

**Technical impact**  
The refactoring centralizes KV cache creation using backend-provided shape and stride methods, ensuring proper memory layout compatibility across different attention backends. The addition of `set_current_vllm_config` context and mock layer parameters enables FlashInfer to run correctly in benchmark environments. Batch type classification and enhanced table output improve benchmark result interpretability.

**Potential risks**  
The mock implementation for FlashInfer's `get_per_layer_parameters` uses hardcoded values that may not match real model configurations. The `spec_decode_threshold` parameter in `get_batch_type` is arbitrary (set to 8) and may not align with actual speculative decoding thresholds used in production. The B200 smoke test is marked optional, which could lead to unnoticed regressions if it fails.

**Key insights**  
Developers should verify that the mocked parameters for FlashInfer match real usage patterns. The batch type classification should be reviewed to ensure it aligns with the system's actual definitions of speculative decoding versus context extension. The KV cache layout abstraction is a positive step toward backend-agnostic benchmarking but requires thorough validation across all supported hardware and attention backends.

---

## 30. [[MODEL] Adding Support for Qwen3.5 Models](https://github.com/vllm-project/vllm/pull/34110)


### Base Information

- **PR Number:** #34110
- **Author:** [JJJYmmm](https://github.com/JJJYmmm)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-09 05:12:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34110/files) (11):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/config/model.py`
  - `vllm/config/speculative.py`
  - `vllm/model_executor/layers/mamba/abstract.py`
  - `vllm/model_executor/models/qwen3_5.py`
  - `vllm/model_executor/models/qwen3_5_mtp.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
This PR adds support for Qwen3.5 models (both dense and MoE variants) to the vLLM inference engine. The changes include new model implementation files (`qwen3_5.py` and `qwen3_5_mtp.py`), updates to configuration handling, registry entries, and documentation. The purpose is to enable inference for upcoming Qwen3.5 models, including support for speculative decoding (MTP) and integration with existing multimodal pipelines.

**Technical impact**  
The changes extend the model registry and configuration system to recognize Qwen3.5 architectures, adding them alongside existing Qwen family models. The implementation reuses components from Qwen3-Next (e.g., attention, MoE blocks) but introduces a new `Qwen3_5GatedDeltaNet` layer for Mamba-style linear attention. Speculative decoding support is added via MTP configurations, and the model is integrated into the multimodal framework. A minor fix ensures `hf_text_config` is used consistently for hybrid models.

**Potential risks**  
The new `Qwen3_5GatedDeltaNet` layer introduces complex state management for Mamba-style attention, which may have edge cases in distributed or speculative decoding scenarios. The reuse of Qwen3-Next components assumes architectural similarity, which could lead to subtle deviations if Qwen3.5 differs. The MTP support adds new configuration paths that require thorough testing to avoid regressions in speculative decoding. Additionally, the conditional checks for MoE attributes (e.g., `num_experts`) rely on `getattr` defaults, which might mask misconfigurations.

**Key insights**  
Developers should verify that the Qwen3.5 implementation aligns with the official HuggingFace transformers reference. The Mamba integration requires careful attention to state caching and speculative decoding compatibility. The consistent use of `hf_text_config` for hybrid models is a positive pattern to follow. Testing should include end-to-end inference with both dense and MoE variants, as well as MTP scenarios, to ensure stability. The PR demonstrates good collaboration with external teams, but internal validation of weight loading and distributed execution is essential.

---

## 31. [[XPU][6/N] add xpu scaled_mm kernel](https://github.com/vllm-project/vllm/pull/34117)


### Base Information

- **PR Number:** #34117
- **Author:** [zufangzhu](https://github.com/zufangzhu)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-09 04:17:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34117/files) (4):**
  - `.buildkite/scripts/hardware_ci/run-xpu-test.sh`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/xpu.py`

### Summary

**What changed and why**  
This PR adds XPU support for FP8 scaled matrix multiplication kernels. The changes include: 1) Adding a new XPU-specific kernel implementation (`XPUFP8ScaledMMLinearKernel`), 2) Removing the XPU exception in the FP8 config to enable FP8 quantization on XPU, 3) Disabling Marlin optimization for XPU (similar to ROCm), and 4) Adding an FP8 quantization test to the XPU CI pipeline.

**Technical impact**  
The changes extend FP8 quantization support to XPU devices by integrating a new kernel that leverages XPU-specific operations (`torch.ops._xpu_C.fp8_gemm_w8a16`). This aligns XPU with existing GPU platforms (CUDA/ROCm) for FP8 inference, though the Marlin optimization remains disabled for XPU due to platform constraints.

**Potential risks**  
The `apply_scaled_mm` method in the new kernel is unimplemented (just a `pass`), which could cause runtime errors if called. Additionally, disabling Marlin for XPU may impact performance compared to CUDA platforms. The kernel only supports FP8 weight dtypes (`float8_e5m2` and `float8_e4m3fn`), limiting flexibility for other quantization types.

**Key insights**  
Developers should ensure the `apply_scaled_mm` method is implemented before the kernel is used in scaled MM contexts. The CI test addition validates basic FP8 functionality on XPU, but performance benchmarking is recommended. The changes follow a modular pattern, making it easier to add future XPU optimizations.

---

## 32. [[Fix] [CPU Backend] : Prepack weights for w8a8 oneDNN matmul](https://github.com/vllm-project/vllm/pull/33901)


### Base Information

- **PR Number:** #33901
- **Author:** [nikhil-arm](https://github.com/nikhil-arm)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-09 02:04:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33901/files) (1):**
  - `csrc/cpu/dnnl_helper.cpp`

### Summary

**What changed and why**  
The change introduces conditional compilation for ARM64 (__aarch64__) to use a fixed probe size (kProbeM = 128) when prepacking weights for w8a8 oneDNN matmul operations, instead of a runtime dimension value. This optimizes weight prepacking to avoid runtime reorders and improve performance, particularly on Neoverse v1 CPUs.

**Technical impact**  
On ARM64 architectures, weight prepacking now uses a static probe size, enabling more efficient kernel selection and memory layout during initialization. This reduces overhead during inference by eliminating dynamic reordering at runtime, leading to significant throughput gains (83% improvement shown). Non-ARM64 platforms continue using runtime dimensions for flexibility.

**Potential risks**  
If the fixed probe size (128) does not align with actual input dimensions during execution, it could lead to suboptimal performance or fallback paths. The change is architecture-specific, so any future portability concerns or variations in ARM64 implementations need consideration. Additionally, the impact on memory usage for prepacked weights should be verified across different workloads.

**Key insights**  
This optimization leverages architecture-specific tuning for substantial performance gains, demonstrating the value of hardware-aware optimizations in compute kernels. Developers should ensure the probe size is appropriate for typical workloads and consider similar tunable parameters for other platforms. The change highlights the trade-off between runtime flexibility and initialization-time optimization.

---

## 33. [[ASR] Fix audio benchmark and add RTFx metric](https://github.com/vllm-project/vllm/pull/32300)


### Base Information

- **PR Number:** #32300
- **Author:** [ekagra-ranjan](https://github.com/ekagra-ranjan)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-09 02:02:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32300/files) (4):**
  - `docs/benchmarking/cli.md`
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/lib/endpoint_request_func.py`
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
This PR fixes the ASR (Automatic Speech Recognition) benchmark functionality and adds RTFx (Inverse Real-Time Factor) metric support. Key changes include fixing the broken ASR benchmark, replacing the hardcoded 30-second audio filter with configurable min/max length filters, making the prompt template Whisper-specific, and making the HuggingFace split configurable.

**Technical impact**  
The changes enable proper ASR benchmarking with vLLM by adding audio duration filtering parameters (`--asr-min-audio-len-sec` and `--asr-max-audio-len-sec`), calculating RTFx metrics for ASR performance evaluation, and improving dataset handling by passing `trust_remote_code` flag. The benchmark now supports multiple ASR datasets and provides detailed audio duration statistics.

**Potential risks**  
The removal of the 30-second hard limit could lead to processing very long audio files that might exceed model capabilities or cause memory issues. The Whisper-specific prompt template condition (`if "openai" in tokenizer.name_or_path`) might not correctly identify all Whisper model variants. The increased `DEFAULT_OUTPUT_LEN` from 128 to 1024 could affect memory usage for large batch sizes.

**Key insights**  
Developers should use the new audio length parameters to filter appropriate samples for their ASR models. The RTFx metric provides valuable insight into ASR efficiency (higher is better). When benchmarking non-Whisper ASR models, ensure the prompt template logic correctly handles your model's requirements. Consider setting reasonable max audio length bounds to prevent resource exhaustion.

---

## 34. [[CI] Remove empty image_size_factors for fuyu, glm4_1v, glm_ocr](https://github.com/vllm-project/vllm/pull/34107)


### Base Information

- **PR Number:** #34107
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-09 01:37:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34107/files) (1):**
  - `tests/models/multimodal/generation/test_common.py`

### Summary

**What changed and why**  
Removed empty tuples `()` from the `image_size_factors` list for three multimodal models (fuyu, glm4_1v, glm_ocr) in `test_common.py`. This fixes test failures caused by PR #34039, which introduced a new `render_prompts` method that raises a `ValueError` when given an empty prompt list—empty `image_size_factors` entries were producing zero prompts.

**Technical impact**  
The change aligns these test configurations with the updated renderer protocol, ensuring that all test cases generate at least one prompt. It maintains test coverage for valid image scaling factors while eliminating invalid zero-prompt scenarios that now violate the renderer's input validation.

**Potential risks**  
If any downstream tests or model configurations implicitly depend on empty `image_size_factors` to represent "no images," those cases may now be untested. Additionally, similar empty entries could exist in other test files not yet updated, potentially causing sporadic test failures.

**Key insights**  
Always validate test data against new API constraints during refactoring. Use systematic searches for patterns like `[(), ...]` across the codebase to prevent oversight. Consider adding a lint rule or pre-commit check to reject empty prompt lists in test configurations.

---

## 35. [[Model] GLM adaptation](https://github.com/vllm-project/vllm/pull/34124)


### Base Information

- **PR Number:** #34124
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-02-09 01:32:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34124/files) (7):**
  - `benchmarks/kernels/benchmark_moe.py`
  - `tests/models/registry.py`
  - `tests/models/test_initialization.py`
  - `vllm/config/speculative.py`
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`

### Summary

**What changed and why**  
This PR adds support for the GLM-5 model (`GlmMoeDsaForCausalLM`) to the vLLM codebase. The changes register the new model architecture, configure it to inherit from the existing `DeepseekV2ForCausalLM` implementation, and update related configuration files and tests to accommodate it. This enables vLLM to load and run inference with the GLM-5 model.

**Technical impact**  
The GLM-5 model is integrated as a subclass of `DeepseekV2ForCausalLM`, leveraging its existing MoE (Mixture of Experts) and DSA (likely a specific attention variant) implementations. Key adjustments include a modified RoPE (Rotary Positional Embedding) initialization condition based on a new config attribute (`indexer_rope_interleave`) and inclusion in speculative decoding and model architecture detection logic. This reuse minimizes new code but ties GLM-5's behavior closely to DeepSeek-V2's.

**Potential risks**  
The conditional `is_neox_style=not getattr(config, "indexer_rope_interleave", True)` assumes a default value of `True` if the attribute is missing, which could lead to incorrect RoPE initialization if the GLM-5 config differs unexpectedly. Inheriting from `DeepseekV2ForCausalLM` might hide model-specific nuances not captured by the shared implementation. The test availability is marked as `is_available_online=False`, limiting immediate validation.

**Key insights**  
The integration efficiently reuses the DeepSeek-V2 code path, demonstrating a pattern for supporting architecturally similar models. Developers should verify the `indexer_rope_interleave` default aligns with the actual GLM-5 Hugging Face config. Future work may require isolating model-specific logic if divergences emerge. Ensure the GLM-5 model weights are compatible with the expected tensor layouts and operations.

---

## 36. [fix(cpu): fix mla_decode compilation on x86 without AVX512](https://github.com/vllm-project/vllm/pull/34052)


### Base Information

- **PR Number:** #34052
- **Author:** [ihb2032](https://github.com/ihb2032)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-09 00:55:42
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34052/files) (1):**
  - `csrc/cpu/mla_decode.cpp`

### Summary

**What changed and why**  
The change removes architecture-specific specializations for s390x and aarch64 in the `KernelVecType<c10::BFloat16>` template, consolidating them under a single `#else` branch. This resolves a compilation failure on x86_64 systems without AVX512 support by ensuring a consistent vector type definition is available.

**Technical impact**  
This simplifies the conditional compilation logic, making the code more maintainable. However, it may remove optimized vector type mappings for s390x and aarch64 architectures, potentially impacting performance on those platforms if they previously relied on distinct specializations.

**Potential risks**  
If s390x or aarch64 architectures required unique vector type definitions for optimal performance, this change could introduce performance regressions. Additionally, the consolidation might hide platform-specific nuances that could affect correctness or efficiency in edge cases.

**Key insights**  
Reviewers should verify that the consolidated vector types are appropriate for all non-AVX512 architectures, including s390x and aarch64. Consider adding a comment explaining why these specializations were merged and whether performance benchmarks are needed for affected platforms.

---

