# vLLM Merged PR Report

**Report Date:** 2026-02-24 PST

**Total Merged PRs:** 43

---

## 1. [docs: document committer proposal process in governance](https://github.com/vllm-project/vllm/pull/35225)


### Base Information

- **PR Number:** #35225
- **Author:** [simon-mo](https://github.com/simon-mo)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-24 23:58:48
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35225/files) (1):**
  - `docs/governance/process.md`

### Summary

**What changed and why**  
The PR documents the formal committer proposal process in the governance documentation. It replaces a brief "Nomination Process" section with a detailed, five-step "Committer Proposal Process" that clarifies roles, timelines, and parallel workflows for permissions and onboarding.

**Technical impact**  
This change has no direct impact on the codebase or build systems, as it only updates documentation. However, it establishes a clearer, more transparent governance process that may affect how committers are onboarded and how code ownership (CODEOWNERS) updates are coordinated.

**Potential risks**  
The two-week feedback period could slow down urgent onboarding if not managed flexibly. Relying on parallel steps (permissions assignment and PR preparation) introduces a risk of misalignment if communication between lead maintainers and the nominator breaks down.

**Key insights**  
The updated process adds valuable structure, especially the explicit feedback period and parallel onboarding steps. Developers should ensure all committers are aware of this documented process to maintain consistency. Consider adding a checklist or template for the nomination email to further standardize contributions evidence.

---

## 2. [[Perf] Add opt-in SM100 Oink RMSNorm custom-op path](https://github.com/vllm-project/vllm/pull/31828)


### Base Information

- **PR Number:** #31828
- **Author:** [Laurawly](https://github.com/Laurawly)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 23:01:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31828/files) (4):**
  - `tests/model_executor/test_oink_integration.py`
  - `vllm/_oink_ops.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/layernorm.py`

### Summary

**What changed and why**  
This PR adds an optional integration path for external SM100 (Blackwell) RMSNorm kernels via the Oink/CuTeDSL library. The changes introduce environment variable control (`VLLM_USE_OINK_OPS`), thin wrapper functions to probe and call external ops, and modifications to `RMSNorm.forward_cuda` to conditionally route eligible calls to the external kernels. The purpose is to enable performance improvements on SM100+ GPUs without introducing hard dependencies, maintaining fallback to vLLM's existing implementation.

**Technical impact**  
The integration is opt-in and non-breaking: default behavior remains unchanged unless explicitly enabled. When activated, the system performs one-time availability checks during RMSNorm initialization and conditionally uses external kernels for compatible tensor layouts and data types. This enables potential speedups (1.35–1.48x kernel-level, ~1.6% end-to-end) on SM100 devices, particularly under `torch.compile` with CUDA graphs. The architecture remains modular, as the external ops are loaded via plugin registration.

**Potential risks**  
- The performance gain depends on workload characteristics and may be limited in communication-heavy tensor-parallel settings.  
- Incorrect `torch.compile` configuration (missing `+rms_norm` in `custom_ops`) will silently disable the Oink path, as noted in the warning.  
- Stride compatibility checks (`_is_oink_stride_compatible_2d`) could inadvertently exclude valid tensor layouts, though fallback mechanisms exist.  
- Plugin availability and version mismatches could lead to runtime errors, though these are caught and logged.

**Key insights**  
- Enable the Oink path only on SM100+ GPUs with the external plugin installed and `VLLM_USE_OINK_OPS=1`.  
- When using Inductor, ensure `compilation_config={"custom_ops": ["none", "+rms_norm"]}` to prevent bypassing the custom-op path.  
- The implementation wisely isolates external dependencies and includes comprehensive fallbacks, making it safe for production use. Developers should verify tensor layout compatibility (contiguous row-major with stride constraints) to maximize performance benefits.

---

## 3. [[Perf] Optimize FP8 gemm of sm120.](https://github.com/vllm-project/vllm/pull/34424)


### Base Information

- **PR Number:** #34424
- **Author:** [wenshuai-xiaomi](https://github.com/wenshuai-xiaomi)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-24 22:25:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34424/files) (1):**
  - `csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm120_fp8_dispatch.cuh`

### Summary

**What changed and why**  
This PR introduces specialized GEMM configurations for FP8 operations on SM120 (compute capability 12.0) GPUs to optimize performance for small batch sizes (M ≤ 256). It adds three new tile configurations (M=16, M=32, M=64) that use a ping-pong kernel schedule and custom epilogue tiles, while the default configuration handles larger M values.

**Technical impact**  
The changes improve FP8 GEMM performance significantly for small input shapes, as shown in the benchmark results where throughput increases by up to ~10x for batch sizes 1–256. The implementation maintains backward compatibility by falling back to the default configuration for larger M values, ensuring no regression for existing workloads.

**Potential risks**  
The custom configurations hardcode cluster shapes to 1x1x1 and rely on specific kernel schedules (e.g., `KernelTmaWarpSpecializedPingpong`), which may limit scalability or compatibility with future hardware or CUDA versions. Edge cases like very small K or N dimensions are not explicitly handled and could lead to suboptimal performance or compilation errors.

**Key insights**  
Developers should note that the optimization is specific to SM120 and FP8 data types (`float_e4m3_t`). The dispatch logic is based solely on M (batch size), so changes to input tensor shapes may affect kernel selection. Consider validating performance across a broader range of shapes and integrating similar optimizations for other compute capabilities if needed.

---

## 4. [[XPU]Support CUDAGraph on XPU Platform](https://github.com/vllm-project/vllm/pull/34482)


### Base Information

- **PR Number:** #34482
- **Author:** [xinyu-intel](https://github.com/xinyu-intel)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:22:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34482/files) (3):**
  - `vllm/platforms/xpu.py`
  - `vllm/utils/torch_utils.py`
  - `vllm/v1/worker/xpu_model_runner.py`

### Summary

**What changed and why**  
This PR adds support for CUDAGraph functionality on the XPU platform by enabling XPU graph capture in PyTorch nightly builds (≥2.11.0.dev). It modifies the XPU platform configuration to allow graph modes, adds a utility function to check XPU graph support, and patches CUDA APIs to use XPU equivalents in the model runner.

**Technical impact**  
The changes enable static graph execution on XPU devices, which can improve inference performance by reducing kernel launch overhead. The implementation conditionally enables graph modes based on PyTorch version, distributed execution, and attention backends—restricting FLASH_ATTN to PIECEWISE mode while allowing TRITON_ATTN full support. The platform now reports `support_static_graph_mode()` as `True`.

**Potential risks**  
Graph capture is disabled for distributed scenarios due to unsupported communication ops, which may silently fall back to non-graph execution. The reliance on nightly PyTorch builds (torch≥2.11.0.dev) could lead to instability or version compatibility issues. There is also a risk of incorrect API patching if future PyTorch updates change XPU graph interfaces.

**Key insights**  
Developers should verify their PyTorch version meets the minimum requirement and avoid using graph modes with distributed training. The fallback logic for unsupported configurations is robust but may produce warning logs. Ensure thorough testing with both FLASH_ATTN and TRITON_ATTN backends to validate performance gains and correctness.

---

## 5. [[Platform] Add current_platform.num_compute_units interface](https://github.com/vllm-project/vllm/pull/35042)


### Base Information

- **PR Number:** #35042
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:22:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35042/files) (24):**
  - `tests/kernels/attention/test_cutlass_mla_decode.py`
  - `tests/kernels/quantization/test_allspark_gemm.py`
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/allspark.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/rocm.py`
  - `vllm/model_executor/layers/batch_invariant.py`
  - `vllm/model_executor/layers/fla/ops/layernorm_guard.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils.py`
  - `vllm/model_executor/layers/utils.py`
  - `vllm/model_executor/warmup/deep_gemm_warmup.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/interface.py`
  - `vllm/platforms/rocm.py`
  - `vllm/platforms/xpu.py`
  - `vllm/utils/platform_utils.py`
  - `vllm/v1/attention/backends/mla/cutlass_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/indexer.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/sample/ops/topk_topp_triton.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_ubatch_wrapper.py`
  - `vllm/v1/worker/xpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces a unified `current_platform.num_compute_units` interface to replace direct calls to `torch.cuda.get_device_properties().multi_processor_count` across the codebase. The goal is to abstract hardware-specific details and support non-CUDA platforms (like XPU and NPU) by providing a platform-agnostic way to query compute units (SMs for NVIDIA, CUs for AMD, EUs for Intel).

**Technical impact**  
The changes centralize compute unit retrieval through the platform abstraction layer. Each platform implementation (CUDA, ROCm, XPU) now provides its own `num_compute_units` method, and a cached utility function `num_compute_units()` in `platform_utils.py` delegates to the current platform. This improves code maintainability and enables cross-platform compatibility without modifying kernel logic.

**Potential risks**  
If a platform implementation is missing or incorrect, calls may raise `NotImplementedError` or return invalid values, potentially affecting kernel performance tuning or workspace sizing. The caching mechanism assumes compute units are static per device; dynamic changes (e.g., GPU partitioning) could lead to stale values. Additionally, the XPU implementation uses `max_compute_units`, which may differ semantically from NVIDIA’s `multi_processor_count`.

**Key insights**  
Developers should use the new `num_compute_units()` utility for all compute unit queries. Ensure any new platform implementations (e.g., NPU) add the method. The caching improves performance but be cautious in environments where device properties might change. Verify that the XPU `max_compute_units` aligns with the expected behavior for kernel configurations.

---

## 6. [remove cuda check in `top_k_top_p_triton` kernel](https://github.com/vllm-project/vllm/pull/35011)


### Base Information

- **PR Number:** #35011
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:22:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35011/files) (2):**
  - `vllm/v1/sample/ops/topk_topp_sampler.py`
  - `vllm/v1/sample/ops/topk_topp_triton.py`

### Summary

**What changed and why**  
Removed CUDA-specific device checks in the Triton-based `top_k_top_p` sampling kernel to support non-CUDA devices like XPU and NPU. The changes eliminate hardcoded `is_cuda` assertions that were preventing the kernel from running on alternative GPU architectures.

**Technical impact**  
The kernel is now device-agnostic, allowing it to execute on any PyTorch-supported device that Triton can target. This improves cross-platform compatibility without altering the kernel's core functionality or performance characteristics for CUDA devices.

**Potential risks**  
Removing device validation could lead to runtime errors if the kernel is invoked on unsupported devices where Triton compilation fails. There's also a risk of silent performance degradation on non-CUDA devices if memory access patterns or hardware capabilities differ significantly.

**Key insights**  
This is a necessary change for multi-architecture support, but consider adding a more generic device capability check (e.g., checking for GPU-like devices with sufficient compute capability). Ensure comprehensive testing across all target device types to validate both correctness and performance.

---

## 7. [[Misc] Add shard_id validation for MergedColumnLinear](https://github.com/vllm-project/vllm/pull/35055)


### Base Information

- **PR Number:** #35055
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:12:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35055/files) (1):**
  - `vllm/model_executor/layers/linear.py`

### Summary

**What changed and why**  
Added `validate_shard_id` methods to `MergedColumnLinear` and `QKVParallelLinear` classes to enforce proper shard ID validation, preventing unexpected shard IDs during weight loading. Also updated type annotations and signatures for several linear utility functions to improve clarity and type safety.

**Technical impact**  
The validation ensures shard IDs are within expected bounds (`0` to `len(output_sizes)-1` for `MergedColumnLinear`, or `"q"`, `"k"`, `"v"` for `QKVParallelLinear`), catching invalid inputs early. This enhances robustness in distributed or quantized model loading scenarios, particularly for fused modules. The updated type hints aid static analysis and developer understanding.

**Potential risks**  
The validation may break existing code that relies on loose shard ID handling, though this is likely intentional. The `FIXME` comment indicates tuple shard IDs are not yet supported for BitsAndBytes quantization, which could cause issues if attempted. Edge cases like negative shard IDs or non-consecutive tuples are now explicitly rejected, which could surface previously silent bugs.

**Key insights**  
Developers should ensure shard IDs passed to these linear layers comply with the new validation rules. The tuple shard ID restriction for BNB quantization requires attention if expanding support. The added type annotations improve maintainability, but runtime validation now adds a small overhead during weight loading.

---

## 8. [[Misc] Enable weights loading tracking for quantized models](https://github.com/vllm-project/vllm/pull/35074)


### Base Information

- **PR Number:** #35074
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:11:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35074/files) (1):**
  - `vllm/model_executor/model_loader/default_loader.py`

### Summary

**What changed and why**  
The PR enables weight loading tracking for quantized models by extracting the tracking logic into a separate method and adding special handling for KV cache quantization parameters. Previously, weight loading validation was skipped for quantized models, but now it includes quantized layers while ignoring missing KV cache scale parameters that may not be present in checkpoints.

**Technical impact**  
This change improves weight loading validation consistency across model types by extending strict weight tracking to quantized models. The refactoring into a dedicated `track_weights_loading` method enhances code organization and maintainability while preserving backward compatibility for non-quantized models.

**Potential risks**  
The assumption that KV cache scale parameters can be missing in checkpoints might mask actual weight loading issues if other quantized parameters are incorrectly omitted. There's also a risk of incomplete weight tracking if the `BaseKVCacheMethod` detection doesn't cover all quantization implementations or edge cases.

**Key insights**  
Developers should verify that all quantization method classes properly inherit from `BaseKVCacheMethod` for consistent behavior. Consider adding logging when KV cache parameters are skipped to improve debuggability. The separation of tracking logic makes it easier to extend weight validation for future quantization schemes.

---

## 9. [[compile] Improve error message during artifacts load failure.](https://github.com/vllm-project/vllm/pull/35115)


### Base Information

- **PR Number:** #35115
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:01:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35115/files) (1):**
  - `vllm/compilation/decorators.py`

### Summary

**What changed and why**  
The PR improves error messaging when AOT compilation artifacts fail to load. Instead of the generic "Cannot load aot compilation" message, it now clearly states "Compiling model again due to a load failure" and provides a specific reason—either the original exception message or a dedicated "Compile cache file corrupted." message for EOFError cases.

**Technical impact**  
These changes enhance user clarity during fallback scenarios by explicitly indicating that a recompilation is occurring due to a load failure. The differentiation of EOFError helps users quickly identify cache corruption issues, while maintaining the existing fallback behavior to recompile when loading fails.

**Potential risks**  
If the exception message (`str(e)`) contains sensitive information, it could be exposed in logs. Additionally, the change does not address the root cause of cache corruption or other load failures, which may recur if underlying issues persist.

**Key insights**  
The updated logging is more user-friendly and actionable, especially for EOFError cases. Developers should ensure that exception messages are safe for logging and consider adding metrics or alerts for frequent cache corruption to investigate systemic problems.

---

## 10. [[Linear Attention] fix bug for linear attention + prefix caching + reset_prefix_cache](https://github.com/vllm-project/vllm/pull/35157)


### Base Information

- **PR Number:** #35157
- **Author:** [heheda12345](https://github.com/heheda12345)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:00:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35157/files) (2):**
  - `tests/v1/worker/test_mamba_utils.py`
  - `vllm/v1/worker/mamba_utils.py`

### Summary

**What changed and why**  
The fix addresses a bug where `mamba_state_idx` entries for force-preempted requests (e.g., during `reset_prefix_cache` or KV cache flush) were not being cleared. These requests appear in `resumed_req_ids` but not in `preempted_req_ids`, leaving stale indices that could reference invalid block allocations after cache resizing.

**Technical impact**  
The change ensures `preprocess_mamba` now also clears `mamba_state_idx` for requests in `resumed_req_ids`, preventing out-of-bounds block index errors. This maintains consistency between the Mamba state tracking and the block allocator after prefix cache resets or KV cache flushes.

**Potential risks**  
If `resumed_req_ids` contains requests that are also in `finished_req_ids` or `preempted_req_ids`, redundant `pop` operations are harmless. However, the fix assumes `resumed_req_ids` accurately identifies all force-preempted requests; any mismatch could lead to missed cleanups or premature removals.

**Key insights**  
Developers should note that force-preempted requests bypass the normal preemption flow, requiring special handling in state cleanup. The added unit test validates this edge case, and similar patterns may be needed elsewhere in the codebase where `resumed_req_ids` is used without corresponding preemption entries.

---

## 11. [Remove requirement to use `--hf-overrides` for `DeepseekVLV2ForCausalLM`](https://github.com/vllm-project/vllm/pull/35203)


### Base Information

- **PR Number:** #35203
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:00:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35203/files) (5):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh`
  - `tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh`
  - `vllm/transformers_utils/configs/deepseek_vl2.py`

### Summary

**What changed and why**  
The changes remove the requirement for users to manually specify `--hf-overrides '{"architectures": ["DeepseekVLV2ForCausalLM"]}'` when loading DeepSeek-VL2 models. This is achieved by modifying the `DeepseekVLV2Config` class to automatically set the `architectures` field to `["DeepseekVLV2ForCausalLM"]` if it is missing from the checkpoint, eliminating the need for user-provided overrides.

**Technical impact**  
These modifications simplify the user experience by making the architecture detection automatic, reducing configuration complexity. The changes also update related documentation, test configurations, and integration scripts to reflect that the manual override is no longer necessary, ensuring consistency across the codebase.

**Potential risks**  
If the checkpoint already contains an `architectures` field that differs from the expected value, the automatic assignment might override it incorrectly, potentially leading to model loading issues. Additionally, removing the override from test scripts could affect reproducibility if future checkpoints change their architecture definitions.

**Key insights**  
This improvement enhances usability by reducing manual configuration steps, aligning with the principle of sensible defaults. Developers should verify that the automatic architecture detection works correctly with all supported DeepSeek-VL2 variants and consider adding validation to handle edge cases where checkpoints might have conflicting architecture definitions.

---

## 12. [[Bugfix] Fix AttributeError when passing StructuredOutputsParams to CompletionRequest](https://github.com/vllm-project/vllm/pull/35237)


### Base Information

- **PR Number:** #35237
- **Author:** [pks](https://github.com/pks)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 22:00:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35237/files) (2):**
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/completion/protocol.py`

### Summary

**What changed and why**  
The fix addresses an AttributeError that occurs when passing a `StructuredOutputsParams` dataclass instance to `CompletionRequest` or `ChatCompletionRequest`. The validator `check_structured_outputs_count` previously assumed `structured_outputs` was always a dict and called `.get()`, which fails on dataclass objects. The change adds type checking to handle both dict and dataclass inputs by using `getattr` for dataclass instances.

**Technical impact**  
This change ensures compatibility with the existing type annotation (`StructuredOutputsParams \| None`) and maintains backward compatibility for dict inputs. It allows the validator to correctly count active structured output constraints (json, regex, choice) regardless of input format, preserving the validation logic that ensures only one constraint type is used.

**Potential risks**  
If `StructuredOutputsParams` is subclassed or its attribute names change, the `getattr` approach may fail silently or miscount constraints. Additionally, the fix assumes the dataclass has default `None` values for optional fields; if not, `getattr(..., None)` could incorrectly treat missing attributes as present. Edge cases with mixed or unexpected input types (e.g., other objects with `get` methods) are not handled.

**Key insights**  
The fix is minimal and focused, but consider adding a unit test for hybrid input types to prevent regression. For robustness, validate the input type more explicitly (e.g., using `hasattr` for dict-like objects) or refactor the validator to normalize inputs early. Ensure documentation clarifies that both dict and dataclass inputs are supported.

---

## 13. [[Responses][CI] Filter negative token IDs in schema fuzz test to avoid 500 errors](https://github.com/vllm-project/vllm/pull/35231)


### Base Information

- **PR Number:** #35231
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-24 21:52:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35231/files) (2):**
  - `tests/entrypoints/openai/test_completion_error.py`
  - `vllm/entrypoints/openai/completion/protocol.py`

### Summary

**What changed and why**  
Added server-side validation to reject negative token IDs in the `prompt` field of `POST /v1/completions` requests. Previously, negative token IDs caused unhandled 500 errors; now they return proper 400 validation errors with descriptive messages, improving API robustness and consistency with existing field validations.

**Technical impact**  
The change modifies the `CompletionRequest` schema in Pydantic to enforce non-negative integer constraints on `prompt` token IDs using `Annotated[int, Field(ge=0)]`. This ensures validation occurs at the request parsing layer, preventing invalid data from propagating to downstream processing and aligning with validation patterns used for other fields like `logprobs`.

**Potential risks**  
If there are existing clients or internal code that rely on negative token IDs for special signaling (though unlikely), this change could break them. Additionally, the validation only covers explicit negative integers; edge cases like extremely large negative numbers or non-integer types should already be handled by Pydantic's type system but warrant verification.

**Key insights**  
This fix enhances API reliability by converting silent server crashes into client-friendly validation errors. Developers should ensure similar validation is applied across all token ID fields in other endpoints to maintain consistency. The addition of explicit test cases for both flat and nested prompt structures is a best practice that should be emulated for other validation rules.

---

## 14. [[FIX] fused moe with lora shared expert dual stream (1.07x otps)](https://github.com/vllm-project/vllm/pull/34933)


### Base Information

- **PR Number:** #34933
- **Author:** [jhaotingc](https://github.com/jhaotingc)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-24 20:40:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34933/files) (1):**
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
The fix addresses a performance regression in fused MoE (Mixture of Experts) with LoRA (Low-Rank Adaptation) where dual-stream overlap for shared experts was lost when LoRA modules were enabled. By setting `shared_experts = None` in the modular kernel, the runner can overlap shared expert computations with routed experts using a separate CUDA stream, restoring the original performance benefit.

**Technical impact**  
This change modifies the fused MoE kernel initialization for LoRA-enabled layers, ensuring that shared experts are not owned by the kernel. This allows the execution engine to schedule shared expert computations concurrently with routed experts via stream parallelism, improving throughput (1.02–1.08× speedup per GPU in benchmarks) without altering numerical correctness.

**Potential risks**  
If the runner does not properly handle `shared_experts = None`, shared expert computations might be skipped or misaligned. Additionally, the change assumes the quant method’s modular kernel (`moe_mk`) can safely relinquish control of shared experts; any hidden dependencies could cause errors. Edge cases with non-standard quantization methods or custom kernels may need validation.

**Key insights**  
The fix is minimal and targeted, leveraging existing stream overlap mechanisms. Developers should verify that all quantization paths and kernel implementations correctly support detached shared experts. Performance testing across varied TP (tensor parallelism) configurations and model architectures is recommended to ensure robustness.

---

## 15. [[ROCm]: Enable customop and rope+kvcache fusion for AITER RoPE](https://github.com/vllm-project/vllm/pull/35180)


### Base Information

- **PR Number:** #35180
- **Author:** [Rohan138](https://github.com/Rohan138)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-24 20:36:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35180/files) (9):**
  - `tests/compile/passes/test_rope_kvcache_fusion.py`
  - `vllm/_aiter_ops.py`
  - `vllm/compilation/passes/fusion/matcher_utils.py`
  - `vllm/compilation/passes/utility/scatter_split_replace.py`
  - `vllm/config/compilation.py`
  - `vllm/config/vllm.py`
  - `vllm/envs.py`
  - `vllm/model_executor/layers/rotary_embedding/base.py`
  - `vllm/platforms/rocm.py`

### Summary

**What changed and why**  
This PR enables the AITER RoPE custom op by default on ROCm and allows RoPE+KVCache fusion. It modifies environment variables to default to using the AITER Triton RoPE kernel, updates configuration logic to enable fusion when appropriate, and extends test coverage to validate both enabled and disabled states.

**Technical impact**  
The changes introduce a new custom op registration for `rocm_aiter_triton_rotary_embedding`, which modifies query and key tensors in-place. This op is now integrated into the RoPE layer and fusion passes, replacing the previous vLLM native custom op during prefill when token count exceeds 256. The configuration logic now conditionally enables RoPE+KVCache fusion based on AITER activation and custom op status, removing prior restrictions that blocked fusion when AITER RoPE was enabled.

**Potential risks**  
In-place modification of query and key tensors could lead to unexpected side effects if other parts of the code assume these tensors are immutable. The conditional enabling of fusion based on dynamic configuration increases system complexity and may introduce subtle bugs if environment variables or config states are inconsistent. Test coverage now includes both enabled and disabled states, but edge cases around tensor shapes or concurrent modifications need careful validation.

**Key insights**  
Developers should note that the AITER RoPE kernel is now the default on ROCm, providing a ~1% performance uplift in benchmarks. The fusion of RoPE with KVCache is now possible with AITER RoPE, which was previously blocked. Ensure that any custom RoPE implementations (like `DeepseekScalingRotaryEmbedding`) are compatible with the new in-place kernel, as this is a prerequisite for future MLA RoPE+KVCache fusion on ROCm.

---

## 16. [[Responses] Decouple SSE event helpers from Harmony context](https://github.com/vllm-project/vllm/pull/35148)


### Base Information

- **PR Number:** #35148
- **Author:** [sfeng33](https://github.com/sfeng33)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 20:05:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35148/files) (5):**
  - `tests/entrypoints/openai/responses/conftest.py`
  - `tests/entrypoints/openai/responses/test_harmony.py`
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/openai/responses/streaming_events.py`

### Summary

**What changed and why**  
This PR refactors SSE event helpers to decouple event-building logic from Harmony-specific context, creating a two-layer architecture. The core change splits `streaming_events.py` into Harmony-specific dispatchers that extract values from context objects, and backend-agnostic leaf helpers that build events from primitive types. This enables reuse by future backends without Harmony dependencies, while also fixing a bug where Python tools generated incorrect event types.

**Technical impact**  
The architecture now separates concerns: dispatchers handle Harmony object extraction while leaf helpers focus purely on event construction from strings and primitives. This improves code organization and reduces coupling. The `HarmonyStreamingState` class is renamed to `StreamingState` to reflect its broader utility. Test infrastructure is enhanced with comprehensive validation for event pairing, ordering, and field consistency.

**Potential risks**  
The significant refactor (+319/-422 lines) could introduce subtle bugs in event sequencing or field mapping, especially for edge cases like MCP tool calls. The removal of MCP-specific tests (`test_mcp_tools.py`) reduces coverage for that integration path. Changes to state management (`current_call_id` addition) need careful validation across all tool types.

**Key insights**  
Developers should understand the new two-layer pattern: use dispatchers for Harmony integration and leaf helpers for generic event building. The enhanced test validation provides stronger guarantees about event structure consistency. When adding new event types, ensure they follow the separation pattern and include appropriate test coverage in the validation framework.

---

## 17. [[Frontend] Use init_app_state and FrontendArgs in run_batch](https://github.com/vllm-project/vllm/pull/32967)


### Base Information

- **PR Number:** #32967
- **Author:** [pooyadavoodi](https://github.com/pooyadavoodi)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 19:40:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32967/files) (4):**
  - `tests/entrypoints/instrumentator/test_metrics.py`
  - `tests/entrypoints/openai/test_run_batch.py`
  - `vllm/entrypoints/openai/cli_args.py`
  - `vllm/entrypoints/openai/run_batch.py`

### Summary

**What changed and why**  
This PR refactors `run_batch` to use `init_app_state` and introduces a shared `BaseFrontendArgs` class. The primary goal is to enable advanced features like tool calling in batch processing by reusing the initialization logic from the API server, thereby reducing code duplication between `api_server.py` and `run_batch.py`.

**Technical impact**  
The changes centralize frontend argument handling and server-state initialization, ensuring consistency between interactive and batch modes. Batch processing now inherits the full feature set of the API server (e.g., chat templates, LoRA, tool parsers) without maintaining separate serving-object construction logic. The refactor also deprecates the `--url` flag in favor of `--host` for metrics.

**Potential risks**  
The tight coupling with `api_server.py`'s `init_app_state` may introduce unintended side effects if that function changes. The deprecation of `--url` could break existing scripts that rely on it, though backward compatibility is maintained with a warning. Increased complexity in argument parsing (via the new `BaseFrontendArgs` hierarchy) might lead to subtle bugs if subclasses override methods incorrectly.

**Key insights**  
Developers should adopt `--host` instead of `--url` for metrics configuration. The shared argument base class promotes consistency but requires careful review when adding new frontend arguments to ensure they apply appropriately to both server and batch contexts. The elimination of duplicate serving-object initialization code is a significant maintainability win.

---

## 18. [[Core] Cleanup engine pause/sleep logic](https://github.com/vllm-project/vllm/pull/34528)


### Base Information

- **PR Number:** #34528
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 19:33:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34528/files) (10):**
  - `tests/v1/distributed/test_async_llm_dp.py`
  - `tests/v1/engine/test_engine_core_client.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/serve/sleep/api_router.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/output_processor.py`

### Summary

**What changed and why**  
This PR refactors engine pause/sleep logic to centralize and unify behavior across different engine modes (multiprocess vs inline). Key changes include: moving pause/sleep functionality from `EngineCoreProc` to base `EngineCore`, adding pause mode support to sleep methods, deduplicating `LLM.enqueue` logic, cleaning up output processor state tracking, and updating tests for DPEP (Data Parallel Expert Parallel) support.

**Technical impact**  
The changes create a more consistent pause/sleep interface that works across both multiprocess and inline engine modes. Centralizing cache resetting and pause logic in `EngineCore` reduces code duplication. The addition of pause mode parameter to sleep methods allows finer control over request handling during sleep operations. Test updates ensure DPEP scenarios are properly validated.

**Potential risks**  
The refactored `_idle_state_callbacks` mechanism replaces the previous `per_step_hooks` approach, which could affect custom engine extensions. The removal of `_requests_drained` event from `OutputProcessor` might impact external monitoring of request completion. Changes to the busy loop logic (`has_work()` method) could affect engine responsiveness in edge cases.

**Key insights**  
Developers should note that pause mode "wait" is now explicitly unsupported in inline-engine mode (raises `ValueError`). The sleep method now always pauses the scheduler before sleeping, regardless of level. Test coverage has been expanded to include both MoE+EP and non-MoE DP scenarios. The `LLM.enqueue` method now delegates to `_add_completion_requests` for consistency with other completion methods.

---

## 19. [[Bugfix] Fix expert_ids padding values in moe_align_block_size kernel](https://github.com/vllm-project/vllm/pull/35161)


### Base Information

- **PR Number:** #35161
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 17:14:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35161/files) (2):**
  - `csrc/moe/moe_align_sum_kernels.cu`
  - `tests/kernels/moe/test_moe_align_block_size.py`

### Summary

**What changed and why**  
The PR fixes a bug where the `moe_align_block_size` kernel incorrectly padded invalid `expert_ids` with 0 instead of -1. Downstream fused MoE kernels expect -1 for invalid entries, as referenced in the fused_moe.py code. Test cases were updated to reflect this change and validate the padding behavior.

**Technical impact**  
This change ensures consistency between the kernel's output and downstream processing, preventing potential misinterpretation of padded values as valid expert indices. The kernel now correctly marks unused blocks with -1, aligning with the fused MoE layer's expectations.

**Potential risks**  
If any downstream code still assumes 0 for invalid entries, it could lead to incorrect expert routing or indexing errors. The test updates only validate the kernel in isolation; integration tests across the full MoE pipeline would help catch any residual assumptions.

**Key insights**  
Always verify padding conventions across kernel boundaries, especially when multiple components interact. The fix is minimal but critical for correctness. Developers should review other kernels for similar inconsistencies and ensure end-to-end tests cover these integration points.

---

## 20. [Adding Nemotron fp8 Triton MoE Config](https://github.com/vllm-project/vllm/pull/34674)


### Base Information

- **PR Number:** #34674
- **Author:** [yugong333](https://github.com/yugong333)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 15:56:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34674/files) (1):**
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_H200,dtype=fp8_w8a8.json`

### Summary

**What changed and why**  
A new Triton kernel configuration file (`E=128,N=1856,device_name=NVIDIA_H200,dtype=fp8_w8a8.json`) was added for the Nemotron-3-Nano-30B-A3B-FP8 model with LoRA adapter. This file contains tuned parameters (e.g., block sizes, warps, stages) for the fused MoE (Mixture of Experts) layer, optimized for FP8 precision on NVIDIA H200 hardware.

**Technical impact**  
The configuration enables more efficient execution of the MoE layer for the specified model and hardware, potentially improving inference throughput. It integrates into vLLM's existing fused MoE system, allowing the runtime to select these tuned parameters automatically when the model, device, and data type match.

**Potential risks**  
The configuration is hardware-specific (NVIDIA H200) and may not perform optimally on other GPU architectures. Additionally, the impact on numerical stability or accuracy in FP8 mode should be validated, as reduced precision can affect model output quality.

**Key insights**  
Developers should ensure this configuration is only applied to compatible hardware and FP8 workloads. Consider adding similar tuned configs for other GPU architectures if needed. The reported ~5-7% OTPS improvement is promising, but verify end-to-end correctness with the target model.

---

## 21. [Convert wvSplitKQ to 16x16 MFMA in prep for mi4xx.](https://github.com/vllm-project/vllm/pull/34100)


### Base Information

- **PR Number:** #34100
- **Author:** [amd-hhashemi](https://github.com/amd-hhashemi)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-24 15:35:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34100/files) (1):**
  - `csrc/rocm/skinny_gemms.cu`

### Summary

**What changed and why**  
The PR converts the `wvSplitKQ` kernel from using 32x32 MFMA (Matrix-Fused Multiply-Add) instructions to 16x16 MFMA for FP8 skinny GEMMs. This change reduces register pressure and improves performance, particularly in preparation for upcoming mi4xx hardware support.

**Technical impact**  
Switching from 32x32 to 16x16 MFMA reduces the accumulation register footprint (from `floatx16` to `scalar8`), simplifying the reduction logic. The kernel now uses `__shfl_down` for warp-level reductions instead of complex DPP (Data Parallel Primitives) and `__shfl` combinations, streamlining the code and likely improving occupancy.

**Potential risks**  
The reduction logic changes could affect numerical precision or correctness in edge cases, though tests pass. The new MFMA shape processes different tile dimensions per instruction, which may alter performance characteristics on non-target architectures. Ensure the kernel remains compatible with all supported ROCm versions and GPU architectures.

**Key insights**  
The change yields significant performance gains (36% throughput improvement) while maintaining accuracy. Developers should verify that the 16x16 MFMA is optimal across all target GPUs and consider adding architecture-specific dispatch if needed. The simplified reduction code improves maintainability but requires careful validation for all data types and shapes.

---

## 22. [[Bug][DSV3.2] Always prepare metadata for DeepGEMM Sparse Attention](https://github.com/vllm-project/vllm/pull/35075)


### Base Information

- **PR Number:** #35075
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 14:55:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35075/files) (1):**
  - `vllm/v1/attention/backends/mla/indexer.py`

### Summary

**What changed and why**  
The fix addresses a crash when using DeepSeek V3.2 with `VLLM_USE_DEEP_GEMM=0` and `FLASHMLA_SPARSE` attention. The issue occurred because `is_deep_gemm_supported()` was replaced with a more accurate condition that checks both CUDA platform and DeepGEMM availability (`has_deep_gemm()`), ensuring metadata is only prepared when DeepGEMM is actually available.

**Technical impact**  
This change ensures that `get_paged_mqa_logits_metadata()` is only called when DeepGEMM is supported on CUDA devices, preventing the use of uninitialized/stale scheduler metadata in `fp8_paged_mqa_logits`. The fix maintains correct behavior for both DeepGEMM-enabled and disabled configurations.

**Potential risks**  
If `has_deep_gemm()` has different semantics than the original `is_deep_gemm_supported()`, it could affect other attention backends or hardware configurations. The change assumes CUDA is required for paged MQA logits, which may need validation for future platform support.

**Key insights**  
Always validate both platform compatibility and feature availability before preparing hardware-specific metadata. The fix demonstrates the importance of precise condition checking to avoid stale state issues in performance-critical attention backends.

---

## 23. [[CI] Fix Distributed Tests](https://github.com/vllm-project/vllm/pull/35236)


### Base Information

- **PR Number:** #35236
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-24 14:31:56
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35236/files) (1):**
  - `.buildkite/test_areas/distributed.yaml`

### Summary

**What changed and why**  
The change fixes a nightly CI failure by replacing a `cd` command followed by a relative path execution with a direct absolute path execution. This prevents the directory change from affecting subsequent commands in the same CI step.

**Technical impact**  
This modification ensures command isolation within the CI pipeline step. By removing the `cd` command, all subsequent commands in the step will execute from the original working directory, maintaining consistent environment state across the entire step.

**Potential risks**  
The main risk is minimal - if the Python script had dependencies on being executed from its own directory (like relative file imports or data file access), those could break. However, the script path is now fully qualified, which typically handles this correctly.

**Key insights**  
Always use absolute paths in CI command sequences to avoid unintended side effects from directory changes. This pattern improves reliability and makes CI steps more predictable and easier to debug. The fix demonstrates good CI hygiene by eliminating state mutation between commands.

---

## 24. [[ROCm][CI] Added MI325 mirrors](https://github.com/vllm-project/vllm/pull/34923)


### Base Information

- **PR Number:** #34923
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-24 13:37:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34923/files) (7):**
  - `.buildkite/scripts/hardware_ci/run-amd-test.sh`
  - `.buildkite/test_areas/engine.yaml`
  - `.buildkite/test_areas/entrypoints.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `.buildkite/test_areas/models_language.yaml`
  - `docker/Dockerfile.rocm`
  - `tests/v1/kv_connector/unit/test_moriio_connector.py`

### Summary

**What changed and why**  
This PR adds AMD MI325 GPU support to the CI pipeline by introducing new test mirrors and enhancing the ROCm test runner script. The changes include restructuring the test runner with helper functions, adding multi-node detection, fixing pytest marker quoting issues, applying ROCm-specific test overrides, and adding RDMA dependencies for MoRIIO tests.

**Technical impact**  
The modifications significantly improve the ROCm CI infrastructure by making it more maintainable and robust. The test runner now handles multi-node configurations systematically, prevents pytest marker parsing errors, and centralizes ROCm test exclusions. Adding MI325 mirrors expands hardware coverage while maintaining dependency management through the CI pipeline.

**Potential risks**  
The extensive string manipulation for pytest commands could break if test patterns change unexpectedly. Multi-node detection relying on bracket syntax might be fragile with future command variations. The new RDMA availability check in tests assumes `ibv_devinfo` is installed in all environments, which could cause false negatives.

**Key insights**  
Developers should note the centralized ROCm test overrides in `apply_rocm_test_overrides()` for future exclusions. The pytest marker re-quoting fix is critical for maintaining test integrity across shell expansions. When adding new hardware mirrors, ensure dependency chains in YAML files are correctly sequenced to avoid CI failures.

---

## 25. [[Model][Spec Decode] Nemotron-H MTP and Mamba Speculative Decoding Support](https://github.com/vllm-project/vllm/pull/33726)


### Base Information

- **PR Number:** #33726
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 09:49:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33726/files) (19):**
  - `tests/models/registry.py`
  - `tests/v1/attention/test_mamba_update_block_table.py`
  - `vllm/config/speculative.py`
  - `vllm/config/vllm.py`
  - `vllm/model_executor/layers/mamba/abstract.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`
  - `vllm/model_executor/layers/mamba/mamba_utils.py`
  - `vllm/model_executor/layers/mamba/ops/causal_conv1d.py`
  - `vllm/model_executor/layers/mamba/short_conv.py`
  - `vllm/model_executor/models/mamba2.py`
  - `vllm/model_executor/models/nemotron_h.py`
  - `vllm/model_executor/models/nemotron_h_mtp.py`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/configs/nemotron_h.py`
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/attention/backends/mamba_attn.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR adds support for Nemotron-H MTP (Multi-Token Prediction) speculative decoding and extends speculative decoding support to Mamba attention backends. The changes enable MTP for Nemotron-H models and generalize Mamba speculative decoding beyond Qwen3-Next by refactoring attention metadata to handle speculative tokens on the second axis.

**Technical impact**  
The implementation modifies Mamba attention backends to separate prefill and decode state indices, adding `num_accepted_tokens` and `query_start_loc_d` for indexing into speculative decode batches. It introduces a new `NemotronHMTPModel` class with MTP-specific layers and updates configuration handling to recognize Nemotron-H MTP variants. Kernel support for these changes already exists.

**Potential risks**  
Several features are incompatible with speculative decoding for linear attention models: chunked prefill, prefix caching, and `supports_update_block_table` are disabled or cause issues. Asynchronous scheduling is ineffective due to synchronization in `_update_states_after_model_execute`. The MTP implementation currently supports only one MTP layer, and testing coverage for the Mamba speculative decoding pathway is limited.

**Key insights**  
Developers should be aware that enabling speculative decoding with Mamba models disables certain performance features and requires careful testing. The changes are designed to minimize complexity but introduce new metadata fields that must be handled consistently across Mamba backends. Future work should address the identified incompatibilities and optimize scheduling overlap.

---

## 26. [Add @MatthewBonanni to CODEOWNERS](https://github.com/vllm-project/vllm/pull/35207)


### Base Information

- **PR Number:** #35207
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-24 09:45:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35207/files) (1):**
  - `.github/CODEOWNERS`

### Summary

**What changed and why**  
MatthewBonanni has been added as a code owner for four key directories: `/vllm/model_executor/layers/attention`, `/vllm/vllm_flash_attn`, `/vllm/v1/attention`, and `/vllm/v1/spec_decode`. This change reflects their expanded responsibilities in reviewing and shepherding attention-related and speculative decoding work, as noted in the PR description.

**Technical impact**  
These updates modify the CODEOWNERS file to include MatthewBonanni in the review process for critical components of the vLLM codebase. This will distribute review workload and ensure expertise is applied to attention mechanisms and speculative decoding features, potentially accelerating development in these areas.

**Potential risks**  
The primary risk is inconsistent review standards if new and existing code owners have differing interpretations of code quality or architectural patterns. Additionally, if MatthewBonanni's availability changes, it could affect review turnaround times for these components. There are no direct technical risks to the codebase itself.

**Key insights**  
This change strategically aligns code ownership with contributor expertise, particularly for attention and speculative decoding—areas highlighted as priorities. Teams should ensure clear communication channels among code owners to maintain review consistency. The update also demonstrates healthy project growth by formally recognizing active contributors.

---

## 27. [Revert "[CI/Build] Remove redundant OpenTelemetry pip install from CI configs"](https://github.com/vllm-project/vllm/pull/35211)


### Base Information

- **PR Number:** #35211
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 09:26:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35211/files) (2):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/misc.yaml`

### Summary

**What changed and why**  
This PR reverts the removal of OpenTelemetry pip install commands from CI configuration files. The original removal (#35032) broke the `test_traces` test in the tracing module, so this revert restores the OpenTelemetry dependencies until a proper fix can be implemented.

**Technical impact**  
The changes restore explicit installation of OpenTelemetry packages in three CI pipeline steps that run tracing tests. This ensures the required dependencies are available during test execution, maintaining test stability but reintroducing redundant installations if OpenTelemetry is already available through other means.

**Potential risks**  
The revert temporarily reintroduces dependency redundancy in CI configurations. There's a risk that future changes might again attempt to remove these installations without addressing the underlying dependency issue, potentially causing recurring test failures.

**Key insights**  
The tracing tests have an implicit dependency on OpenTelemetry that isn't properly declared elsewhere. Developers should investigate why the tests fail without explicit installation—likely missing from project dependencies or test environment setup—and fix the root cause rather than relying on CI workarounds.

---

## 28. [[CI/Build] Fix kernels test location](https://github.com/vllm-project/vllm/pull/35205)


### Base Information

- **PR Number:** #35205
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 09:20:34
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35205/files) (1):**
  - `.buildkite/test_areas/kernels.yaml`

### Summary

**What changed and why**  
The change fixes a file path in the CI configuration, correcting `kernels/moe/test_flashinfer_moe.py` to `tests/kernels/moe/test_flashinfer_moe.py`. This ensures the test runner can locate and execute the correct test file, addressing a broken test reference.

**Technical impact**  
This update resolves a CI pipeline failure by aligning the test path with the project's directory structure. It ensures the FlashInfer MOE kernel tests are executed as intended, maintaining test coverage for this component.

**Potential risks**  
If the corrected path does not exist or contains different tests, the CI could still fail. Additionally, other similar path discrepancies in the configuration might remain undetected, potentially causing intermittent test execution issues.

**Key insights**  
Always verify test file paths in CI configurations match the actual repository structure. Consider adding a validation step in the CI to catch such mismatches early. This fix is minimal but critical for maintaining reliable test automation.

---

## 29. [[Perf] Optimize Python Slice for Structured Output using `islice` instead of [:]](https://github.com/vllm-project/vllm/pull/33593)


### Base Information

- **PR Number:** #33593
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 09:02:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33593/files) (8):**
  - `vllm/reasoning/abs_reasoning_parsers.py`
  - `vllm/reasoning/basic_parsers.py`
  - `vllm/reasoning/deepseek_v3_reasoning_parser.py`
  - `vllm/reasoning/identity_reasoning_parser.py`
  - `vllm/reasoning/mistral_reasoning_parser.py`
  - `vllm/reasoning/step3_reasoning_parser.py`
  - `vllm/reasoning/step3p5_reasoning_parser.py`
  - `vllm/v1/structured_output/__init__.py`

### Summary

**What changed and why**  
The PR replaces Python list slicing (`[:]`) with `itertools.islice()` for performance optimization in structured output processing. It changes multiple reasoning parser methods to accept `Iterable[int]` instead of `Sequence[int]` for `delta_ids`, and uses `islice` to avoid creating intermediate list copies when checking for reasoning end tokens.

**Technical impact**  
These changes reduce memory overhead by eliminating temporary list allocations during streaming token processing. The switch from `Sequence` to `Iterable` for `delta_ids` makes the API more flexible and aligns with the lazy evaluation of `islice`. Performance improvements are demonstrated in microbenchmarks showing lower peak memory usage and slightly faster execution times.

**Potential risks**  
Using `islice` on a list multiple times may cause repeated iteration overhead if the underlying list is large. The `max(0, len(input_ids) - 1)` pattern in `extract_content_ids` could be simplified. There’s also a risk that some callers might rely on `delta_ids` being a concrete sequence (e.g., for multiple iterations), though the change to `Iterable` is backward compatible.

**Key insights**  
The optimization is most beneficial in high-throughput streaming scenarios where reducing memory allocations improves scalability. Developers should ensure that `delta_ids` iterables are not consumed multiple times unintentionally. Consider using `islice` consistently across similar patterns in the codebase for further memory savings.

---

## 30. [Remove `padding_index` from models that don't use it for better Transformers v5 compatibility](https://github.com/vllm-project/vllm/pull/35189)


### Base Information

- **PR Number:** #35189
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 08:04:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35189/files) (14):**
  - `vllm/model_executor/models/ernie45_moe.py`
  - `vllm/model_executor/models/ernie45_vl_moe.py`
  - `vllm/model_executor/models/granitemoeshared.py`
  - `vllm/model_executor/models/grok1.py`
  - `vllm/model_executor/models/hunyuan_v1.py`
  - `vllm/model_executor/models/jais2.py`
  - `vllm/model_executor/models/kimi_linear.py`
  - `vllm/model_executor/models/longcat_flash.py`
  - `vllm/model_executor/models/minimax_text_01.py`
  - `vllm/model_executor/models/nemotron_nas.py`
  - `vllm/model_executor/models/openpangu.py`
  - `vllm/model_executor/models/plamo2.py`
  - `vllm/model_executor/models/plamo3.py`
  - `vllm/model_executor/models/qwen3_moe.py`

### Summary

**What changed and why**  
Removed the `padding_idx` attribute initialization from 14 model implementations where it was unused. This change improves compatibility with Transformers v5, where `pad_token_id` is no longer universally present in `PreTrainedConfig` but only in configs that actually require it.

**Technical impact**  
The removal eliminates a dependency on `config.pad_token_id`, which may be absent in Transformers v5 for models that do not use padding. This simplifies the codebase by removing unused attributes and ensures these models can initialize without errors when `pad_token_id` is not defined in their configs.

**Potential risks**  
If any downstream code (e.g., in parent classes or utilities) still references `self.padding_idx`, it could lead to `AttributeError`. Additionally, models that do use padding but were incorrectly included in this cleanup may lose necessary functionality, though the PR description indicates these models do not actually use the attribute.

**Key insights**  
Verify that no inherited methods or external components rely on `self.padding_idx`. Consider adding a linting rule or test to catch unused attributes early. This cleanup aligns with Transformers v5’s modular design and reduces technical debt, but thorough testing is recommended to ensure no regression in model behavior.

---

## 31. [[CI] Remove Duplicated Tests](https://github.com/vllm-project/vllm/pull/35199)


### Base Information

- **PR Number:** #35199
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-24 07:53:30
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35199/files) (1):**
  - `.buildkite/test_areas/kernels.yaml`

### Summary

**What changed and why**  
The PR removes a duplicated test configuration for MoE (Mixture of Experts) kernels on B200 devices and adds a missing test file to the main test step. The `kernels/moe/test_flashinfer_moe.py` test was previously run in both a dedicated B200 step and the main step, causing duplicate execution.

**Technical impact**  
This change consolidates test execution by eliminating a redundant Buildkite pipeline step, reducing overall CI runtime and resource usage. The test suite remains comprehensive as all tests are now executed in a single, unified step, improving CI efficiency without sacrificing test coverage.

**Potential risks**  
If the B200-specific test step had unique configurations or environment dependencies not present in the main step, those tests might now fail or behave differently. There's also a risk that the removed step contained other unique tests not listed in the diff, though the description suggests only duplication was addressed.

**Key insights**  
This is a straightforward CI optimization that reduces redundant work. Developers should verify that all necessary test configurations and environment setups are preserved in the remaining test step, particularly any device-specific requirements for B200 hardware that might have been implicit in the removed configuration.

---

## 32. [Integrate flashinfer mm_mxfp8 in ModelOpt MXFP8](https://github.com/vllm-project/vllm/pull/35053)


### Base Information

- **PR Number:** #35053
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-24 07:45:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35053/files) (3):**
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/mxfp8_utils.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR integrates FlashInfer's new MXFP8 GEMM kernel (`mm_mxfp8`) into vLLM's ModelOpt MXFP8 quantization. It adds support for the `FLASHINFER_CUTLASS` backend, which replaces the previous emulation-only implementation. The changes include a new swizzling function for weight scales, backend-specific weight processing, and the integration of the FlashInfer kernel via a custom PyTorch operator.

**Technical impact**  
The integration enables faster MXFP8 matrix multiplication using FlashInfer's optimized CUTLASS kernel, improving inference throughput. The architecture now supports two backends (emulation and FlashInfer CUTLASS), with automatic selection based on configuration. Weight scales are swizzled into a 1D layout for the CUTLASS kernel, while the emulation backend retains the original 2D layout.

**Potential risks**  
The CUTLASS kernel imposes strict dimension constraints (K ≥ 128, N ≥ 128, and divisibility by 32), which may cause failures for small models or layers. The swizzling process adds complexity and could introduce errors if scale tensors are malformed. There is also a risk of performance degradation if input dimensions require padding, though the implementation handles this.

**Key insights**  
Developers should ensure model dimensions meet the CUTLASS kernel requirements before enabling the FlashInfer backend. The swizzled scale format is critical for performance and accuracy; validate scale tensors post-swizzling. The PR shows significant performance gains (≈44% throughput increase) with minimal accuracy drop, making it a valuable optimization for supported hardware and model sizes.

---

## 33. [Fix fallback to default tactic (flashinfer autotuner) with trtllm_fp4_block_scale_moe](https://github.com/vllm-project/vllm/pull/35088)


### Base Information

- **PR Number:** #35088
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 07:25:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35088/files) (1):**
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
The PR replaces `.flatten()` with `.reshape(*hidden_states_fp4.shape[:-1], -1)` for the `hidden_states_scale` tensor in two FlashInfer FP4 MoE functions. This ensures the scale tensor's first dimension matches `num_tokens` (the batch dimension), aligning with FlashInfer's autotuner expectations and preventing fallback to a default tactic.

**Technical impact**  
This change optimizes kernel selection by allowing the FlashInfer autotuner to correctly interpret tensor dimensions, potentially improving inference performance for FP4 MoE models. It maintains compatibility with existing model execution patterns, as evidenced by unchanged accuracy and throughput in tests.

**Potential risks**  
If the `hidden_states_fp4` tensor shape is inconsistent (e.g., unexpected dimensions), the reshape could produce incorrect tensor layouts. Additionally, the change assumes the scale tensor's last dimension aligns with the hidden states' structure, which may not hold for all model configurations.

**Key insights**  
Always verify tensor dimension assumptions when interfacing with external kernels like FlashInfer. The fix is minimal and mirrors patterns used elsewhere in the codebase (e.g., `Mxfp4MoEMethod.apply_monolithic`), reinforcing consistency. Developers should test with varied model architectures to ensure robustness.

---

## 34. [[CPU][Perf] Accelerate Attention head for s390x using vector intrinsics](https://github.com/vllm-project/vllm/pull/34434)


### Base Information

- **PR Number:** #34434
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 07:25:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34434/files) (6):**
  - `csrc/cpu/cpu_attn.cpp`
  - `csrc/cpu/cpu_attn_impl.hpp`
  - `csrc/cpu/cpu_attn_vxe.hpp`
  - `csrc/cpu/generate_cpu_attn_dispatch.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/v1/attention/backends/cpu_attn.py`

### Summary

**What changed and why**  
This PR adds s390x CPU architecture support for accelerated attention computation using VXE (Vector Extension Facility) intrinsics. It introduces a new VXE ISA implementation in the CPU attention system, enabling vectorized GEMM kernels for QK and PV attention phases to improve token generation throughput on s390x platforms.

**Technical impact**  
The changes extend the CPU attention dispatch mechanism to include VXE as a new ISA option, adding s390x-specific vectorized kernels that handle float, bfloat16, and half precision. This enables s390x systems to utilize chunked prefill and prefix caching features previously disabled for this architecture, bringing performance parity with other supported CPU architectures.

**Potential risks**  
The VXE implementation assumes big-endian byte ordering for bfloat16 handling, which could cause portability issues if used on little-endian s390x variants. The manual unrolling for half precision (c10::Half) uses stack-allocated arrays without explicit vectorization, potentially missing optimization opportunities. There's also a risk of compilation failures on non-s390x platforms due to the inclusion of s390x-specific headers.

**Key insights**  
Developers should verify the big-endian assumptions in the bfloat16 conversion code match their target s390x environment. The half precision implementation should be reviewed for potential vectorization improvements. The PR successfully removes s390x from the disabled architectures list, enabling advanced features that now have optimized implementations.

---

## 35. [Fix GLM4 parser tests](https://github.com/vllm-project/vllm/pull/34905)


### Base Information

- **PR Number:** #34905
- **Author:** [RNabel](https://github.com/RNabel)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-24 06:27:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34905/files) (1):**
  - `tests/tool_parsers/test_glm4_moe_tool_parser.py`

### Summary

**What changed and why**  
The changes re-enable previously skipped GLM4 parser tests by fixing test setup issues. The core fix involves adding a `mock_request` fixture that provides a proper `ChatCompletionRequest` with tools defined, since the GLM4 parser requires the `request.tools` attribute to be non-empty to activate tool parsing logic. All test methods were updated to use this mock request instead of passing `None`.

**Technical impact**  
These changes ensure the GLM4 tool parser's unit tests correctly exercise the tool extraction logic by simulating real request conditions. The removal of `pytest.skip` re-enables test execution, and the consistent use of a mocked request object validates that the parser behaves as expected when tools are defined in the request.

**Potential risks**  
If the mock request does not accurately reflect production request structures (e.g., missing optional fields or incorrect tool definitions), tests may pass while masking integration issues. Additionally, changes to content expectations (e.g., trailing spaces or newlines in extracted content) could affect downstream consumers if not aligned with actual parser behavior.

**Key insights**  
Always ensure unit tests mirror real usage scenarios—here, providing a valid request object was critical. Developers should verify that mock fixtures comprehensively represent production data structures. Consider adding a test case with `request=None` to explicitly validate the parser's behavior when tools are disabled.

---

## 36. [[Attn,KV-cache] Use per-head scales in the attention selector](https://github.com/vllm-project/vllm/pull/34281)


### Base Information

- **PR Number:** #34281
- **Author:** [eldarkurtic](https://github.com/eldarkurtic)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-24 06:02:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34281/files) (6):**
  - `tests/kernels/attention/test_attention_selector.py`
  - `vllm/model_executor/layers/attention/attention.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/selector.py`

### Summary

**What changed and why**  
The changes enforce that attention backends must support per-head quantization scales when a model's KV-cache scheme uses the "attn_head" strategy. This validation occurs during backend selection, ensuring early failure if no compatible backend is available, rather than at runtime.

**Technical impact**  
A new `supports_per_head_quant_scales` class method is added to the attention backend base class, with FlashAttention v3 being the only backend currently returning `True`. The attention selector now includes `use_per_head_quant_scales` in its configuration, and backends are filtered accordingly. The compressed-tensors layer no longer checks `layer.impl.supports_per_head_quant_scales` at initialization, moving validation to the selector.

**Potential risks**  
If future backends incorrectly report support for per-head scales, it could lead to runtime errors. The removal of the assertion in `compressed_tensors.py` shifts validation entirely to the selector, which assumes correct backend reporting. Additionally, the test suite may need expansion to cover edge cases like mixed backend configurations or unsupported GPU architectures.

**Key insights**  
This change centralizes compatibility checks in the attention selector, improving error handling and maintainability. Developers adding new backends must implement `supports_per_head_quant_scales` accurately. The validation logic is now more consistent, but thorough testing of per-head scale support across different hardware and backend combinations is recommended.

---

## 37. [[Frontend] Always pass `supported_tasks` to validation](https://github.com/vllm-project/vllm/pull/35186)


### Base Information

- **PR Number:** #35186
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 04:16:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35186/files) (4):**
  - `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
This PR resolves a TODO by ensuring `supported_tasks` is always passed to the validation logic in `input_processor.py`. The changes modify the `add_request` methods in both async and synchronous engines to pass `supported_tasks` as a required parameter, and update the `_validate_params` method to require this parameter and validate generation tasks.

**Technical impact**  
The validation logic now properly checks if the model supports generation tasks when `SamplingParams` are used, in addition to the existing pooling task validation. This ensures consistent validation across all request types and prevents runtime errors by making `supported_tasks` a required parameter rather than optional.

**Potential risks**  
If any code paths call `process_inputs` without providing `supported_tasks`, they will now fail with a type error. The parameter reordering in method signatures could potentially break external integrations that rely on positional arguments, though keyword arguments are used consistently.

**Key insights**  
The PR completes the validation framework by extending it to generation tasks. Developers should ensure all calls to `process_inputs` provide the `supported_tasks` parameter. The fix also includes a minor documentation correction in the speech-to-text module unrelated to the main change.

---

## 38. [[Bugfix] Fix failing FunASR processor test](https://github.com/vllm-project/vllm/pull/35111)


### Base Information

- **PR Number:** #35111
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 04:13:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35111/files) (2):**
  - `tests/models/multimodal/processing/test_common.py`
  - `vllm/transformers_utils/processors/funasr_processor.py`

### Summary

**What changed and why**  
The PR fixes a bug in the FunASR processor implementation that was causing mm cache to fail. The changes remove a test skip for the FunASR model and correct the processor's frontend initialization to properly pass the `dither` parameter and use a local instance.

**Technical impact**  
The fix enables proper caching for the FunASR model by ensuring the audio feature extraction frontend is correctly configured with the required `dither` setting. This allows the previously skipped test to pass and improves the model's performance through effective multimodal caching.

**Potential risks**  
If the `dither` parameter configuration is incorrect or incompatible with the `WavFrontend` class, it could lead to silent failures in audio processing. The change from an instance variable (`self.frontend`) to a local variable (`frontend`) might affect other methods if they rely on the shared frontend instance.

**Key insights**  
The fix demonstrates the importance of proper parameter passing in processor initialization for caching compatibility. Developers should verify that all required parameters are correctly propagated when creating processor components, and consider whether component instances should be shared or created locally based on usage patterns.

---

## 39. [[glm-asr] change defaults dummy audio size](https://github.com/vllm-project/vllm/pull/35108)


### Base Information

- **PR Number:** #35108
- **Author:** [eustlb](https://github.com/eustlb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 04:13:33
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35108/files) (1):**
  - `tests/models/multimodal/processing/test_common.py`

### Summary

**What changed and why**  
The PR modifies the test data generation for audio inputs in multimodal processing tests to accommodate GLM-ASR model requirements. Specifically, it increases the minimum dummy audio length from 512 samples to 1120 samples (70ms at 16kHz) for GLM-ASR models to ensure sufficient mel-spectrogram features are generated during processing.

**Technical impact**  
This change ensures that GLM-ASR model tests pass by providing audio inputs that meet the model's minimum length requirement of 4 mel-features (which translates to 1120 samples). The modification is isolated to test code and doesn't affect production logic, maintaining backward compatibility for other model types that still use the 512-sample minimum.

**Potential risks**  
The risk is minimal since this only affects test data generation. However, there's a slight risk that other audio models might have different minimum length requirements that aren't accounted for. The hardcoded 1120 value assumes specific GLM-ASR architecture details (merged_factor=4, hop_length=160) that could change in future model versions.

**Key insights**  
Test data generation should account for model-specific requirements. Consider making minimum audio lengths configurable per model type rather than using hardcoded values. The fix correctly addresses the immediate test failure while maintaining the existing test structure for other models.

---

## 40. [[Perf] Optimize pooling model redundant copy, 1.8% throughput improvement](https://github.com/vllm-project/vllm/pull/35127)


### Base Information

- **PR Number:** #35127
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 04:13:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35127/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The change optimizes pooling model output copying by eliminating redundant full-batch copies. Previously, all pooling outputs were copied to CPU regardless of completion status; now only outputs from finished requests are selectively copied using a new helper function `_copy_pooler_output_to_cpu`.

**Technical impact**  
This reduces memory bandwidth and CPU-GPU transfer overhead, particularly for partial batches where some requests are still pending. The optimization maintains correctness by preserving `None` for unfinished requests and handles both tensor and list-based pooling outputs.

**Potential risks**  
Edge cases include mismatched batch sizes between `raw_pooler_output` and `finished_mask`, which are validated but could cause runtime errors. The `index_select` path assumes contiguous finished indices; non-contiguous masks may slightly reduce efficiency but remain correct.

**Key insights**  
The 1.8% throughput improvement demonstrates meaningful gains from minimizing unnecessary data movement. Developers should ensure `finished_mask` accurately reflects request completion. Consider extending similar selective-copy patterns to other model outputs where partial batch completion is common.

---

## 41. [[compile] Save aot compile artifacts atomically.](https://github.com/vllm-project/vllm/pull/35117)


### Base Information

- **PR Number:** #35117
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 04:11:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35117/files) (1):**
  - `vllm/compilation/decorators.py`

### Summary

**What changed and why**  
The change modifies the AOT compilation artifact saving process to be atomic by first writing to a temporary file and then using `os.replace()` to rename it to the final destination. This prevents corrupted files if the save operation is interrupted, and the comment indicates this pattern should be upstreamed to PyTorch.

**Technical impact**  
This improves the robustness of the AOT compilation cache by ensuring that either a complete, valid file exists at the target path or no file exists (if the temporary save fails). The atomic rename operation is filesystem-dependent but generally provides the required atomicity on modern systems.

**Potential risks**  
The temporary filename uses `os.getpid()` which could cause collisions in rare cases if multiple processes with the same PID (after PID reuse) attempt to save to the same cache directory concurrently. Additionally, `os.replace()` may not be fully atomic on all filesystems or platforms, though it's the best available option.

**Key insights**  
The change is a standard pattern for atomic file writes. Consider using a more robust temporary naming scheme (e.g., adding a random component) to further reduce collision risk in highly concurrent scenarios. The upstreaming comment suggests this fix may be temporary until PyTorch incorporates similar atomic save functionality.

---

## 42. [[Feature] Add LoRA tower/connector support for Llama 4 Vision (mllama4)](https://github.com/vllm-project/vllm/pull/35147)


### Base Information

- **PR Number:** #35147
- **Author:** [dorhuri123](https://github.com/dorhuri123)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-24 04:10:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35147/files) (1):**
  - `vllm/model_executor/models/mllama4.py`

### Summary

**What changed and why**  
The changes enable LoRA adapter support for the vision tower and connector components in Llama 4 Vision models. Previously, LoRA could only target language model layers; now, with the `--enable-tower-connector-lora` flag, LoRA can also be applied to the vision encoder's attention layers (tower) and the vision adapter MLP plus multi-modal projector (connector).

**Technical impact**  
The update modifies the multimodal module mapping to include both connector components and adds two new token conversion methods. These methods handle the token count translation between the vision encoder (which includes a CLS token) and the connector/language model (after pixel-shuffle downsampling), ensuring proper LoRA application across the vision processing pipeline.

**Potential risks**  
The token conversion logic assumes consistent chunking and could produce incorrect counts if `num_image_tokens` isn't perfectly divisible by `patches_per_chunk`. Additionally, the expanded connector mapping might inadvertently include unrelated modules if future changes add new submodules under the `vision_model.vision_adapter.` prefix.

**Key insights**  
This implementation follows established patterns from InternVL2 and Qwen2VL, demonstrating good architectural consistency. Developers should verify that any future modifications to the vision adapter structure don't conflict with the longest-prefix matching used for LoRA module mapping. The token flow documentation is crucial for understanding the vision processing pipeline.

---

## 43. [Make voxtral compile friendly](https://github.com/vllm-project/vllm/pull/33959)


### Base Information

- **PR Number:** #33959
- **Author:** [tugsbayasgalan](https://github.com/tugsbayasgalan)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-24 00:33:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33959/files) (1):**
  - `vllm/model_executor/models/voxtral_realtime.py`

### Summary

**What changed and why**  
The PR introduces a conditional workaround for a torch.compile compatibility issue in the Voxtral model. For PyTorch versions older than 2.11, it replaces a `.view()` operation with `.reshape().clone()` to break a view chain that causes serialization errors in AOT autograd cache. This is a temporary fix for a known PyTorch issue (#174299) that will be resolved starting with PyTorch 2.11.

**Technical impact**  
This change ensures the Voxtral model can be compiled with `torch.compile` on PyTorch versions <2.11 by avoiding a specific cache serialization bug related to symbolic integer shapes in view operations. The model behavior remains functionally identical, as the clone operation preserves tensor data while creating a new memory layout that bypasses the problematic view metadata chain.

**Potential risks**  
The `.clone()` operation introduces a memory copy, which may slightly increase memory usage and computation time for the affected tensor reshaping. There's a risk that developers might forget to remove this workaround after upgrading to PyTorch ≥2.11, though the conditional check helps mitigate this. The fix assumes the PyTorch issue is fully resolved in version 2.11, which should be verified when upgrading.

**Key insights**  
This is an excellent example of a targeted, version-gated workaround for a third-party library issue. The code includes clear documentation linking to the upstream bug report and explaining the temporary nature of the fix. Developers should monitor PyTorch releases and remove the workaround once the minimum supported version reaches 2.11. The use of `is_torch_equal_or_newer()` utility demonstrates good practices for version-dependent code paths.

---

