# vLLM Merged PR Report

**Report Date:** 2026-02-05 PST

**Total Merged PRs:** 36

---

## 1. [[cpu][performance] CPU Paged Attention NEON BFMMLA BF16 Implementation](https://github.com/vllm-project/vllm/pull/32263)


### Base Information

- **PR Number:** #32263
- **Author:** [gassan-arm](https://github.com/gassan-arm)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-05 23:01:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32263/files) (4):**
  - `csrc/cpu/cpu_attn_impl.hpp`
  - `csrc/cpu/cpu_attn_neon.hpp`
  - `csrc/cpu/cpu_attn_neon_bfmmla.hpp`
  - `vllm/v1/attention/backends/cpu_attn.py`

### Summary

**What changed and why**  
This PR introduces a NEON-optimized BF16 implementation for CPU Paged Attention using the BFMMLA instruction, specifically targeting Arm architectures. The changes include a new kernel implementation (`cpu_attn_neon_bfmmla.hpp`) that leverages BFMMLA for matrix multiplication, along with integration into the existing NEON attention backend. The primary goal is to achieve significant performance improvements—2.32x for prefill and 2.07x for decode—over the current NEON implementation.

**Technical impact**  
The implementation adds a specialized BF16 kernel that uses BFMMLA tile-based operations (2x4x2 tiles) for efficient attention computation. It integrates conditionally via `ARM_BF16_SUPPORT` and reuses the existing attention framework with 32-token alignment. The changes also adjust the ISA selection logic in Python to correctly detect Arm CPUs and ensure the NEON backend is used for BF16 with block size 32.

**Potential risks**  
The implementation relies on compile-time detection of `ARM_BF16_SUPPORT`, which may not be available on all Arm platforms, potentially leading to fallback paths or compilation errors. The tail-handling logic for partial K dimensions (e.g., `K_tail`) introduces additional branching that could impact performance on edge cases. There is also a risk of regression if the BFMMLA kernel is incorrectly selected on non-compatible hardware.

**Key insights**  
Developers should ensure that the target Arm CPU supports BFMMLA and that `ARM_BF16_SUPPORT` is properly defined. The performance gains are substantial, but the implementation adds complexity with specialized packing and accumulation macros. It is critical to validate the fallback paths for partial tiles and ensure the Python-side ISA detection aligns with the hardware capabilities to avoid runtime errors.

---

## 2. [Onboard voyage-4-nano](https://github.com/vllm-project/vllm/pull/33720)


### Base Information

- **PR Number:** #33720
- **Author:** [chengchengpei](https://github.com/chengchengpei)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-05 22:23:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33720/files) (8):**
  - `docs/models/supported_models.md`
  - `tests/models/language/pooling_mteb_test/test_voyage.py`
  - `tests/models/registry.py`
  - `vllm/config/model.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/qwen3.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/voyage.py`

### Summary

**What changed and why**  
This PR adds support for the `voyage-4-nano` embedding model by implementing a new model class `VoyageQwen3BidirectionalEmbedModel`. The changes include model registration, configuration updates, weight loading logic, and integration tests to ensure the model works correctly with vLLM's pooling and embedding features.

**Technical impact**  
The model extends `Qwen3Model` but uses bidirectional (encoder-only) attention and adds a linear embedding head. Key modifications include a custom weight loader that fuses QKV and gate/up projections to match vLLM's expected tensor formats, and configuration adjustments to mark the model as non-causal. The embedding size is now correctly derived from the Hugging Face config's `num_labels` field.

**Potential risks**  
The custom weight-loading pipeline bypasses the parent's `stacked_params_mapping`, which could lead to inconsistencies if future changes affect the base class. The model requires `enforce_eager=True` due to encoder-only attention and CUDA graph compatibility. There is also a risk of weight fusion errors if the checkpoint structure deviates from expectations.

**Key insights**  
Developers should note that this model uses a specialized weight loader and is explicitly configured as non-causal. The implementation demonstrates how to adapt a causal model for bidirectional embedding tasks. Ensure any future updates to `Qwen3Model` account for this derived class, and always test with `enforce_eager=True` for encoder-only models.

---

## 3. [[XPU]Replace pip in docker.xpu with uv pip](https://github.com/vllm-project/vllm/pull/31112)


### Base Information

- **PR Number:** #31112
- **Author:** [1643661061leo](https://github.com/1643661061leo)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-05 22:02:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31112/files) (1):**
  - `docker/Dockerfile.xpu`

### Summary

**What changed and why**  
The Dockerfile.xpu has been updated to replace standard pip with uv pip for Python package management. This change aims to significantly improve Docker image build speeds by leveraging uv's faster dependency resolution and caching mechanisms. A virtual environment is now created at `/opt/venv` to isolate Python packages, and build arguments and environment variables are introduced to support uv's configuration.

**Technical impact**  
The migration to uv pip introduces a more efficient and reproducible build process. Key changes include using uv's cache mounts (`/root/.cache/uv`), bind mounts for requirement files, and environment variables like `UV_EXTRA_INDEX_URL` and `UV_LINK_MODE`. The build now consistently uses the virtual environment's Python, ensuring package isolation and reducing conflicts with system Python. The removal of pip-specific configurations (e.g., `break-system-packages`) simplifies the setup.

**Potential risks**  
There is a risk of incompatibility with packages that rely on specific pip behaviors or legacy installation methods. The use of `UV_INDEX_STRATEGY="unsafe-best-match"` may lead to unexpected package versions if index URLs are not carefully managed. Additionally, the NIXL script installation now uses the virtual environment's Python, which could fail if the script has hardcoded Python path dependencies. The removal of PyJWT cleanup steps might cause conflicts if system packages interfere.

**Key insights**  
Developers should verify that all Python dependencies, especially those with complex build processes like `arctic-inference`, are compatible with uv's installation model. The build speed improvements are substantial (from 192.3s to 0.7s on repeat builds), but initial builds may still be slower due to uv's cache population. Ensure that the `UV_EXTRA_INDEX_URL` and `PIP_EXTRA_INDEX_URL` are correctly set for XPU-specific packages. Monitor for any regressions in package resolution or runtime behavior due to the virtual environment isolation.

---

## 4. [[XPU][4/N] add mxfp4 moe model support](https://github.com/vllm-project/vllm/pull/33679)


### Base Information

- **PR Number:** #33679
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-05 21:03:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33679/files) (1):**
  - `vllm/model_executor/layers/quantization/mxfp4.py`

### Summary

**What changed and why**  
This PR refactors MXFP4 MoE support for XPU devices by replacing the IPEX-specific implementation with a custom XPU kernel approach. The `IpexMxfp4MoEMethod` class is renamed to `XpuMxfp4MoEMethod` and its implementation is changed from using Intel's IPEX library to calling a custom `xpu_fused_moe` kernel from `vllm_xpu_kernels`.

**Technical impact**  
The changes decouple the MXFP4 MoE implementation from the IPEX library, moving to a more direct kernel invocation model. This affects weight processing (now a no-op) and the forward pass computation, which now uses custom XPU kernels for routing and expert computation instead of IPEX's fused operations.

**Potential risks**  
The refactored implementation introduces new dependencies on `vllm_xpu_kernels.fused_moe_interface`. There's a risk of regression if the custom kernel doesn't match IPEX's behavior exactly, especially for edge cases in routing logic or activation handling. The removal of padding logic (`hidden_size_pad`) might affect performance or correctness for certain tensor sizes.

**Key insights**  
This is part of a larger refactoring effort to abstract XPU kernels. Developers should verify that the new kernel implementation provides equivalent numerical results and performance compared to the IPEX version. The change simplifies the codebase by removing IPEX dependencies but adds maintenance responsibility for the custom XPU kernels.

---

## 5. [[CPU] Add BF16 Kernel type for s390x](https://github.com/vllm-project/vllm/pull/33788)


### Base Information

- **PR Number:** #33788
- **Author:** [R3hankhan123](https://github.com/R3hankhan123)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-05 20:57:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33788/files) (1):**
  - `csrc/cpu/mla_decode.cpp`

### Summary

**What changed and why**  
Added BF16 kernel type specialization for s390x architecture in `mla_decode.cpp`. This enables bfloat16 tensor operations on IBM Z systems by defining appropriate vector types for query/key/value computations in the MLA decode kernel.

**Technical impact**  
Extends CPU support for bfloat16 precision to s390x platforms, allowing models using BF16 weights (like the tested Granite model) to run efficiently. The change aligns s390x with other architectures (x86, ARM) by providing architecture-specific vector type mappings for BF16 operations.

**Potential risks**  
The implementation uses `FP32Vec16` for `qk_vec_type` while other architectures use `BF16Vec32`, which may indicate different precision handling or performance characteristics. No runtime validation is included for the vector operation implementations on s390x.

**Key insights**  
This is a necessary platform enablement change for s390x BF16 support. Developers should verify that the chosen vector type mappings provide correct numerical behavior and performance. Consider adding architecture-specific unit tests to validate BF16 operations on s390x hardware.

---

## 6. [[Misc] Update code for encoder-decoder models](https://github.com/vllm-project/vllm/pull/33900)


### Base Information

- **PR Number:** #33900
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-05 19:38:39
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33900/files) (2):**
  - `vllm/multimodal/inputs.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
Updated documentation links from neuralmagic/bart-plugin to vllm-project/bart-plugin in two locations, and added validation logic for encoder-decoder models. The changes ensure proper multimodal interface implementation by enforcing that encoder-decoder models use at most one modality.

**Technical impact**  
The validation prevents encoder-decoder models from incorrectly using multiple modalities, which could cause runtime errors. The conditional check for `mm_max_toks_per_item` prevents potential index errors when the list is empty. Documentation updates maintain accurate references to project resources.

**Potential risks**  
If existing encoder-decoder models incorrectly implement multiple modalities, the new assertion will fail at scheduler initialization. The conditional check assumes `mm_max_toks_per_item` being empty is valid, which might need verification against the multimodal interface specification.

**Key insights**  
These are defensive programming improvements that catch configuration errors early. Developers should verify that all encoder-decoder models comply with the single-modality constraint. The documentation updates reflect organizational changes in project ownership.

---

## 7. [feat(frontend): early-fail tokenization guard for user requests](https://github.com/vllm-project/vllm/pull/31366)


### Base Information

- **PR Number:** #31366
- **Author:** [scratch-ml](https://github.com/scratch-ml)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-05 19:38:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31366/files) (7):**
  - `tests/renderers/test_completions.py`
  - `vllm/renderers/params.py`
  - `vllm/tokenizers/deepseek_v32.py`
  - `vllm/tokenizers/grok2.py`
  - `vllm/tokenizers/hf.py`
  - `vllm/tokenizers/mistral.py`
  - `vllm/tokenizers/protocol.py`

### Summary

**What changed and why**  
This PR implements early-fail tokenization guards to prevent resource monopolization by overlength prompts. It introduces protective truncation by default (`truncation=True`, `max_length=max_model_len+1`) and raises `ValueError` immediately when encoded length exceeds limits, avoiding async tokenizer stalls. Callers can override with explicit `truncate_prompt_tokens=0` or custom truncation settings.

**Technical impact**  
The changes add a new `max_chars_per_token` property to tokenizer protocols and implementations, enabling character-level length validation before tokenization. Default truncation behavior is now enforced in `TokenizeParams.get_encode_kwargs()`, and validation logic is refactored into separate text/token check methods (`_text_len_check`, `_token_len_check`). This affects all tokenization paths in serving engines and renderers.

**Potential risks**  
The default truncation behavior (`max_length=max_model_len+1`) may subtly alter existing workflows that rely on untruncated tokenization. The character-length heuristic (`max_chars_per_token`) could be inaccurate for tokenizers with variable-length tokens, potentially causing false positives/negatives. Override mechanisms require careful documentation to avoid misuse leading to OOMs.

**Key insights**  
Developers should note the new `max_chars_per_token` property requirement for custom tokenizers. The `truncate_prompt_tokens=None` now triggers protective truncation instead of no truncation—explicit `truncate_prompt_tokens=0` is needed to disable. Test updates reflect these behavioral changes, emphasizing validation before tokenization to improve system resilience.

---

## 8. [fix(ROCm): Make flash_attn import optional in MLA attention](https://github.com/vllm-project/vllm/pull/33511)


### Base Information

- **PR Number:** #33511
- **Author:** [rabi](https://github.com/rabi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-05 18:22:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33511/files) (1):**
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
The changes make the `flash_attn` import optional in MLA attention for ROCm platforms. Previously, the import was unconditional, causing failures for non-MLA models on ROCm when `flash_attn` wasn't installed. Now, a try/except block attempts the import only on ROCm and provides a fallback with a clear error message if MLA is used without `flash_attn`.

**Technical impact**  
This decouples the dependency on `flash_attn` for ROCm users who don't use MLA attention, allowing non-MLA models to load successfully. The architecture now gracefully handles missing dependencies by delaying the error until MLA is actually required, improving compatibility and user experience on ROCm.

**Potential risks**  
If `flash_attn` is missing and a user attempts to use MLA with the TRITON_MLA backend, they will encounter a runtime error. The error message is clear, but there's a risk of confusion if users overlook the need to install `flash_attn` or switch to the AITER_MLA backend. Additionally, the `flash_attn_varlen_func` variable could be `None` in some code paths, requiring careful handling.

**Key insights**  
Always use optional imports for third-party dependencies that aren't universally required. This change enhances platform-specific flexibility without sacrificing functionality. Developers should ensure error messages guide users toward actionable solutions, as demonstrated here by suggesting alternative backends or installations.

---

## 9. [[Docs] Add reo analytics](https://github.com/vllm-project/vllm/pull/33957)


### Base Information

- **PR Number:** #33957
- **Author:** [simon-mo](https://github.com/simon-mo)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-05 17:42:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33957/files) (2):**
  - `docs/mkdocs/javascript/reo.js`
  - `mkdocs.yaml`

### Summary

**What changed and why**  
Added Reo.Dev analytics tracking to the vLLM documentation by including a JavaScript file that initializes the Reo analytics client with a specific client ID. This enables community analytics tracking for documentation usage as referenced in the Reo.Dev integration guide.

**Technical impact**  
The changes add a third-party analytics script to the documentation site, which will load asynchronously and initialize when the script loads. This modifies the site's behavior by sending analytics data to Reo.Dev's servers, potentially affecting page load performance and user privacy.

**Potential risks**  
The third-party script introduces external dependency risks, including potential service outages or changes to Reo.Dev's tracking behavior. There may be privacy compliance considerations depending on user jurisdiction, and the script could impact page load times if the external service is slow or unavailable.

**Key insights**  
This is a standard analytics integration for documentation tracking, but developers should monitor for any performance impacts and ensure privacy policies are updated to reflect this data collection. Consider implementing a privacy-first approach with user consent mechanisms if required by regulations.

---

## 10. [[Perf] Disable clean_logits in deepgemm fp8_mqa_logits kernel](https://github.com/vllm-project/vllm/pull/33568)


### Base Information

- **PR Number:** #33568
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-05 17:34:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33568/files) (4):**
  - `tests/kernels/attention/test_deepgemm_attention.py`
  - `tests/kernels/test_top_k_per_row.py`
  - `vllm/model_executor/layers/sparse_attn_indexer.py`
  - `vllm/utils/deep_gemm.py`

### Summary

**What changed and why**  
This PR disables the `clean_logits` parameter in DeepGEMM's FP8 MQA logits kernels to eliminate an unnecessary kernel launch (`deep_gemm::smxx_clean_logits`). The change is based on the observation that downstream sampling kernels (`topKPerRowDecode` and `topKPerRowPrefill`) already ignore padding values by reading only within the valid `rowEnd` range, making the cleaning step redundant.

**Technical impact**  
Removing the `clean_logits` kernel launch reduces GPU kernel overhead, improving overall inference latency. The modification affects both prefill and decode phases in sparse attention operations, with benchmark results showing a 15% improvement in output token throughput on H200 hardware. The change maintains functional correctness because the sampling kernels already handle padding correctly.

**Potential risks**  
If any downstream kernel or process implicitly relies on padding values being set to `-inf`, disabling cleaning could lead to incorrect behavior. Additionally, the change assumes that all future sampling implementations will continue to respect the `rowEnd` boundaries. Edge cases involving extremely large or malformed inputs should be validated to ensure no out-of-bounds accesses occur.

**Key insights**  
This optimization demonstrates that redundant GPU kernel launches can be identified and removed through careful profiling and understanding of kernel dependencies. Developers should verify that all dependent kernels properly handle padding without relying on cleaned values. The added test parameterization ensures both `clean_logits=True` and `False` paths are validated, preserving test coverage for the original behavior.

---

## 11. [[Feature] OTEL tracing during loading](https://github.com/vllm-project/vllm/pull/31162)


### Base Information

- **PR Number:** #31162
- **Author:** [emricksini-h](https://github.com/emricksini-h)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-05 16:59:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31162/files) (29):**
  - `requirements/common.txt`
  - `setup.py`
  - `tests/tracing/__init__.py`
  - `tests/tracing/conftest.py`
  - `tests/tracing/test_loading_tracing.py`
  - `tests/v1/tracing/test_tracing.py`
  - `vllm/compilation/backends.py`
  - `vllm/config/observability.py`
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/model_executor/model_loader/base_loader.py`
  - `vllm/model_executor/model_loader/default_loader.py`
  - `vllm/model_executor/model_loader/utils.py`
  - `vllm/model_executor/model_loader/weight_utils.py`
  - `vllm/model_executor/warmup/deep_gemm_warmup.py`
  - `vllm/tracing.py`
  - `vllm/tracing/__init__.py`
  - `vllm/tracing/otel.py`
  - `vllm/tracing/utils.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/output_processor.py`
  - `vllm/v1/executor/abstract.py`
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/worker/cpu_model_runner.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/worker_base.py`

### Summary

**What changed and why**  
This PR adds OpenTelemetry (OTel) tracing instrumentation to vLLM's loading and initialization processes. The primary goal is to replace unstructured log-based performance monitoring with precise span-based observability, enabling better visualization of startup timelines and eliminating fragile log parsing dependencies.

**Technical impact**  
The changes introduce a modular tracing architecture (`vllm/tracing/`) with OTel as the first backend. Key loading phases—model loading, weight download, compilation, KV cache allocation, and worker initialization—are now instrumented with spans. The system supports distributed context propagation across processes via environment variables, and tracing is optional via the `otel` extra dependency.

**Potential risks**  
1. **Performance overhead**: The `@instrument` decorator adds function call overhead; synchronous spans may impact hot paths during loading.
2. **Dependency management**: Optional OTel dependencies could lead to runtime import errors if not properly handled in all code paths.
3. **Context propagation complexity**: Environment-based trace context propagation may fail in edge cases (e.g., case-sensitive header handling).
4. **Testing fragility**: The fake gRPC server in tests may introduce flakiness under high load or network variability.

**Key insights**  
- The modular tracing design allows future backends (e.g., profiling) without disrupting existing instrumentation.
- Developers should use `is_tracing_available()` to guard expensive tracing logic and prefer manual instrumentation for precise timing control.
- Ensure the `otel` extra is included in deployment environments where tracing is required, and validate OTLP endpoint configuration in production.

---

## 12. [[Bugfix] Fix DeepSeek v3.2 tokenizer outputting None issue](https://github.com/vllm-project/vllm/pull/33832)


### Base Information

- **PR Number:** #33832
- **Author:** [wzhao18](https://github.com/wzhao18)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-05 15:50:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33832/files) (1):**
  - `vllm/tokenizers/detokenizer_utils.py`

### Summary

**What changed and why**  
Added a guard to handle cases where `new_tokens` contains `None` values, which can occur when processing out-of-vocabulary prompt token IDs (e.g., when using dummy weights). The fix calls `_replace_none_with_empty()` to convert `None` entries to empty strings before further processing.

**Technical impact**  
This change ensures the tokenizer output remains consistent and prevents potential crashes when `None` values appear in the token list. It maintains backward compatibility by preserving existing behavior for valid tokens while gracefully handling edge cases in tokenization.

**Potential risks**  
If `_replace_none_with_empty()` is not properly implemented or doesn't handle all edge cases, some `None` values might still propagate. The type hint suppression (`type: ignore[arg-type]`) suggests potential type system inconsistencies that could mask other issues.

**Key insights**  
The fix addresses a specific failure scenario in DeepSeek v3.2 tokenization but likely benefits other models with similar tokenization edge cases. Developers should verify that `_replace_none_with_empty()` robustly handles all `None` scenarios and consider whether similar protections are needed elsewhere in the tokenization pipeline.

---

## 13. [Adds padding and perf improvements to wvSplitK_fp8](https://github.com/vllm-project/vllm/pull/33527)


### Base Information

- **PR Number:** #33527
- **Author:** [amd-hhashemi](https://github.com/amd-hhashemi)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-05 14:16:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33527/files) (3):**
  - `csrc/rocm/skinny_gemms.cu`
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/rocm.py`

### Summary

**What changed and why**  
This PR adds activation padding support to the `wvSplitKQ_fp8` kernel and introduces performance improvements for bias handling and DPP (Data Parallel Primitives) reduction. The kernel now accepts separate padded dimensions for A and B matrices (`Kap`, `Kbp`), enabling support for non-16-byte-aligned inputs. Additionally, inline assembly for DPP reductions is replaced with built-in functions, and bias loading is optimized by pre-fetching into registers.

**Technical impact**  
The changes extend the kernel's compatibility to handle padded activations and weights, which is essential for memory-aligned operations in mixed-precision workloads. The shift from inline assembly to `__builtin_amdgcn_mov_dpp` improves code portability and maintainability across AMD GPU architectures. The bias optimization reduces memory access latency by caching bias values in registers, potentially improving throughput for bias-enabled operations.

**Potential risks**  
The introduction of padded dimensions (`Kap`, `Kbp`) increases kernel complexity and may introduce off-by-one errors if boundary conditions are not carefully handled. The reliance on architecture-specific built-ins (e.g., `__builtin_amdgcn_global_load_lds`) could affect portability to non-AMD platforms. Additionally, the expanded test coverage includes edge cases with padding, which must be validated to ensure correctness across all configurations.

**Key insights**  
Developers should verify that padding logic aligns with memory access patterns to avoid performance degradation. The use of built-in DPP functions enhances code clarity but requires awareness of target architecture support. The bias pre-fetching optimization is beneficial but must be tested with varying bias shapes to ensure correctness. Ensure all new test cases (including padded and biased scenarios) pass consistently to maintain robustness.

---

## 14. [[Minor] Sort safetensors files to ensure deterministic loading order](https://github.com/vllm-project/vllm/pull/33491)


### Base Information

- **PR Number:** #33491
- **Author:** [Lumosis](https://github.com/Lumosis)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-05 14:05:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33491/files) (1):**
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
This PR introduces a natural sort function (`_natural_sort_key`) and applies it to sort safetensors files by filename before iteration in `safetensors_weights_iterator`. The purpose is to ensure a deterministic loading order, which is a prerequisite for upcoming TPU sharding logic that requires predictable, layer-by-layer weight availability for optimal memory initialization.

**Technical impact**  
The change guarantees that weight files are processed in a consistent, numerically ordered sequence (e.g., model-00001-of-00005.safetensors before model-00002-of-...). This deterministic ordering is critical for downstream processes, particularly for TPU execution where weights must be sharded immediately after loading. It does not alter the final loaded model state, only the order of intermediate operations.

**Potential risks**  
The natural sort relies on `regex` (imported as `re`), which is now a new dependency for this module; ensure it's properly declared in project requirements. While the sort aims for numeric ordering, extremely complex or non-standard filenames might still lead to unexpected ordering. The change assumes that filename ordering correlates with logical layer ordering, which should be validated for all supported model repositories.

**Key insights**  
This is a well-scoped, preparatory change for TPU optimization. The `_natural_sort_key` function is reusable and clearly documented. Developers should verify that the regex-based sort behaves correctly across all expected filename patterns in production. Consider adding a unit test for the sort function with a variety of filename examples to prevent regression.

---

## 15. [[Bugfix] Make MM batching more robust](https://github.com/vllm-project/vllm/pull/33817)


### Base Information

- **PR Number:** #33817
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-05 12:40:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33817/files) (13):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/models_basic.yaml`
  - `tests/models/test_terratorch.py`
  - `tests/multimodal/media/test_connector.py`
  - `tests/multimodal/test_hasher.py`
  - `tests/multimodal/test_inputs.py`
  - `tests/multimodal/test_utils.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/multimodal/hasher.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/utils.py`

### Summary

**What changed and why**  
This PR fixes several bugs in multimodal batching to make it more robust. The main issues addressed are: Terratorch model inference failures, hashing problems with 0-D tensors and key order sensitivity, and test organization. The changes improve the MultiModalHasher to handle edge cases and ensure consistent batching behavior across different multimodal inputs.

**Technical impact**  
The modifications enhance the reliability of multimodal batching by fixing hashing inconsistencies and improving tensor handling. The `group_and_batch_mm_items` function now properly groups items based on shared field values, preventing invalid batches. Changes to Terratorch model handling ensure correct tensor dimensions and field configurations during inference.

**Potential risks**  
The added hashing overhead, while described as negligible, could impact performance with complex shared fields. The refactored batching logic introduces new grouping constraints that might affect batch composition efficiency. There's also a risk of regression in other multimodal models due to changes in tensor schema and field handling.

**Key insights**  
Developers should verify that multimodal inputs with shared fields (especially complex ones) maintain expected performance. The fixed key order invariance in hashing ensures deterministic behavior but requires consistent serialization. The test reorganization improves maintainability but necessitates updating any custom tests referencing moved utilities.

---

## 16. [[Bugfix] Fix DSV3.2 NVFP4](https://github.com/vllm-project/vllm/pull/33932)


### Base Information

- **PR Number:** #33932
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-05 11:22:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33932/files) (1):**
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
The changes fix an issue with FP8 attention in DeepSeek V3.2 models by adding conditional checks. Specifically, FP8 dtype conversion for the key-value cache is now skipped when using `fp8_ds_mla` KV cache dtype, and FP8 query concatenation is only applied when the attention implementation supports quantized query input.

**Technical impact**  
These modifications prevent incorrect dtype conversions and operations that could cause runtime errors or numerical inaccuracies in FP8 attention paths. The addition of `supports_quant_query_input = True` ensures the implementation correctly handles quantized queries for compatible attention backends.

**Potential risks**  
If `supports_quant_query_input` is incorrectly set for other attention implementations, it could lead to unsupported operations. The conditional logic assumes `fp8_ds_mla` is mutually exclusive with general `fp8_attention` handling, which may need validation across all model configurations.

**Key insights**  
Developers should verify that `supports_quant_query_input` aligns with all applicable attention backends. Testing should include edge cases with mixed FP8 configurations to ensure dtype conversions remain consistent. The fix is narrowly scoped but critical for FP8 support in specific model architectures.

---

## 17. [[Misc] Rename `translations` to `speech_to_text` for OAI serving component](https://github.com/vllm-project/vllm/pull/33904)


### Base Information

- **PR Number:** #33904
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-05 11:16:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33904/files) (7):**
  - `vllm/entrypoints/openai/api_server.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/speech_to_text/__init__.py`
  - `vllm/entrypoints/openai/speech_to_text/api_router.py`
  - `vllm/entrypoints/openai/speech_to_text/protocol.py`
  - `vllm/entrypoints/openai/speech_to_text/serving.py`
  - `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`

### Summary

**What changed and why**  
The changes rename the `translations` directory to `speech_to_text` for the OpenAI serving component, updating import paths and references throughout the codebase. This follows feedback that "translations" is too reductive for ASR (Automatic Speech Recognition) endpoints, which include both transcription and translation tasks.

**Technical impact**  
These changes are purely structural and do not affect functionality. The renaming improves clarity by aligning the directory name with the broader speech-to-text capabilities (transcription and translation) rather than implying only translation services. All import paths and router registrations are updated consistently.

**Potential risks**  
The risk is minimal since this is a straightforward rename with no logic changes. However, any external dependencies or scripts referencing the old `translations` path will break unless updated. Additionally, if any references were missed during the rename, it could cause import errors at runtime.

**Key insights**  
This is a maintainability improvement that enhances code clarity. Developers should ensure all references to the old path are updated, including in documentation or configuration files. The change aligns with the component's actual functionality, making the codebase more intuitive for new contributors.

---

## 18. [Fix tokenizer test for renamed attr on Transformers v5](https://github.com/vllm-project/vllm/pull/33902)


### Base Information

- **PR Number:** #33902
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-05 11:16:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33902/files) (1):**
  - `tests/entrypoints/openai/test_serving_tokens.py`

### Summary

**What changed and why**  
The PR updates a tokenizer test to handle a renamed attribute in Transformers v5. Specifically, `additional_special_tokens_ids` has been renamed to `extra_special_tokens_ids`. The change introduces a helper function `getattr_iter` to check both attribute names, ensuring backward compatibility with Transformers v4 while supporting v5.

**Technical impact**  
This change maintains test compatibility across Transformers library versions without breaking existing functionality. It abstracts version-specific attribute names behind a utility function, reducing conditional logic in the test and centralizing version adaptation. The test now dynamically resolves the correct attribute name, making it more robust to future library changes.

**Potential risks**  
If `getattr_iter` is not implemented correctly or fails to handle missing attributes gracefully, the test could produce false positives or negatives. Additionally, relying on a utility function introduces a dependency—if `getattr_iter` changes or has bugs, it could affect multiple tests. There’s also a risk that other parts of the codebase may still use the old attribute name directly, leading to inconsistencies.

**Key insights**  
This approach demonstrates a clean pattern for handling library version differences: centralize attribute resolution and avoid hardcoded names in tests. Developers should verify that `getattr_iter` is thoroughly tested and consider auditing other uses of `additional_special_tokens_ids` in the codebase. Future similar changes should follow this pattern to minimize technical debt.

---

## 19. [[Bugfix] Suppress non-TTY color output on the process name part of the log](https://github.com/vllm-project/vllm/pull/29714)


### Base Information

- **PR Number:** #29714
- **Author:** [a4lg](https://github.com/a4lg)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-05 10:47:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29714/files) (1):**
  - `vllm/utils/system_utils.py`

### Summary

**What changed and why**  
The change modifies the `_add_prefix` function in `vllm/utils/system_utils.py` to suppress ANSI color escape sequences in process name log prefixes (e.g., `(APIServer pid=xxxx)`) when output is directed to a non-TTY (like a file or pipe). This aligns the function's color logic with the existing `_use_color` function in `vllm/logger.py`, fixing a bug where colored prefixes appeared in redirected logs.

**Technical impact**  
This ensures consistent color output behavior across all logging components. The system now uniformly respects the `NO_COLOR` environment variable, the `VLLM_LOGGING_COLOR` setting, and TTY detection, preventing unwanted escape sequences in log files and non-interactive outputs while preserving colors for terminals and when explicitly forced.

**Potential risks**  
The logic depends on correct detection of TTY status via `file.isatty()`, which may behave unexpectedly in certain environments (e.g., some CI systems or subprocesses). Additionally, the condition `envs.VLLM_LOGGING_COLOR != "1"` could be misinterpreted if the variable is set to an invalid value, though this mirrors existing logger behavior.

**Key insights**  
The fix is minimal and correctly mirrors established patterns, but consider adding a unit test for the color logic to prevent regression. Developers should note that `VLLM_LOGGING_COLOR="1"` forces colors regardless of TTY status, which is useful for tools like `less -R`. Ensure any future logging color changes update both this function and the main logger.

---

## 20. [[Models] Consolidate Deepseek-OCR2 processor](https://github.com/vllm-project/vllm/pull/33909)


### Base Information

- **PR Number:** #33909
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-05 10:29:20
- **Type:** `None`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33909/files) (5):**
  - `vllm/model_executor/models/deepencoder2.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/deepseek_ocr2.py`
  - `vllm/transformers_utils/processors/deepseek_ocr.py`
  - `vllm/transformers_utils/processors/deepseek_ocr2.py`

### Summary



---

## 21. [[Moe Refactor] Make Inplace Flag for FusedMoEModularKernel part of the constructor](https://github.com/vllm-project/vllm/pull/33375)


### Base Information

- **PR Number:** #33375
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-05 10:07:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33375/files) (37):**
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `tests/kernels/moe/test_batched_deepgemm.py`
  - `tests/kernels/moe/test_block_fp8.py`
  - `tests/kernels/moe/test_cutlass_moe.py`
  - `tests/kernels/moe/test_deepep_deepgemm_moe.py`
  - `tests/kernels/moe/test_deepep_moe.py`
  - `tests/kernels/moe/test_deepgemm.py`
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/kernels/moe/test_flashinfer_moe.py`
  - `tests/kernels/moe/test_modular_oai_triton_moe.py`
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/moe/test_nvfp4_moe.py`
  - `tests/kernels/moe/test_pplx_cutlass_moe.py`
  - `tests/kernels/moe/test_pplx_moe.py`
  - `tests/kernels/moe/utils.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_method_base.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/modular_kernel.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/quantization/awq_marlin.py`
  - `vllm/model_executor/layers/quantization/bitsandbytes.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/layers/quantization/experts_int8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/moe_wna16.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`

### Summary

**What changed and why**  
The PR moves the `inplace` flag from being a runtime parameter of the `FusedMoEModularKernel.forward()` method to a constructor argument. This change simplifies runtime logic in `layer.py` by making the inplace behavior a fixed property of the kernel instance, which helps with handling shared experts and cloning of `hidden_states`.

**Technical impact**  
This refactor changes the kernel's API by removing the `inplace` parameter from the forward call and adding it to the constructor. All test files and kernel creation sites have been updated to pass `inplace=False` explicitly. The `disable_inplace` flag in `FusedMoEConfig` now controls whether kernels can use inplace operations, and this is propagated through the constructor.

**Potential risks**  
The main risk is that any code calling `FusedMoEModularKernel.forward()` directly must now remove the `inplace` parameter, which could cause runtime errors if not updated. Additionally, the assertion `assert not inplace or not disable_inplace()` in `fused_moe.py` ensures safety but could introduce new failures if configurations are mismatched.

**Key insights**  
Developers should note that `inplace` is now a kernel property, not a runtime choice. When creating kernels, consider the `moe_config.disable_inplace` flag and shared expert presence. All existing tests have been updated, but external integrations must adapt to the new API. The change centralizes inplace control, improving consistency.

---

## 22. [[Bugfix] Fix swapped engine_ids in NIXL Llama 4 local attention path](https://github.com/vllm-project/vllm/pull/33795)


### Base Information

- **PR Number:** #33795
- **Author:** [zackyoray](https://github.com/zackyoray)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-05 09:51:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33795/files) (1):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
The fix corrects swapped `engine_id` parameters in the NIXL Llama 4 local attention path within `_read_blocks`. Previously, `layer_local_desc_ids` used `dst_engine_id` and `layer_remote_desc_ids` used `self.engine_id`, causing incorrect `num_blocks` calculations during descriptor ID generation. This mismatch led to "remote index out of range" errors in heterogeneous tensor parallelism scenarios (e.g., prefill TP=2, decode TP=4).

**Technical impact**  
This correction ensures that local descriptor IDs are computed using the local engine's block count (`self.engine_id`), while remote descriptor IDs use the destination engine's block count (`dst_engine_id`). This aligns the local attention path with the global attention path's logic, restoring proper KV cache transfer functionality and preventing index errors during NIXL operations with varying tensor parallelism degrees.

**Potential risks**  
The primary risk is that the fix assumes the existing `_get_block_descs_ids` method correctly handles the `block_size_ratio` parameter for both local and remote calls; any latent issues in that method could persist. Additionally, the parameter order change (moving `block_size_ratio` only to the local call) must be verified to match the method's signature and intended behavior for both descriptor types.

**Key insights**  
Developers should ensure the `_get_block_descs_ids` method's implementation and signature are consistent with this updated usage. This bug highlights the importance of rigorous testing for heterogeneous tensor parallelism configurations, particularly when attention paths diverge. Consider adding assertions or validation to catch similar parameter swaps early in the development cycle.

---

## 23. [[Misc] Add debug logs](https://github.com/vllm-project/vllm/pull/33931)


### Base Information

- **PR Number:** #33931
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-05 09:42:41
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33931/files) (2):**
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
Two debug log statements were added to aid in debugging KV cache behavior. The first logs the shape of a mock KV cache during initialization, and the second logs the shape of each KV cache being registered. These changes provide visibility into cache dimensions without requiring physical machine access.

**Technical impact**  
The changes are purely additive and have no functional impact on system behavior. They introduce low-overhead debug logging that will only be visible when debug logging is enabled, helping developers understand KV cache shapes during connector initialization and cache registration phases.

**Potential risks**  
There's a minor risk of log spam if debug logging is enabled in production environments, though the volume is limited to initialization and registration events. The log in `__post_init__` uses a mock shape for dimension checking, which could be slightly misleading if interpreted as an actual runtime cache shape.

**Key insights**  
These are safe, non-invasive debugging aids. Developers should ensure debug logging remains disabled in production to avoid unnecessary overhead. The mock shape log should be understood as a test configuration, not a reflection of real cache dimensions during inference.

---

## 24. [[Spec Decode] Unified Parallel Drafting](https://github.com/vllm-project/vllm/pull/32887)


### Base Information

- **PR Number:** #32887
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-02-05 09:37:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32887/files) (14):**
  - `examples/offline_inference/spec_decode.py`
  - `tests/v1/e2e/test_spec_decode.py`
  - `tests/v1/spec_decode/test_eagle.py`
  - `tests/v1/spec_decode/test_mtp.py`
  - `vllm/config/speculative.py`
  - `vllm/config/vllm.py`
  - `vllm/model_executor/models/llama_eagle3.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/flashinfer.py`
  - `vllm/v1/attention/backends/utils.py`
  - `vllm/v1/spec_decode/draft_model.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/spec_decode/utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces unified parallel drafting support for speculative decoding, enabling both AMD's PARD (parallel drafting for fine-tuned external draft models) and AWS's P-EAGLE (parallel prediction for EAGLE3). It adds a single input preparation kernel and extends the speculative decoding framework to handle parallel token generation in one pass, improving throughput.

**Technical impact**  
The changes unify the input preparation logic across EAGLE, draft models, and parallel drafting methods, reducing code duplication. The system now supports generating all speculative tokens in parallel (when `parallel_drafting=True`) instead of sequentially, which increases token generation speed. The compile range calculation and attention backend configurations are updated to account for the extra tokens introduced by parallel drafting.

**Potential risks**  
- Parallel drafting requires draft models trained specifically for this mode (with `pard_token` or `ptd_token_id` in config). Using untrained models may cause errors or degraded performance.  
- The increased token count per request (multiplied by `num_speculative_tokens`) could strain memory or KV cache capacity, especially at high batch sizes.  
- The removal of the standalone `merge_toks_kernel` and refactoring of draft model logic may introduce regressions in non-parallel drafting scenarios if not thoroughly tested.

**Key insights**  
- Parallel drafting provides significant speedups (up to 3.1x in benchmarks) but requires compatible draft models and careful configuration of `num_speculative_tokens`.  
- Developers should validate that their draft model config includes the necessary parallel drafting token ID and that `parallel_drafting` is only enabled when supported.  
- The changes maintain backward compatibility—existing EAGLE and draft model workflows continue to work with `parallel_drafting=False`.

---

## 25. [[BugFix] Fix LoRA Fp8](https://github.com/vllm-project/vllm/pull/33879)


### Base Information

- **PR Number:** #33879
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-05 09:25:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33879/files) (1):**
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
The fix addresses a LoRA compatibility issue with FP8-quantized Mixture of Experts (MoE) models like Nemotron Nano FP8. Previously, the code attempted to call `select_gemm_impl()` on FP8 MoE quant methods that use modular kernel initialization, which is unsupported and caused a `ValueError`. The change adds a conditional check to use the existing modular kernel (`moe_mk`) when available, avoiding the problematic call.

**Technical impact**  
This change enables LoRA adapter support for FP8 MoE models by respecting the quantization method's internal modular kernel initialization logic. It maintains backward compatibility for non-FP8 MoE models by falling back to the original `select_gemm_impl()` path when the `supports_internal_mk` flag is absent or false.

**Potential risks**  
If the `supports_internal_mk` flag is incorrectly set or missing in other quantization methods, it could lead to runtime errors or silent fallback to the wrong kernel path. Additionally, the change assumes that `moe_mk` is always properly initialized when the flag is true, which may not hold in edge cases during model loading.

**Key insights**  
Developers should ensure that any new FP8 MoE quantization methods properly set the `supports_internal_mk` flag and initialize `moe_mk`. Consider adding a validation step or logging to clarify which kernel path is used during LoRA injection. This fix highlights the need for consistent modular kernel interfaces across quantization backends.

---

## 26. [[Feat][RL][1/2] Native Weight Syncing API: NCCL](https://github.com/vllm-project/vllm/pull/31943)


### Base Information

- **PR Number:** #31943
- **Author:** [hao-aaron](https://github.com/hao-aaron)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-05 09:13:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31943/files) (27):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/distributed.yaml`
  - `examples/offline_inference/new_weight_syncing/rlhf.py`
  - `examples/offline_inference/new_weight_syncing/rlhf_async_new_apis.py`
  - `examples/online_serving/rlhf_http.py`
  - `tests/distributed/test_packed_tensor.py`
  - `tests/distributed/test_weight_transfer.py`
  - `tests/entrypoints/openai/test_openai_schema.py`
  - `tests/entrypoints/weight_transfer/__init__.py`
  - `tests/entrypoints/weight_transfer/test_weight_transfer_llm.py`
  - `vllm/config/__init__.py`
  - `vllm/config/vllm.py`
  - `vllm/config/weight_transfer.py`
  - `vllm/distributed/weight_transfer/__init__.py`
  - `vllm/distributed/weight_transfer/base.py`
  - `vllm/distributed/weight_transfer/factory.py`
  - `vllm/distributed/weight_transfer/nccl_engine.py`
  - `vllm/distributed/weight_transfer/packed_tensor.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/engine/protocol.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/serve/rlhf/api_router.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/model_loader/reload/layerwise.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR introduces native weight syncing APIs to vLLM, enabling efficient transfer of updated model weights from a trainer to inference workers during reinforcement learning workflows (RLHF, PPO). It addresses the need for a standardized, high‑performance weight syncing mechanism, replacing ad‑hole solutions currently implemented by third‑party projects. The core addition is a pluggable `WeightTransferEngine` abstraction with an NCCL backend, plus new endpoints (`init_weight_transfer`, `update_weights`, `get_world_size`) exposed via `LLM`, `AsyncLLM`, and the HTTP server.

**Technical impact**  
The changes extend vLLM’s architecture with a new weight‑transfer subsystem that integrates cleanly with the existing parallel config and worker model. The engine abstraction allows future backends (e.g., CUDA IPC) to be added without modifying core logic. Packed tensor broadcasting optimizes NCCL communication by batching small tensors, reducing latency. The APIs are designed to work concurrently with generation (via `pause_generation`/`resume_generation`), enabling live weight updates without stopping the inference server.

**Potential risks**  
- The NCCL backend requires careful rank‑offset calculation across TP/PP/DP groups; misconfiguration could lead to deadlock or incorrect weight mapping.  
- Weight reloading for quantized models (FP8) depends on resetting a flag (`_already_called_process_weights_after_loading`); if other quantization methods are added, similar handling must be ensured.  
- The HTTP endpoints are gated behind `VLLM_SERVER_DEV_MODE`, which may confuse users if they attempt to use them in production without enabling the flag.  
- Packed tensor broadcasting uses fixed buffer sizes (default 1GB); extremely large models or unusual tensor distributions could cause suboptimal batching or memory spikes.

**Key insights**  
- The design elegantly separates transport logic from worker implementation, making it easy to extend with new backends.  
- Developers should ensure the trainer and inference workers agree on `packed_buffer_size_bytes` and `packed_num_buffers` when using packed mode.  
- The examples provide a solid reference for integrating weight syncing into RL pipelines, but users must be mindful of GPU placement (via Ray placement groups) to avoid interference with vLLM’s memory management.  
- The flag reset for FP8 quantization is a critical detail; any future weight‑reloading features must preserve similar handling to avoid double‑processing.

---

## 27. [[Bugfix] Fix step3p5 parser when using mtp](https://github.com/vllm-project/vllm/pull/33690)


### Base Information

- **PR Number:** #33690
- **Author:** [mariohong128](https://github.com/mariohong128)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-05 08:04:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33690/files) (2):**
  - `tests/tool_parsers/test_step3p5_tool_parser.py`
  - `vllm/tool_parsers/step3p5_tool_parser.py`

### Summary

**What changed and why**  
The fix addresses a parser bug in the Step-3.5 tool-call parser when model outputs contain the sequence `</tool_call><tool_call><` (more likely with MTP). Previously, this could cause the parser to incorrectly start a new empty tool call, leading to validation errors. The changes introduce a `fallback_call_id` logic to better track call boundaries during streaming XML chunk processing.

**Technical impact**  
The parser now more accurately distinguishes between consecutive tool calls by using entry-state tracking (`entry_call_id`, `entry_tool_call_index`) and a computed `fallback_call_id`. This ensures that closing tags (`</function>`, `</tool_call>`) only affect the appropriate call ID, preventing malformed tool-call objects and subsequent Pydantic validation failures.

**Potential risks**  
If the `fallback_call_id` logic misidentifies the active call (e.g., in complex nested or interleaved tool calls), it could still produce incorrect deltas. Edge cases with rapid successive tool calls or malformed XML chunks may not be fully covered, though the existing test suite passes.

**Key insights**  
The fix is narrowly targeted to the specific parsing edge case and maintains backward compatibility. Developers should ensure future tool-call parser changes preserve the entry-state tracking pattern. Consider adding fuzz tests with varied MTP outputs to further stress the parser.

---

## 28. [[Docs] Add bart-plugin to docs](https://github.com/vllm-project/vllm/pull/33905)


### Base Information

- **PR Number:** #33905
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-05 04:20:25
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33905/files) (3):**
  - `docs/design/plugin_system.md`
  - `docs/models/supported_models.md`
  - `docs/usage/v1_guide.md`

### Summary

**What changed and why**  
Documentation updates to reference the new `bart-plugin` repository as an official example of vLLM's plugin system for supporting out-of-tree models. The changes clarify that `BartForConditionalGeneration` is now supported via plugin, update the supported models list, and adjust guidance on encoder-decoder model support.

**Technical impact**  
These changes formalize the plugin system as the recommended approach for adding encoder-decoder model support beyond Whisper. The documentation now provides a concrete reference implementation (`bart-plugin`) that developers can follow to extend vLLM with custom architectures, particularly for models requiring cross-attention between separate encoder and decoder components.

**Potential risks**  
Users might misinterpret the plugin as a built-in feature rather than an external dependency, potentially leading to runtime errors if the plugin isn't properly installed. The documentation doesn't specify compatibility requirements or version constraints between vLLM and the plugin. There's also a risk of fragmentation if multiple unofficial plugins emerge without clear quality standards.

**Key insights**  
The plugin system is being positioned as the primary extension mechanism for non-native model architectures. Developers should study the bart-plugin repository as a reference implementation when adding support for other encoder-decoder models. Clear installation and dependency instructions should be added to avoid user confusion about plugin requirements.

---

## 29. [[Bugfix] Fix corner case of sparse embedding](https://github.com/vllm-project/vllm/pull/33886)


### Base Information

- **PR Number:** #33886
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-05 02:51:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33886/files) (2):**
  - `tests/models/language/pooling/test_bge_m3.py`
  - `vllm/model_executor/layers/pooler/special.py`

### Summary

**What changed and why**  
The PR fixes a corner case in sparse embedding generation for the BGE-M3 model by modifying the pooling layer's squeeze operation and adding a corresponding test. The fix addresses issue #33873 where sparse embeddings for single-token inputs could produce incorrect tensor shapes.

**Technical impact**  
Changing `squeeze()` to `squeeze(-1)` ensures consistent tensor dimensionality when pooling single-token sequences, preventing shape errors in downstream sparse embedding calculations. The added test validates that sparse embeddings for the input "Hi" produce expected values at a specific token index.

**Potential risks**  
The fix assumes the issue only occurs with single-token inputs, but similar shape issues could arise with other edge cases like empty sequences or specific token combinations. The test only validates one specific input ("Hi"), which may not cover all potential corner cases.

**Key insights**  
Always use explicit dimension arguments in tensor operations like `squeeze()` to avoid unexpected behavior with single-element dimensions. When fixing shape-related bugs, consider adding tests for various input lengths (0, 1, 2+ tokens) to ensure robustness. The sparse embedding logic appears sensitive to tensor shape consistency throughout the pooling pipeline.

---

## 30. [[Refactor] Clean up input preprocessing](https://github.com/vllm-project/vllm/pull/33687)


### Base Information

- **PR Number:** #33687
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-05 02:43:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33687/files) (4):**
  - `vllm/inputs/data.py`
  - `vllm/inputs/parse.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/multimodal/inputs.py`

### Summary

**What changed and why**  
This refactor removes text-only encoder-decoder model handling from the input preprocessing pipeline. The changes simplify the codebase by treating all encoder-decoder models as multi-modal models, even if they are text-only. This aligns with the architectural decision to implement encoder-decoder models uniformly through the multi-modal framework.

**Technical impact**  
The refactor significantly reduces code complexity by eliminating special-case logic for text-only encoder-decoder models. Key changes include removing the `_get_default_enc_dec_decoder_prompt` method, simplifying `get_decoder_start_token_id` to always return a value (raising an error if unavailable), and consolidating encoder-decoder input validation into separate `_validate_enc_inputs` and `_validate_dec_inputs` methods. The `_build_enc_dec_inputs` method now handles both multi-modal and token inputs more uniformly.

**Potential risks**  
The removal of fallback logic in `get_decoder_start_token_id` could cause runtime errors if a model lacks both a decoder start token ID and a BOS token. The refactor assumes all encoder-decoder models are registered as multi-modal, which may break compatibility with existing text-only encoder-decoder models not configured this way. Additionally, the simplified input validation may miss edge cases previously handled by the removed special-case logic.

**Key insights**  
Developers should ensure all encoder-decoder models are properly registered as multi-modal models, even if they are text-only. The updated `get_decoder_start_token_id` method now raises a `RuntimeError` instead of returning `None`, so callers must handle this exception. The refactor improves maintainability but requires careful testing to confirm backward compatibility with existing encoder-decoder model implementations.

---

## 31. [[Bugfix] Fix Kimi-K2.5 NVFP4 checkpoints weight loading](https://github.com/vllm-project/vllm/pull/33876)


### Base Information

- **PR Number:** #33876
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-05 02:29:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33876/files) (2):**
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/model_executor/models/kimi_k25.py`

### Summary

**What changed and why**  
This PR adds backward compatibility for loading NVFP4 quantized checkpoints of the `nvidia/Kimi-K2.5-NVFP4` model. The changes update weight mapping to handle the legacy layer naming (`language_model.layers.*`) that was refactored in a previous PR (#33346), ensuring the model loads correctly in v0.15.0+.

**Technical impact**  
The modifications introduce a new weight mapping rule in `kimi_k25.py` to translate legacy parameter names to the current architecture, and adjust `deepseek_v2.py` to safely handle `None` layer names during weight loading. This maintains compatibility with older checkpoints without disrupting existing functionality.

**Potential risks**  
If the mapping rule is overly broad or conflicts with other model variants, it could cause incorrect weight assignments. Additionally, the `None` check in `deepseek_v2.py` might mask underlying issues in weight loading for other models if `name` is unexpectedly `None` elsewhere.

**Key insights**  
Developers should verify that the weight mapping only applies to the intended legacy checkpoints and does not affect newer models. Consider adding a version or format flag to explicitly distinguish checkpoint types, and ensure similar legacy compatibility is reviewed for other models that underwent similar refactoring.

---

## 32. [[perf] Integrate flashinfer concat_mla_k](https://github.com/vllm-project/vllm/pull/31171)


### Base Information

- **PR Number:** #31171
- **Author:** [jiahanc](https://github.com/jiahanc)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-05 02:23:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31171/files) (2):**
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/utils/flashinfer.py`

### Summary

**What changed and why**  
This PR integrates Flashinfer's optimized `concat_mla_k` kernel into the MLA attention layer. The kernel is specifically tuned for DeepSeek V3 model dimensions (128 heads, 128 nope dimensions, 64 rope dimensions) to accelerate the concatenation of key tensors during prefill. A conditional check enables the kernel only when Flashinfer is available and the model matches these exact dimensions, otherwise falling back to the existing broadcast-based concatenation.

**Technical impact**  
The change introduces a performance optimization for a specific model configuration (DeepSeek V3) by replacing a manual tensor copy with a hardware-optimized kernel. The kernel leverages warp-based processing, vectorized memory access, and software pipelining to improve memory bandwidth utilization. This is a targeted optimization that does not affect other models or configurations, maintaining backward compatibility through the fallback path.

**Potential risks**  
The optimization is tightly coupled to specific model dimensions (128/128/64), which could lead to silent performance degradation if the kernel is incorrectly enabled for other configurations. The in-place modification of the `k` tensor requires careful handling to avoid aliasing issues. Additionally, the dependency on Flashinfer introduces a runtime requirement that must be satisfied for the optimization to activate.

**Key insights**  
Developers should note that this is a model-specific optimization; extending support to other configurations would require additional kernel tuning or condition checks. The implementation correctly isolates the optimization behind a feature flag and maintains a fallback, which is a good practice for maintainability. Consider adding validation or logging to confirm the kernel is active when expected, and ensure the Flashinfer dependency is well-documented for deployment environments.

---

## 33. [Enable Cross layers KV cache layout at NIXL Connector V2](https://github.com/vllm-project/vllm/pull/33339)


### Base Information

- **PR Number:** #33339
- **Author:** [liranschour](https://github.com/liranschour)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-05 02:17:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33339/files) (6):**
  - `docs/features/nixl_connector_usage.md`
  - `tests/v1/kv_connector/nixl_integration/config_sweep_accuracy_test.sh`
  - `tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh`
  - `tests/v1/kv_connector/unit/test_nixl_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`

### Summary

**What changed and why**  
This PR enables cross-layer KV cache layout support in the NIXL Connector V2, allowing KV cache blocks to be contiguous across layers in physical memory. The change reduces transfer buffer fragmentation, leading to significant performance improvements (2x+ in tokens/sec and TTFT) as shown in the benchmark data.

**Technical impact**  
The implementation introduces a new `enable_cross_layers_blocks` configuration option and modifies KV cache registration logic to handle contiguous cross-layer memory layouts. Key changes include updates to `TpKVTopology` to detect cross-layer layouts, adjustments in `NixlConnector` to compute compatibility hashes accordingly, and enhanced test coverage to validate both traditional and cross-layer configurations.

**Potential risks**  
Cross-layer support is currently limited to FlashAttention and FlashInfer backends with HND layout; enabling it on unsupported backends may cause failures. The lazy initialization of compatibility hashes and topology in `NixlConnector` could lead to runtime errors if registration order is disrupted. Additionally, the complex stride and shape calculations for cross-layer tensors increase the risk of dimension mismatches.

**Key insights**  
Developers should enable this feature only with supported attention backends (FlashAttention/FlashInfer) and HND layout to avoid performance degradation or errors. The PR includes comprehensive test updates, but thorough validation is recommended when integrating with heterogeneous TP/DP setups. The performance gains justify the added complexity, but the feature should be considered experimental until broader backend support is added.

---

## 34. [[ROCm][Bugfix][CI] Fix hybrid models and their tests (Mamba/Jamba/Bamba)](https://github.com/vllm-project/vllm/pull/32710)


### Base Information

- **PR Number:** #32710
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-05 02:01:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32710/files) (2):**
  - `tests/models/language/generation/test_hybrid.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer.py`

### Summary

**What changed and why**  
Two changes were made to address incorrect Mamba model outputs on ROCm. The first adds a `.contiguous()` call to `time_step` in the Mamba mixer's SSM transform to ensure contiguous input for GEMM operations. The second modifies a test to limit batch size on ROCm, reducing variance due to unsupported batch invariance.

**Technical impact**  
The fix ensures tensor contiguity before GEMM operations on ROCm, correcting discretization time step calculations and restoring proper SSM recurrence. The test adjustment mitigates platform-specific batch processing differences, improving test reliability without affecting core functionality.

**Potential risks**  
The `.contiguous()` call introduces a memory copy overhead, potentially impacting performance. Platform-specific code increases maintenance complexity. The batch size limitation in tests may mask underlying issues if batch invariance support is later added to ROCm.

**Key insights**  
Always verify tensor contiguity before GEMM operations, especially on ROCm. Consider adding contiguity checks for other similar operations in the codebase. Monitor performance impacts and evaluate if a more generalized fix is needed for other affected layers like Mamba2.

---

## 35. [[Refactor] Move `task` outside of `PoolingParams.verify`](https://github.com/vllm-project/vllm/pull/33796)


### Base Information

- **PR Number:** #33796
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-05 01:33:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33796/files) (24):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/misc.yaml`
  - `tests/entrypoints/pooling/classify/test_online.py`
  - `tests/entrypoints/pooling/embed/test_online.py`
  - `tests/entrypoints/pooling/score/test_online_colbert.py`
  - `tests/entrypoints/pooling/score/test_online_rerank.py`
  - `tests/test_pooling_params.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/pooling/__init__.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/classify/serving.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/embed/serving.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/pooling_params.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The refactor moves the `task` parameter out of `PoolingParams.verify()` and into `PoolingParams` initialization, centralizing task validation within `InputProcessor._validate_params()`. This ensures task assignment and verification happen in a single place, simplifying the pooling logic across endpoints (`/pooling`, `LLM.encode`, and score/classify/embed endpoints). Additionally, the CI configuration was updated to run the previously excluded `test_pooling_params.py`.

**Technical impact**  
Pooling task validation is now unified in `InputProcessor`, reducing duplication and improving consistency. The `PoolingParams` class no longer accepts a `task` argument in `verify()`; instead, the task must be set during initialization or inferred from `supported_tasks`. This change affects all pooling-related endpoints and the `LLM.encode` method, requiring updates to error handling and parameter passing.

**Potential risks**  
If `supported_tasks` is not correctly passed to `InputProcessor._validate_params()`, pooling tasks may fail validation. The removal of task-specific verification in endpoint-serving code could mask model-config mismatches if the `InputProcessor` validation is bypassed. Additionally, the caching of `supported_tasks` in async/sync engines introduces a risk of stale cache if the model’s supported tasks change dynamically.

**Key insights**  
Developers must ensure `supported_tasks` is propagated to all `process_inputs` calls, especially for pooling tasks. The centralized validation improves maintainability but shifts responsibility to the `InputProcessor`. Reviewers should verify that all pooling endpoints correctly construct `PoolingParams` with the appropriate task and that error messages remain user-friendly. The CI fix ensures better test coverage for pooling parameter logic.

---

## 36. [[Bugfix] Kimi-K2 grouped_topk usage for Flashinfer monolithic kernels.](https://github.com/vllm-project/vllm/pull/33858)


### Base Information

- **PR Number:** #33858
- **Author:** [pavanimajety](https://github.com/pavanimajety)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-05 01:32:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33858/files) (1):**
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
The PR removes logic that conditionally set `n_group` and `topk_group` to `None` (and `use_grouped_topk` to `False`) when both values were `(1, 1)`. Instead, it now always passes the raw values from the config (defaulting to 1) and sets `use_grouped_topk=True`. This fixes a bug where the Flashinfer monolithic kernel for routing + MoE failed when these parameters were `None`, specifically impacting the Kimi-K2 model.

**Technical impact**  
This change ensures the `SharedFusedMoE` layer always receives explicit integer values for `num_expert_group` and `topk_group`, which is required by the Flashinfer kernel's grouped_topk implementation. The Marlin kernel path was unaffected because it doesn't use a monolithic kernel for routing. The fix restores correct behavior for models like Kimi-K2 that rely on this kernel.

**Potential risks**  
The primary risk is that other models (like Mistral, as noted) might depend on the previous behavior where `use_grouped_topk=False` when groups were `(1, 1)`. Passing `True` with group values of 1 could theoretically introduce performance regressions or correctness issues if the kernel handles the `(1, 1)` case differently. The PR author explicitly requests validation for Mistral compatibility.

**Key insights**  
Always pass explicit group values to the `SharedFusedMoE` layer to satisfy kernel requirements. The fix is critical for Kimi-K2 with Flashinfer, as shown by the dramatic accuracy recovery (0.299 to 0.917). However, cross-validation with other affected models (e.g., Mistral) is essential before merging to avoid unintended side effects. Consider if a more robust solution, like kernel-side handling of the `(1, 1)` default, is needed.

---

