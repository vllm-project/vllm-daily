# vLLM Merged PR Report

**Report Date:** 2026-02-13 PST

**Total Merged PRs:** 27

---

## 1. [[Misc] Update tests and examples for Prithvi/Terratorch models](https://github.com/vllm-project/vllm/pull/34416)


### Base Information

- **PR Number:** #34416
- **Author:** [christian-pinto](https://github.com/christian-pinto)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 23:03:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34416/files) (9):**
  - `examples/pooling/plugin/prithvi_geospatial_mae_io_processor.py`
  - `examples/pooling/plugin/prithvi_geospatial_mae_offline.py`
  - `examples/pooling/plugin/prithvi_geospatial_mae_online.py`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/models/multimodal/pooling/test_prithvi_mae.py`
  - `tests/models/test_terratorch.py`
  - `tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py`
  - `tests/plugins_tests/test_io_processor_plugins.py`

### Summary

**What changed and why**  
This PR updates tests and examples for Prithvi/Terratorch models to use official model releases, adapt to newer Terratorch versions (≥1.2.2), and enhance test robustness by validating output images via perceptual hashing. It replaces community-hosted model references with official IBM/NASA repositories and switches from a Git dependency to a pinned PyPI release.

**Technical impact**  
The changes improve test reliability by adding perceptual hash validation for output correctness, ensure compatibility with updated Terratorch APIs (simplifying transform configurations), and align dependencies with official releases. This reduces reliance on third-party model forks and ensures tests reflect production-ready model artifacts.

**Potential risks**  
Perceptual hashing may be sensitive to minor image variations (e.g., compression artifacts or library version differences), potentially causing flaky tests. The removal of `--trust-remote-code` and `--model-impl` flags assumes Terratorch ≥1.2.2 handles these implicitly, which could break if older versions are inadvertently used. Dependency constraints on `datasets` (3.3.0–3.6.0) may conflict with other library requirements.

**Key insights**  
Developers should verify Terratorch ≥1.2.2 is installed in all environments. The perceptual hash thresholds may need adjustment if test images change. The simplified transform configuration in `prithvi_processor.py` reflects Terratorch API updates—ensure any custom plugins align with these changes. Monitor for dependency conflicts, especially with `datasets` and `torchcoded`.

---

## 2. [[new model] add COLQwen3 code & Inference](https://github.com/vllm-project/vllm/pull/34398)


### Base Information

- **PR Number:** #34398
- **Author:** [craftsangjae](https://github.com/craftsangjae)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-13 20:15:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34398/files) (10):**
  - `docs/models/pooling_models.md`
  - `examples/pooling/score/colqwen3_rerank_online.py`
  - `examples/pooling/token_embed/colqwen3_token_embed_online.py`
  - `tests/models/multimodal/pooling/test_colqwen3.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/colqwen3.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/colqwen3.py`

### Summary

**What changed and why**  
This PR adds native support for ColQwen3 multi-modal late interaction models to vLLM. ColQwen3 extends Qwen3-VL with a ColBERT-style linear projection head that produces per-token L2-normalized embeddings, enabling MaxSim late interaction scoring for retrieval and reranking across both text and image inputs. The implementation supports multiple model families including TomoroAI/tomoro-colqwen3-embed-4b and OpenSearch-AI/Ops-Colqwen3-4B.

**Technical impact**  
The changes extend the vLLM architecture to handle multi-modal pooling models with late interaction capabilities. A new `ColQwen3Model` class inherits from `Qwen3VLForConditionalGeneration` and adds projection layers for embedding generation. The model is registered as a pooling model with `supports_late_interaction=True`, enabling `/score` and `/rerank` endpoints. The implementation includes flexible weight loading to handle three different checkpoint naming conventions and automatic embed dimension inference.

**Potential risks**  
The weight loading logic handles multiple naming conventions which could lead to incorrect mappings if new variants emerge. The `ColQwen3ProcessingInfo` overrides strict type checks similar to `OpenCUAProcessingInfo`, which might bypass important validation. NVIDIA Nemotron models are currently blocked due to upstream tokenizer compatibility issues. The projection layer initialization depends on config fields that may not be consistently named across all model variants.

**Key insights**  
Developers should note that image inputs require the chat-style `messages` field for proper multimodal processing. The implementation reuses the Qwen3-VL backbone extensively, minimizing code duplication. Testing shows successful cross-modal MaxSim scoring between text queries and image documents. The added examples provide clear patterns for both token embedding retrieval and reranking workflows.

---

## 3. [[CI] Heavy refactoring of Voxtral multimodal audio model tests](https://github.com/vllm-project/vllm/pull/34294)


### Base Information

- **PR Number:** #34294
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 20:04:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34294/files) (11):**
  - `requirements/rocm-test.txt`
  - `tests/conftest.py`
  - `tests/models/multimodal/generation/test_voxtral.py`
  - `tests/models/multimodal/generation/test_voxtral_realtime.py`
  - `tests/models/multimodal/generation/vlm_utils/model_utils.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/whisper_causal.py`
  - `vllm/reasoning/mistral_reasoning_parser.py`
  - `vllm/tokenizers/mistral.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
This PR introduces comprehensive testing for the Voxtral multimodal audio model (`mistralai/Voxtral-Mini-3B-2507`), addressing issue #34283. It adds three-layer accuracy validation (offline vLLM, HF Transformers reference, and online API serving) and includes a patch for `HfRunner` to handle Voxtral's unique conversation-based audio processing interface. The changes also fix related issues in tokenizer initialization and attention backends for Whisper models.

**Technical impact**  
The refactoring establishes a robust testing framework for Voxtral, ensuring consistency across inference paths. The `voxtral_patch_hf_runner` function overrides `HfRunner`'s input processing to support multi-audio prompts and proper prompt-token stripping. Modifications to `test_common.py` improve dummy input handling for non-image modalities, while updates to `whisper_causal.py` expand supported attention backends and fix KV cache shape calculations.

**Potential risks**  
The patch to `HfRunner` introduces a custom `get_inputs` method that may not generalize to other multimodal models. The base64 audio encoding in memory could increase memory usage for large audio batches. Changes to tokenizer initialization (removing `dtype` parameters) might affect other models if not carefully reviewed. The expanded attention backend support for Whisper could introduce subtle performance or accuracy regressions on untested hardware.

**Key insights**  
Developers should note that Voxtral requires a distinct testing approach due to its conversation-based audio input format. The three-layer validation strategy is a best practice for multimodal models. The `HfRunner` patch is model-specific and should be kept isolated; consider abstracting if similar patterns emerge. Ensure tokenizer changes do not break existing model tests, and monitor the newly enabled attention backends for Whisper on ROCm platforms.

---

## 4. [Add explicit validation error for tool calls.](https://github.com/vllm-project/vllm/pull/34438)


### Base Information

- **PR Number:** #34438
- **Author:** [juliendenize](https://github.com/juliendenize)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 20:04:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34438/files) (1):**
  - `vllm/tokenizers/mistral.py`

### Summary

**What changed and why**  
The PR adds explicit validation for `tool_calls` in assistant messages within Mistral chat completion requests. Previously, invalid `tool_calls` (e.g., a string instead of an iterable) would cause obscure errors; now, a clear `ValueError` is raised with a descriptive message, addressing issue #34225.

**Technical impact**  
This change improves error handling in the tokenization layer by catching `ValidationError` from Pydantic when `tool_calls` is not iterable. It ensures that malformed `tool_calls` are caught early with a user-friendly error, enhancing debugging and API robustness.

**Potential risks**  
The validation only triggers when `tool_calls` is not `None`, which could miss cases where `tool_calls` is an empty string or other non-iterable but truthy values. Additionally, relying on `list(tool_calls_validator)` may not catch all invalid structures if the iterable itself contains malformed items.

**Key insights**  
Developers should ensure `tool_calls` is either `None` or a proper iterable of tool call objects. The error message is clearer, but consider extending validation to check each tool call’s structure. This fix is a stopgap until Pydantic v2.11 improves native validation.

---

## 5. [fix: use `__annotations__` instead of `get_type_hints()` for dynamic `kwargs` detection](https://github.com/vllm-project/vllm/pull/34527)


### Base Information

- **PR Number:** #34527
- **Author:** [perone](https://github.com/perone)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 20:03:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34527/files) (1):**
  - `vllm/transformers_utils/processor.py`

### Summary

**What changed and why**  
The fix replaces `get_type_hints()` with direct `__annotations__` traversal via the MRO (Method Resolution Order) in `_collect_dynamic_keys_from_processing_kwargs`. This avoids `NameError` exceptions caused by unresolved forward references (e.g., `PILImageResampling`) that were silently caught, leading to empty dynamic key detection and LRU cache misses.

**Technical impact**  
Dynamic keys (e.g., `size`, `fps`) are now correctly identified and filtered from cache keys, enabling proper LRU cache hits in `cached_get_processor_without_dynamic_kwargs`. This eliminates redundant `from_pretrained` calls for multimodal processors, reducing preprocessing time from ~28s to ~6.3s (cold) and <0.5s (subsequent requests).

**Potential risks**  
Using `__annotations__` directly may miss type hints from parent classes not defined in the current module or those added dynamically. The traversal relies on `__annotations__` being present; if a base class lacks it, keys could be omitted. Additionally, the added logging in the exception handler could expose sensitive stack traces in production.

**Key insights**  
The change is a targeted workaround for forward-reference resolution issues, but consider validating that `__annotations__` captures all necessary keys across inheritance chains. Ensure logging is appropriately filtered in production to avoid noise. This fix significantly improves performance for multimodal models and should be tested with diverse processor configurations.

---

## 6. [[bug] Make sure get_modality_with_max_tokens is deterministic](https://github.com/vllm-project/vllm/pull/34533)


### Base Information

- **PR Number:** #34533
- **Author:** [842974287](https://github.com/842974287)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 20:02:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34533/files) (1):**
  - `vllm/multimodal/encoder_budget.py`

### Summary

**What changed and why**  
The PR modifies the `get_modality_with_max_tokens` method to ensure deterministic behavior when multiple modalities have identical `max_toks_per_item` values. Previously, using `max()` with only the value as the key could lead to non-deterministic selection due to Python's dictionary iteration order, which caused issues in distributed settings (e.g., NCCL hangs during profiling with tensor parallelism).

**Technical impact**  
This change stabilizes modality selection across different ranks by making the `max()` function compare both the token count and the modality name (as a tuple). This ensures consistent profiling and encoder loading in multi-GPU environments, preventing synchronization problems in tensor-parallel configurations.

**Potential risks**  
If modality names are not consistently defined across ranks (e.g., due to configuration mismatches), deterministic ordering may still fail. Additionally, the fix assumes lexicographic ordering of modality strings is sufficient; non-ASCII or case-sensitive names could introduce subtle ordering differences.

**Key insights**  
Always ensure deterministic behavior in distributed systems by using fully comparable keys in sorting/max operations. Consider adding a test to verify consistent modality selection across runs with identical token counts. For future similar cases, document the assumption that modality names are uniform and sortable.

---

## 7. [[Feature][Perf] Support Selective CPU Weight Offloading](https://github.com/vllm-project/vllm/pull/34535)


### Base Information

- **PR Number:** #34535
- **Author:** [wzhao18](https://github.com/wzhao18)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 20:02:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34535/files) (4):**
  - `vllm/config/cache.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR introduces selective CPU weight offloading based on parameter name matching, allowing users to offload only specific parameters (e.g., MoE expert weights) to CPU memory. This is enabled via a new `--cpu-offload-params` CLI argument, which accepts a set of name segments to match. The change aims to improve performance in low-concurrency settings by reducing unnecessary offloading.

**Technical impact**  
The modification extends the existing CPU offloading mechanism by adding a filtering step in `maybe_offload_to_cpu`. Parameters are only offloaded if their names contain any of the specified segments as exact dot-delimited parts (e.g., `"experts.w2_weight"` matches `"mlp.experts.w2_weight"` but not `"mlp.experts.w2_weight_scale"`). The offloading logic remains per-parameter, and total offloaded memory is logged after layer creation.

**Potential risks**  
Incorrect segment matching could lead to unintended offloading or missed offloads, especially if parameter naming conventions vary across models. The segment matching relies on exact substring matching within dot-delimited names, which may be fragile if names contain special characters or unexpected structures. Additionally, the global state variables (`_CPU_OFFLOAD_PARAMS`) could introduce thread-safety concerns in multi-process scenarios.

**Key insights**  
The feature provides fine-grained control over CPU offloading, which can significantly boost throughput (as shown by the 2× improvement in the test). Developers should ensure parameter names are well-documented for target models. Consider adding validation for name segments to catch potential mismatches early, and evaluate thread safety if offloading is used in concurrent environments.

---

## 8. [[Bugfix] Fix ROCm UVA CPU weight offloading broken by #32993](https://github.com/vllm-project/vllm/pull/34543)


### Base Information

- **PR Number:** #34543
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 20:01:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34543/files) (1):**
  - `vllm/utils/torch_utils.py`

### Summary

**What changed and why**  
The change adds ROCm platform detection to an existing CUDA branch in `get_accelerator_view_from_cpu_tensor`. Previously, PR #32993 introduced UVA support for CPU weight offloading but only checked for CUDA and XPU platforms, causing ROCm systems to raise a `ValueError`. Since ROCm shares the same device type and dispatch key as CUDA, and the underlying C++ layer already supports ROCm via hipification, this fix ensures ROCm is recognized at the Python level.

**Technical impact**  
This modification enables ROCm-based systems to utilize CPU weight offloading with UVA, aligning Python-level platform checks with existing C++ support. The change maintains backward compatibility and does not affect CUDA or XPU functionality, as it simply extends the conditional branch to include ROCm. The system behavior now correctly routes ROCm requests through the same CUDA tensor view mechanism.

**Potential risks**  
The primary risk is minimal, as the underlying C++ operations are already hipified and tested for ROCm. However, if there are subtle differences between CUDA and ROCm memory management or UVA behavior, they may not be fully accounted for. Additionally, the fix assumes ROCm and CUDA share identical semantics for pinned CPU tensors, which should be validated in ROCm-specific environments.

**Key insights**  
Developers should ensure that platform-specific checks consider all compatible accelerators, especially when code is hipified automatically. This fix highlights the importance of synchronizing Python and C++ platform guards. Future similar updates should include ROCm in initial implementations to prevent regression, and ROCm-specific testing should be prioritized to catch any hidden discrepancies.

---

## 9. [[Hybrid] Enable spec decoding in mamba cache align mode](https://github.com/vllm-project/vllm/pull/33705)


### Base Information

- **PR Number:** #33705
- **Author:** [peakcrosser7](https://github.com/peakcrosser7)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-02-13 13:02:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33705/files) (2):**
  - `tests/v1/e2e/test_mamba_prefix_cache.py`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
This PR re-enables speculative decoding for Mamba models using the 'align' cache mode by removing the assertion that previously blocked it. The test file was updated to remove the skip decorator and add proper cleanup procedures, confirming that the underlying issues have been resolved.

**Technical impact**  
The removal of the compatibility assertion allows speculative decoding to be used with Mamba's align cache mode, potentially improving inference performance through draft token proposals. The test changes ensure proper resource management (GPU memory cleanup, distributed environment teardown) and enable the previously skipped end-to-end test.

**Potential risks**  
If the speculative decoding issues were not fully resolved, re-enabling it could reintroduce bugs related to cache alignment or state management in Mamba models. The test relies on monkey-patched functions, which may not fully represent production behavior, and the cleanup steps indicate previous memory/distributed environment issues.

**Key insights**  
Developers should verify that speculative decoding works correctly in all Mamba align cache scenarios, especially with distributed setups. The added cleanup calls (`cleanup_dist_env_and_memory`) are critical for preventing resource leaks in tests. Consider adding more integration tests beyond the single e2e test to cover edge cases.

---

## 10. [[Bugfix]: Fix structured output in multi-turn gpt-oss](https://github.com/vllm-project/vllm/pull/34454)


### Base Information

- **PR Number:** #34454
- **Author:** [bbrowning](https://github.com/bbrowning)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 11:12:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34454/files) (4):**
  - `tests/entrypoints/openai/test_gptoss_structural_tags_integration.py`
  - `tests/reasoning/test_gptoss_reasoning_parser.py`
  - `tests/v1/structured_output/test_gptoss_structural_tags.py`
  - `vllm/reasoning/gptoss_reasoning_parser.py`

### Summary

**What changed and why**  
The fix addresses a bug in the GPT-OSS reasoning parser where the detection logic for the final output channel incorrectly matched tokens from previous conversation turns in multi-turn scenarios. This premature application of grammar constraints caused invalid or empty outputs. The change ensures the parser only searches within the current message by stopping the backward search when it encounters an end-of-message token (`<\|end\|>`).

**Technical impact**  
This correction restores proper structured output generation (e.g., JSON objects) in multi-turn conversations for GPT-OSS models. It ensures grammar bitmasks are applied only after the model genuinely starts generating the final channel, aligning with the trained Harmony format and preserving output validity.

**Potential risks**  
If the tokenizer’s vocabulary lacks the `<\|end\|>` token or if its ID is inconsistent, the parser may revert to the faulty behavior. Additionally, the fix assumes that `<\|end\|>` uniquely marks message boundaries; any edge cases where this token appears within message content could cause false early exits.

**Key insights**  
The fix is minimal and targeted, leveraging existing token boundaries to constrain the search scope. Developers should verify that the tokenizer consistently maps `<\|end\|>` across all GPT-OSS model variants. This change also highlights the importance of testing multi-turn state handling alongside structured output features.

---

## 11. [Revert "[Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides"](https://github.com/vllm-project/vllm/pull/34530)


### Base Information

- **PR Number:** #34530
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 10:35:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34530/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
This PR reverts a previous change that explicitly typed stride parameters as `tl.int64` in two fused MoE kernel functions. The revert is due to reported large performance degradations, with plans to find a similar solution after more careful performance analysis.

**Technical impact**  
Removing explicit `tl.int64` type annotations returns stride parameters to implicit typing, which may affect Triton compiler optimizations and kernel performance. This change directly impacts the fused MoE implementation's execution efficiency, particularly for GPTQ/AWQ quantized models and standard fused MoE operations.

**Potential risks**  
Reverting may reintroduce the original bug that the int64 change was meant to fix, potentially causing incorrect stride calculations in certain scenarios. The performance impact could vary across different hardware configurations or input sizes, making it difficult to predict behavior consistently.

**Key insights**  
Performance regressions from type annotations suggest Triton's compiler handles implicit vs explicit types differently. Developers should benchmark both versions under diverse workloads before reintroducing changes. Consider adding performance tests for stride-related operations to catch regressions early.

---

## 12. [[Misc] vLLM's --enforce-eager should turn off compile and cudagraphs only](https://github.com/vllm-project/vllm/pull/34523)


### Base Information

- **PR Number:** #34523
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 09:52:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34523/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The change modifies how the `--enforce-eager` flag behaves. Previously, it forced the optimization level to `-O0`, which disabled many optimizations including compile and CUDAGraphs, but also disabled unrelated features like flashinfer autotuning. Now, it directly and specifically disables only torch.compile and CUDAGraphs by setting their respective modes to `NONE`, making it a more precise debugging tool.

**Technical impact**  
This change decouples the `enforce_eager` flag from the broader `optimization_level` setting. The system's overall optimization level (`-O0`, `-O1`, `-O2`) is now preserved when `enforce_eager` is used, allowing other non-compilation-related optimizations to remain active. This provides a more targeted way to disable just the compilation subsystem for debugging.

**Potential risks**  
If any other features were implicitly disabled by the old `-O0` setting that are critical for debugging compilation issues, they may now remain active and could obscure the root cause. There is also a risk if the `compilation_config` object is not fully initialized at this point in `__post_init__`, which could lead to attribute errors.

**Key insights**  
This is a positive refinement that aligns the flag's behavior with its intent. Developers using `--enforce-eager` for debugging should be aware it now only isolates compilation/CUDAGraph issues. The warning message is updated to clearly state the new equivalent command-line arguments (`-cc.mode=none -cc.cudagraph_mode=none`), which is helpful for user education.

---

## 13. [[Bugfix] Replace c10::optional with std::optional in topk kernel](https://github.com/vllm-project/vllm/pull/34467)


### Base Information

- **PR Number:** #34467
- **Author:** [FloatingVertex](https://github.com/FloatingVertex)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 08:30:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34467/files) (1):**
  - `csrc/topk.cu`

### Summary

**What changed and why**  
The change replaces `c10::optional<torch::Tensor>` with `std::optional<torch::Tensor>` in the `large_context_topk` kernel function signature, aligning with the deprecation of `c10::optional`. This is part of a broader migration effort, as indicated by prior PRs, and ensures consistency with the header already using `std::optional`.

**Technical impact**  
This update standardizes the codebase to use C++17's `std::optional`, reducing dependency on deprecated PyTorch utilities. It maintains functional equivalence since `std::nullopt` replaces `c10::nullopt`, and the function's behavior remains unchanged, assuming the header and implementation are now synchronized.

**Potential risks**  
The risk is minimal if the header and implementation are correctly aligned, but mismatches could cause compilation errors or runtime issues. Additionally, any downstream code relying on implicit conversions between `c10::optional` and `std::optional` might need adjustment, though this is unlikely given the prior migrations.

**Key insights**  
Developers should verify that all related headers and source files consistently use `std::optional` to avoid compilation errors. This change reinforces the importance of tracking deprecations in foundational libraries and highlights the need for thorough CI testing to catch any inconsistencies early.

---

## 14. [[Feature] Support CPU Offloading without Pytorch Pinned Memory that leads to doubled allocation](https://github.com/vllm-project/vllm/pull/32993)


### Base Information

- **PR Number:** #32993
- **Author:** [wzhao18](https://github.com/wzhao18)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 08:11:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32993/files) (6):**
  - `csrc/cuda_view.cu`
  - `tests/basic_correctness/test_cpu_offload.py`
  - `vllm/envs.py`
  - `vllm/model_executor/model_loader/utils.py`
  - `vllm/model_executor/models/utils.py`
  - `vllm/utils/torch_utils.py`

### Summary

**What changed and why**  
This PR introduces a workaround for PyTorch's pinned memory allocation issue where allocations are rounded up to the next power of two, potentially doubling memory usage. It adds support for CPU offloading via UVA (Unified Virtual Addressing) without relying on PyTorch's pinned memory, controlled by two environment variables: `VLLM_WEIGHT_OFFLOADING_DISABLE_PIN_MEMORY` and `VLLM_WEIGHT_OFFLOADING_DISABLE_UVA`. The changes enable more efficient memory usage for large models.

**Technical impact**  
The core change modifies `csrc/cuda_view.cu` to handle both pinned and non-pinned CPU tensors, allocating pinned memory via `cudaHostAlloc` when needed. This allows UVA-based offloading without PyTorch's `pin_memory`. The CPU offloading logic in `model_loader/utils.py` and `models/utils.py` is updated to respect the new environment variables, ensuring proper tensor movement and UVA offloading state management. The architecture now supports three offloading modes: original (pinned + UVA), UVA-only, and explicit CPU-GPU transfers.

**Potential risks**  
The workaround introduces additional `cudaHostAlloc` and `cudaMemcpy` operations for non-pinned tensors, which may impact performance. There is a risk of memory leaks if `cudaFreeHost` is not called correctly in error paths. The `_vllm_is_uva_offloaded` attribute management adds complexity and could lead to inconsistent states if not handled carefully across all code paths. The changes assume CUDA UVA availability, which may not hold on all systems.

**Key insights**  
Developers should test all environment variable combinations to ensure correct behavior, especially for large models. The performance trade-off between memory savings and additional copy operations should be evaluated. The PR is a temporary workaround; monitor the upstream PyTorch PR (#171662) for a permanent fix. Ensure error handling in `cuda_view.cu` is robust to prevent memory leaks. The `tie_weights=False` addition in `functional_call` is critical to avoid issues with tied weights during offloading.

---

## 15. [[Bugfix] Add quant_config in ViT of Kimi-K2.5](https://github.com/vllm-project/vllm/pull/34501)


### Base Information

- **PR Number:** #34501
- **Author:** [LoganJane](https://github.com/LoganJane)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-13 08:05:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34501/files) (2):**
  - `vllm/model_executor/models/kimi_k25.py`
  - `vllm/model_executor/models/kimi_k25_vit.py`

### Summary

**What changed and why**  
The changes add quantization configuration support to the ViT (Vision Transformer) components of the Kimi-K2.5 model. Specifically, the `quant_config` parameter is now passed through the ViT tower and multimodal projector initialization, enabling proper weight loading for quantized models (W4A8 format).

**Technical impact**  
These modifications ensure that when using quantized weights for Kimi-K2.5, the quantization configuration correctly propagates to all relevant linear layers within the vision tower and projector. This maintains consistency with the model's quantization scheme and enables successful execution with compressed weights on target hardware (910B NPU).

**Potential risks**  
The `_maybe_ignore_quant_config` method introduces conditional logic that returns `None` for `CompressedTensorsConfig` while passing through other quantization configs. This could lead to inconsistent behavior if different quantization methods require different handling. Additionally, there's a risk of missing quantization configuration propagation in other model components not covered by this change.

**Key insights**  
The implementation successfully enables quantized weight support for Kimi-K2.5's vision components. Developers should ensure that similar quantization configuration handling is consistently applied across all model components. The conditional logic in `_maybe_ignore_quant_config` warrants documentation to clarify when quantization configs should be ignored versus propagated.

---

## 16. [[Bugfix] Exclude `language_model_only` key from MM AOT compile hash but include in model one](https://github.com/vllm-project/vllm/pull/34508)


### Base Information

- **PR Number:** #34508
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-13 05:59:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34508/files) (3):**
  - `tests/config/test_multimodal_config.py`
  - `vllm/config/model.py`
  - `vllm/config/multimodal.py`

### Summary

**What changed and why**  
The changes modify how the `language_model_only` configuration parameter is included in hash calculations. This key is removed from the MultiModalConfig hash but added to the ModelConfig hash, ensuring it doesn't affect ViT computation graph caching while still influencing language model input preparation.

**Technical impact**  
This separation ensures that AOT compilation for vision encoders remains consistent regardless of `language_model_only` settings, while model-level caching correctly accounts for differences in language model computation graphs. The hash changes affect cache invalidation for models with multimodal components like Qwen3-VL.

**Potential risks**  
If other multimodal configuration parameters similarly affect language model computation, they may need similar treatment. The change could cause cache collisions if `language_model_only` interacts with other hash factors in unexpected ways. Test coverage appears limited to hash behavior without validating actual model execution differences.

**Key insights**  
The fix correctly distinguishes between vision encoder and language model caching concerns. Developers should verify that `language_model_only` is the only multimodal parameter affecting language model computation. Consider adding integration tests to validate that the hash separation prevents the described deepstack input buffer error in real scenarios.

---

## 17. [[Misc] Port Qwen3.5 Configs](https://github.com/vllm-project/vllm/pull/34512)


### Base Information

- **PR Number:** #34512
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 05:24:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34512/files) (6):**
  - `vllm/model_executor/models/qwen3_5.py`
  - `vllm/model_executor/models/qwen3_5_mtp.py`
  - `vllm/transformers_utils/config.py`
  - `vllm/transformers_utils/configs/__init__.py`
  - `vllm/transformers_utils/configs/qwen3_5.py`
  - `vllm/transformers_utils/configs/qwen3_5_moe.py`

### Summary

**What changed and why**  
This PR ports Qwen3.5 and Qwen3.5-MoE configuration classes from the HuggingFace Transformers library into vLLM's internal `transformers_utils/configs` directory. The changes replace direct imports from `transformers.models.qwen3_5` with imports from the new local modules, and register the configs in vLLM's configuration registry. The purpose is to eliminate the dependency on installing the Transformers library for users who want to run Qwen3.5 models.

**Technical impact**  
The changes decouple vLLM from the external Transformers library for these specific model configurations, allowing vLLM to load Qwen3.5 models without requiring a full Transformers installation. This aligns with vLLM's strategy of internalizing model configurations to reduce external dependencies and improve deployment flexibility. The architecture remains consistent with how other model configs are handled in the codebase.

**Potential risks**  
There is a risk of configuration drift if the internalized configs are not kept in sync with upstream changes in the Transformers library. The PR lacks test plans and results, making it difficult to verify functional correctness. Additionally, the changes assume the ported configs are complete and compatible with all Qwen3.5 variants, which could lead to runtime errors if any fields are missing or misinterpreted.

**Key insights**  
Developers should ensure the ported configs are periodically updated to match the latest Transformers versions. Comprehensive testing is needed to validate that the models load and run correctly with the new configs. This change is part of a broader effort to reduce external dependencies, but it increases maintenance overhead for keeping internal configs up-to-date.

---

## 18. [Extend ColBERT support to non-standard BERT backbones](https://github.com/vllm-project/vllm/pull/34170)


### Base Information

- **PR Number:** #34170
- **Author:** [ieBoytsov](https://github.com/ieBoytsov)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-13 01:53:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34170/files) (9):**
  - `docs/models/pooling_models.md`
  - `examples/pooling/score/colbert_rerank_online.py`
  - `tests/entrypoints/pooling/score/test_online_colbert.py`
  - `tests/models/language/pooling/test_colbert.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/colbert.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
This PR extends ColBERT support beyond standard BERT backbones by introducing a `ColBERTMixin` that encapsulates shared ColBERT logic (projection layer initialization, weight loading, pooler building). Two new model classes are added: `ColBERTModernBertModel` for ModernBERT backbones and `ColBERTJinaRobertaModel` for Jina XLM-RoBERTa backbones. The existing `HF_ColBERT` class is refactored to use the mixin, maintaining backward compatibility while enabling new architectures with minimal code duplication.

**Technical impact**  
The changes modularize ColBERT support, reducing the code required to add new backbones to ~40 lines. The mixin handles dimension auto-detection from config or weight shapes, projection layer management, and weight loading separation. Tests are now parametrized across all three backbones (BERT, ModernBERT, Jina), ensuring consistent behavior. Documentation and examples are updated to reflect the new architectures and required `--hf-overrides` for non-BERT models.

**Potential risks**  
The weight loading logic assumes ColBERT projection weights are identifiable by specific suffixes (`linear.weight`, `colbert_linear.weight`); custom or future backbones with different naming conventions may require adjustments. The `--trust-remote-code` flag is needed for Jina models, which could introduce security or compatibility concerns. There is a risk of regression for existing BERT-based ColBERT models if the refactored `HF_ColBERT` class does not perfectly replicate previous behavior.

**Key insights**  
Developers adding new ColBERT backbones should inherit from `ColBERTMixin` and implement only the backbone-specific components. The mixin’s `_load_colbert_weights` method simplifies projection layer integration. Always verify that the model’s config or weight naming aligns with the expected patterns. For production use, ensure thorough testing across all supported backbones, especially for edge cases in tokenization and pooling.

---

## 19. [[GDN] Use CPU tensors to build GDN metadata](https://github.com/vllm-project/vllm/pull/34498)


### Base Information

- **PR Number:** #34498
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-13 01:24:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34498/files) (2):**
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/utils.py`

### Summary

**What changed and why**  
The PR addresses a CPU-GPU synchronization bottleneck in the GDN metadata builder by replacing GPU tensor operations with CPU tensor equivalents. Specifically, it modifies the code to use CPU tensors (`query_lens_cpu`, `spec_sequence_masks_cpu`) instead of GPU tensors when computing metadata counts, and adds `non_blocking=True` to tensor copy operations to improve asynchronous data transfer.

**Technical impact**  
These changes eliminate blocking synchronization points caused by `.item()` calls on GPU tensors, which improves overall throughput in mixed CPU-GPU workloads. The metadata computation now operates entirely on CPU tensors, reducing GPU idle time and potentially lowering latency for attention operations in speculative decoding scenarios.

**Potential risks**  
The CPU tensor operations assume that `query_lens_cpu` and `spec_sequence_masks_cpu` are properly synchronized from GPU before use. If these CPU tensors become stale due to asynchronous GPU operations, the computed metadata could be incorrect. The `non_blocking=True` copies require proper synchronization later to ensure data consistency.

**Key insights**  
This optimization is crucial for performance in speculative decoding workloads where metadata computation occurs frequently. Developers should ensure CPU tensors remain synchronized with their GPU counterparts and verify that `non_blocking=True` operations have appropriate synchronization points downstream. Consider applying similar patterns elsewhere in the codebase where GPU tensor `.item()` calls cause synchronization bottlenecks.

---

## 20. [[Feature] Pipeline Parallel Async send/recv, 2.9% E2E throughput improvement](https://github.com/vllm-project/vllm/pull/33368)


### Base Information

- **PR Number:** #33368
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-02-13 00:38:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33368/files) (3):**
  - `tests/distributed/test_comm_ops.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR introduces asynchronous send/recv operations for pipeline parallel (PP) communication to improve throughput. It adds non-blocking `isend_tensor_dict` and `irecv_tensor_dict` methods to `GroupCoordinator`, along with an `AsyncIntermediateTensors` class that lazily waits for communication completion. The changes aim to overlap communication with computation, yielding a measured 2.9% end-to-end throughput improvement.

**Technical impact**  
The synchronous `send_tensor_dict` and `recv_tensor_dict` methods now delegate to their async counterparts, maintaining backward compatibility. The `AsyncIntermediateTensors` class defers synchronization until tensor data is accessed, allowing the GPU worker to proceed with other work while communication happens in the background. This reduces idle time in the PP forward pass, especially beneficial when using `--async-scheduling`.

**Potential risks**  
If the async handles are not properly waited on (e.g., if `AsyncIntermediateTensors.wait_for_comm` is skipped), tensor data may be incomplete or corrupted. The lazy wait mechanism adds a subtle side effect to attribute access (`__getattribute__`), which could confuse developers. There is also a risk of increased memory usage if sends are not synchronized and tensors are retained longer.

**Key insights**  
The async communication is transparently integrated—existing synchronous calls still work. Developers must ensure that any custom use of `irecv_tensor_dict` properly calls the returned postprocess functions. The `_pp_send_work` list in the GPU worker ensures send operations from previous iterations complete before starting new ones, preventing race conditions. Consider adding timeout or error handling for async operations in production.

---

## 21. [[Core] Move pause and resume functions into engine](https://github.com/vllm-project/vllm/pull/34125)


### Base Information

- **PR Number:** #34125
- **Author:** [hao-aaron](https://github.com/hao-aaron)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 00:15:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34125/files) (9):**
  - `examples/online_serving/data_parallel_pause_resume.py`
  - `tests/v1/distributed/test_async_llm_dp.py`
  - `tests/v1/engine/test_async_llm.py`
  - `tests/v1/engine/test_engine_core_client.py`
  - `vllm/v1/core/sched/interface.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`

### Summary

**What changed and why**  
This PR moves pause and resume functionality from `AsyncLLM` to `EngineCore` to enable coordinated pause states across multiple `AsyncLLM` instances in data-parallel deployments. It introduces a `FutureUtility` mechanism that allows pause operations to block until completion (e.g., draining in-flight requests) and ensures all engine instances share the same paused state via broadcast.

**Technical impact**  
The changes centralize pause/resume logic in `EngineCore`, making it a core engine feature rather than a per-`AsyncLLM` concern. This enables consistent pause behavior across distributed engines, supports multiple API servers, and introduces a flexible callback system (`per_step_hooks`) for deferred utility completion. The scheduler now tracks pause states (`PAUSED_NEW`, `PAUSED_ALL`) to control request scheduling.

**Potential risks**  
The new `FutureUtility` mechanism adds complexity to the engine's busy loop and could introduce subtle bugs if callbacks misbehave or block indefinitely. Edge cases around aborting queued requests while paused require careful handling, as shown in the added test. Distributed synchronization (via all-reduce) for pause states is noted as future work (DPEP support), so current multi-engine coordination may be incomplete.

**Key insights**  
Developers should note that pause/resume is now a first-class engine operation with three modes (`abort`, `wait`, `keep`). The `FutureUtility` pattern enables blocking until pause conditions are met, which is critical for correct synchronization. Ensure that any new engine utilities that require deferred completion follow the same pattern. The changes are backward-compatible for single-engine setups but enable future data-parallel extensions.

---

## 22. [[KVConnector] Clean up redundant code in KV connectors](https://github.com/vllm-project/vllm/pull/34147)


### Base Information

- **PR Number:** #34147
- **Author:** [hickeyma](https://github.com/hickeyma)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 00:14:31
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34147/files) (4):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/__init__.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`

### Summary

**What changed and why**  
This PR performs minor code cleanup across KV connector modules, addressing several code quality issues: fixing a malformed `noqa` comment, removing unused `reshape()` calls, eliminating unreachable code after an assertion, removing a redundant TYPE_CHECKING import, and ensuring type consistency by initializing a float variable properly.

**Technical impact**  
These changes improve code clarity and maintainability without altering functional behavior. The removal of no-op `reshape()` calls eliminates unnecessary operations, while the unreachable code removal and import cleanup reduce cognitive load for developers. The float initialization ensures type consistency in statistical calculations.

**Potential risks**  
The removal of `reshape()` calls could theoretically affect tensor memory layout if the operations weren't truly no-op, though the PR indicates they were. The unreachable None check removal is safe since it followed an `assert isinstance(metadata, ExampleConnectorMetadata)` which would raise an exception if metadata were None.

**Key insights**  
This is a well-scoped cleanup PR that follows good software engineering practices. Developers should verify that the removed `reshape()` calls were indeed redundant by checking tensor shape invariants in the surrounding code. The type consistency fix for `total_time` prevents potential integer division issues in future calculations.

---

## 23. [[Perf] fused_moe: add int4_w4a16 benchmark support and tuning config](https://github.com/vllm-project/vllm/pull/34130)


### Base Information

- **PR Number:** #34130
- **Author:** [mgehre-amd](https://github.com/mgehre-amd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 00:14:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34130/files) (2):**
  - `benchmarks/kernels/benchmark_moe.py`
  - `vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=Radeon_8060S_Graphics,dtype=int4_w4a16.json`

### Summary

**What changed and why**  
Added support for int4_w4a16 quantization to the MoE benchmark/tuning script, including proper uint8-packed weight generation and group-wise scales. The changes enable benchmarking and tuning of int4-quantized MoE layers, with automatic extraction of group_size from model configs (supporting AWQ/GPTQ and compressed-tensors formats). Pre-tuned kernel configurations for AMD Radeon 8060S Graphics are also provided.

**Technical impact**  
The modifications extend the existing benchmark infrastructure to handle int4 quantization, requiring adjustments to weight tensor shapes (packed uint8), scale tensor dimensions, and search-space logic. The kernel configurations now include a fixed `SPLIT_K=1` parameter for the gptq_awq kernel, and block_quant_shape filtering is disabled during tuning to allow full exploration. This enables performance optimization for int4-quantized MoE models, as demonstrated by the reported 22% TTFT improvement.

**Potential risks**  
If a model's quantization config lacks the expected group_size field (or nested config_groups), the script will raise a ValueError, potentially breaking automated workflows. The hardcoded `SPLIT_K=1` assumption may not generalize to all hardware or kernel implementations. Additionally, the int4 path introduces new tensor shape calculations that could lead to dimension mismatches if not carefully validated across different model architectures.

**Key insights**  
Developers should ensure that models using int4_w4a16 quantization have properly structured quantization_configs. The tuning process for int4 treats weights similarly to fp8 (no matrix_instr_nonkdim/kpack exploration), which may affect performance on non-AMD hardware. The added kernel configurations are hardware-specific; when porting to other GPUs, re-tuning is recommended to achieve optimal performance.

---

## 24. [[Bugfix] fix the import path in moe test utils.py](https://github.com/vllm-project/vllm/pull/34245)


### Base Information

- **PR Number:** #34245
- **Author:** [michalowski-arm](https://github.com/michalowski-arm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 00:13:46
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34245/files) (1):**
  - `tests/kernels/moe/utils.py`

### Summary

**What changed and why**  
The import paths for `TritonExperts`, `fused_experts`, and `fused_topk` were updated. These components were moved from `vllm.model_executor.layers.fused_moe` to more specific submodules (`fused_moe` and `router.fused_topk_router`), likely as part of a recent code reorganization (#33375). This fix resolves a failing Arm CPU test caused by incorrect import paths.

**Technical impact**  
This change aligns the test utility imports with the new modular structure of the fused MoE (Mixture of Experts) layer. It ensures the test can correctly access the required implementations, preventing import errors during test execution. The rest of the code's functionality remains unchanged.

**Potential risks**  
If other files still import these components from the old paths, they will also fail. The change is localized to a test file, but the original reorganization (#33375) might have introduced similar breakages elsewhere that need to be checked. There is a minor risk of circular imports if the new module structure isn't carefully designed.

**Key insights**  
Always verify import paths after significant code reorganizations. The build failure was a direct symptom of the architectural change. Developers should run a broader search for similar outdated imports, especially in test files, to prevent future CI failures. Consider adding linting or automated checks for import consistency.

---

## 25. [[Bug Fix] Fix MambaManager.cache_blocks() crash on null blocks in align mode](https://github.com/vllm-project/vllm/pull/34418)


### Base Information

- **PR Number:** #34418
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 00:13:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34418/files) (2):**
  - `tests/v1/core/test_prefix_caching.py`
  - `vllm/v1/core/single_type_kv_cache_manager.py`

### Summary

**What changed and why**  
The fix addresses a crash in `MambaManager.cache_blocks()` when operating in `mamba_cache_mode="align"`. In this mode, `allocate_new_blocks()` pads `req_to_blocks` with null blocks, which `cache_full_blocks()` correctly skips, but `cache_blocks()` previously did not, leading to an assertion failure on `block.block_hash`. The change adds null-block skipping in `cache_blocks()` to maintain consistency.

**Technical impact**  
This ensures that hybrid KV caching with Mamba’s align mode functions correctly, preventing assertion failures during block caching. The fix aligns the behavior of `cache_blocks()` with `cache_full_blocks()`, maintaining system stability when null blocks are present in the request-to-blocks mapping.

**Potential risks**  
If null blocks are introduced in other cache modes or scenarios, similar issues could arise elsewhere. Additionally, the fix assumes null blocks are only present in align mode; if future changes introduce null blocks in other contexts, the same crash might reoccur unless broader safeguards are implemented.

**Key insights**  
Developers should verify that null-block handling is consistent across all block-iteration logic in the caching system. Consider adding a helper method to skip null blocks uniformly or documenting the invariant that null blocks may appear only in specific modes. The regression test added is crucial for catching similar issues early.

---

## 26. [[BugFix] Fix and optimize max_num_blocks_per_req calculation for MambaSpec](https://github.com/vllm-project/vllm/pull/34440)


### Base Information

- **PR Number:** #34440
- **Author:** [peakcrosser7](https://github.com/peakcrosser7)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 00:13:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34440/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The PR fixes a bug in `max_num_blocks_per_req` calculation where using separate loops for `block_sizes` and `max_num_blocks` could cause index misalignment when `EncoderOnlyAttentionSpec` groups are present. It also optimizes the calculation for Mamba models by removing redundant `max()` logic, since without prefix caching they require at most `1 + num_speculative_blocks` blocks.

**Technical impact**  
The changes ensure correct block allocation for KV cache groups by constructing `block_sizes` and `max_num_blocks` in the same loop, preventing index mismatches. For Mamba models, the logic is simplified and more efficient, as the previous `max()` operation was unnecessary when prefix caching is disabled.

**Potential risks**  
If `EncoderOnlyAttentionSpec` appears in `kv_cache_groups` after other spec types, the previous indexing bug could have caused incorrect block size references, potentially leading to memory allocation errors or performance issues. The simplified Mamba logic assumes the condition `max_num_blocks_per_req` is always >= `1 + num_speculative_blocks` when prefix caching is disabled, which should be validated.

**Key insights**  
Always maintain data structure consistency when filtering elements—constructing dependent arrays in the same loop avoids subtle index bugs. For specialized models like Mamba, tailor resource calculations to their specific caching behavior to eliminate unnecessary operations. Reviewers should verify the Mamba block calculation invariant holds across all expected configurations.

---

## 27. [[New Model] support new model ovis2.6](https://github.com/vllm-project/vllm/pull/34426)


### Base Information

- **PR Number:** #34426
- **Author:** [myselvess](https://github.com/myselvess)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-13 00:12:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34426/files) (6):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/model_executor/models/siglip2navit.py`
  - `vllm/transformers_utils/processors/ovis2_5.py`

### Summary

**What changed and why**  
This PR adds support for the new Ovis2.6 model family (including a 30B MoE variant) and fixes two bugs: one in the Ovis2_5 processor to handle prompt token IDs directly, and another in Siglip2VisionTransformer to remove an unnecessary post_layernorm on the last hidden state.

**Technical impact**  
The changes extend model registry support for Ovis2.6, reuse the existing Ovis2_5 implementation via registry mapping, and update token handling to use explicit token IDs from the vocabulary instead of hardcoded negative values. This improves compatibility with the new model's tokenizer and ensures proper visual token processing.

**Potential risks**  
Reusing the Ovis2_5 implementation for Ovis2_6 may introduce subtle incompatibilities if the models differ in architecture or tokenization. The removal of `post_layernorm` in Siglip2 could affect output normalization if the model expects it. The processor now requires specific special tokens in the tokenizer config; missing tokens will cause runtime errors.

**Key insights**  
Developers should verify that Ovis2.6 models are fully compatible with the Ovis2_5 code path. The tokenizer must include all required special tokens (e.g., `<ovis_image_start>`). The Siglip2 change should be validated against the original model implementation to ensure correctness. Documentation updates are minimal but sufficient for basic model listing.

---

