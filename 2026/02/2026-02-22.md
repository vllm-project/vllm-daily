# vLLM Merged PR Report

**Report Date:** 2026-02-22 PST

**Total Merged PRs:** 10

---

## 1. [[Bugfix] Fix  kernel benchmark](https://github.com/vllm-project/vllm/pull/33752)


### Base Information

- **PR Number:** #33752
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-22 21:18:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33752/files) (14):**
  - `benchmarks/fused_kernels/layernorm_rms_benchmarks.py`
  - `benchmarks/kernels/benchmark_activation.py`
  - `benchmarks/kernels/benchmark_block_fp8_gemm.py`
  - `benchmarks/kernels/benchmark_fp8_gemm.py`
  - `benchmarks/kernels/benchmark_int8_gemm.py`
  - `benchmarks/kernels/benchmark_layernorm.py`
  - `benchmarks/kernels/benchmark_mrope.py`
  - `benchmarks/kernels/benchmark_mxfp4_qutlass.py`
  - `benchmarks/kernels/benchmark_nvfp4_gemm.py`
  - `benchmarks/kernels/benchmark_nvfp4_quant.py`
  - `benchmarks/kernels/benchmark_nvfp4_qutlass.py`
  - `benchmarks/kernels/benchmark_per_token_quant_fp8.py`
  - `benchmarks/kernels/benchmark_rope.py`
  - `vllm/benchmarks/lib/utils.py`

### Summary

**What changed and why**  
This PR fixes a bug where kernel benchmarks were failing due to CustomOp initialization requiring a vLLM config context. The fix adds a `default_vllm_config()` context manager decorator to all benchmark functions that use CustomOps, ensuring proper configuration context. Additionally, it renames `bench_*` files to `benchmark_*` for naming consistency.

**Technical impact**  
The changes ensure kernel benchmarks can run independently without requiring a full vLLM engine context. The `default_vllm_config()` context manager provides a minimal VllmConfig instance during benchmark execution, allowing CustomOp initialization to succeed. This maintains benchmark isolation while preserving the configuration dependency required by the custom operation system.

**Potential risks**  
The default VllmConfig may not match specific benchmark requirements (e.g., quantization settings, compilation options). Benchmarks testing configuration-sensitive operations might produce misleading results. There's also a risk of configuration leakage if benchmarks modify the global config state. The decorator approach assumes all benchmark functions are properly decorated.

**Key insights**  
Always use `@default_vllm_config()` decorator when benchmarking code paths that involve CustomOps or `get_current_vllm_config()`. The context manager should be imported from `vllm.benchmarks.lib.utils`. Ensure benchmark configurations align with the default VllmConfig's assumptions, particularly for quantization and compilation-related tests.

---

## 2. [[Refactor] Simplify dummy data generation](https://github.com/vllm-project/vllm/pull/35025)


### Base Information

- **PR Number:** #35025
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-22 20:55:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35025/files) (78):**
  - `docs/contributing/model/multimodal.md`
  - `tests/models/multimodal/processing/test_audioflamingo3.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `vllm/config/multimodal.py`
  - `vllm/model_executor/models/aria.py`
  - `vllm/model_executor/models/audioflamingo3.py`
  - `vllm/model_executor/models/aya_vision.py`
  - `vllm/model_executor/models/bagel.py`
  - `vllm/model_executor/models/bee.py`
  - `vllm/model_executor/models/blip2.py`
  - `vllm/model_executor/models/chameleon.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/colmodernvbert.py`
  - `vllm/model_executor/models/deepseek_ocr.py`
  - `vllm/model_executor/models/deepseek_ocr2.py`
  - `vllm/model_executor/models/deepseek_vl2.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/funasr.py`
  - `vllm/model_executor/models/funaudiochat.py`
  - `vllm/model_executor/models/fuyu.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4v.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/hyperclovax_vision.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/interns1.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/isaac.py`
  - `vllm/model_executor/models/kanana_v.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/kimi_k25.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/llava_next_video.py`
  - `vllm/model_executor/models/llava_onevision.py`
  - `vllm/model_executor/models/midashenglm.py`
  - `vllm/model_executor/models/minicpmo.py`
  - `vllm/model_executor/models/minicpmv.py`
  - `vllm/model_executor/models/mistral3.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/nano_nemotron_vl.py`
  - `vllm/model_executor/models/nemotron_parse.py`
  - `vllm/model_executor/models/nvlm_d.py`
  - `vllm/model_executor/models/ovis.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen2_audio.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_asr.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen_vl.py`
  - `vllm/model_executor/models/rvl.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/skyworkr1v.py`
  - `vllm/model_executor/models/step3_vl.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/multimodal.py`
  - `vllm/model_executor/models/ultravox.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/whisper.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/dummy_inputs.py`
  - `vllm/multimodal/registry.py`

### Summary

**What changed and why**  
This refactor simplifies dummy data generation by removing the `mm_processor_kwargs` parameter and making `mm_options` non-optional. The changes ensure that HuggingFace processor kwargs are always passed from the multimodal config, eliminating the need for explicit per-call overrides. This reduces boilerplate and centralizes configuration handling.

**Technical impact**  
The API for `get_dummy_mm_data` and `get_dummy_processor_inputs` is now stricter: `mm_options` is required (must be a dictionary), and `mm_processor_kwargs` is removed. Processor configuration is managed via `MultiModalConfig.mm_processor_kwargs`, which is automatically merged in `MultimodalProcessingContext`. This streamlines dummy data generation across all multimodal models.

**Potential risks**  
Breaking changes for any external code calling these methods directly, as `mm_options` can no longer be `None`. The removal of `mm_processor_kwargs` could affect customizations that relied on per-call overrides. Edge cases where `mm_options` was previously `None` must now pass an empty dict, as seen in updated tests.

**Key insights**  
Developers must update calls to dummy data generation methods to pass an empty dict `{}` for `mm_options` if no overrides are needed. Processor configuration should be set via `MultiModalConfig` rather than per-call kwargs. This change improves consistency and reduces complexity in multimodal input handling.

---

## 3. [[Model Runner V2] Remove propose_draft method](https://github.com/vllm-project/vllm/pull/35070)


### Base Information

- **PR Number:** #35070
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-22 18:27:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35070/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The `propose_draft` method has been removed from the `ModelRunner` class because it was deemed redundant. Instead, the code now directly calls `self.speculator.propose` within the `sample_tokens` method, passing the required arguments inline.

**Technical impact**  
This change simplifies the codebase by eliminating an unnecessary abstraction layer. The `ModelRunner` class now has one fewer method, reducing complexity and making the flow of draft token generation more direct and explicit in the `sample_tokens` method.

**Potential risks**  
There is a risk of missing argument validation previously handled in `propose_draft` (e.g., the `assert self.speculator is not None` check). Additionally, any future changes to the speculator's `propose` method signature will require updates in the calling code, as the arguments are now passed directly.

**Key insights**  
This refactoring improves code clarity by removing an indirect wrapper. Developers should ensure that the `self.speculator` is always initialized when draft token generation is needed, and consider adding a null check if not already guaranteed elsewhere.

---

## 4. [[Model Runner V2][Minor] Remove redundant `do_spec_decode` field](https://github.com/vllm-project/vllm/pull/35039)


### Base Information

- **PR Number:** #35039
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-22 16:18:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35039/files) (1):**
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
The change removes the redundant `do_spec_decode` boolean field and simplifies the initialization logic for speculative decoding. Instead of setting `do_spec_decode = True/False` based on the presence of `speculative_config`, the code now directly checks if `self.speculator` is not `None`. This makes the logic cleaner and more direct.

**Technical impact**  
This refactoring reduces code complexity by eliminating an intermediate boolean flag. The system behavior remains unchanged because `self.speculator` is initialized to `None` when `speculative_config` is absent, and the conditional logic now uses this direct check. The initialization of `num_speculative_steps` is also moved outside the conditional block for consistency.

**Potential risks**  
The main risk is in the `load_model` method where the condition changed from checking `self.do_spec_decode` to `self.speculator is not None`. This should be safe since `self.speculator` is only set when `speculative_config` exists. However, if `self.speculator` could be `None` in a speculative decoding scenario due to an initialization error, this could lead to missed buffer preparation.

**Key insights**  
The refactoring improves code clarity by removing redundant state. Developers should ensure that `self.speculator` is always properly initialized when speculative decoding is enabled. The change also highlights that `num_speculative_steps` defaults to 0, which is a sensible default for non-speculative execution.

---

## 5. [[Spec Decode] Reduce TP communication for speculative decoding draft token generation](https://github.com/vllm-project/vllm/pull/34049)


### Base Information

- **PR Number:** #34049
- **Author:** [zixi-qi](https://github.com/zixi-qi)
- **Merged By:** [houseroad](https://github.com/houseroad)
- **Merged time:** 2026-02-22 14:59:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34049/files) (4):**
  - `vllm/config/speculative.py`
  - `vllm/model_executor/layers/logits_processor.py`
  - `vllm/model_executor/models/llama4_eagle.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
Added a `use_local_argmax_reduction` option to reduce TP communication during Eagle speculative decoding draft generation. Instead of all-gathering full vocabulary logits (O(vocab_size)), each TP rank computes local argmax on its vocab shard and gathers only (max_value, global_index) pairs (O(2 × tp_size)), then reduces to find the global argmax.

**Technical impact**  
This optimization significantly reduces inter-rank communication volume for large vocabularies (e.g., 200K+ tokens), improving throughput and reducing latency in TP setups. The change introduces a new `get_top_tokens` method in the logits processor and model, with fallback to full logits when vocab remapping (`draft_id_to_target_id`) is active or for tree attention scenarios.

**Potential risks**  
The optimization is incompatible with non-positive logit scaling factors (raises `ValueError`). It also bypasses the optimization when vocab remapping is used, negating benefits in those cases. Edge cases include precision loss from bf16 indices (mitigated by using float32 for gathered pairs) and potential performance regression for small batch sizes or vocabularies where overhead may outweigh gains.

**Key insights**  
Defaulting the option to `False` is prudent; enable only when batch_size × vocab_size is large enough to justify the reduction in communication. Developers should verify that target models implement `get_top_tokens` and be aware of the fallback conditions. Benchmarking is essential to validate gains, as shown by the marginal but positive results in the provided test.

---

## 6. [[Model] Add NVFP4 quantization support for Step3.5-Flash](https://github.com/vllm-project/vllm/pull/34478)


### Base Information

- **PR Number:** #34478
- **Author:** [tacos8me](https://github.com/tacos8me)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-22 11:30:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34478/files) (5):**
  - `tests/kernels/moe/test_nvfp4_moe.py`
  - `vllm/model_executor/layers/fused_moe/cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`
  - `vllm/model_executor/models/step3p5.py`

### Summary

**What changed and why**  
This PR adds NVFP4 quantization support for Step-3.5-Flash models, specifically enabling the `swiglustep` activation function used in MoE layers 43–44. Changes include adding activation support to kernel backends, fixing an assertion that blocked non-SiLU activations, extending weight loading to handle per-expert NVFP4 checkpoint formats, and adding comprehensive tests.

**Technical impact**  
The modifications extend NVFP4 quantization compatibility to models with `swiglustep` activations, leveraging existing non-fused activation fallback paths. The weight loading system now supports both traditional packed 3D and newer per-expert checkpoint formats, improving flexibility. Kernel-level changes are minimal and focused on allowlists and validation logic.

**Potential risks**  
The per-tensor to per-expert scale broadcasting assumes consistent semantics across all NVFP4 checkpoints, which could cause issues if other quantization tools use different formats. The reliance on non-fused activation paths for `swiglustep` may impact performance compared to fused kernels. Any future activations will require similar manual additions to allowlists.

**Key insights**  
Developers should note that `swiglustep` uses a fallback path, so performance may differ from SiLU. The weight loading changes are backward compatible but introduce a new per-expert format that must be maintained. Ensure any new activation types are added to both `_supports_activation` methods and tested accordingly.

---

## 7. [[Bug] Refactor max_num_batched_tokens to account for drafting](https://github.com/vllm-project/vllm/pull/34898)


### Base Information

- **PR Number:** #34898
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-02-22 08:18:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34898/files) (5):**
  - `vllm/config/scheduler.py`
  - `vllm/config/speculative.py`
  - `vllm/config/vllm.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
This PR introduces a new `max_num_scheduled_tokens` configuration to separate the maximum tokens the scheduler can issue from the maximum tokens the worker can process. This addresses a crash in speculative decoding where draft models could append extra tokens to the batch, exceeding the worker's capacity. The fix adjusts scheduling limits based on speculative decoding settings while preserving the original batch token limit.

**Technical impact**  
The changes decouple scheduling constraints from batch processing limits, allowing the scheduler to account for speculative decoding overhead. The `max_num_scheduled_tokens` is dynamically reduced when speculative decoding is active, ensuring the total tokens (scheduled + draft-added tokens) never exceed `max_num_batched_tokens`. This also simplifies compile range calculations by removing speculative decoding adjustments.

**Potential risks**  
If `max_num_scheduled_tokens` is manually set too high relative to speculative decoding settings, validation may fail, causing initialization errors. Edge cases in non-standard speculative configurations (e.g., custom draft methods) might not be fully covered by the slot calculation logic. The removal of speculative adjustments from compile ranges assumes the new scheduling limit is sufficient, which should be verified under high load.

**Key insights**  
Developers should note that `max_num_scheduled_tokens` now defaults to `max_num_batched_tokens` but is automatically reduced for speculative decoding. This change centralizes scheduling limit logic in `VllmConfig`, improving consistency. Ensure any custom speculative configurations implement `max_num_new_slots_for_drafting` correctly to avoid scheduling overflow.

---

## 8. [[Spec Decode] Defer clearing KV connector metadata for EAGLE3 speculative decode + prefill / decode disagg setup](https://github.com/vllm-project/vllm/pull/34529)


### Base Information

- **PR Number:** #34529
- **Author:** [zixi-qi](https://github.com/zixi-qi)
- **Merged By:** [houseroad](https://github.com/houseroad)
- **Merged time:** 2026-02-22 08:08:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34529/files) (3):**
  - `vllm/v1/worker/gpu/kv_connector.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/kv_connector_model_runner_mixin.py`

### Summary

**What changed and why**  
The changes defer clearing KV connector metadata when speculative decoding is enabled with disaggregated prefill/decode. Previously, metadata was cleared after the target model forward but before the draft model forward, causing the draft model's attention layers to skip saving its KV cache. The fix adds a `clear_metadata` parameter to control when metadata is cleared and explicitly clears it after the draft model runs.

**Technical impact**  
This ensures the draft model’s KV cache is properly saved and transferred via the KV connector in disaggregated setups. The changes are conditional—only applied when speculative decoding is active—and maintain backward compatibility for non-speculative decoding scenarios. The architecture now supports correct KV cache propagation between prefill and decode hosts in EAGLE3 speculative decoding.

**Potential risks**  
If the `clear_metadata` flag is incorrectly set or omitted in future modifications, it could lead to similar cache transfer failures. The fix assumes draft prefill runs on the prefill host; if draft prefill runs on the decode host, the original issue does not occur, but the changes are still applied uniformly when speculative decoding is enabled.

**Key insights**  
Developers should ensure the `clear_metadata` parameter is consistently used in all code paths involving KV connector operations. The addition of explicit `clear_metadata` methods improves clarity but requires careful integration. Testing should validate both speculative and non-speculative decoding workflows to prevent regressions.

---

## 9. [[ROCm][CI] Fix realtime test timeouts caused by aiter JIT compilation delays](https://github.com/vllm-project/vllm/pull/35052)


### Base Information

- **PR Number:** #35052
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-22 02:07:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35052/files) (1):**
  - `tests/entrypoints/openai/test_realtime_validation.py`

### Summary

**What changed and why**  
The PR addresses intermittent timeout failures in ROCm builds for two realtime API tests. The root cause is JIT compilation of aiter modules during the first inference request, which exceeds the original 30-60s timeouts. The fix introduces a warm-up step in `test_multi_chunk_streaming` to trigger JIT compilation beforehand, increases timeouts to 360s for initial requests, and adds synchronization by waiting for `session.updated` events after `session.update` calls.

**Technical impact**  
These changes make the tests more robust to JIT compilation delays, particularly on ROCm hardware. The warm-up step ensures compilation occurs before the actual test audio is processed, while extended timeouts accommodate slow first requests. The addition of `session.updated` waiting improves protocol compliance and reduces race conditions in session setup.

**Potential risks**  
The 360s timeout may mask other performance issues or hangs unrelated to JIT compilation. The warm-up step adds complexity and could affect test execution time, though it’s limited to ROCm builds. If the server does not implement `session.updated`, warnings are issued but the test continues, which might hide integration issues.

**Key insights**  
Developers should note that JIT compilation on first inference is a known bottleneck on ROCm. The warm-up pattern is a practical workaround, but consider if compilation can be moved out of the critical path in production. Ensure timeouts are monitored to avoid hiding regressions, and verify that `session.updated` events are properly handled in server implementations.

---

## 10. [[ROCm][CI] Fix flaky embedding chat test by using tolerance-based comparison](https://github.com/vllm-project/vllm/pull/35050)


### Base Information

- **PR Number:** #35050
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-22 01:03:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35050/files) (1):**
  - `tests/entrypoints/pooling/embed/test_online.py`

### Summary

**What changed and why**  
The PR addresses flaky embedding test failures on ROCm by replacing exact equality comparisons with tolerance-based checks for embedding vectors. This is necessary because separate forward passes can produce slightly different floating-point results due to non-deterministic kernel reductions, particularly evident on MI355 ROCm hardware. The fix also forces `--max-num-seqs 1` on ROCm to reduce non-determinism from varying batch geometry.

**Technical impact**  
The changes make the test more robust by aligning it with existing tolerance-based comparison patterns used elsewhere in the same test file (e.g., `check_embeddings_close`). The ROCm-specific determinism arguments ensure consistent execution paths, while metadata fields continue to be validated with exact equality checks. This maintains test reliability without compromising validation rigor.

**Potential risks**  
The tolerance threshold in `check_embeddings_close` isn't visible in the diff; if set too loosely, it might mask genuine regressions. Forcing `--max-num-seqs 1` could alter performance characteristics during testing, though this is likely acceptable for CI. The fix is ROCm-specific, but similar non-determinism could theoretically affect other platforms in the future.

**Key insights**  
Always use tolerance-based comparisons for floating-point embeddings rather than exact equality. The pattern of separating vector comparisons (tolerance-based) from metadata comparisons (exact) is a best practice for embedding tests. When dealing with GPU-specific non-determinism, consider adding platform-specific execution constraints to ensure test stability.

---

