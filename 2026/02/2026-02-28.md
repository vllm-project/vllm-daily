# vLLM Merged PR Report

**Report Date:** 2026-02-28 PST

**Total Merged PRs:** 18

---

## 1. [[AMD][CI] Support Triton attention with ExampleConnector](https://github.com/vllm-project/vllm/pull/34931)


### Base Information

- **PR Number:** #34931
- **Author:** [rjrock](https://github.com/rjrock)
- **Merged By:** [orozery](https://github.com/orozery)
- **Merged time:** 2026-02-28 23:58:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34931/files) (3):**
  - `tests/v1/kv_connector/unit/test_example_connector.py`
  - `tests/v1/kv_connector/unit/test_multi_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py`

### Summary

**What changed and why**  
The changes enable Triton attention support in ExampleConnector by handling its different KV cache layout compared to Flash attention. This is necessary because ROCm uses Triton attention, which has a distinct memory layout that wasn't previously supported during KV cache transfer operations.

**Technical impact**  
The ExampleConnector now dynamically handles multiple attention backends (Flash, Triton, MLA) by checking the attention metadata type during KV cache injection and extraction. This adds backend awareness to the connector while maintaining compatibility with existing Flash attention workflows. Test modifications parameterize attention backends to validate both configurations.

**Potential risks**  
The conditional logic based on `isinstance(attn_metadata, TritonAttentionMetadata)` creates tight coupling between the connector and attention backend implementations. If new backends are added, similar conditionals must be extended. The changes assume `attn_metadata` can be a dictionary in some contexts (line 191), which may need clearer documentation about when this occurs.

**Key insights**  
Consider refactoring to a more extensible pattern (e.g., strategy pattern or registry) for handling different KV cache layouts. Ensure all attention backends are covered in tests—note that ROCm tests only use TRITON_ATTN while CUDA tests both backends. The removal of ROCm-specific test skips indicates underlying platform issues may have been resolved.

---

## 2. [Fix typo: implictly -> implicitly in isaac.py docstring](https://github.com/vllm-project/vllm/pull/35646)


### Base Information

- **PR Number:** #35646
- **Author:** [lin-shh](https://github.com/lin-shh)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-28 23:34:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35646/files) (1):**
  - `vllm/model_executor/models/isaac.py`

### Summary

**What changed and why**  
Fixed a typo in a docstring where "implictly" was corrected to "implicitly" in the `process_vision_for_patches` function documentation. This change improves documentation clarity and professionalism without altering any code logic.

**Technical impact**  
This is a documentation-only change with zero impact on system behavior, performance, or functionality. The correction maintains consistency in spelling and enhances the readability of the docstring for developers.

**Potential risks**  
No functional risks exist since this only affects documentation. The only minimal risk is if automated tools parse docstrings for code generation or validation, but the correction aligns with standard English spelling and should not cause issues.

**Key insights**  
Always maintain clean, correctly spelled documentation as it reflects codebase quality. While minor, such fixes contribute to overall code hygiene and should be encouraged during reviews. Ensure similar typos are checked across the codebase for consistency.

---

## 3. [[Bugfix][Model] Fix Qwen3.5/Qwen3Next ignoring --dtype flag on older GPUs](https://github.com/vllm-project/vllm/pull/35617)


### Base Information

- **PR Number:** #35617
- **Author:** [lailoo](https://github.com/lailoo)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-28 19:27:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35617/files) (2):**
  - `vllm/model_executor/models/qwen3_5.py`
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
Removed explicit `dtype=config.dtype` parameters from `torch.zeros()` and `torch.empty()` calls in Qwen3.5 and Qwen3Next model implementations. This fixes a bug where the models ignored the user's `--dtype float16` flag, instead creating bfloat16 parameters that crashed on older GPUs lacking bfloat16 support.

**Technical impact**  
The parameters (`RMSNormGated.weight`, `attn_layer_scale`, `ffn_layer_scale`) now correctly inherit the default PyTorch dtype set by vLLM's model loader, which respects the user-specified `--dtype` flag. This ensures model compatibility with a wider range of GPU hardware while maintaining the intended parameter initialization.

**Potential risks**  
If any other code paths rely on these specific parameters being bfloat16 for numerical stability or performance, removing the explicit dtype could introduce subtle behavioral changes. However, since the user explicitly chooses the dtype, this aligns with expected behavior. There's a low risk that other model files might have similar hidden `config.dtype` usage that could cause analogous issues.

**Key insights**  
The root cause was a confusion between HuggingFace's `PretrainedConfig.dtype` (static, from config.json) and vLLM's runtime `ModelConfig.dtype` (dynamic, from user flags). Developers should be cautious when using `config.dtype` in model implementations and prefer the model loader's default dtype for parameter initialization to respect user overrides.

---

## 4. [Add TMA support to fused_moe_lora kernel](https://github.com/vllm-project/vllm/pull/32195)


### Base Information

- **PR Number:** #32195
- **Author:** [gnovack](https://github.com/gnovack)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-28 18:55:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32195/files) (5):**
  - `tests/lora/test_fused_moe_lora_kernel.py`
  - `tests/lora/test_olmoe_tp.py`
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`
  - `vllm/lora/ops/triton_ops/utils.py`
  - `vllm/triton_utils/allocation.py`

### Summary

**What changed and why**  
This PR adds Tensor Memory Access (TMA) support to the fused MoE LoRA kernel to improve performance on SM90+ GPUs. The implementation conditionally enables TMA for loading LoRA A and B weight matrices when compute capability supports it and fully-sharded mode is disabled. Key changes include adding TMA descriptor handling, modifying memory layout for sorted token access, and updating kernel logic to support both TMA and non-TMA paths.

**Technical impact**  
The changes introduce a performance optimization path that can significantly improve throughput, as demonstrated by benchmark results showing ~17% improvement in output token throughput. The kernel now supports both traditional pointer-based access and TMA-based access with automatic descriptor management. The architecture maintains backward compatibility by falling back to non-TMA paths when conditions aren't met (e.g., fully-sharded mode or older hardware).

**Potential risks**  
TMA is disabled when `fully_sharded` is enabled due to inconsistent token ordering across TP ranks, creating a performance disparity between sharded and non-sharded configurations. Device-side TMA descriptor allocation in parallel execution (PDL) could cause conflicts. The implementation adds complexity with multiple code paths (TMA vs non-TMA, single-slice vs multi-slice), increasing maintenance burden and testing requirements.

**Key insights**  
The performance gains (340.79 to 398.37 tokens/sec) justify the added complexity for supported hardware. Developers should ensure proper testing across different configurations (single/multi-slice, sharded/non-sharded). The token ordering limitation with fully-sharded mode indicates a future optimization opportunity if MoE Align kernel modifications can guarantee deterministic token ordering. The new `set_triton_allocator` utility enables device-side descriptor allocation but requires careful handling in parallel execution scenarios.

---

## 5. [[Model Runner V2] Add ModelStateInterface [4/N]](https://github.com/vllm-project/vllm/pull/35621)


### Base Information

- **PR Number:** #35621
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-28 13:19:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35621/files) (5):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/model_states/__init__.py`
  - `vllm/v1/worker/gpu/model_states/default.py`
  - `vllm/v1/worker/gpu/model_states/interface.py`

### Summary

**What changed and why**  
This PR introduces a `ModelStateInterface` as part of a larger Model Runner V2 refactoring. It extracts an abstract base class (`ModelState`) to define a clear interface for model state management, while renaming the existing concrete implementation to `DefaultModelState`. A factory function `init_model_state` is added to centralize instantiation.

**Technical impact**  
The changes decouple the interface from implementation, promoting better abstraction and future extensibility. The `model_runner.py` now depends on the interface via the factory, reducing direct coupling to the concrete class. This sets the stage for pluggable model state implementations without modifying core runner logic.

**Potential risks**  
The factory function currently hardcodes `DefaultModelState`, which could lead to confusion if multiple implementations are added later without updating the factory. There’s also a risk of incomplete interface coverage if `DefaultModelState` has methods not yet defined in the abstract class, though the PR appears to be part of a series addressing this.

**Key insights**  
This is a foundational step toward a more modular architecture. Developers should ensure all required methods are abstract in the interface as the refactoring progresses. The factory pattern should be extended to support dynamic implementation selection if needed. Consistency in import paths (e.g., `model_states.interface`) should be maintained across the codebase.

---

## 6. [[Bugfix] Fix Anthropic API base64 image handling in Messages endpoint](https://github.com/vllm-project/vllm/pull/35557)


### Base Information

- **PR Number:** #35557
- **Author:** [voipmonitor](https://github.com/voipmonitor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-28 12:57:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35557/files) (2):**
  - `tests/entrypoints/openai/test_anthropic_messages_conversion.py`
  - `vllm/entrypoints/anthropic/serving.py`

### Summary

**What changed and why**  
The fix addresses two bugs in Anthropic-to-OpenAI request conversion for the Messages API: base64 images were missing the required `data:` URI prefix, causing 400 errors, and images within `tool_result` content blocks were being silently dropped due to improper stringification. A new helper method `_convert_image_source_to_url()` now properly constructs data URIs for base64 sources and handles URL sources, while tool result images are injected as follow-up user messages since OpenAI tool messages only support string content.

**Technical impact**  
This change ensures that image/vision support works correctly in the Anthropic Messages adapter, enabling proper handling of both direct image content blocks and images returned by tools. The conversion logic now aligns with OpenAI's expected format, allowing downstream media connectors to process images without errors. The architecture remains consistent, with a clear separation between base64 and URL source handling.

**Potential risks**  
If the `media_type` field is missing for base64 sources, the code defaults to `image/jpeg`, which could cause issues if the actual image format differs. The injection of images from tool results as separate user messages might subtly alter conversation flow or ordering in multi-turn interactions. Edge cases like malformed source dictionaries or unsupported media types may not be fully validated.

**Key insights**  
The helper method centralizes image source conversion, improving maintainability and consistency. Developers should ensure that any future image source types added by Anthropic are handled in `_convert_image_source_to_url()`. The workaround for tool result images highlights a semantic mismatch between Anthropic and OpenAI tool message formats—this should be documented as a known limitation.

---

## 7. [[Chore] Cleanup BNB utilization dead code](https://github.com/vllm-project/vllm/pull/35620)


### Base Information

- **PR Number:** #35620
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-28 11:22:41
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35620/files) (1):**
  - `vllm/model_executor/layers/linear.py`

### Summary

**What changed and why**  
Removed the `left_shift_bitsandbytes_4bit_shard` function and its associated imports, as this code became dead after the deprecation of `QKVCrossParallelLinear`. The cleanup eliminates unused functionality related to BitsAndBytes 4-bit quantization shard handling.

**Technical impact**  
This reduces code complexity and maintenance overhead by removing a specialized utility that is no longer referenced elsewhere in the codebase. The removal does not affect runtime behavior since the function was unused, but it may impact any downstream projects that imported or depended on this internal helper.

**Potential risks**  
If any external or internal code still imports or calls this function indirectly, it will result in `NameError` or import failures. Additionally, the removal of the TODO comment about needing a "more flexible structure" could obscure future design considerations if BitsAndBytes quantization support is revisited.

**Key insights**  
This is a straightforward dead-code cleanup that improves code hygiene. Developers should verify no other modules or tests depend on this function, and consider whether the underlying quantization sharding logic might be needed for future features. The PR description could be enhanced by confirming that no regression tests are required.

---

## 8. [[Deprecation] Deprecate code in 0.17 as scheduled](https://github.com/vllm-project/vllm/pull/35441)


### Base Information

- **PR Number:** #35441
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-28 09:32:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35441/files) (22):**
  - `tests/entrypoints/pooling/embed/test_online.py`
  - `vllm/entrypoints/grpc_server.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/completion/protocol.py`
  - `vllm/entrypoints/openai/translations/__init__.py`
  - `vllm/entrypoints/openai/translations/api_router.py`
  - `vllm/entrypoints/openai/translations/protocol.py`
  - `vllm/entrypoints/openai/translations/serving.py`
  - `vllm/entrypoints/openai/translations/speech_to_text.py`
  - `vllm/entrypoints/pooling/base/protocol.py`
  - `vllm/entrypoints/pooling/classify/protocol.py`
  - `vllm/entrypoints/pooling/embed/protocol.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/score/protocol.py`
  - `vllm/model_executor/layers/mamba/mamba_utils.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/multimodal/utils.py`
  - `vllm/pooling_params.py`
  - `vllm/sampling_params.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This PR removes deprecated code scheduled for removal in v0.17, following the release of v0.16. Changes include removing the `truncate_prompt_tokens` parameter from `SamplingParams` and `PoolingParams`, renaming `normalize` to `use_activation` in embedding APIs, deleting deprecated translation module aliases, and cleaning up other deprecated attributes and functions.

**Technical impact**  
The removal simplifies the codebase by eliminating deprecated pathways, reducing maintenance overhead. The `truncate_prompt_tokens` parameter is now exclusively handled via `tokenization_kwargs`, and the `normalize` parameter is fully replaced by `use_activation`. Backward-compatible aliases and warnings have been removed, which may break existing code that still relies on the deprecated interfaces.

**Potential risks**  
Codebases that have not migrated from deprecated parameters (e.g., `truncate_prompt_tokens`, `normalize`) or still import from removed aliases (e.g., `vllm.entrypoints.openai.translations`) will encounter errors after upgrading to v0.17. The gRPC server changes require careful validation to ensure the new `tokenization_kwargs` handling works correctly across all request types.

**Key insights**  
Developers must update their code to use `tokenization_kwargs` for truncation and `use_activation` for embedding normalization. All imports from deprecated translation modules should be redirected to `vllm.entrypoints.openai.speech_to_text`. Thorough testing is recommended to confirm compatibility, especially for gRPC and multimodal processing integrations.

---

## 9. [[Benchmark] Avoid unnecessary video download in MMVU](https://github.com/vllm-project/vllm/pull/35618)


### Base Information

- **PR Number:** #35618
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-28 09:07:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35618/files) (1):**
  - `vllm/benchmarks/datasets.py`

### Summary

**What changed and why**  
The PR modifies the MMVU dataset class to avoid unnecessary video downloads during benchmarking. It replaces remote video URLs with local file paths by downloading the dataset once using `snapshot_download` and then substituting the URL base path with the local directory path.

**Technical impact**  
This change improves benchmark performance by eliminating repeated video downloads from Hugging Face for each sample. The dataset is now downloaded locally once during initialization, and all subsequent video processing uses local files, reducing network overhead and potential latency.

**Potential risks**  
If the dataset is large, the initial `snapshot_download` could consume significant disk space and time. There's also a risk of path replacement logic failing if video URLs don't exactly match the expected remote path format, or if the dataset structure changes in future versions.

**Key insights**  
The solution effectively addresses the performance issue but should include error handling for the path replacement. Consider adding a fallback mechanism if local files aren't found, and document the disk space requirements since the entire dataset will be cached locally.

---

## 10. [[Fix] Avoid sending image input to other PP ranks](https://github.com/vllm-project/vllm/pull/35405)


### Base Information

- **PR Number:** #35405
- **Author:** [emricksini-h](https://github.com/emricksini-h)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-28 08:14:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35405/files) (1):**
  - `vllm/v1/executor/ray_utils.py`

### Summary

**What changed and why**  
The PR adds logic to strip `mm_features` (containing large CPU image tensors) from `scheduler_output.scheduled_new_reqs` after the first pipeline parallel (PP) rank executes. This prevents Ray's Compiled DAG from attempting to serialize and transmit these tensors to subsequent PP ranks, which is unnecessary since only the first rank processes raw image inputs. The fix addresses fatal crashes and deadlocks in Vision-Language Model serving when using PP > 1 with Ray's NCCL channel type.

**Technical impact**  
This change reduces unnecessary data transfer across PP ranks, mitigating serialization errors and deadlocks in Ray's DAG pipeline. It maintains correctness because subsequent PP ranks only require intermediate tensors, not the raw multimodal features. The modification is conditional, applying only when the model supports multimodal inputs and the current rank is the first in its PP group.

**Potential risks**  
If `scheduler_output.scheduled_new_reqs` is missing or its structure changes unexpectedly, accessing `req.mm_features` could raise an `AttributeError`. Additionally, any downstream logic that inadvertently depends on `mm_features` being present in later PP ranks could break, though this is unlikely given the stated design.

**Key insights**  
This is a targeted workaround for known Ray issues (#61309, #61358) and should be revisited once Ray fixes those bugs. Developers should ensure that no other parts of the codebase assume `mm_features` availability beyond the first PP rank. The fix is minimal and localized, but monitoring for regressions in multimodal request handling across distributed setups is advised.

---

## 11. [Fix Qwen3_5MTP packed_modules_mapping for gate_up_proj](https://github.com/vllm-project/vllm/pull/35581)


### Base Information

- **PR Number:** #35581
- **Author:** [cwazai](https://github.com/cwazai)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-02-28 06:50:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35581/files) (1):**
  - `vllm/model_executor/models/qwen3_5_mtp.py`

### Summary

**What changed and why**  
Fixed a typo in the `packed_modules_mapping` dictionary for the `Qwen3_5MTP` class. The `gate_up_proj` key incorrectly mapped to `["up_proj", "down_proj"]`; it has been corrected to `["gate_proj", "up_proj"]`. This ensures the mapping aligns with the model's actual architecture and the weight-loading logic in `Qwen3_5MultiTokenPredictor.load_weights()`, which expects `gate_up_proj` to pack `gate_proj` and `up_proj`.

**Technical impact**  
The correction enables the intended fused kernel optimization for the MLP layer, allowing packed weight tensors to be processed efficiently. This restores the performance benefits of fused operations, improving inference throughput and reducing latency, as evidenced by the benchmark results showing a 2–8% performance gain.

**Potential risks**  
If any downstream code or serialized models incorrectly assumed the previous mapping, they may encounter compatibility issues during weight loading. Additionally, the change assumes that all `Qwen3_5MTP` instances use the same MLP structure; variations in model configurations should be validated to ensure consistency.

**Key insights**  
This fix is critical for achieving optimal performance in Qwen3.5 MTP models. Developers should verify that similar mappings in other model classes are correct and consider adding unit tests (as done here) to prevent regressions. The performance improvement highlights the importance of accurate parameter packing for leveraging fused kernels in high-throughput serving scenarios.

---

## 12. [custom dataset img support base64](https://github.com/vllm-project/vllm/pull/35280)


### Base Information

- **PR Number:** #35280
- **Author:** [flutist](https://github.com/flutist)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-28 03:49:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35280/files) (1):**
  - `vllm/benchmarks/datasets.py`

### Summary

**What changed and why**  
The `process_image` function was updated to support base64-encoded image data URLs in addition to existing URL and file path handling. Specifically, strings starting with `"data:image/"` are now recognized as base64 data and passed through directly without modification, while the error message was clarified to reflect the new supported input types.

**Technical impact**  
This change extends the function's compatibility with inline image data, allowing benchmarks or datasets to use base64-encoded images without requiring separate file storage or HTTP fetching. The modification is backward-compatible, as existing URL and file path logic remains unchanged.

**Potential risks**  
If base64 strings are malformed or excessively large, they could cause processing overhead or failures downstream. The validation is minimal—only checking the prefix—so invalid base64 data may not be caught until later stages. Additionally, the function does not differentiate between base64 and other `data:` URLs, which might lead to unintended behavior.

**Key insights**  
Developers can now pass base64 image data directly to the benchmark pipeline, simplifying workflows that generate images programmatically. Ensure that base64 strings are properly formatted and consider adding size validation if large images are expected. The update aligns with broader support for flexible image inputs in multimodal applications.

---

## 13. [[Feat] Add CUDA torch fallbacks for fp8_mqa_logits/fp8_paged_mqa_logits_torch function](https://github.com/vllm-project/vllm/pull/35271)


### Base Information

- **PR Number:** #35271
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-28 02:12:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35271/files) (3):**
  - `vllm/model_executor/layers/sparse_attn_indexer.py`
  - `vllm/utils/deep_gemm.py`
  - `vllm/v1/attention/backends/mla/indexer.py`

### Summary

**What changed and why**  
This PR adds PyTorch fallback implementations for FP8 multi-query attention (MQA) logit computation when DeepGEMM is unavailable on CUDA platforms. It replaces a hard runtime error with a warning and provides `fp8_mqa_logits_torch` and `fp8_paged_mqa_logits_torch` functions to maintain functionality, addressing issue #35021 where systems without DeepGEMM would fail.

**Technical impact**  
The changes introduce conditional logic in `sparse_attn_indexer.py` to select between DeepGEMM-optimized and PyTorch fallback paths based on `is_deep_gemm_supported()`. This ensures backward compatibility and allows the system to operate without DeepGEMM, albeit with potentially reduced performance. The fallback implementations handle both non-paged and paged KV-cache scenarios using standard PyTorch operations.

**Potential risks**  
The PyTorch fallbacks may have significantly lower performance compared to DeepGEMM-optimized kernels, especially for large batch sizes or sequence lengths. The manual looping in `fp8_paged_mqa_logits_torch` could become a bottleneck. Additionally, the fallback uses `bfloat16` and `float` conversions which may introduce slight numerical differences compared to the DeepGEMM path.

**Key insights**  
Developers should be aware that the system will now run without DeepGEMM but with a performance trade-off. The warning encourages users to install DeepGEMM for optimal performance. The fallback implementations are designed for correctness over speed, so performance-critical deployments should ensure DeepGEMM is available. The changes maintain the same API, minimizing disruption to existing code.

---

## 14. [add io_process_plugin for sparse embedding](https://github.com/vllm-project/vllm/pull/34214)


### Base Information

- **PR Number:** #34214
- **Author:** [staugust](https://github.com/staugust)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-28 01:16:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34214/files) (14):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/plugins.yaml`
  - `docs/design/io_processor_plugins.md`
  - `tests/plugins/bge_m3_sparse_plugin/bge_m3_sparse_processor/__init__.py`
  - `tests/plugins/bge_m3_sparse_plugin/bge_m3_sparse_processor/sparse_embeddings_processor.py`
  - `tests/plugins/bge_m3_sparse_plugin/bge_m3_sparse_processor/types.py`
  - `tests/plugins/bge_m3_sparse_plugin/setup.py`
  - `tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py`
  - `tests/plugins_tests/test_bge_m3_sparse_io_processor_plugins.py`
  - `tests/plugins_tests/test_io_processor_plugins.py`
  - `vllm/plugins/io_processors/__init__.py`
  - `vllm/plugins/io_processors/interface.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
Added a new IO processor plugin (`bge_m3_sparse_plugin`) to support sparse embedding output for the BGE-M3 model, which returns token IDs with corresponding weights. This addresses issue #33882 and enables both online (HTTP API) and offline (direct library call) usage for sparse embeddings.

**Technical impact**  
The changes extend the IO processor plugin framework to handle a new data type (sparse embeddings) and modify the plugin interface to require a `renderer` parameter for tokenization. Existing plugins are backward compatible with a deprecation warning. The system now supports a structured sparse embedding response format that matches FlagEmbedding's output.

**Potential risks**  
The backward compatibility mechanism for plugins without a `renderer` parameter may cause issues if plugins rely on the old signature beyond v0.18. The sparse embedding aggregation logic assumes special tokens are stripped when `add_special_tokens` is true, which could break if the model's tokenization behavior differs. Memory management for stored requests in `online_requests` and `offline_requests` dictionaries could lead to leaks if requests are not properly cleaned up.

**Key insights**  
Developers should update existing IO processor plugins to accept the `renderer` parameter to avoid future breakage. The plugin demonstrates a clean pattern for extending vLLM with custom output formats, but careful attention is needed for token alignment and request lifecycle management. Integration tests are comprehensive but should be expanded to cover edge cases like empty inputs or tokenization mismatches.

---

## 15. [[Feature]Supports Anthropic Thinking Block](https://github.com/vllm-project/vllm/pull/33671)


### Base Information

- **PR Number:** #33671
- **Author:** [mariohong128](https://github.com/mariohong128)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-28 01:02:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33671/files) (2):**
  - `vllm/entrypoints/anthropic/protocol.py`
  - `vllm/entrypoints/anthropic/serving.py`

### Summary

**What changed and why**  
This PR adds support for Anthropic's thinking blocks in the message stream converter. It extends the data models to include thinking content types and deltas, and refactors the streaming logic to handle mixed content chunks containing reasoning, text, and multiple tool calls simultaneously.

**Technical impact**  
The changes introduce a state machine (`_ActiveBlockState`) to manage concurrent content blocks during streaming, enabling proper sequencing of thinking, text, and tool call deltas. This improves compatibility with Anthropic's /v1/messages API by supporting complex multi-content responses that were previously unhandled.

**Potential risks**  
The refactored streaming logic increases complexity, which could introduce subtle bugs in edge cases like empty reasoning deltas or rapid content type switches. The UUID generation for thinking signatures may not guarantee uniqueness across distributed systems if not properly coordinated.

**Key insights**  
Developers should verify that the state machine correctly handles all possible content interleavings, especially when tool calls and reasoning appear in the same chunk. Consider adding integration tests for mixed-content scenarios and reviewing the signature generation strategy for production use.

---

## 16. [Add padding support to wvSplitK solution for skinny GEMMs](https://github.com/vllm-project/vllm/pull/33762)


### Base Information

- **PR Number:** #33762
- **Author:** [amd-hhashemi](https://github.com/amd-hhashemi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-28 01:02:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33762/files) (3):**
  - `csrc/rocm/skinny_gemms.cu`
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`
  - `vllm/model_executor/layers/utils.py`

### Summary

**What changed and why**  
The PR adds padding support to the `wvSplitK` kernel for handling skinny GEMMs (matrix multiplications where dimensions may not be perfectly aligned). This enables the kernel to work with matrices that have dimensions not multiples of tile sizes, improving performance for certain model configurations (e.g., Falcon-Mamba) as shown by the 0.89% throughput increase and reduced latency in the provided benchmarks.

**Technical impact**  
The changes modify the kernel to accept padded dimensions (`Kbp`, `Kap`) and adjust memory access patterns to safely handle out-of-bounds reads via `min__` bounds checking. This allows the kernel to process matrices with arbitrary dimensions while maintaining correctness, extending its applicability beyond perfectly aligned cases. The test suite is updated to include padded and non-aligned dimensions, ensuring robustness.

**Potential risks**  
The introduction of bounds checking and padded logic could introduce minor overhead for perfectly aligned cases, though benchmarks show net positive impact. Edge cases where padding leads to misaligned memory accesses or incorrect bias indexing (e.g., when `M != Bx`) need careful validation, as bias handling was refactored. The kernel now relies on `__builtin_amdgcn` intrinsics for DPP reductions, which may have portability implications across GPU architectures.

**Key insights**  
Developers should verify that padding is correctly computed and passed to the kernel in all calling contexts. The removal of the `x.is_contiguous()` check in `rocm_unquantized_gemm_impl` increases flexibility but assumes callers ensure appropriate memory layouts. Performance testing should include a mix of aligned and unaligned dimensions to validate gains across workloads.

---

## 17. [[ROCm][CI] Parametrize vision score tests across attention backends with per-backend tolerances](https://github.com/vllm-project/vllm/pull/35571)


### Base Information

- **PR Number:** #35571
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-28 00:59:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35571/files) (1):**
  - `tests/entrypoints/pooling/score/test_online_score_vision.py`

### Summary

**What changed and why**  
The changes parametrize vision score tests across ROCm attention backends (`ROCM_ATTN`, `ROCM_AITER_FA`, `TRITON_ATTN`, `FLEX_ATTENTION`) using a module-scoped fixture with `--attention-config`. This enables backend-specific testing and prepares for future integration work. On non-ROCm platforms, the backend list remains empty, preserving existing behavior.

**Technical impact**  
The test suite now launches separate servers per backend, applying backend-specific tolerances and ROCm-specific stability fixes (disabling skinny GEMM, prefix caching, and limiting batch size). This increases test coverage for ROCm platforms while maintaining isolation between backends. The `assert_score` helper provides detailed diagnostic logging for easier failure analysis.

**Potential risks**  
The `ROCM_ATTN` backend shows a consistent ~8.5% deviation in `text_vs_text` scores, indicating a potential numerical accuracy bug. While mitigated with a higher tolerance (0.09), this could mask real regressions. The ROCm stability fixes (environment variables and arguments) add platform-specific complexity that must be kept in sync with future ROCm improvements.

**Key insights**  
Developers should note the per-backend tolerance dictionary and the diagnostic `assert_score` helper, which streamline debugging. The known `ROCM_ATTN` issue is tracked separately; test failures in other backends should be investigated promptly. Ensure any new ROCm backends are added to `ROCM_ATTN_BACKENDS` and `BACKEND_TOL` with appropriate tolerances.

---

## 18. [[Benchmark] Improve UX of sweep scripts](https://github.com/vllm-project/vllm/pull/35600)


### Base Information

- **PR Number:** #35600
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-28 00:36:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35600/files) (6):**
  - `docs/benchmarking/sweeps.md`
  - `vllm/benchmarks/sweep/plot.py`
  - `vllm/benchmarks/sweep/plot_pareto.py`
  - `vllm/benchmarks/sweep/serve.py`
  - `vllm/benchmarks/sweep/serve_workload.py`
  - `vllm/benchmarks/sweep/startup.py`

### Summary

**What changed and why**  
This PR improves the UX of benchmark sweep scripts by introducing an explicit `--experiment-name` argument and making `--resume` a boolean flag. Previously, users had to manually specify a timestamp directory for resuming; now the experiment directory is constructed as `output_dir/experiment_name` (defaulting to a timestamp). The plotting utilities were updated to accept a required `EXPERIMENT_DIR` argument instead of an optional `OUTPUT_DIR`.

**Technical impact**  
The changes centralize experiment directory management within the `*Args` classes via new helper methods (`resolve_experiment_dir`, `run_ctx`). This reduces code duplication and ensures consistent validation across all sweep scripts (serve, serve_workload, startup). The plotting scripts now require an explicit experiment directory, eliminating ambiguity about which results to plot.

**Potential risks**  
Existing workflows that relied on the old `--resume <timestamp>` syntax will break, requiring users to update their commands. The removal of default values for `EXPERIMENT_DIR` in plotting scripts could cause errors if callers omit the argument. There is also a risk of accidental overwrites if users specify an existing `experiment_name` without `--resume`.

**Key insights**  
The refactoring significantly improves script usability and maintainability. Developers should update any automation or documentation that uses the old resume syntax. The new `run_ctx` method provides better error handling and dry-run support, but care must be taken to ensure all sweep scripts implement it consistently. The changes are backward-incompatible, so release notes should highlight the required command updates.

---

