# vLLM Merged PR Report

**Report Date:** 2026-02-12 PST

**Total Merged PRs:** 39

---

## 1. [[Refactor] Call renderer for online IO processor request](https://github.com/vllm-project/vllm/pull/34490)


### Base Information

- **PR Number:** #34490
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 22:48:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34490/files) (4):**
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/pooling/pooling/protocol.py`
  - `vllm/entrypoints/pooling/pooling/serving.py`

### Summary

**What changed and why**  
The PR refactors completion request preprocessing to use a unified `_preprocess_cmpl` method instead of `_preprocess_completion`. This change addresses an issue where the online IO processor wasn't properly calling the renderer for completion requests, ensuring consistent prompt processing across different entrypoints.

**Technical impact**  
This change centralizes completion preprocessing logic into a single method (`_preprocess_cmpl`) that's now used by both the LLM entrypoint and OpenAI serving layer. The IO processor protocol now includes a `build_tok_params` method to properly construct tokenization parameters, ensuring consistent behavior between different request types.

**Potential risks**  
The rename from `_preprocess_completion` to `_preprocess_cmpl` could break any external code that directly calls the old method. The changes in tokenization parameter handling for pooling requests need careful validation to ensure backward compatibility, especially with the conditional logic around `pooling_params.task`.

**Key insights**  
Developers should verify that all completion request paths now properly use `_preprocess_cmpl` and that the renderer is correctly invoked for all request types. The new `build_tok_params` method in `IOProcessorRequest` provides a cleaner abstraction for tokenization parameter construction that should be adopted consistently across the codebase.

---

## 2. [[Bugfix] Fix mamba state dtype setting for Qwen3-Next and Qwen3.5](https://github.com/vllm-project/vllm/pull/34489)


### Base Information

- **PR Number:** #34489
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 22:48:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34489/files) (4):**
  - `vllm/model_executor/layers/mamba/mamba_utils.py`
  - `vllm/model_executor/models/config.py`
  - `vllm/model_executor/models/qwen3_5.py`
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
This PR fixes a bug where Qwen3-Next and Qwen3.5 models incorrectly coupled convolution and SSM state dtypes. The changes decouple these dtypes by updating `gated_delta_net_state_dtype` to accept a separate `mamba_ssm_cache_dtype` parameter, ensuring proper dtype initialization for both models. A config hook for Qwen3.5 aligns the cache configuration with the Hugging Face config's `mamba_ssm_dtype` when the CLI flag is set to `"auto"`.

**Technical impact**  
The fix ensures that convolution and SSM states can have independent dtypes, improving consistency between allocation and runtime. For Qwen3-Next, SSM state now respects `--mamba-ssm-cache-dtype`, while convolution state defaults to the model dtype. For Qwen3.5, the system now correctly reads from `cache_config.mamba_ssm_cache_dtype`, which is populated from the HF config when not overridden by CLI.

**Potential risks**  
If users previously relied on the buggy coupling behavior, their SSM state dtype may change unexpectedly (e.g., when `--mamba-ssm-cache-dtype` was not explicitly set). The warning for Qwen3.5 when CLI overrides HF config could be missed in logs, leading to confusion. Edge cases where `mamba_ssm_dtype` is missing in the HF config may default incorrectly.

**Key insights**  
Developers should ensure that Qwen3-Next is initialized with `--mamba-ssm-cache-dtype float32` if float32 SSM cache is required, as this was previously a silent bug. The fix centralizes dtype logic via `cache_config.mamba_ssm_cache_dtype`, promoting consistency. Reviewers should verify that all Mamba-based models follow similar patterns to avoid future coupling issues.

---

## 3. [[Refactor] Pass full VllmConfig to Renderer](https://github.com/vllm-project/vllm/pull/34485)


### Base Information

- **PR Number:** #34485
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 22:48:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34485/files) (18):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/renderers/test_completions.py`
  - `tests/renderers/test_mistral.py`
  - `tests/test_inputs.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/renderers/base.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/renderers/grok2.py`
  - `vllm/renderers/hf.py`
  - `vllm/renderers/mistral.py`
  - `vllm/renderers/registry.py`
  - `vllm/renderers/terratorch.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
The refactor modifies renderer initialization to accept the full `VllmConfig` object instead of just `ModelConfig`. This change prepares the renderer to access additional configuration fields (beyond model-specific settings) that will be required for multimodal (MM) processing.

**Technical impact**  
All renderer classes (`BaseRenderer` and its implementations) now receive `VllmConfig` and internally access `model_config` via `config.model_config`. This maintains backward compatibility for existing model-config-dependent logic while enabling future access to broader configuration (e.g., observability, multimodal settings). The change propagates through the initialization chain—from engine classes to input processors and tests.

**Potential risks**  
If any code directly accesses renderer attributes expecting the old `config` to be a `ModelConfig`, it could break. The test updates show widespread adjustments to mock objects, indicating this risk is mitigated. Additionally, the `skip_tokenizer_init` checks now use `model_config`; any oversight in updating similar config references could cause runtime errors.

**Key insights**  
Developers should update any custom renderers or direct renderer usage to expect `VllmConfig`. The pattern `self.model_config = config.model_config` ensures consistency. Future MM features can now leverage the full config without further refactoring. Verify that all config references in renderer subclasses are updated to use `self.model_config`.

---

## 4. [[CI/Build] Fix CUDA re-initialization error in distributed model tests](https://github.com/vllm-project/vllm/pull/34491)


### Base Information

- **PR Number:** #34491
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-12 22:43:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34491/files) (1):**
  - `tests/models/multimodal/generation/test_voxtral_realtime.py`

### Summary

**What changed and why**  
The change addresses a CUDA re-initialization error in distributed model tests by moving the import of `VoxtralRealtimeBuffer` inside the test function. This lazy import pattern prevents early CUDA initialization that can conflict with distributed testing setups.

**Technical impact**  
This modification localizes the CUDA-dependent import to the specific test execution context, allowing the test framework to manage CUDA context properly. The change maintains the same functionality while deferring CUDA initialization until the test actually runs.

**Potential risks**  
If other tests or imports in the same file also cause early CUDA initialization, this fix might be insufficient. There's also a risk that the lazy import could be overlooked when refactoring or adding new tests that use the same module.

**Key insights**  
Lazy imports are an effective pattern for managing CUDA context in distributed testing scenarios. Developers should apply this approach consistently across all tests that import CUDA-dependent modules and consider adding a comment explaining why the lazy import is necessary for future maintainers.

---

## 5. [[Bugfix] Fix encoder cache underestimation for GLM-4V/GLM-OCR single image](https://github.com/vllm-project/vllm/pull/34483)


### Base Information

- **PR Number:** #34483
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 21:04:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34483/files) (1):**
  - `vllm/model_executor/models/glm4_1v.py`

### Summary

**What changed and why**  
The fix addresses a critical bug where single-image processing for GLM-4V/GLM-OCR models was using a video default of `num_frames=16` in token budget calculations. This caused the `smart_resize` logic to constrain `16*H*W <= max_pixels`, severely underestimating the spatial budget for a single image and leading to encoder cache overflow errors. The changes ensure single-image methods use `num_frames=1` and dynamically read the `max_pixels` configuration from the Hugging Face image processor instead of using a hardcoded value.

**Technical impact**  
This correction ensures accurate encoder cache pre-allocation for single-image inputs, preventing runtime errors when processing large images. The system now properly calculates the maximum possible token count (~6084 tokens for large images) instead of the underestimated value (~3721 tokens). The change also improves maintainability by reading configuration from the image processor rather than using magic numbers.

**Potential risks**  
If the HF image processor's `size["longest_edge"]` configuration is missing or incorrectly defined, the `_get_image_max_pixels()` method could fail or return an invalid value. The fix assumes `longest_edge` consistently represents a pixel area constraint across all GLM vision model variants, which may not hold for future model versions. There's also a risk if other model files have similar issues but weren't updated.

**Key insights**  
Always validate configuration assumptions when reading from external components like HF processors. Consider adding a fallback value or validation in `_get_image_max_pixels()`. This bug highlights the importance of ensuring parameter defaults (like `num_frames=16`) match the actual use case—single-image vs. video processing. Developers should audit other vision model implementations for similar frame count mismatches.

---

## 6. [[Bugfix] Standardize getting number of image patches/tokens](https://github.com/vllm-project/vllm/pull/34358)


### Base Information

- **PR Number:** #34358
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 20:47:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34358/files) (29):**
  - `tests/kernels/core/test_mrope.py`
  - `tests/models/multimodal/generation/test_common.py`
  - `tests/models/multimodal/processing/test_gemma3.py`
  - `tests/models/multimodal/processing/test_idefics3.py`
  - `tests/models/multimodal/processing/test_qwen2_vl.py`
  - `tests/models/multimodal/processing/test_smolvlm.py`
  - `vllm/model_executor/models/cohere2_vision.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/gemma3_mm.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/h2ovl.py`
  - `vllm/model_executor/models/hunyuan_vision.py`
  - `vllm/model_executor/models/idefics3.py`
  - `vllm/model_executor/models/interns1.py`
  - `vllm/model_executor/models/internvl.py`
  - `vllm/model_executor/models/keye.py`
  - `vllm/model_executor/models/lfm2_vl.py`
  - `vllm/model_executor/models/molmo.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/model_executor/models/ovis2_5.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/phi3v.py`
  - `vllm/model_executor/models/phi4mm.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/skyworkr1v.py`
  - `vllm/model_executor/models/smolvlm.py`
  - `vllm/multimodal/processing/context.py`

### Summary

**What changed and why**  
This PR standardizes how multimodal models calculate the number of image patches/tokens by consistently passing `mm_kwargs` to processor methods and removing support for `processor=None` arguments. The changes ensure that processor configurations (like image size parameters) are properly considered when determining token counts, fixing tests for Idefics3 and SmolVLM that weren't passing these kwargs.

**Technical impact**  
The changes affect 20+ multimodal model implementations by modifying their `get_num_image_tokens`, `get_num_patches`, and related methods. All methods now require explicit processor arguments and `mm_kwargs` parameters, eliminating internal fallback logic. This creates a more consistent interface and ensures processor configurations are properly merged via the new `get_merged_mm_kwargs()` helper.

**Potential risks**  
Removing the `processor=None` fallback could break external code that relies on this convenience feature. Some models (like Gemma3n) still have methods without `mm_kwargs` parameters, creating inconsistency. The extensive changes across many files increase the risk of regression if any model-specific logic was incorrectly adapted.

**Key insights**  
Developers should ensure all multimodal processor calls include both processor and mm_kwargs arguments. The new `get_merged_mm_kwargs()` method provides a standardized way to merge configuration overrides. Test coverage appears comprehensive, but special attention should be paid to models with complex token calculation logic (like Cohere2Vision and Lfm2Vl) during integration testing.

---

## 7. [[ROCm][CI] Fix serving tokens test failures](https://github.com/vllm-project/vllm/pull/34047)


### Base Information

- **PR Number:** #34047
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-12 19:27:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34047/files) (1):**
  - `tests/entrypoints/openai/test_serving_tokens.py`

### Summary

**What changed and why**  
The PR fixes two issues in the OpenAI serving tokens test: non-deterministic failures on ROCm due to prefix cache tiling divergence and an incorrect assertion for `logprobs=0`. The fix disables prefix caching in the test server to eliminate numerical divergence and corrects the logprobs validation logic.

**Technical impact**  
Disabling prefix caching ensures deterministic GEMM execution on ROCm by preventing batch dimension changes that cause different tile configurations. The logprobs fix ensures the test correctly handles the server's `null` response when `logprobs=0`. These changes make the test reliable across runs without affecting production code.

**Potential risks**  
The test now runs without prefix caching, which reduces test coverage for prefix caching functionality. The environment variable `VLLM_ROCM_USE_SKINNY_GEMM` is hardcoded to "0", which may not align with other test configurations or future defaults.

**Key insights**  
The comment provides valuable documentation about ROCm's numerical sensitivity to batch dimensions and offers alternative solutions (`--deterministic-prefix-caching`). Developers should consider creating separate test suites for prefix caching to maintain coverage while keeping deterministic tests for core functionality.

---

## 8. [Add new sections to CODEOWNERS](https://github.com/vllm-project/vllm/pull/34309)


### Base Information

- **PR Number:** #34309
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:39:28
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34309/files) (1):**
  - `.github/CODEOWNERS`

### Summary

**What changed and why**  
This PR restructures the CODEOWNERS file by adding new sections for API-related entrypoints and IO processing components, splitting the monolithic `entrypoints/` section into specific subdirectories, and adding ownership for `tokenizers/` and `renderers/`. It also aligns `sampling_params.py` with `pooling_params.py` for consistency and removes outdated directory references.

**Technical impact**  
The changes improve code ownership granularity, ensuring more targeted reviews for API endpoints and IO processing modules. By splitting `entrypoints/` into specific subdirectories (e.g., OpenAI, CLI, MCP), review responsibilities are better distributed. The cleanup of outdated paths reduces confusion and maintains file accuracy.

**Potential risks**  
If any newly assigned owners lack context or availability, it could slow down reviews for critical areas like API entrypoints. The removal of outdated directories is safe but should be verified to ensure no active code paths are inadvertently unowned. Over-splitting ownership could lead to fragmented responsibility.

**Key insights**  
This update enhances maintainability by clarifying ownership, especially for growing areas like multimodal and API endpoints. Reviewers should confirm their assigned areas align with their expertise. Future changes should continue to validate ownership assignments against team capacity and project priorities.

---

## 9. [[Hybrid] Fix and optimize block-aligned splitting in mamba cache align mode](https://github.com/vllm-project/vllm/pull/33706)


### Base Information

- **PR Number:** #33706
- **Author:** [peakcrosser7](https://github.com/peakcrosser7)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:21:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33706/files) (1):**
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The PR fixes a bug where block-aligned splitting for Mamba cache alignment was only applied during prefill for new requests, not resumed requests. This caused resumed requests to be scheduled without proper block alignment, leading to incorrect Mamba state retrieval on cache hits. The change ensures block-aligned splitting is performed during prefill for both new and resumed requests.

**Technical impact**  
The logic now uses `num_computed_tokens` to determine if a request is in the prefill phase, replacing the check `request.num_output_tokens == 0`. This correctly identifies resumed requests that still have remaining prompt tokens to process. The `last_cache_position` calculation is also updated to use `request.num_tokens` instead of `request.num_prompt_tokens`, ensuring alignment accounts for the total tokens processed so far.

**Potential risks**  
If `num_computed_tokens` includes external computed tokens in future scenarios (currently asserted as zero), the alignment logic may need re-evaluation. The condition `num_computed_tokens < max(request.num_prompt_tokens, request.num_tokens - 1)` could be subtle; edge cases where `num_tokens - 1` is less than `num_prompt_tokens` might cause unintended behavior. Additionally, the Eagle mode adjustment (`last_cache_position - block_size`) could potentially result in a negative value, though `max(..., 0)` mitigates this.

**Key insights**  
Developers should verify that `num_external_computed_tokens` remains zero as asserted, as any future support would break this logic. The updated condition for prefill detection is more robust but requires careful validation with various request states (e.g., very short prompts). Testing should include resumed requests with partial prompt processing to ensure Mamba states are cached correctly and cache hits retrieve valid states.

---

## 10. [[Bugfix] Fix Random Dataset Prefix Length Inaccuracy](https://github.com/vllm-project/vllm/pull/33907)


### Base Information

- **PR Number:** #33907
- **Author:** [frankwang28](https://github.com/frankwang28)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:21:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33907/files) (1):**
  - `vllm/benchmarks/datasets.py`

### Summary

**What changed and why**  
The PR fixes prefix length inaccuracies in `RandomDataset` by applying decode-encode tokenization to prefix generation, aligning it with how input prompts are generated. This ensures the actual tokenized prefix length matches the expected length, improving prefix cache hit rate accuracy.

**Technical impact**  
The change modifies `get_prefix()` to use `gen_prompt_decode_to_target_len()` for token length adjustment, ensuring consistency between prefix and prompt tokenization. This affects all random dataset benchmarks using prefix caching, making cache hit rate calculations more reliable and predictable.

**Potential risks**  
The decode-encode process may still produce token mismatches (logged as warnings), which could slightly affect prefix length precision. Additionally, the added tokenization overhead may marginally impact benchmark performance, though the test results show minimal throughput differences.

**Key insights**  
Developers should note that prefix caching behavior is now more accurate, but token mismatches are expected due to sampling limitations. The fix ensures benchmark reliability, but any downstream logic depending on exact prefix lengths should handle potential minor deviations.

---

## 11. [[Kernel] [Helion] [5/N] Add Helion Autotuning infrastructure](https://github.com/vllm-project/vllm/pull/34025)


### Base Information

- **PR Number:** #34025
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:21:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34025/files) (9):**
  - `scripts/autotune_helion_kernels.py`
  - `tests/kernels/helion/test_register.py`
  - `tests/kernels/helion/test_silu_mul_fp8.py`
  - `vllm/kernels/helion/__init__.py`
  - `vllm/kernels/helion/config_manager.py`
  - `vllm/kernels/helion/configs/silu_mul_fp8.json`
  - `vllm/kernels/helion/ops/__init__.py`
  - `vllm/kernels/helion/ops/silu_mul_fp8.py`
  - `vllm/kernels/helion/register.py`

### Summary

**What changed and why**  
This PR adds autotuning infrastructure for Helion kernels in vLLM. It introduces a script (`scripts/autotune_helion_kernels.py`) to autotune registered kernels, enhances the kernel wrapper to support input generation for autotuning, and adds configuration management improvements. The changes enable automatic performance optimization of Helion kernels across different GPU platforms.

**Technical impact**  
The infrastructure allows kernels to register representative inputs for autotuning, which generates platform-specific configurations stored in JSON files. This enables runtime selection of optimal kernel configurations based on input shapes and GPU architecture. The system supports incremental autotuning, checkpointing, and graceful handling of failures.

**Potential risks**  
Autotuning may fail silently if input generators are not properly implemented or if GPU memory is insufficient for large inputs. The configuration selection logic relies on exact or closest matches, which may not always yield optimal performance for unseen input sizes. Concurrent autotuning processes could corrupt configuration files without proper locking.

**Key insights**  
Developers must implement `register_input_generator` for each kernel to enable autotuning. The system intelligently skips already-tuned configurations unless forced. Testing shows robust error handling and detailed logging, but ensure autotuning scripts are run in controlled environments to avoid production interference. The added test for `silu_mul_fp8` provides a template for future kernel integrations.

---

## 12. [[Bugfix] Delete unused redundant code in Kimi-K2.5](https://github.com/vllm-project/vllm/pull/34427)


### Base Information

- **PR Number:** #34427
- **Author:** [LoganJane](https://github.com/LoganJane)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:18:42
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34427/files) (1):**
  - `vllm/model_executor/models/kimi_k25.py`

### Summary

**What changed and why**  
Removed 5 lines of unused code from the Kimi-K2.5 model implementation. The deleted lines involved creating a deep copy of the `vllm_config` and modifying its `hf_config` attribute, which was not actually utilized in the model initialization process.

**Technical impact**  
This change simplifies the code by eliminating redundant operations that had no functional effect. The `sub_vllm_config` variable was created but never used, so removing it reduces complexity and potential overhead without altering model behavior.

**Potential risks**  
Minimal risk since the removed code was dead/unused. However, ensure that no other parts of the codebase implicitly relied on the deep copy or the modified `hf_config` structure, though the context suggests this is unlikely.

**Key insights**  
This is a straightforward cleanup that improves code maintainability. Developers should regularly audit for similar unused code segments, especially in model implementations, to keep the codebase lean and reduce cognitive load during reviews.

---

## 13. [[Refactor] Simplify BOS/EOS token handling](https://github.com/vllm-project/vllm/pull/34435)


### Base Information

- **PR Number:** #34435
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:18:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34435/files) (29):**
  - `tests/detokenizer/test_min_tokens.py`
  - `tests/detokenizer/test_stop_string_while_stop_model_terminates.py`
  - `tests/tokenizers_/test_detokenize.py`
  - `tests/tool_parsers/test_step3p5_tool_parser.py`
  - `tests/v1/core/test_kv_cache_utils.py`
  - `tests/v1/core/test_prefix_caching.py`
  - `tests/v1/core/test_priority_scheduler_random.py`
  - `tests/v1/core/test_scheduler.py`
  - `tests/v1/core/utils.py`
  - `tests/v1/engine/test_engine_core.py`
  - `tests/v1/engine/test_engine_core_client.py`
  - `tests/v1/engine/test_fast_incdec_prefix_err.py`
  - `tests/v1/engine/test_output_processor.py`
  - `tests/v1/engine/test_parallel_sampling.py`
  - `tests/v1/engine/utils.py`
  - `tests/v1/kv_connector/unit/test_decode_bench_connector.py`
  - `tests/v1/kv_connector/unit/test_lmcache_integration.py`
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `tests/v1/kv_connector/unit/utils.py`
  - `tests/v1/streaming_input/test_scheduler_streaming.py`
  - `tests/v1/structured_output/test_backend_guidance.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/renderers/base.py`
  - `vllm/sampling_params.py`
  - `vllm/v1/core/sched/utils.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/request.py`
  - `vllm/v1/structured_output/utils.py`

### Summary

**What changed and why**  
The PR refactors BOS/EOS token handling by moving `eos_token_id` from `Request` to `SamplingParams`, moving `get_bos_token_id` and `get_eos_token_id` methods from `InputPreprocessor` to `Renderer`, and removing fallback logic since `tokenizer.eos_token_id` is now required. This centralizes token ID management and simplifies the codebase.

**Technical impact**  
This change consolidates token ID logic into `SamplingParams` and `Renderer`, making the architecture cleaner. The `Request` and `EngineCoreRequest` classes now expose `eos_token_id` as a deprecated property that delegates to `sampling_params.eos_token_id`, ensuring backward compatibility while guiding users toward the new API.

**Potential risks**  
The deprecation warnings may cause confusion if not properly communicated, and there's a risk of breaking existing code that directly accesses `request.eos_token_id` without updating to use `sampling_params`. Additionally, the assumption that `tokenizer.eos_token_id` is always available could cause issues with custom tokenizers that don't implement this property.

**Key insights**  
Developers should update their code to access `eos_token_id` via `sampling_params.eos_token_id` instead of the deprecated `request.eos_token_id`. The refactor improves code organization but requires careful testing to ensure all tokenizer implementations provide the required `eos_token_id` property.

---

## 14. [[Bugfix] Remove assert that's no longer valid](https://github.com/vllm-project/vllm/pull/34443)


### Base Information

- **PR Number:** #34443
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:18:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34443/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`

### Summary

**What changed and why**  
Removed an `assert` statement that was checking whether `old_quant_method.is_monolithic` is `False`. This assertion became invalid due to recent Mixture of Experts (MoE) refactoring work and was causing test failures, specifically in `tests/lora/test_gptoss_tp.py`.

**Technical impact**  
The removal eliminates a hard failure that was blocking test execution, allowing the test to proceed past the assertion. However, the test still fails with incorrect outputs, indicating that the underlying issue is not resolved—only the immediate crash is avoided.

**Potential risks**  
Without the assertion, the code may now accept `old_quant_method.is_monolithic == True` in scenarios where it shouldn’t, potentially leading to silent logical errors or incorrect behavior in MoE-related operations. The test’s continued failure with wrong answers suggests unresolved functional defects.

**Key insights**  
This change is a temporary workaround rather than a fix. Developers should investigate why the test produces wrong answers and ensure the `old_quant_method` is handled correctly in the refactored MoE logic. Consider adding a more appropriate validation or logging if the monolithic condition is now permissible.

---

## 15. [[BugFix] Add block_size validation for mamba cache align mode](https://github.com/vllm-project/vllm/pull/34445)


### Base Information

- **PR Number:** #34445
- **Author:** [peakcrosser7](https://github.com/peakcrosser7)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:18:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34445/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
Added a validation assertion in the VLLM configuration to ensure that when Mamba cache align mode is enabled, the `block_size` does not exceed `max_num_batched_tokens`. This prevents a scenario where prefill requests cannot be scheduled due to alignment constraints, which previously caused the engine to hang.

**Technical impact**  
The change enforces a runtime constraint during engine initialization, ensuring that the scheduler can properly split tokens into block-aligned chunks. This prevents deadlocks in the scheduling logic when token counts are misaligned with the block size, improving system reliability in Mamba cache align mode.

**Potential risks**  
If users have existing configurations where `block_size` exceeds `max_num_batched_tokens`, engine initialization will now fail with an assertion error, requiring configuration adjustments. Additionally, the validation only occurs at startup; dynamic changes to these parameters during runtime are not covered.

**Key insights**  
Developers should review their Mamba cache align configurations to ensure `block_size` is appropriately set relative to `max_num_batched_tokens`. Consider documenting this constraint in configuration guides. For future enhancements, dynamic validation or automatic adjustment of these parameters could improve user experience.

---

## 16. [Fix num_logprobs parameter description in sampler.py](https://github.com/vllm-project/vllm/pull/34451)


### Base Information

- **PR Number:** #34451
- **Author:** [zhuohan123](https://github.com/zhuohan123)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:18:03
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34451/files) (1):**
  - `vllm/v1/sample/sampler.py`

### Summary

**What changed and why**  
The change corrects a documentation error in the `gather_logprobs` function's parameter description. The `num_logprobs` parameter was incorrectly described as specifying the "minimum number of logprobs to retain per token," but it actually defines the "maximum number of logprobs to retain per token."

**Technical impact**  
This is a documentation-only change that clarifies the intended behavior of the parameter. It does not affect runtime functionality, but ensures developers correctly understand that `num_logprobs` sets an upper bound on the number of log probabilities returned per token.

**Potential risks**  
No functional risks exist since only a comment was modified. However, if developers had previously misinterpreted the parameter as a minimum requirement, they might have written code based on incorrect assumptions about the function's behavior.

**Key insights**  
Always verify parameter descriptions against actual implementation logic. While this fix is minor, accurate documentation is critical for API usability. Consider reviewing other parameter descriptions in the same module for similar inconsistencies.

---

## 17. [[CI/Build] Update video URLs for testing](https://github.com/vllm-project/vllm/pull/34446)


### Base Information

- **PR Number:** #34446
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:15:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34446/files) (1):**
  - `tests/entrypoints/openai/test_video.py`

### Summary

**What changed and why**  
The test video URLs were updated from Google's gtv-videos-bucket to publicly accessible OpenCV sample videos because the previous URLs are no longer accessible. The maximum number of test videos was reduced from 4 to 3 to match the new list.

**Technical impact**  
This change ensures video processing tests remain functional by using reliable, open-source video samples. The reduction in test videos may slightly decrease test coverage for video count edge cases but maintains core functionality.

**Potential risks**  
The new URLs point to third-party repositories (GitHub) and external sites, which could become unavailable or change over time. The PR description mentions plans to host these videos on an AWS bucket, but until that's done, tests remain dependent on external resources.

**Key insights**  
Developers should prioritize migrating to the internal AWS bucket as suggested in the PR description to avoid future test failures. Consider adding a fallback mechanism or local test assets for critical video processing tests to reduce external dependencies.

---

## 18. [[Kernel] [Helion] [4/N] Add silu_mul_fp8 Helion kernel](https://github.com/vllm-project/vllm/pull/33373)


### Base Information

- **PR Number:** #33373
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 18:13:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33373/files) (7):**
  - `tests/kernels/helion/test_register.py`
  - `tests/kernels/helion/test_silu_mul_fp8.py`
  - `vllm/kernels/helion/__init__.py`
  - `vllm/kernels/helion/config_manager.py`
  - `vllm/kernels/helion/configs/silu_mul_fp8.json`
  - `vllm/kernels/helion/ops/__init__.py`
  - `vllm/kernels/helion/ops/silu_mul_fp8.py`

### Summary

**What changed and why**  
This PR adds a new Helion kernel for the `silu_mul_fp8` operation with FP8 quantization, introducing the first operator built using the new vLLM+Helion integration framework. It includes GPU configurations for H100 and H200 platforms, establishes a file structure for future ops with wildcard importing, and provides comprehensive unit tests.

**Technical impact**  
The changes extend vLLM's kernel system with a new Helion-based operator that leverages FP8 quantization for improved performance and memory efficiency. The auto-import mechanism in `vllm/kernels/helion/ops/__init__.py` ensures all future Helion ops are automatically registered, streamlining the addition of new kernels. The config picker logic dynamically selects optimal kernel configurations based on input tensor dimensions.

**Potential risks**  
The kernel currently only supports the `float8_e4m3fn` data type, limiting flexibility for other FP8 variants. The config picker assumes a fixed batch size of 256, which may lead to suboptimal performance for other batch sizes. Additionally, the tight test tolerances (5% absolute/relative) may mask subtle numerical differences in edge cases.

**Key insights**  
Developers should note the new auto-registration pattern for Helion ops, which simplifies future kernel additions. The config picker should be extended to support variable batch sizes and additional FP8 subtypes. The comprehensive test suite provides a good template for validating future Helion kernels, but attention should be paid to the numerical precision requirements for FP8 operations.

---

## 19. [[Core] Profiler improvements and lazy initialization](https://github.com/vllm-project/vllm/pull/33198)


### Base Information

- **PR Number:** #33198
- **Author:** [jaewonlee-fb](https://github.com/jaewonlee-fb)
- **Merged By:** [zhuohan123](https://github.com/zhuohan123)
- **Merged time:** 2026-02-12 16:16:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33198/files) (10):**
  - `vllm/distributed/utils.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/executor/abstract.py`
  - `vllm/v1/metrics/loggers.py`
  - `vllm/v1/worker/cpu_worker.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR introduces lazy profiler initialization and comprehensive rank naming for distributed tracing. The profiler is now only created when `start_profile()` is called, allowing proper trace naming with all parallel dimensions (DP, PP, TP, DCP, EP). A new `profile_prefix` parameter enables custom trace naming, and trace files now follow the format `{prefix}_dp{X}_pp{Y}_tp{Z}_dcp{W}_ep{V}_rank{R}`.

**Technical impact**  
The changes affect profiling across the entire codebase by modifying the profiler initialization flow. A new utility function `get_worker_rank_suffix()` provides standardized rank naming, and the profiler interface now accepts an optional `profile_prefix` parameter that propagates through multiple abstraction layers (LLM engine, core, workers). Profiler creation is deferred until first use, ensuring trace names include complete rank information.

**Potential risks**  
The lazy initialization could cause issues if profiler configuration validation happens after worker initialization. The fallback logic in `get_worker_rank_suffix()` might mask parallel state initialization problems. There's also a risk that restarting profiling (calling `start_profile()` multiple times) might not update trace names after the first initialization, as indicated by the comment about keeping the "original trace name."

**Key insights**  
Developers should ensure parallel state is properly initialized before profiling. The new naming convention provides better debugging capabilities in distributed environments. When using custom prefixes, be aware that the profiler maintains its initial trace name across restarts. The changes maintain backward compatibility while adding valuable diagnostic information for multi-dimensional parallelism setups.

---

## 20. [[Core] Add sleep level 0 mode with enqueue/wait pattern](https://github.com/vllm-project/vllm/pull/33195)


### Base Information

- **PR Number:** #33195
- **Author:** [jaewonlee-fb](https://github.com/jaewonlee-fb)
- **Merged By:** [zhuohan123](https://github.com/zhuohan123)
- **Merged time:** 2026-02-12 16:16:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33195/files) (5):**
  - `vllm/entrypoints/llm.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
This PR introduces a new sleep level 0 mode that pauses request scheduling while continuing to accept requests, along with explicit `enqueue()` and `wait_for_completion()` methods. The changes enable batched inference patterns where requests can be queued first and processed together, providing precise scheduling control for debugging and optimization.

**Technical impact**  
The implementation adds a new scheduling pause mechanism at the engine core level that doesn't affect GPU memory (unlike levels 1-2). The `enqueue()` method allows request submission without immediate processing, while `wait_for_completion()` triggers batch execution. This creates a two-phase execution pattern that separates request ingestion from processing.

**Potential risks**  
The new sleep level 0 introduces a state where requests accumulate in the queue without processing, which could lead to unbounded memory growth if not managed properly. There's also a risk of deadlock if `wait_for_completion()` is never called after `enqueue()`. The conditional logic in `_run_engine()` and scheduler pausing adds complexity to the execution flow.

**Key insights**  
Developers should use the `enqueue()`/`wait_for_completion()` pattern carefully with proper error handling and timeout mechanisms. The sleep level 0 requires explicit wake-up with `tags=["scheduling"]`, unlike other sleep levels. This feature is particularly valuable for benchmarking and controlled batch processing scenarios where precise timing of request execution is needed.

---

## 21. [[Frontend] Enable generic structured_outputs for responses API](https://github.com/vllm-project/vllm/pull/33709)


### Base Information

- **PR Number:** #33709
- **Author:** [alecsolder](https://github.com/alecsolder)
- **Merged By:** [zhuohan123](https://github.com/zhuohan123)
- **Merged time:** 2026-02-12 16:15:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33709/files) (2):**
  - `tests/entrypoints/openai/responses/test_sampling_params.py`
  - `vllm/entrypoints/openai/responses/protocol.py`

### Summary

**What changed and why**  
The changes enable the Responses API to accept generic `structured_outputs` parameter, allowing more complex output constraints like grammars, regexes, and choices. Previously, only JSON schema via `text.format` was supported, limiting functionality for advanced use cases.

**Technical impact**  
The `ResponsesRequest` protocol now includes an optional `structured_outputs` field that passes `StructuredOutputsParams` directly to sampling parameters. The implementation maintains backward compatibility by checking for conflicts between the new `structured_outputs` field and the existing `text.format` JSON schema approach, raising a `ValueError` if both are specified.

**Potential risks**  
The conflict validation only occurs when both `structured_outputs` and `text.format` are present, but there may be edge cases where other structured output types (beyond JSON schema) could conflict. The test coverage appears limited to basic grammar and JSON schema cases, potentially missing complex interaction scenarios with other API parameters.

**Key insights**  
Developers should note that `structured_outputs` provides more flexibility than the previous JSON schema approach but cannot be used simultaneously with `text.format`. The implementation correctly preserves the existing behavior for JSON schema while enabling new structured output capabilities. Ensure thorough testing when migrating from `text.format` to `structured_outputs` for JSON schema use cases.

---

## 22. [Use paged_attention_v1 for sliding window decode in rocm_aiter_fa](https://github.com/vllm-project/vllm/pull/34378)


### Base Information

- **PR Number:** #34378
- **Author:** [iseeyuan](https://github.com/iseeyuan)
- **Merged By:** [zhuohan123](https://github.com/zhuohan123)
- **Merged time:** 2026-02-12 16:14:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34378/files) (1):**
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
The change replaces the Triton-based `unified_attention` implementation for sliding window decode with `paged_attention_v1` in the ROCm flash-attention backend. This unification simplifies the code by using a single attention kernel that natively supports sliding window via its `sliding_window` parameter, eliminating a separate code path.

**Technical impact**  
This reduces code complexity and maintenance overhead by consolidating sliding window and non-sliding window decode paths. The sliding window value is derived from `self.sliding_window[0] + 1` to match flash-attention conventions, where 0 disables the feature. The change removes 29 lines of specialized Triton code and adds 2 parameters to the existing `paged_attention_v1` call.

**Potential risks**  
The conversion from `self.sliding_window` to `self.sliding_window[0] + 1` requires careful validation that this mapping correctly translates sliding window configurations. There's a risk if the original Triton implementation had different edge-case behavior or performance characteristics that aren't preserved in the new unified path.

**Key insights**  
This is a positive refactoring that reduces code duplication and leverages existing kernel capabilities. Developers should verify the sliding window parameter transformation works correctly across all model configurations, particularly ensuring backward compatibility for models without sliding windows (where the value should be 0).

---

## 23. [[Kernel] Support Flashinfer trtllm fused MoE non gated FP8 & NVFP4](https://github.com/vllm-project/vllm/pull/33506)


### Base Information

- **PR Number:** #33506
- **Author:** [amitz-nv](https://github.com/amitz-nv)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 13:06:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33506/files) (5):**
  - `tests/kernels/moe/test_flashinfer.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`

### Summary

**What changed and why**  
This PR adds support for Flashinfer's TRT-LLM fused MoE kernels with non-gated activations (specifically `relu2_no_mul`) for FP8 and NVFP4 quantization. It extends kernel support to handle both gated (e.g., SiLU) and non-gated activations, adds DeepSeek routing to the FP8 kernel's allowed routing methods, and ensures proper weight alignment and scaling for non-gated flows.

**Technical impact**  
The changes enable Flashinfer MoE kernels to work with models like Nemotron 3 Nano that use non-gated activations and NVFP4/FP8 quantization. Kernel selection logic now supports `relu2_no_mul` alongside SiLU, and weight preparation functions have been updated to handle gated vs. non-gated differences in tensor shapes, scaling factors, and alignment requirements (e.g., using `min_alignment=128` for non-gated activations).

**Potential risks**  
- The alignment padding logic for non-gated activations (`min_alignment=128`) could introduce memory overhead or performance degradation if intermediate sizes are padded significantly.  
- The expanded activation support may not be fully validated across all routing methods or edge cases (e.g., tensor-parallel configurations).  
- Changes to weight reordering and scaling could affect numerical accuracy for existing gated activation models if not thoroughly tested.

**Key insights**  
- Developers should verify that non-gated activation models (e.g., Nemotron 3 Nano) use the correct `activation_type` parameter when invoking Flashinfer kernels.  
- The test suite has been extended to cover `relu2_no_mul` for both CUTLASS and TRT-LLM backends, but additional validation for mixed precision and routing scenarios is recommended.  
- Note that the `activation_type` argument is now properly passed to the underlying Flashinfer kernels, replacing a hardcoded value.

---

## 24. [small adjustment to wvSplitKrc](https://github.com/vllm-project/vllm/pull/34410)


### Base Information

- **PR Number:** #34410
- **Author:** [amd-hhashemi](https://github.com/amd-hhashemi)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-12 12:26:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34410/files) (1):**
  - `csrc/rocm/skinny_gemms.cu`

### Summary

**What changed and why**  
The change modifies the `kOffcp` calculation in the ROCm kernel to explicitly clamp the offset using `min__(K - A_CHUNK, k_str + kOff)`. Previously, the commented-out clamp was inactive, which could cause out-of-bounds memory access in certain vLLM Docker environments, leading to prompt failures.

**Technical impact**  
This fix ensures the kernel respects tensor boundaries during memory loads, preventing potential segmentation faults or incorrect results when `k_str + kOff` exceeds `K - A_CHUNK`. The change is minimal and localized, affecting only the indexing logic for the `A` tensor in GPU memory.

**Potential risks**  
If `K - A_CHUNK` is incorrectly computed (e.g., due to an off-by-one error), it could still lead to boundary issues. Additionally, the fix assumes the `min__` macro is correctly defined and behaves as expected for unsigned integers. No changes to tests suggest the fix addresses a specific edge case not covered by existing tests.

**Key insights**  
Always validate tensor indexing in GPU kernels, especially when offsets depend on runtime values. Consider adding a comment explaining why clamping is necessary here. Reviewers should verify that `K` and `A_CHUNK` are defined consistently across the kernel to ensure the clamp logic is robust.

---

## 25. [[Bugfix] Enforce DeepGEMM when using sparse_attn_indexer on CUDA](https://github.com/vllm-project/vllm/pull/34374)


### Base Information

- **PR Number:** #34374
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 12:08:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34374/files) (1):**
  - `vllm/model_executor/layers/sparse_attn_indexer.py`

### Summary

**What changed and why**  
Added a runtime check in the `SparseAttentionIndexer.__init__` method to enforce DeepGEMM installation when using the sparse attention indexer on CUDA platforms. This prevents crashes during inference on Blackwell GPUs where DeepGEMM is required for `fp8_paged_mqa_logits` operations.

**Technical impact**  
The change ensures that initialization fails early with a clear error message if DeepGEMM is missing on CUDA, rather than allowing the model to proceed and crash later during kernel execution. This improves debuggability and user experience by providing immediate feedback about missing dependencies.

**Potential risks**  
The check only runs during initialization, so any subsequent changes to the environment (e.g., DeepGEMM being uninstalled after initialization) won't be caught. Additionally, if the condition `current_platform.is_cuda()` is evaluated incorrectly on hybrid or non-standard CUDA setups, it might either miss a required check or block valid configurations unnecessarily.

**Key insights**  
This is a defensive programming improvement that aligns with fail-fast principles. Developers should ensure that the DeepGEMM dependency is clearly documented in installation guides. Consider adding similar checks for other platform-specific dependencies to maintain consistency across the codebase.

---

## 26. [[ROCm][CI] Pin TorchCodec to v0.10.0 for ROCm compatibility](https://github.com/vllm-project/vllm/pull/34447)


### Base Information

- **PR Number:** #34447
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-12 10:47:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34447/files) (1):**
  - `tools/install_torchcodec_rocm.sh`

### Summary

**What changed and why**  
The script was modified to pin TorchCodec from the `main` branch to version `v0.10.0` by default. This change addresses a build failure in ROCm environments where the `main` branch introduced a dependency on PyTorch's stable ABI headers (`torch/csrc/stable/device.h`), which are unavailable in ROCm PyTorch builds.

**Technical impact**  
This ensures ROCm builds remain reproducible and compatible by avoiding the unstable `main` branch. The `TORCHCODEC_BRANCH` environment variable can still override the pinned version, preserving flexibility for testing or future updates when ROCm support is added.

**Potential risks**  
Pinning to an older release may cause ROCm builds to miss newer features or bug fixes from TorchCodec. If the stable ABI header dependency is eventually supported in ROCm, the pinned version could become outdated unless manually updated, potentially leading to technical debt.

**Key insights**  
This is a necessary workaround to maintain ROCm compatibility, but it requires periodic review to update the pinned version once ROCm supports the stable ABI headers. Developers should use the `TORCHCODEC_BRANCH` override for testing newer versions and monitor upstream TorchCodec and PyTorch ROCm support for changes.

---

## 27. [[Voxtral Realtime] Refactor & Improve buffering logic](https://github.com/vllm-project/vllm/pull/34428)


### Base Information

- **PR Number:** #34428
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 09:46:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34428/files) (7):**
  - `requirements/common.txt`
  - `requirements/nightly_torch_test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`
  - `tests/models/multimodal/generation/test_voxtral_realtime.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_realtime.py`

### Summary

**What changed and why**  
This PR refactors Voxtral realtime audio processing to eliminate base64 encoding/decoding of audio data and improve buffering logic. The changes update the mistral_common dependency to 1.9.1+ and implement a cleaner buffer management system based on upstream improvements from mistral-common.

**Technical impact**  
The refactoring significantly simplifies audio streaming by removing the need for audio hashing and base64 conversion. The new `VoxtralRealtimeBuffer` class provides more efficient audio chunk management with proper look-ahead/look-back handling and eliminates the pre-allocation of large audio buffers. The test suite is streamlined by replacing custom test harness code with the production buffer implementation.

**Potential risks**  
The dependency upgrade to mistral_common 1.9.1+ could introduce breaking changes if not properly tested. The new buffer logic relies on precise timing calculations for audio frames, which may be sensitive to edge cases with variable audio sampling rates. The removal of pre-allocated buffers could affect performance with very long audio streams if memory fragmentation becomes an issue.

**Key insights**  
The refactoring demonstrates good practice by leveraging upstream library improvements rather than maintaining custom solutions. Developers should verify that the new buffer logic handles all audio edge cases, particularly with variable-length streams and different sampling rates. The simplified test structure improves maintainability but requires thorough validation of the realtime streaming behavior across different scenarios.

---

## 28. [[Voxstral Realtime] Enable tests](https://github.com/vllm-project/vllm/pull/33803)


### Base Information

- **PR Number:** #33803
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 09:43:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33803/files) (5):**
  - `tests/entrypoints/openai/test_realtime_validation.py`
  - `tests/models/multimodal/generation/test_voxtral_realtime.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/voxtral.py`

### Summary

**What changed and why**  
The changes enable previously skipped tests for the Voxtral-Mini-4B-Realtime-2602 model now that it's publicly available on Hugging Face. The PR removes `@pytest.mark.skip` decorators from three test files, updates the model registry to point to the real model, adds a skip for a processing correctness test that doesn't apply to Voxtral Realtime, and includes a validation override in the model implementation.

**Technical impact**  
Enabling these tests will now run realtime Voxtral model validation in CI, ensuring the integration works correctly with the public model. The server arguments are updated with `--max-model-len 2048` for the OpenAI endpoint test, and the model registry now correctly configures the realtime model with `enforce_eager=True` and `tokenizer_mode="mistral"`. The processing correctness test is skipped for Voxtral Realtime because it doesn't use placeholder tokens.

**Potential risks**  
The CI may produce slightly different transcription results since the test fixtures were created on an H200 GPU, and the CI environment likely uses different hardware. The override of `_validate_mm_placeholders` could mask validation issues if the model's tokenizer behavior changes in the future. There's also a risk that the `--max-model-len 2048` argument might not be optimal for all test scenarios.

**Key insights**  
Developers should monitor CI test results for any hardware-induced discrepancies in audio transcription outputs. The placeholder validation skip is model-specific and should be documented; consider creating a separate test suite for realtime models that don't use placeholders. Ensure the `enforce_eager` and `max-model-len` settings align with production deployment requirements.

---

## 29. [[ROCm][quantization] improve OCP weight quant parser robust](https://github.com/vllm-project/vllm/pull/34431)


### Base Information

- **PR Number:** #34431
- **Author:** [xuebwang-amd](https://github.com/xuebwang-amd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 09:40:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34431/files) (1):**
  - `vllm/model_executor/layers/quantization/quark/quark.py`

### Summary

**What changed and why**  
This PR adds a type check in the `_is_w_ocp_mx_a_x` method to handle cases where `weight_quant` is a list instead of a dictionary. The change prevents an `AttributeError` when the quantization configuration for certain models (like `amd/Kimi-K2-Thinking-W4A8`) uses a list format (e.g., `fp8_w4a8`) that is incompatible with the OCP_MX quantization format expectation.

**Technical impact**  
The fix makes the OCP weight quantization parser more robust by gracefully handling incompatible quantization configurations. It adds a defensive check that logs a debug message and returns `False` when `weight_quant` is a list, preventing runtime crashes during model loading while maintaining existing functionality for valid configurations.

**Potential risks**  
The change only addresses the symptom (type error) but doesn't resolve the underlying incompatibility between the model's quantization format and OCP_MX expectations. There's a risk that models with list-based quantization configs might silently fall back to non-OCP_MX quantization paths without clear user feedback beyond debug logs.

**Key insights**  
Developers should ensure quantization configuration consistency across the codebase. Consider whether list-based quantization formats should be properly supported or explicitly rejected earlier in the pipeline. The debug logging provides useful diagnostics but production systems might benefit from more prominent warnings when encountering incompatible quantization formats.

---

## 30. [[Bugfix] Remove broken raw url GGUF model loading support](https://github.com/vllm-project/vllm/pull/34433)


### Base Information

- **PR Number:** #34433
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 09:40:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34433/files) (2):**
  - `tests/models/test_gguf_download.py`
  - `vllm/model_executor/model_loader/gguf_loader.py`

### Summary

**What changed and why**  
Removed support for loading GGUF models directly from raw HTTP/HTTPS URLs. This feature was broken and posed security risks due to potential URL redirection attacks. The change simplifies the codebase by eliminating rarely used functionality that was already non-functional.

**Technical impact**  
The GGUF loader now only supports local files and Hugging Face repository references (repo_id/filename.gguf format). This reduces code complexity and eliminates a code path that could bypass security controls. Users attempting to use raw URLs will now receive a clear error message indicating unrecognized GGUF references.

**Potential risks**  
Users who were relying on raw URL loading (despite it being broken) will experience service disruptions. There's a risk that some deployment scripts or documentation might still reference this functionality. The removal could affect automated workflows that programmatically generated GGUF model URLs.

**Key insights**  
This is a security-focused cleanup that removes vulnerable code. Developers should update any documentation or examples that mention raw URL support. Consider adding a deprecation warning in a future release before complete removal if there are concerns about breaking existing user workflows. The simplified error message now accurately reflects supported formats.

---

## 31. [Fix MoE for the Transformers modelling backend](https://github.com/vllm-project/vllm/pull/34436)


### Base Information

- **PR Number:** #34436
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 09:29:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34436/files) (1):**
  - `vllm/model_executor/models/transformers/moe.py`

### Summary

**What changed and why**  
The PR updates the Transformers backend's MoE (Mixture of Experts) integration to align with recent refactoring of the `FusedMoE` class. Specifically, it modifies the initialization order to pass a custom routing function via kwargs and updates the forward method call to use `self.runner.forward` instead of `self.forward_impl`. This ensures compatibility with the refactored `FusedMoE` and allows users with Transformers v5 to utilize MoE support.

**Technical impact**  
These changes maintain the functional behavior of the MoE layer while adapting to the internal refactoring of `FusedMoE`. The initialization adjustment ensures the custom routing function is properly registered, and the forward method now delegates to the correct runner object, preserving the expected in-place mutation handling of hidden states.

**Potential risks**  
If the `FusedMoE` base class expects `forward_impl` to be called directly, switching to `self.runner.forward` could introduce subtle behavioral differences or errors. Additionally, the removal of the explicit `super().__init__` call before setting up the custom routing function might cause issues if the base class initialization has side effects that depend on the routing function being set earlier.

**Key insights**  
Developers should verify that `self.runner.forward` is the intended interface in the refactored `FusedMoE` and ensure no regression in distributed routing logic. Testing with Transformers v5 is critical, and the changes highlight the importance of keeping backend integrations synchronized with core refactoring efforts.

---

## 32. [[Attention] Add FlashInfer Sparse MLA backend](https://github.com/vllm-project/vllm/pull/33451)


### Base Information

- **PR Number:** #33451
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-12 09:21:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33451/files) (24):**
  - `benchmarks/attention_benchmarks/benchmark.py`
  - `benchmarks/attention_benchmarks/common.py`
  - `benchmarks/attention_benchmarks/configs/mla_decode.yaml`
  - `benchmarks/attention_benchmarks/configs/mla_mixed_batch.yaml`
  - `benchmarks/attention_benchmarks/configs/mla_prefill.yaml`
  - `benchmarks/attention_benchmarks/configs/reorder_threshold.yaml`
  - `benchmarks/attention_benchmarks/configs/speculative_decode.yaml`
  - `benchmarks/attention_benchmarks/configs/standard_attention.yaml`
  - `benchmarks/attention_benchmarks/mla_runner.py`
  - `benchmarks/attention_benchmarks/runner.py`
  - `docs/design/attention_backends.md`
  - `tests/v1/attention/test_sparse_mla_backends.py`
  - `tools/pre_commit/generate_attention_backend_docs.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/platforms/cpu.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/interface.py`
  - `vllm/platforms/rocm.py`
  - `vllm/platforms/xpu.py`
  - `vllm/v1/attention/backends/mla/flashinfer_mla_sparse.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/sparse_utils.py`
  - `vllm/v1/attention/backends/registry.py`
  - `vllm/v1/attention/selector.py`

### Summary

**What changed and why**  
This PR integrates FlashInfer's new sparse MLA kernel into vLLM, enabling it by default when the head count is ≤16 (typically TP8 or higher). The changes add a new `FLASHINFER_MLA_SPARSE` backend, update backend selection logic to prioritize it for low head counts, and enhance benchmarking tools to evaluate sparse MLA performance. The integration targets models like DeepSeek-V3.2 that use index-based sparse attention.

**Technical impact**  
The addition expands vLLM's sparse attention capabilities, providing an alternative to the existing `FLASHMLA_SPARSE` backend. Backend selection now dynamically prefers `FLASHINFER_MLA_SPARSE` for ≤16 heads due to its performance advantage in tensor-parallel configurations. Benchmarking infrastructure is updated to support sparse backends, including new configuration files and improved sorting of results. The changes also refactor sparse utility functions into a shared module to reduce code duplication.

**Potential risks**  
The new backend is limited to SM 10.x (Blackwell) GPUs and requires specific model configurations (e.g., `qk_nope_head_dim == 128` and `index_topk`). Performance gains are head-count dependent, and incorrect backend selection could degrade performance for models with >16 heads. The forced block size adjustment to 64 may conflict with user configurations. Sparse attention correctness depends on accurate index mapping, which is complex and could introduce subtle bugs.

**Key insights**  
Developers should verify that their hardware and model configurations meet the backend's requirements before enabling sparse MLA. The performance benchmarks show clear wins for ≤16 heads but penalties for higher counts, highlighting the importance of dynamic backend selection. The refactored sparse utilities improve maintainability, and the updated benchmarking tools provide better insights into sparse attention behavior across different tensor parallelism levels.

---

## 33. [[Docs] Spec decoding docs warning removal](https://github.com/vllm-project/vllm/pull/34439)


### Base Information

- **PR Number:** #34439
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-12 09:01:51
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34439/files) (1):**
  - `docs/features/spec_decode/README.md`

### Summary

**What changed and why**  
Removed a warning about speculative decoding performance limitations that was likely carried over from v0, while keeping the warning about incompatibility with pipeline parallelism. This suggests the performance concerns may no longer be current or relevant.

**Technical impact**  
This change only affects documentation, not the actual codebase or system behavior. It reduces potential confusion for users by removing outdated performance caveats while maintaining important compatibility warnings.

**Potential risks**  
If the performance limitations still exist in the current version, removing this warning could mislead users about speculative decoding's effectiveness. There's also a risk that users might overlook the remaining pipeline parallelism incompatibility warning.

**Key insights**  
Documentation should accurately reflect current system capabilities—outdated warnings create confusion. Consider verifying whether the performance concerns are truly resolved before removing such warnings. The pipeline parallelism warning remains critical for users implementing distributed setups.

---

## 34. [[BUG] Reset running requests when clearing cache for pause/resume](https://github.com/vllm-project/vllm/pull/34382)


### Base Information

- **PR Number:** #34382
- **Author:** [hao-aaron](https://github.com/hao-aaron)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-12 08:19:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34382/files) (1):**
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
The change modifies the `pause_generation` method to pass `reset_running_requests=True` when calling `reset_prefix_cache`. This addresses a bug where in-progress requests block prefix cache clearing in "keep" mode, ensuring cache reset proceeds correctly during pause/resume operations.

**Technical impact**  
This parameter ensures that running requests are properly handled during cache reset, preventing deadlocks or stalls in cache clearing. The change is minimal but critical for the correct behavior of the pause/resume functionality, particularly in scenarios with active requests.

**Potential risks**  
If `reset_running_requests=True` introduces side effects like prematurely terminating or interfering with ongoing requests, it could lead to request failures or inconsistent states. The impact depends on the implementation of `reset_prefix_cache` and how it manages running requests during reset.

**Key insights**  
Developers should verify that `reset_prefix_cache` with this flag handles running requests gracefully—either by waiting, canceling, or preserving their state appropriately. Testing should include edge cases with multiple concurrent requests to ensure no regression in request handling or cache consistency.

---

## 35. [Add config file for fused MoE for Nemotron (TP4, B200)](https://github.com/vllm-project/vllm/pull/34411)


### Base Information

- **PR Number:** #34411
- **Author:** [danisereb](https://github.com/danisereb)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 06:09:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34411/files) (1):**
  - `vllm/model_executor/layers/fused_moe/configs/E=512,N=672,device_name=NVIDIA_B200.json`

### Summary

**What changed and why**  
A new configuration file (`E=512,N=672,device_name=NVIDIA_B200.json`) was added to tune the fused MoE (Mixture of Experts) kernel for the Nemotron-H model on NVIDIA B200 GPUs with tensor parallelism degree 4. This JSON file provides optimized Triton kernel parameters (like block sizes and staging) for different batch sizes to improve inference performance, as evidenced by benchmark gains of 13–18%.

**Technical impact**  
The configuration enables the fused MoE kernel to use hardware-specific optimizations for the given model and GPU architecture, directly impacting kernel execution efficiency. This change is additive and only activates when the specific model (Nemotron with E=512, N=672) runs on B200 hardware, improving throughput without altering core kernel logic.

**Potential risks**  
The configuration is tightly coupled to the specific model parameters (E=512, N=672) and GPU (B200), so it may not generalize to other setups or future hardware. Incorrect parameters for unsupported batch sizes could degrade performance, and there is a risk of configuration drift if model or kernel versions change without updates.

**Key insights**  
This is a targeted performance optimization that demonstrates significant gains (up to 18%) for a specific workload. Developers should ensure similar configurations are validated for other model/GPU combinations and consider automating parameter tuning to reduce manual effort. The approach highlights the value of hardware-aware kernel tuning in production systems.

---

## 36. [[ROCm] Enable MXFP4 MoE weight pre-shuffling on gfx950 and update aiter](https://github.com/vllm-project/vllm/pull/34192)


### Base Information

- **PR Number:** #34192
- **Author:** [dllehr-amd](https://github.com/dllehr-amd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-12 05:06:34
- **Type:** `None`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34192/files) (2):**
  - `docker/Dockerfile.rocm_base`
  - `vllm/model_executor/layers/quantization/quark/quark_moe.py`

### Summary



---

## 37. [[V0 Deprecation] Remove code related to per-request logits processors](https://github.com/vllm-project/vllm/pull/34400)


### Base Information

- **PR Number:** #34400
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-12 04:44:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34400/files) (12):**
  - `tests/entrypoints/openai/test_chat_error.py`
  - `tests/entrypoints/openai/test_completion_error.py`
  - `tests/entrypoints/openai/test_lora_resolvers.py`
  - `tests/entrypoints/openai/test_serving_chat.py`
  - `tests/v1/sample/test_sampling_params_e2e.py`
  - `vllm/config/model.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/protocol.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/sampling_params.py`

### Summary

**What changed and why**  
Removed per-request logits processor functionality as part of V1 deprecation. This includes removing the `logits_processor_pattern` configuration field, removing `logits_processors` from request protocols, and eliminating per-request logits processor validation and processing logic.

**Technical impact**  
The changes simplify the API by removing dynamic logits processor injection at request time. Logits processors can now only be configured at model initialization via `ModelConfig.logits_processors`. The validation logic has been moved from request processing to `SamplingParams._validate_logits_processors()` to ensure compatibility with model-level configuration.

**Potential risks**  
Existing code using per-request logits processors will break with a clear error message. The validation now occurs later in the pipeline (during `SamplingParams.verify()`), which could affect error handling flow. There may be edge cases where previously valid configurations now fail due to the stricter model-level-only approach.

**Key insights**  
This is a breaking change that aligns with V1 API simplification goals. Developers must migrate from per-request logits processors to model-level configuration. The error messaging should clearly guide users to update their configurations. Consider documenting this change prominently in migration guides.

---

## 38. [Fix Mistral config remap to accept compressed-tensors quantization #34028](https://github.com/vllm-project/vllm/pull/34104)


### Base Information

- **PR Number:** #34104
- **Author:** [baonudesifeizhai](https://github.com/baonudesifeizhai)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-12 00:22:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34104/files) (1):**
  - `vllm/transformers_utils/configs/mistral.py`

### Summary

**What changed and why**  
Added support for `compressed-tensors` quantization in Mistral model configurations by normalizing the `quant_method` key and passing through the quantization config. This fixes an issue where models using compressed-tensors quantization (like the referenced FP8 model) would fail to load due to unrecognized quantization settings.

**Technical impact**  
The change extends the quantization argument remapping logic to accept `compressed-tensors` as a valid quantization method, ensuring compatibility with models quantized using the Compressed-Tensors library. This allows vLLM to properly initialize and serve such models without raising a `ValueError`.

**Potential risks**  
If the `quantization` dict lacks expected keys or has malformed structure, it could lead to runtime errors during model loading. Additionally, the normalization logic assumes `compressed-tensors` is the only variant spelling; other aliases (e.g., `compressed_tensors`) might not be handled.

**Key insights**  
This is a targeted fix that enhances model compatibility with minimal code changes. Developers should ensure any future quantization method additions follow similar normalization patterns and include comprehensive testing with actual quantized models to validate end-to-end functionality.

---

## 39. [Vllm CPU benchmark suite improvement](https://github.com/vllm-project/vllm/pull/34128)


### Base Information

- **PR Number:** #34128
- **Author:** [louie-tsai](https://github.com/louie-tsai)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-12 00:04:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34128/files) (6):**
  - `.buildkite/performance-benchmarks/scripts/compare-json-results.py`
  - `.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh`
  - `.buildkite/performance-benchmarks/tests/serving-tests-cpu-embed.json`
  - `.buildkite/performance-benchmarks/tests/serving-tests-cpu-text.json`
  - `.buildkite/performance-benchmarks/tests/serving-tests-cpu.json`
  - `docs/getting_started/installation/cpu.md`

### Summary

**What changed and why**  
This PR enhances the vLLM CPU benchmark suite with three main improvements: 1) Added Excel export capability alongside HTML reports for better post-processing of performance data, 2) Reorganized test configurations by splitting the monolithic CPU test file into specialized JSON files for text, multimodal, and embedding models to keep default tests shorter, and 3) Introduced a dry-run mode that outputs execution commands without running benchmarks, addressing user requests for configuration details.

**Technical impact**  
The changes significantly improve benchmark data accessibility through structured Excel exports and modular test organization. The new dry-run capability with filtering options (MODEL_FILTER, DTYPE_FILTER) enables users to extract specific configuration commands efficiently. The architecture now supports separate test suites for different model types while maintaining backward compatibility through environment variable selection.

**Potential risks**  
The Excel sheet name sanitization function (_sanitize_sheet_name) could potentially truncate important identifying information due to the 31-character limit. The dry-run mode's filtering logic depends on jq processing which might fail with malformed JSON. Splitting test configurations across multiple files could lead to maintenance overhead if defaults diverge between files.

**Key insights**  
Developers should leverage the new Excel export for automated performance data analysis and use the dry-run mode to quickly generate deployment commands. The modular test organization establishes a scalable pattern for adding new model categories. When modifying test configurations, ensure consistency across the three specialized JSON files to maintain benchmark reliability.

---

