# vLLM Merged PR Report

**Report Date:** 2026-02-18 PST

**Total Merged PRs:** 29

---

## 1. [[ROCm][CI] Removing all blocking labels from MI355 until stable infra](https://github.com/vllm-project/vllm/pull/34879)


### Base Information

- **PR Number:** #34879
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-18 23:53:09
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34879/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
Removed the `grade: Blocking` label from all MI355 test steps and deleted a dead "Custom Models Test" configuration. This change prevents infrastructure issues on the MI355 platform from causing the AMD CI pipeline to fail, as the infrastructure is currently under testing and not yet stable.

**Technical impact**  
The CI pipeline will no longer block merges due to failures in MI355 tests, allowing development to proceed even if the new hardware encounters issues. The removal of the dead test configuration cleans up the pipeline definition, reducing maintenance overhead.

**Potential risks**  
Test failures on MI355 hardware may go unnoticed, potentially allowing regressions specific to that platform to slip into the codebase. There is also a risk that the `blocking` status might not be restored once the infrastructure stabilizes, leading to permanently reduced test coverage enforcement.

**Key insights**  
Developers should monitor MI355 test results separately during this transitional period and plan to re-enable blocking status once the infrastructure is stable. Consider implementing a temporary reporting mechanism for MI355 failures to maintain visibility without blocking the pipeline.

---

## 2. [[Voxtral Realtime] Fix engine crash on empty multimodal embeddings](https://github.com/vllm-project/vllm/pull/34862)


### Base Information

- **PR Number:** #34862
- **Author:** [talnirnx](https://github.com/talnirnx)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 23:21:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34862/files) (2):**
  - `tests/entrypoints/openai/test_realtime_validation.py`
  - `vllm/model_executor/models/voxtral_realtime.py`

### Summary

**What changed and why**  
The fix replaces fatal assertions in `VoxtralRealtimeGeneration.embed_input_ids()` and `embed_multimodal()` with graceful fallbacks that return zero or empty embeddings when multimodal embeddings are missing. This prevents engine crashes triggered by empty audio commits, encoder cache misses under GPU memory pressure, or client disconnects mid-session.

**Technical impact**  
The change introduces error isolation at the request level: affected requests produce degraded (text-only) output for that decode step, while all other in-flight requests continue unaffected. The engine process remains alive, maintaining service for connected WebSocket clients.

**Potential risks**  
Zero embeddings may lead to subtle quality degradation in audio features for the affected step, though this is preferable to a full engine crash. The warning logs could become noisy under high-frequency edge-case scenarios, potentially obscuring other issues.

**Key insights**  
This fix correctly prioritizes system stability over strict input validation. Developers should monitor warning logs to identify recurring edge cases and consider adding metrics to track the frequency of fallback activations. The comprehensive test validates both crash prevention and continued normal operation.

---

## 3. [[Bug] Fix DeepSeek V3 weight loading caused by incorrect prefix](https://github.com/vllm-project/vllm/pull/34876)


### Base Information

- **PR Number:** #34876
- **Author:** [wzhao18](https://github.com/wzhao18)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 23:20:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34876/files) (1):**
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
The PR fixes a weight loading issue for DeepSeek V3 models by correcting the prefix parameter in the `DeepSeekV2FusedQkvAProj` class. The prefix was incorrectly set to `f"{prefix}.kv_a_proj_with_mqa"` but should simply be the passed `prefix` value to ensure proper weight mapping.

**Technical impact**  
This change ensures that when loading DeepSeek V3 model weights, the fused QKV A projection layer correctly identifies its parameters in the checkpoint file. Without this fix, weight loading would fail or produce incorrect model behavior due to mismatched parameter names.

**Potential risks**  
If there are other layers with similar prefix issues in the codebase, they might still cause weight loading problems. The change could potentially affect backward compatibility if any existing code or checkpoints rely on the old prefix structure.

**Key insights**  
Always verify parameter name mappings when implementing fused layers or complex model architectures. The fix is minimal and targeted, demonstrating the importance of precise string handling in weight loading logic. Developers should ensure similar prefix patterns are checked across other model implementations.

---

## 4. [[Bugfix] Add Quant Config to Llava Next Projector](https://github.com/vllm-project/vllm/pull/34847)


### Base Information

- **PR Number:** #34847
- **Author:** [alex-jw-brooks](https://github.com/alex-jw-brooks)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 23:18:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34847/files) (1):**
  - `vllm/model_executor/models/llava_next.py`

### Summary

**What changed and why**  
The fix adds the `quant_config` parameter and `prefix` argument when initializing the Llava Next multimodal projector. This addresses a crash when loading quantized projectors by ensuring the projector receives the quantization configuration needed for proper initialization.

**Technical impact**  
These changes enable quantized Llava Next models to load successfully by passing the quantization settings through to the projector component. Without this, the projector would lack necessary configuration details, causing initialization failures with quantized weights.

**Potential risks**  
If the `quant_config` is `None` (non-quantized models), the projector must handle this gracefully. There's also a risk that other projector initialization parameters might be missing for different quantization schemes beyond W4A16.

**Key insights**  
Always verify that all subcomponents receive necessary configuration parameters, especially when adding quantization support. Consider adding a test case for `quant_config=None` to ensure backward compatibility with non-quantized models remains intact.

---

## 5. [fix(docs): fix typos in comments and docstrings](https://github.com/vllm-project/vllm/pull/34836)


### Base Information

- **PR Number:** #34836
- **Author:** [machov](https://github.com/machov)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 23:17:41
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34836/files) (5):**
  - `vllm/compilation/backends.py`
  - `vllm/model_executor/layers/fused_moe/oracle/fp8.py`
  - `vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`
  - `vllm/model_executor/models/gpt_oss.py`
  - `vllm/utils/torch_utils.py`

### Summary

**What changed and why**  
This PR fixes six spelling errors in comments and docstrings across five files. The changes correct common misspellings like "seperated" to "separated" and "auxilary" to "auxiliary" to improve code documentation clarity and professionalism.

**Technical impact**  
These changes have no functional impact on the codebase, as they only affect non-executable documentation. The corrections enhance readability and maintain consistency in internal documentation, which aids developer comprehension and onboarding.

**Potential risks**  
The risk is minimal since no runtime code is modified. However, there is a slight chance that automated tools or scripts parsing docstrings could be affected if they rely on exact string matching of the incorrect spellings, though this is unlikely.

**Key insights**  
Maintaining accurate documentation is crucial for long-term code maintainability. While these fixes are minor, they demonstrate attention to detail. Developers should continue to prioritize documentation quality and consider using spell-checking tools in their workflow to catch similar issues early.

---

## 6. [[Frontend] Fix reasoning_tokens for text-based parsers in Responses API](https://github.com/vllm-project/vllm/pull/33513)


### Base Information

- **PR Number:** #33513
- **Author:** [anencore94](https://github.com/anencore94)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 23:16:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33513/files) (7):**
  - `tests/entrypoints/openai/responses/test_simple.py`
  - `tests/entrypoints/openai/test_serving_responses.py`
  - `tests/reasoning/test_base_thinking_reasoning_parser.py`
  - `vllm/entrypoints/openai/responses/context.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/reasoning/abs_reasoning_parsers.py`
  - `vllm/reasoning/basic_parsers.py`

### Summary

**What changed and why**  
This PR fixes missing `reasoning_tokens` counting for text-based reasoning models (like Qwen3/DeepSeek R1) when using the `/v1/responses` API with Simple/Parsable contexts. It adds a `count_reasoning_tokens` hook to reasoning parsers, implements span counting for `<think>...</think>` style models, accumulates output token IDs in contexts, and backfills reasoning tokens during response serving when Harmony channel tracking isn't available.

**Technical impact**  
The changes extend the reasoning parser interface with token counting capability, enabling accurate reasoning token tracking for text-based models in non-Harmony contexts. This maintains backward compatibility (default returns 0) while fixing a critical gap where reasoning tokens remained zero for models using tag-based reasoning formats. The architecture now supports dual counting mechanisms: Harmony channels for chat paths and parser-based counting for Simple/Parsable contexts.

**Potential risks**  
The depth-based counting algorithm could miscount if token sequences contain malformed nesting or unbalanced markers. The accumulation of token IDs in contexts adds memory overhead for long conversations. There's a risk of double-counting if both Harmony and parser-based mechanisms become active simultaneously, though the code guards against this. Edge cases with streaming responses and partial thinking spans need careful validation.

**Key insights**  
Developers should ensure new reasoning parser implementations override `count_reasoning_tokens` for accurate tracking. The fix is complementary to existing Harmony-based counting (#31094), targeting specifically the text-based parser path. Testing shows both streaming and non-streaming responses now correctly report reasoning tokens. Memory-conscious implementations should monitor `_accumulated_token_ids` growth in long-running sessions.

---

## 7. [Deprecate test-pipeline.yaml](https://github.com/vllm-project/vllm/pull/34864)


### Base Information

- **PR Number:** #34864
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-18 18:15:28
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34864/files) (1):**
  - `.buildkite/test-pipeline.yaml`

### Summary

**What changed and why**  
The test-pipeline.yaml file has been deprecated and replaced with a modular pipeline structure. The file content has been reduced from 1524 lines to just 8 lines, with all test configurations migrated to separate directories: test_areas for test jobs, image_build for image building jobs, hardware_tests for specialized hardware tests, and ci_config.yaml for pipeline configuration.

**Technical impact**  
This change transitions from a monolithic pipeline configuration to a modular architecture, making the CI/CD system more maintainable and scalable. The new structure allows for better organization of different test types and hardware configurations, and enables the new pipeline generator that has been running successfully for over two weeks.

**Potential risks**  
The main risk is that some branches may not have been updated/rebased and could still reference the deprecated file. There's also a risk that the migration may have missed some edge cases or test configurations that were present in the original file but not properly migrated to the new structure.

**Key insights**  
This is a significant architectural improvement that follows best practices for CI/CD pipeline management. Developers should ensure their branches are updated to use the new pipeline structure. The deprecation notice clearly indicates where different types of configurations have been migrated, making it easy to locate specific test configurations in the new modular structure.

---

## 8. [[Model Runner V2] Use FP32 for Gumbel Noise](https://github.com/vllm-project/vllm/pull/34854)


### Base Information

- **PR Number:** #34854
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-18 17:07:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34854/files) (1):**
  - `vllm/v1/worker/gpu/sample/gumbel.py`

### Summary

**What changed and why**  
The PR modifies the Gumbel sampling kernel to use FP32 precision for noise generation instead of FP64, and consolidates separate `tl.max` and `tl.argmax` operations into a single `tl.max(..., return_indices=True)` call. These optimizations aim to improve performance, particularly for large batch sizes, with reported gains up to 50%.

**Technical impact**  
Using FP32 for Gumbel noise generation reduces computational overhead and memory bandwidth compared to FP64, which is beneficial for GPU kernels. Merging the max and argmax operations eliminates redundant passes over the logits data, improving kernel efficiency. The function signature documentation was also updated to clarify tensor dimensions.

**Potential risks**  
The change from FP64 to FP32 may slightly reduce numerical precision in the Gumbel noise, though the addition of `tl.maximum(u, 1e-7)` helps maintain stability. The performance improvement claims should be validated across diverse hardware and workload configurations to ensure consistent gains.

**Key insights**  
This is a well-considered optimization that leverages Triton's capabilities effectively. Developers should verify that the reduced precision doesn't affect sampling quality in sensitive applications. The pattern of combining related operations (max/argmax) is a good optimization technique for GPU kernels that can be applied elsewhere.

---

## 9. [[Model Runner V2] Remove unnecessary copies in PW CUDA graph capture](https://github.com/vllm-project/vllm/pull/34849)


### Base Information

- **PR Number:** #34849
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-18 15:52:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34849/files) (1):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`

### Summary

**What changed and why**  
The code removes the assignment of model output to `self.hidden_states` during CUDA graph capture for piecewise (PW) graphs. This is because PW CUDA graphs internally manage the output hidden state buffer, making external handling unnecessary.

**Technical impact**  
This change simplifies the graph capture logic by eliminating a redundant copy operation, which reduces memory bandwidth usage and potentially improves capture performance. The model output is now directly managed within the CUDA graph's internal memory management.

**Potential risks**  
If the assumption that PW CUDA graphs fully manage the hidden state buffer is incorrect, it could lead to missing output data or runtime errors. The removal of the assertion `assert self.hidden_states is not None` also eliminates a safety check that could have caught initialization issues.

**Key insights**  
This optimization aligns with the intended design of PW CUDA graphs and removes unnecessary overhead. Developers should verify that the internal buffer management is robust across all use cases and consider adding logging or validation to ensure the hidden states are properly accessible after graph execution.

---

## 10. [[CI][AMD][BugFix] Use torch.testing.assert_close instead of assert torch.allclose in test_rocm_skinny_gemms.py](https://github.com/vllm-project/vllm/pull/34181)


### Base Information

- **PR Number:** #34181
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-18 15:10:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34181/files) (1):**
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`

### Summary

**What changed and why**  
This PR replaces `assert torch.allclose` with `torch.testing.assert_close` in the AMD ROCm quantization kernel test file. The change improves test failure diagnostics by providing detailed mismatch information when tensor comparisons fail, making it easier to debug numerical discrepancies.

**Technical impact**  
The switch enhances test debuggability without altering test logic or tolerance criteria. Most assertions retain their original `atol`/`rtol` values, though a few now explicitly set `atol=1e-8` where only `rtol` was previously specified, slightly tightening absolute tolerance in those cases.

**Potential risks**  
Introducing an explicit `atol=1e-8` in three tests could make them more sensitive to very small absolute differences that were previously ignored. This might cause sporadic failures in edge cases where results differ minutely but within the original implicit absolute tolerance.

**Key insights**  
Use `torch.testing.assert_close` for better failure messages in PyTorch tests. Review the added `atol=1e-8` values to ensure they align with the numerical precision expectations of the tested kernels, particularly for low-precision operations like FP8 or quantized computations.

---

## 11. [[Model Runner V2] support piecewise & mixed cudagraph](https://github.com/vllm-project/vllm/pull/32771)


### Base Information

- **PR Number:** #32771
- **Author:** [izhuhaoran](https://github.com/izhuhaoran)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-18 15:03:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32771/files) (5):**
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/dp_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle.py`
  - `vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py`

### Summary

**What changed and why**  
This PR adds support for piecewise and mixed CUDA graph modes in Model Runner V2. It extends the existing CUDA graph infrastructure to handle different graph capture strategies (FULL, PIECEWISE, MIXED) and separates uniform decode graph capture from prefill. The changes enable more flexible graph usage across different batch types and speculative decoding scenarios.

**Technical impact**  
The architecture now supports three distinct CUDA graph modes: FULL (explicit graph replay), PIECEWISE (torch.compile auto-capture), and MIXED (combination). The system separates graph capture for uniform decode batches and introduces synchronized graph mode selection across data parallel ranks. This affects graph capture logic, execution path selection, and distributed coordination.

**Potential risks**  
- Synchronization logic across DP ranks could fail if modes mismatch, leading to inconsistent execution paths.  
- Piecewise graph mode relies on torch.compile internals, which may have version compatibility issues.  
- The increased complexity in graph management could introduce subtle bugs in edge cases like zero-token batches or mixed batch types.

**Key insights**  
- Developers must ensure CUDA graph mode configurations align across components (main model, speculative decoder).  
- The `BatchDescriptor` is now essential for piecewise graph dispatch and must be properly initialized.  
- When modifying graph capture logic, verify both single-rank and multi-rank DP scenarios work correctly.

---

## 12. [[MoE Refactor] Convert mxfp4 marlin into modular kernel format](https://github.com/vllm-project/vllm/pull/34588)


### Base Information

- **PR Number:** #34588
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 14:37:15
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34588/files) (2):**
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`
  - `vllm/model_executor/layers/quantization/mxfp4.py`

### Summary

**What changed and why**  
This PR refactors the MXFP4 Marlin implementation to use the modular kernel format. It replaces the direct `fused_marlin_moe` function call with a `FusedMoEModularKernel` instance that encapsulates the Marlin experts and configuration. The changes ensure consistency with the modular kernel architecture used elsewhere in the codebase.

**Technical impact**  
The refactor centralizes the Marlin MXFP4 MoE execution path within the modular kernel framework, improving code maintainability and alignment with the existing fused MoE infrastructure. It introduces a `moe_mk` attribute in `Mxfp4MoEMethod` to hold the kernel instance, which is initialized during `process_weights_after_loading`. This change also removes the `marlin_input_dtype` instance variable, instead calling `get_marlin_input_dtype()` directly where needed.

**Potential risks**  
The initialization of `moe_mk` depends on `prepare_moe_fp4_layer_for_marlin` and `maybe_make_prepare_finalize` succeeding; if either fails or returns `None`, the kernel won't be created, leading to runtime failures. The `assert self.moe_mk is not None` in the `apply` method could cause crashes if the kernel isn't properly set up. Additionally, the removal of explicit `input_dtype` passing in some places relies on the global `get_marlin_input_dtype()` function, which may introduce subtle issues if the dtype context changes.

**Key insights**  
This refactor is a positive step toward unifying MoE kernel interfaces, but developers must ensure that `process_weights_after_loading` is always called before `apply` to avoid `None` kernel errors. The use of `assert` statements for critical invariants should be reviewed; consider replacing them with proper error handling or validation in production code. Verify that `get_marlin_input_dtype()` returns the correct dtype in all execution contexts to maintain numerical accuracy.

---

## 13. [Fix empty tool_call_id in Anthropic messages API tool result conversion](https://github.com/vllm-project/vllm/pull/34745)


### Base Information

- **PR Number:** #34745
- **Author:** [sfeng33](https://github.com/sfeng33)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 14:31:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34745/files) (2):**
  - `vllm/entrypoints/anthropic/protocol.py`
  - `vllm/entrypoints/anthropic/serving.py`

### Summary

**What changed and why**  
Added `tool_use_id` field to `AnthropicContentBlock` in `protocol.py` and updated `serving.py` to use this field instead of `id` when converting tool result blocks. This fixes a mismatch where the Anthropic API uses `tool_use_id` to reference tool calls, but the conversion was incorrectly using `id`, causing empty tool call IDs and request failures.

**Technical impact**  
The change ensures proper mapping between Anthropic and OpenAI message formats for tool calls, allowing multi-turn conversations with tools to work correctly. This maintains compatibility with Anthropic's API specification while preserving correct behavior in downstream tokenizers and model inference.

**Potential risks**  
If any existing code relies on `block.id` being populated for tool result blocks, it may break. Additionally, the fallback to empty string (`""`) could still occur if `tool_use_id` is missing, though this aligns with the original behavior. Edge cases where `tool_use_id` is present but invalid should be handled by upstream validation.

**Key insights**  
Always validate API field mappings when integrating third-party formats—Pydantic's silent discarding of undefined fields can hide critical bugs. Consider adding validation or logging for missing required fields in conversion logic. This fix is minimal and targeted, but reviewing other Anthropic message conversions for similar issues is recommended.

---

## 14. [[BUG] Fixing Weight Sync unit test](https://github.com/vllm-project/vllm/pull/34841)


### Base Information

- **PR Number:** #34841
- **Author:** [hao-aaron](https://github.com/hao-aaron)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-18 14:20:10
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34841/files) (1):**
  - `tests/entrypoints/weight_transfer/test_weight_transfer_llm.py`

### Summary

**What changed and why**  
Added `VLLM_ENABLE_V1_MULTIPROCESSING = "0"` environment variable to three unit tests to force in-process execution. This ensures `mock.patch` works correctly, as the default spawn multiprocessing mode does not inherit mocks.

**Technical impact**  
The change modifies test execution behavior by disabling v1 multiprocessing, allowing mocked objects to persist across processes. This resolves test failures where mocks were lost due to process spawning, ensuring the tests validate the intended functionality.

**Potential risks**  
If other tests rely on spawn-based multiprocessing, this environment variable could inadvertently affect them if not isolated. Additionally, the fix assumes in-process execution is safe for these tests, which may hide concurrency or isolation issues that could surface in production.

**Key insights**  
The fix is targeted and necessary for mock inheritance, but consider whether the tests should be refactored to avoid environment variable manipulation. Ensure that `VLLM_ENABLE_V1_MULTIPROCESSING` is only set for the specific test scope to prevent side effects on other tests.

---

## 15. [[Docs] Clean up speculators docs](https://github.com/vllm-project/vllm/pull/34065)


### Base Information

- **PR Number:** #34065
- **Author:** [kylesayrs](https://github.com/kylesayrs)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 13:48:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34065/files) (15):**
  - `docs/assets/features/speculative_decoding/speculators-user-flow-dark.svg`
  - `docs/assets/features/speculative_decoding/speculators-user-flow-light.svg`
  - `docs/features/README.md`
  - `docs/features/spec_decode/README.md`
  - `docs/features/speculative_decoding/README.md`
  - `docs/features/speculative_decoding/draft_model.md`
  - `docs/features/speculative_decoding/eagle.md`
  - `docs/features/speculative_decoding/mlp.md`
  - `docs/features/speculative_decoding/n_gram.md`
  - `docs/features/speculative_decoding/speculators.md`
  - `docs/features/speculative_decoding/suffix.md`
  - `mkdocs.yaml`
  - `requirements/docs.txt`
  - `tests/v1/test_oracle.py`
  - `vllm/config/speculative.py`

### Summary

**What changed and why**  
This PR reorganizes and updates the speculative decoding documentation to reflect current APIs, remove outdated warnings, and improve clarity. The main changes include breaking out speculation methods into separate pages, updating examples with correct method specifications, and renaming the folder from `spec_decode` to `speculative_decoding` for better naming consistency.

**Technical impact**  
The changes improve documentation structure and accuracy, ensuring users have up-to-date examples and clear guidance. The folder rename is handled with redirects to maintain backward compatibility for existing links. Code changes include better inference logic for speculation methods when not explicitly specified, enhancing usability.

**Potential risks**  
The redirects rely on the `mkdocs-redirects` plugin, which must be correctly configured to avoid broken links. Updated examples assume users have compatible model versions, and the removal of certain warnings (e.g., TP incompatibility for Eagle) could lead to confusion if underlying issues resurface. The added inference logic might incorrectly default to `draft_model` in edge cases.

**Key insights**  
Developers should verify that all redirects work as intended and that the new documentation structure is intuitive. The improved method inference logic simplifies configuration but requires careful testing to ensure it doesn’t misclassify user inputs. Keeping examples updated with current model names and APIs is crucial for user adoption.

---

## 16. [[Bugfix] Fix lora tests](https://github.com/vllm-project/vllm/pull/34834)


### Base Information

- **PR Number:** #34834
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 13:22:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34834/files) (3):**
  - `tests/lora/test_default_mm_loras.py`
  - `tests/lora/test_qwenvl.py`
  - `vllm/entrypoints/llm.py`

### Summary

**What changed and why**  
This PR fixes a regression where `_resolve_lora_reqs` wasn't being called, breaking LoRA functionality for multimodal models. The fix replaces the unused `_resolve_lora_reqs` method with a simpler `_resolve_mm_lora` function that's directly called during request processing. Test files were updated to fix broken assertions and improve test clarity.

**Technical impact**  
The changes restore proper LoRA resolution for multimodal prompts by integrating the resolution logic directly into the request rendering pipeline. This ensures default modality-specific LoRAs are correctly applied when processing multimodal inputs. The test updates reflect corrected behavior expectations, particularly for beam search outputs which now properly validate suffix matching.

**Potential risks**  
The simplified resolution approach loses batch processing optimization present in the original `_resolve_lora_reqs` method. There's a potential performance impact when processing many multimodal prompts simultaneously. The warning logic for multiple modality intersections remains unchanged but could benefit from clearer user guidance.

**Key insights**  
The fix demonstrates the importance of actually calling critical resolution functions. Developers should verify that helper methods are properly integrated into main execution flows. The test changes show improved validation patterns - using `endswith()` for beam search outputs is more robust than exact matching. Consider adding metrics to track multimodal LoRA usage patterns.

---

## 17. [[Bugfix][MoE Kernel] Fix incorrect routing selection for models without expert groups (e.g., MiniMax-M2.1)](https://github.com/vllm-project/vllm/pull/34673)


### Base Information

- **PR Number:** #34673
- **Author:** [wwl2755](https://github.com/wwl2755)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 13:03:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34673/files) (5):**
  - `tests/kernels/moe/test_flashinfer.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py`
  - `vllm/model_executor/layers/fused_moe/router/fused_topk_router.py`
  - `vllm/model_executor/layers/fused_moe/router/grouped_topk_router.py`

### Summary

**What changed and why**  
This PR fixes a bug where `get_routing_method_type()` incorrectly assigned `DeepSeekV3` routing to any sigmoid-scored model with `top_k > 1`, even when the model lacks expert groups (`num_expert_group=None`). The issue caused crashes in the flashinfer TRTLLM kernel for models like MiniMax-M2.1. The fix updates the routing logic to check for `num_expert_group` and `has_e_score_bias`, ensuring `RoutingMethodType.Unspecified` is used when expert groups are absent.

**Technical impact**  
The changes refine routing selection by incorporating `num_expert_group` and `has_e_score_bias` as explicit parameters. This prevents incompatible kernel selections and ensures models without grouped expert routing (e.g., MiniMax-M2.1) use appropriate kernels. The logic is now centralized in `get_routing_method_type()`, simplifying maintenance and improving consistency across router implementations.

**Potential risks**  
If `num_expert_group` is incorrectly passed as `0` instead of `None`, it could still trigger `DeepSeekV3` routing unintentionally. Additionally, the removal of the dedicated test (`test_flashinfer_blockscale_fp8_none_expert_group`) reduces regression coverage for edge cases involving `num_expert_group=None`. Future changes to routing logic must carefully validate all conditional branches.

**Key insights**  
Always validate routing parameters (`num_expert_group`, `has_e_score_bias`) when integrating new MoE models. The centralized routing function improves clarity, but developers should ensure all router subclasses pass the correct parameters. Consider restoring or adapting the removed test to maintain coverage for `num_expert_group=None` scenarios.

---

## 18. [[CI][AMD][BugFix] Skip tests in test_unquantized_backend_selection that should not run on ROCm](https://github.com/vllm-project/vllm/pull/34655)


### Base Information

- **PR Number:** #34655
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-02-18 12:00:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34655/files) (1):**
  - `tests/kernels/moe/test_unquantized_backend_selection.py`

### Summary

**What changed and why**  
Two test functions (`test_select_cuda_flashinfer_trtllm_backend` and `test_select_cuda_flashinfer_cutlass_backend`) now include a `@pytest.mark.skipif` decorator that skips execution on non-NVIDIA platforms (i.e., when `current_platform.is_cuda()` returns `False`). This change ensures these tests are only run on CUDA-compatible hardware, preventing failures on ROCm (AMD) or other unsupported platforms.

**Technical impact**  
The modification restricts test execution to NVIDIA CUDA environments, aligning test coverage with platform-specific backend capabilities. It maintains existing test logic but adds a conditional skip, which reduces false negatives in CI pipelines for heterogeneous hardware setups. No functional changes are made to the underlying backend selection logic.

**Potential risks**  
If `current_platform.is_cuda()` incorrectly identifies the platform (e.g., returns `True` on ROCm), tests may still fail. Additionally, over-skipping tests could mask regressions on CUDA platforms if the skip condition is too broad. The change assumes FlashInfer and TRT-LLM backends are exclusively CUDA-dependent, which should be validated.

**Key insights**  
Platform-specific test skipping is a clean solution for multi-architecture codebases. Developers should ensure `current_platform.is_cuda()` is robust across all environments. Consider adding similar skips for other CUDA-only tests to maintain CI stability, and document platform-specific test assumptions in the codebase.

---

## 19. [[Model Runner V2] Minor simplification for DCP](https://github.com/vllm-project/vllm/pull/34786)


### Base Information

- **PR Number:** #34786
- **Author:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-18 11:04:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34786/files) (5):**
  - `vllm/v1/worker/gpu/attn_utils.py`
  - `vllm/v1/worker/gpu/block_table.py`
  - `vllm/v1/worker/gpu/cp_utils.py`
  - `vllm/v1/worker/gpu/cudagraph_utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`

### Summary

**What changed and why**  
This PR simplifies DCP (Decode Context Parallelism) implementation by refactoring DCP-related logic into a dedicated module (`cp_utils.py`). The changes remove dynamic DCP group detection in favor of precomputed DCP parameters, streamline the slot-mapping kernel, and introduce a Triton kernel for computing local sequence lengths.

**Technical impact**  
The refactoring centralizes DCP logic, improving code organization and maintainability. The slot-mapping kernel now handles the non-DCP case (`CP_SIZE == 1`) as a fast path, potentially improving performance when DCP is disabled. The new Triton kernel for `dcp_local_seq_lens` replaces a Python implementation, which may enhance efficiency but introduces GPU kernel overhead.

**Potential risks**  
The removal of dynamic DCP group detection (`get_dcp_group`) assumes DCP parameters are correctly initialized elsewhere, which could lead to misconfiguration if not properly synchronized. The new Triton kernel introduces additional GPU kernel launches, which may affect latency in small-batch scenarios. Edge cases in the DCP offset calculation (e.g., when `cp_interleave` is not 1) require careful validation.

**Key insights**  
Developers should verify that `dcp_size`, `dcp_rank`, and `cp_interleave` are correctly passed from the parallel configuration. The fast path for `CP_SIZE == 1` in the slot-mapping kernel is a positive optimization. Ensure the new `cp_utils` module is thoroughly tested, especially for varying `cp_interleave` values and DCP group sizes.

---

## 20. [[Bugfix] Remove assert causing hipErrorStreamCaptureUnsupported](https://github.com/vllm-project/vllm/pull/34455)


### Base Information

- **PR Number:** #34455
- **Author:** [JadenMathias](https://github.com/JadenMathias)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-18 10:54:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34455/files) (1):**
  - `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`

### Summary

**What changed and why**  
Removed an assertion that was causing a `hipErrorStreamCaptureUnsupported` error when running certain ROCm-based aiter kernels with CUDA graphs enabled. The assertion was introduced in a previous PR but only surfaced when not using the `--enforce-eager` flag, which bypasses CUDA graph compilation.

**Technical impact**  
This change resolves a runtime failure for ROCm users running models like Mixtral-8x7B-Instruct-v0.1-FP8-KV with aiter kernels and CUDA graphs. It allows the fused MoE layer to proceed without validating the `a2_scale` against the quantization config, which may affect quantization behavior if the scale values diverge.

**Potential risks**  
Removing the assertion could mask mismatches between the provided `a2_scale` and the quantization configuration, potentially leading to incorrect quantization scaling in the fused MoE layer. There is also a risk that other latent issues related to scale consistency may emerge in production scenarios.

**Key insights**  
The fix is minimal and targeted, but developers should verify that quantization scales are correctly aligned elsewhere in the codebase. Consider adding a warning or logging mechanism for scale mismatches in the future, and ensure that the test plan includes runs without `--enforce-eager` to catch similar issues early.

---

## 21. [[Misc] Add mooncake-transfer-engine to kv_connectors requirements](https://github.com/vllm-project/vllm/pull/34826)


### Base Information

- **PR Number:** #34826
- **Author:** [stmatengss](https://github.com/stmatengss)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-18 10:26:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34826/files) (1):**
  - `requirements/kv_connectors.txt`

### Summary

**What changed and why**  
A single dependency (`mooncake-transfer-engine >= 0.3.8`) was added to the `requirements/kv_connectors.txt` file. This change enables direct use of the Mooncake connector in Docker environments by ensuring the required transfer engine is available.

**Technical impact**  
This addition expands the KV connector ecosystem by including a new backend dependency. It allows the system to leverage Mooncake-specific functionality without requiring manual installation, simplifying deployment in containerized setups.

**Potential risks**  
Introducing a new dependency could cause version conflicts with existing packages or increase image size. If the Mooncake engine has its own dependencies, they might not be explicitly listed, potentially leading to missing sub-dependencies in some environments.

**Key insights**  
Ensure compatibility testing between `mooncake-transfer-engine` and other listed packages. Consider documenting any additional configuration needed for the Mooncake connector. Monitor for any downstream impact on build times or deployment workflows due to the new dependency.

---

## 22. [[Bugfix] Redo Qwen3.5/Qwen3-Next GDN projector fusion](https://github.com/vllm-project/vllm/pull/34697)


### Base Information

- **PR Number:** #34697
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 09:46:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34697/files) (3):**
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/models/qwen3_5.py`
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
This PR fixes a bug in Qwen3.5/Qwen3-Next Gated Delta Network (GDN) projector fusion. The root issue was that Qwen3-Next's `qkvz_proj` output sizes were incorrectly defined as `[sum((key_dim, key_dim, value_dim)), value_dim]` instead of the correct `[sum((key_dim, key_dim, value_dim, value_dim))]`. Additionally, it reverts a previous change to restore FP8 support for Qwen3.5.

**Technical impact**  
The changes modify the linear layer weight loading logic to support tuple-based shard IDs and adjust shard offset/size calculations for tensor parallelism. In the model files, the GDN projections are refactored to use merged linear layers with corrected output sizes, simplifying the forward pass and weight loading mappings. This ensures proper tensor partitioning and quantization compatibility.

**Potential risks**  
The introduction of tuple shard IDs in `weight_loader` could break existing code that relies on integer shard IDs, though a guard is added to raise `NotImplementedError`. Changes to shard offset/size division before block quantization adjustments may affect quantized model loading. The refactored GDN forward pass assumes specific tensor splits, which could introduce subtle bugs if dimensions are mismatched.

**Key insights**  
Developers should verify that all GDN-related models use the updated `output_sizes` format. The tuple shard ID support indicates a move towards more flexible weight loading, but existing integrations need updates. Testing with FP8 quantization is critical to ensure the revert doesn't reintroduce prior issues. The consolidated weight mappings improve maintainability but require careful validation of shard indices.

---

## 23. [[Bugfix] Fix NVFP4 TRTLLM MoE non-gated support; add gsm8k for Nemotron-3-Nano FP8+NVFP4](https://github.com/vllm-project/vllm/pull/34725)


### Base Information

- **PR Number:** #34725
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 09:39:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34725/files) (4):**
  - `tests/evals/gsm8k/configs/moe-refactor/Nemotron-Nano-30B-Fp8-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Nemotron-Nano-30B-NvFp4-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/config-b200.txt`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`

### Summary

**What changed and why**  
This PR adds support for NVFP4 TRTLLM MoE models with non-gated activation by fixing a hidden dimension alignment requirement and adds GSM8K evaluation configurations for Nemotron-3-Nano models in FP8 and NVFP4 formats. The changes address an issue with FlashInfer's FP4 MoE kernel compatibility.

**Technical impact**  
The modification to `flashinfer_fp4_moe.py` enforces that the hidden dimension must be divisible by 512 for FlashInfer FP4 MoE kernel compatibility, preventing potential runtime errors. The new YAML configurations enable testing Nemotron-3-Nano models with different quantization formats (FP8 and NVFP4) using specific FlashInfer backends.

**Potential risks**  
The hidden dimension divisibility check may exclude valid model configurations that don't meet this alignment requirement, potentially breaking compatibility with some models. The environment variable configurations (`VLLM_USE_FLASHINFER_MOE_FP4` and `VLLM_USE_FLASHINFER_MOE_FP8`) must be properly set for the tests to run correctly.

**Key insights**  
The PR demonstrates proper validation for hardware-specific kernel requirements and adds comprehensive testing for newly supported model formats. Developers should ensure future MoE model implementations check for similar alignment constraints, and the test configurations provide a template for evaluating different quantization backends.

---

## 24. [[CI][Bugfix] Fix multinode test script](https://github.com/vllm-project/vllm/pull/34820)


### Base Information

- **PR Number:** #34820
- **Author:** [ilmarkov](https://github.com/ilmarkov)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-18 08:45:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34820/files) (1):**
  - `.buildkite/scripts/run-multi-node-test.sh`

### Summary

**What changed and why**  
The change removes quotes around the `$GPU_DEVICES` variable in a `docker run` command within a multi-node test script. This reverts to a previous version after a ShellCheck edit in a prior PR (#34514) caused CI failures, indicating the quoted variable likely interfered with proper argument expansion.

**Technical impact**  
This fix restores the original behavior where `$GPU_DEVICES` expands into separate arguments (e.g., `--gpus all` or device IDs) for Docker. With quotes, the variable was treated as a single string, breaking GPU device assignment and causing container startup to fail in CI.

**Potential risks**  
If `$GPU_DEVICES` contains spaces or special characters, unquoted expansion could lead to word splitting or globbing issues. However, since the variable is expected to hold Docker GPU flags (like `--gpus all`), this is likely safe. The risk of CI regression is now mitigated, but future ShellCheck passes might flag this as a potential shell issue.

**Key insights**  
ShellCheck recommendations (like adding quotes) should be evaluated in context—some variables intentionally require unquoted expansion for multi-word arguments. Developers should test CI changes that modify shell scripts, especially for critical infrastructure like multi-node tests. Consider adding a comment explaining why quotes are omitted here to prevent "fixes" in the future.

---

## 25. [[CI] temporarily disable multi-node tests](https://github.com/vllm-project/vllm/pull/34825)


### Base Information

- **PR Number:** #34825
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-18 08:32:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34825/files) (1):**
  - `.buildkite/test_areas/distributed.yaml`

### Summary

**What changed and why**  
The PR temporarily disables multi-node tests by marking them as optional in the CI configuration. This change is in response to an infrastructure issue where Docker commands fail due to incorrect GPU device flag syntax (`--gpus "device=`), preventing container creation and causing test failures.

**Technical impact**  
This change will cause multi-node distributed tests to be skipped in CI pipelines until the issue is resolved. It prevents CI failures from blocking other development work but reduces test coverage for distributed functionality during this period.

**Potential risks**  
The main risk is that regressions in multi-node functionality could go undetected while tests are disabled. There's also a risk that the temporary change becomes permanent if not tracked properly, as indicated by the TODO comment. The underlying infrastructure issue with Docker GPU configuration needs to be addressed separately.

**Key insights**  
Developers should prioritize fixing the Docker GPU configuration issue to restore full test coverage. The TODO comment provides clear tracking, but a follow-up ticket or issue should be created to ensure the tests are re-enabled. Consider adding a temporary monitoring mechanism for distributed functionality changes during this period.

---

## 26. [[Model Bash] DeepSeek R1 BF16 Min Latency QKV A GEMM (0.5% E2E Speedup)](https://github.com/vllm-project/vllm/pull/34758)


### Base Information

- **PR Number:** #34758
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 07:42:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34758/files) (7):**
  - `CMakeLists.txt`
  - `csrc/dsv3_fused_a_gemm.cu`
  - `csrc/ops.h`
  - `csrc/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/mla.py`
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
This PR adds a specialized BF16 GEMM kernel (`dsv3_fused_a_gemm`) optimized for DeepSeek V2/V3 models' QKV A-projection at small batch sizes (1-16 tokens). The kernel is adapted from TensorRT-LLM via SGLang and aims to reduce latency by using PDL (Pipeline Data Load) to overlap memory operations with computation. While the current E2E speedup is modest (0.5%), this sets up future optimization potential.

**Technical impact**  
The changes introduce a new CUDA kernel that requires SM 9.0+ (Hopper) and specific tensor shapes (mat_a: [num_tokens, 7168], mat_b: [7168, 2112]). The kernel is conditionally enabled via CMake configuration and runtime checks. A new `DeepSeekV2FusedQkvAProj` layer replaces the previous `MergedColumnParallelLinear` for QKV A-projection, providing a fallback to standard GEMM when conditions aren't met. This adds architectural complexity but maintains backward compatibility.

**Potential risks**  
The kernel has strict requirements: BF16 precision, specific weight dimensions (2112x7168), and SM 9.0+ architecture. Runtime shape/dtype mismatches could cause silent fallbacks or errors. The PDL feature depends on an environment variable (`TRTLLM_ENABLE_PDL`), which may lead to inconsistent behavior across deployments. The 0.5% speedup suggests marginal gains for current workloads, potentially not justifying the added code complexity.

**Key insights**  
This is a foundational change enabling future PDL optimizations rather than delivering immediate significant performance gains. Developers should verify GPU architecture compatibility before expecting benefits. The fallback mechanism ensures safety but adds branching logic that should be monitored. Consider whether the complexity is warranted given the current minimal performance improvement, or if this should be treated as experimental until PDL integration proves more substantial benefits.

---

## 27. [Add unit tests for fp8 output fusion of triton_attn](https://github.com/vllm-project/vllm/pull/34228)


### Base Information

- **PR Number:** #34228
- **Author:** [bringlein](https://github.com/bringlein)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-18 03:22:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34228/files) (1):**
  - `tests/kernels/attention/test_triton_unified_attention.py`

### Summary

**What changed and why**  
Added a new unit test `test_triton_unified_attn_fp16_input_fp8_output` to validate fused quantization in the `triton_unified_attention` kernel. The test covers FP16 input with FP8 output using an `output_scale` parameter, addressing debugging needs for issues like #31785. Additionally, expanded `NUM_HEADS` to include `(5, 1)` for broader coverage.

**Technical impact**  
This enhances test coverage for the FP8 output fusion path in the attention kernel, ensuring correctness under various configurations (e.g., sliding windows, soft caps, and sequence lengths). The test leverages existing infrastructure (`ref_paged_attn`) for validation, maintaining consistency with the reference implementation.

**Potential risks**  
The test uses relaxed tolerances (`atol=2e-1, rtol=2e-1`), which may mask subtle numerical inaccuracies in FP8 quantization. Edge cases with extreme sequence lengths (e.g., `[(533, 533)] * 533`) could stress memory or performance. The addition of `(5, 1)` to `NUM_HEADS` may introduce untested interactions with other parameters.

**Key insights**  
Developers should verify that the FP8 quantization logic aligns with kernel expectations, especially for `output_scale` handling. Consider tightening tolerances after kernel stabilization and monitoring performance for large-sequence scenarios. Ensure the test suite runs efficiently across supported hardware (H100/B200).

---

## 28. [[Model Runner V2] Avoid prepare prefill kernel launch overhead](https://github.com/vllm-project/vllm/pull/34780)


### Base Information

- **PR Number:** #34780
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [WoosukKwon](https://github.com/WoosukKwon)
- **Merged time:** 2026-02-18 00:49:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34780/files) (2):**
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu/states.py`

### Summary

**What changed and why**  
The changes optimize kernel launch overhead in decode-only scenarios by conditionally executing the `prepare_prefill_inputs` function. A new `any_prefills` method was added to check if any requests still have pending prefill tokens, avoiding unnecessary kernel launches when all requests are in decode phase.

**Technical impact**  
This reduces computational overhead during pure decoding by skipping prefill preparation entirely. The system now dynamically determines if prefill processing is needed based on request state, improving throughput for decode-heavy workloads without affecting prefill functionality.

**Potential risks**  
The `any_prefills` method relies on NumPy array operations which may introduce CPU-GPU synchronization overhead. Edge cases where `idx_mapping_np` contains invalid indices could cause indexing errors. The condition check adds minimal but non-zero overhead for mixed prefill/decode scenarios.

**Key insights**  
This is a targeted optimization that demonstrates good awareness of performance bottlenecks in inference systems. Developers should verify the NumPy-based check doesn't become a performance issue at scale. Consider adding bounds checking for `idx_mapping_np` indices to prevent potential crashes.

---

## 29. [[Renderer] Deprecate code paths for old input processing](https://github.com/vllm-project/vllm/pull/34775)


### Base Information

- **PR Number:** #34775
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-18 00:35:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34775/files) (6):**
  - `docs/design/plugin_system.md`
  - `vllm/entrypoints/llm.py`
  - `vllm/platforms/interface.py`
  - `vllm/v1/engine/async_llm.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/engine/llm_engine.py`

### Summary

**What changed and why**  
This PR deprecates legacy input processing paths in favor of the Renderer system. It removes `tokenization_kwargs` parameters from multiple method signatures, adds deprecation warnings for passing raw prompts or `EngineCoreRequest` objects directly, and updates the `Platform.validate_request` interface to accept `processed_inputs` instead of raw `prompt`.

**Technical impact**  
The changes centralize input preprocessing within the Renderer component, simplifying the API surface. Engine methods now expect pre-rendered inputs from `Renderer.render_cmpl()` or `Renderer.render_chat()`. The platform interface change (`validate_request`) requires plugin updates, with backward compatibility maintained through runtime detection.

**Potential risks**  
Existing code passing `tokenization_kwargs` or raw prompts will emit deprecation warnings and will break in v0.18. The platform interface change could break custom platforms if they don't update their `validate_request` signature by v0.18. The `truncate_prompt_tokens` deprecation timeline appears inconsistent (v0.16 vs v0.17 warnings).

**Key insights**  
Developers must migrate to using Renderer methods for input preprocessing. Platform plugin maintainers should update `validate_request` signatures promptly. The removal of `tokenization_kwargs` from public APIs simplifies the interface but requires adjusting existing integration code. All deprecations target v0.18 for removal, providing a clear migration timeline.

---

