# vLLM Merged PR Report

**Report Date:** 2026-02-23 PST

**Total Merged PRs:** 34

---

## 1. [[LoRA] Update LoRA expand kernel block_n calculation](https://github.com/vllm-project/vllm/pull/32621)


### Base Information

- **PR Number:** #32621
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 23:17:54
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32621/files) (1):**
  - `vllm/lora/ops/triton_ops/utils.py`

### Summary

**What changed and why**  
The PR simplifies the `block_n` calculation in LoRA's expand kernel configuration. Previously, it used `max(64, next_power_of_2(128 // num_slices))`, which always resulted in 64 when `num_slices > 1`. The new calculation directly sets `block_n` to 64 if `num_slices > 1` else 128, removing unnecessary division and power-of-two operations.

**Technical impact**  
This change maintains identical runtime behavior since the logic is mathematically equivalent, but reduces computational overhead during kernel configuration. It simplifies the code and eliminates redundant operations, potentially improving initialization performance slightly.

**Potential risks**  
The risk is minimal as the change is a direct simplification of an existing invariant. However, developers should ensure that `num_slices` is always a positive integer; edge cases like `num_slices = 0` or negative values could cause unintended behavior if not validated elsewhere.

**Key insights**  
This is a clean optimization that removes unnecessary complexity. Developers should verify that `num_slices` is properly validated in all usage contexts to prevent future issues. The change aligns with good practices by replacing computational logic with a straightforward conditional.

---

## 2. [[Refactor] [1/N] Reorganize kernel abstraction directory](https://github.com/vllm-project/vllm/pull/34055)


### Base Information

- **PR Number:** #34055
- **Author:** [BadrBasowid](https://github.com/BadrBasowid)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-23 22:47:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34055/files) (40):**
  - `tests/compile/passes/test_fusion.py`
  - `tests/compile/passes/test_silu_mul_quant_fusion.py`
  - `tests/kernels/quantization/test_scaled_mm_kernel_selection.py`
  - `tests/utils.py`
  - `vllm/model_executor/kernels/__init__.py`
  - `vllm/model_executor/kernels/linear/__init__.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/MPLinearKernel.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/__init__.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/allspark.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/conch.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/cpu.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/cutlass.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/dynamic_4bit.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/exllama.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/machete.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/marlin.py`
  - `vllm/model_executor/kernels/linear/mixed_precision/xpu.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/ScaledMMLinearKernel.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/__init__.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/aiter.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/cpu.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/cutlass.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/flashinfer.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/pytorch.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/rocm.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/triton.py`
  - `vllm/model_executor/kernels/linear/scaled_mm/xpu.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py`
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py`
  - `vllm/model_executor/layers/quantization/fbgemm_fp8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/ptpc_fp8.py`
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py`
  - `vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py`

### Summary

**What changed and why**  
This PR reorganizes kernel abstraction by moving the `kernels` package from `vllm/model_executor/layers/quantization/` to `vllm/model_executor/`. It introduces a new top-level `kernels` directory with subdirectories for `linear/scaled_mm` and `linear/mixed_precision`, consolidating kernel implementations and providing a stable import interface via updated `__init__.py` files. This is the first step in a broader refactoring effort to improve code organization.

**Technical impact**  
The change centralizes kernel imports under `vllm.model_executor.kernels.linear`, reducing dependency on the quantization layer structure. Existing kernel selection logic and initialization functions are preserved but now exported from the new location. This refactor simplifies future reorganization of kernels by provider (e.g., cutlass, flashinfer) rather than by precision type.

**Potential risks**  
Import paths have been updated across many files, but any missed imports or external dependencies could cause runtime errors. The PR includes extensive test file modifications, but thorough testing is required to ensure all kernel selections and quantization methods work correctly. The refactor is marked as `[1/N]`, indicating subsequent changes may further alter the structure.

**Key insights**  
Developers should now import kernel-related classes from `vllm.model_executor.kernels.linear` instead of the old quantization paths. The new `__init__.py` files provide a stable interface, so future internal reorganizations should not require widespread import updates. Ensure all team members are aware of the new import patterns to avoid broken code.

---

## 3. [[CI/Build] Remove redundant OpenTelemetry pip install from CI configs](https://github.com/vllm-project/vllm/pull/35032)


### Base Information

- **PR Number:** #35032
- **Author:** [vladmihailescu](https://github.com/vladmihailescu)
- **Merged By:** [houseroad](https://github.com/houseroad)
- **Merged time:** 2026-02-23 22:24:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35032/files) (2):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/misc.yaml`

### Summary

**What changed and why**  
Removed explicit `pip install` commands for OpenTelemetry packages from two CI configuration files. This cleanup follows PR #34466 which bundled these dependencies directly with vLLM, eliminating the need for manual installation in test environments.

**Technical impact**  
These changes simplify CI configuration by removing redundant dependency management steps. The OpenTelemetry packages are now consistently provided through vLLM's standard installation, ensuring version compatibility and reducing potential conflicts between explicit and implicit dependencies.

**Potential risks**  
If the bundled OpenTelemetry versions in vLLM differ from those previously specified in CI configs, some tests might exhibit different behavior. There's also a risk that tests relying on specific OpenTelemetry features or versions could fail if the bundled packages don't meet those requirements.

**Key insights**  
This is a straightforward cleanup that improves maintainability by centralizing dependency management. Developers should verify that all OpenTelemetry-related tests pass in CI and ensure any future OpenTelemetry version updates are handled through vLLM's dependency management rather than CI configs.

---

## 4. [[MM] Allow audio chunking for offline LLM](https://github.com/vllm-project/vllm/pull/34628)


### Base Information

- **PR Number:** #34628
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 21:04:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34628/files) (5):**
  - `docs/features/multimodal_inputs.md`
  - `tests/multimodal/test_audio.py`
  - `vllm/config/speech_to_text.py`
  - `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`
  - `vllm/multimodal/audio.py`

### Summary

**What changed and why**  
This PR exposes the `split_audio` utility function from the multimodal audio module, enabling offline chunking of long audio files at low-energy regions. The change is part of the Whisper roadmap to support transcription of audio longer than the model's maximum input length (typically 30 seconds). Previously, chunking logic was internal to the speech-to-text endpoint; now it's a reusable utility with comprehensive documentation and tests.

**Technical impact**  
The refactoring moves audio-splitting logic from the speech-to-text endpoint (`speech_to_text.py`) into a shared utility (`audio.py`), promoting code reuse and separation of concerns. The `allow_audio_chunking` property in `SpeechToTextConfig` now requires both `min_energy_split_window_size` and `max_audio_clip_s` to be non‑null, tightening the configuration validation. The addition of thorough unit tests ensures robustness across different audio lengths and sample rates.

**Potential risks**  
The chunking algorithm uses non‑overlapping windows when scanning for low‑energy regions (`step = min_energy_window`), which could miss optimal split points if the quietest region falls between windows. Additionally, the `find_split_point` function may return suboptimal indices when audio energy is uniform, though it defaults to the start of the search region. Developers must ensure `min_energy_window_size` aligns with the sample rate (e.g., ~100 ms windows) to avoid overly coarse or fine‑grained energy calculations.

**Key insights**  
This utility is essential for offline Whisper transcription pipelines handling long audio. Developers should tune `overlap_duration_s` and `min_energy_window_size` based on the audio content and sample rate to balance chunk length and split quality. The exposed function supports any sample rate and preserves all audio samples without loss. Consider adding an optional parameter for window step size in `find_split_point` to allow overlapping energy scans for finer control.

---

## 5. [[BUGFIX][Qwen3.5] Hardcode `mlp.gate` as not quantizable](https://github.com/vllm-project/vllm/pull/35156)


### Base Information

- **PR Number:** #35156
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 19:42:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35156/files) (1):**
  - `vllm/model_executor/models/qwen3_next.py`

### Summary

**What changed and why**  
The change hardcodes the `mlp.gate` layer's quantization configuration to `None` in the Qwen3.5-Next model, overriding any external quantization settings. This is a workaround to fix weight loading failures for the `nvidia/Qwen3.5-397B-A17B-NVFP4` checkpoint, where `mlp.gate` layers were not properly excluded from quantization in the saved checkpoint.

**Technical impact**  
This modification ensures the `mlp.gate` layer is treated as non-quantizable during model loading, which resolves compatibility issues with specific quantized checkpoints. However, it introduces a model-specific override that bypasses the general quantization configuration mechanism, potentially affecting consistency in how quantization is applied across different model components.

**Potential risks**  
If other layers in the checkpoint also have incorrect quantization metadata, they may still cause loading failures. The hardcoded approach could mask underlying issues in checkpoint generation or quantization logic. Additionally, this change might unintentionally affect other Qwen3.5 variants or future model versions if not properly scoped.

**Key insights**  
This is a targeted fix for a specific checkpoint compatibility issue, but a more robust solution would involve updating the checkpoint generation process to correctly mark `mlp.gate` as non-quantizable. Developers should verify that no other layers exhibit similar problems and consider adding validation for quantization metadata during checkpoint loading to prevent recurrence.

---

## 6. [gpu_model_runner: Cache is_encoder_decoder from model config](https://github.com/vllm-project/vllm/pull/35099)


### Base Information

- **PR Number:** #35099
- **Author:** [pschlan-amd](https://github.com/pschlan-amd)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 19:08:34
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35099/files) (1):**
  - `vllm/config/model.py`

### Summary

**What changed and why**  
The change converts the `is_encoder_decoder` property in the model configuration to a cached property. This optimization addresses performance profiling findings that showed repeated calls to this property were relatively expensive (~12 µs per call in the test environment), and caching prevents redundant computation.

**Technical impact**  
This modification reduces CPU overhead by ensuring the `is_encoder_decoder` flag is computed only once per model configuration instance. Since this property is likely accessed frequently during inference or sampling operations, the caching improves overall runtime efficiency with minimal memory overhead.

**Potential risks**  
If the underlying `hf_config` is mutated after the first property access, the cached value may become stale, leading to incorrect behavior. Additionally, the caching introduces a slight memory overhead per model configuration instance, though this is negligible compared to the performance gain.

**Key insights**  
This is a straightforward performance optimization that leverages Python's `cached_property` to eliminate redundant work. Developers should ensure that `hf_config` remains immutable after initialization to maintain correctness. Similar patterns could be applied to other frequently accessed, immutable properties in the codebase for further performance improvements.

---

## 7. [[Quantization] Support FP8 MoE bias for models like GPT-OSS](https://github.com/vllm-project/vllm/pull/34906)


### Base Information

- **PR Number:** #34906
- **Author:** [jasperjiaguo](https://github.com/jasperjiaguo)
- **Merged By:** [simon-mo](https://github.com/simon-mo)
- **Merged time:** 2026-02-23 19:07:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34906/files) (1):**
  - `vllm/model_executor/layers/quantization/fp8.py`

### Summary

**What changed and why**  
This PR adds support for FP8 quantization of MoE layers with bias parameters, specifically for models like GPT-OSS-120B that include biases in their MoE layers (`gate_up_proj_bias`, `down_proj_bias`). Previously, `Fp8MoEMethod` failed to register these bias parameters during weight loading, causing crashes when serving BF16 models with `--quantization fp8`. The changes ensure biases are properly registered and passed through to the fused MoE kernel.

**Technical impact**  
The modifications extend `Fp8MoEMethod` to conditionally create and manage bias parameters (`w13_bias`, `w2_bias`) when `FusedMoEConfig.has_bias` is enabled. Biases are integrated into the quantization configuration and handled during weight loading, maintaining compatibility with existing FP8 quantization workflows. This enables successful serving of biased MoE models with FP8 quantization, improving throughput as demonstrated in the test results.

**Potential risks**  
A guard is included to prevent unsupported combinations of `FusedMoEModularKernel` with bias, but there may be edge cases if other kernel implementations or quantization backends are used. The bias handling relies on the `has_bias` flag; if this flag is incorrectly set or missing in certain configurations, it could lead to silent failures or performance degradation. Additionally, the changes assume biases are stored in the original dtype (BF16), which may not hold for all model variants.

**Key insights**  
Developers should verify that `FusedMoEConfig.has_bias` is accurately configured for any MoE model with biases. The PR maintains consistency with `UnquantizedFusedMoEMethod` and includes necessary safeguards, but testing with diverse MoE architectures is recommended. The performance boost (≈46% throughput increase) highlights the value of FP8 quantization for biased MoE models, though accuracy should be monitored as shown by the slight drop in GSM8K accuracy.

---

## 8. [[ROCm] AITER fused RoPE+KVCache](https://github.com/vllm-project/vllm/pull/33443)


### Base Information

- **PR Number:** #33443
- **Author:** [Rohan138](https://github.com/Rohan138)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 19:06:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33443/files) (19):**
  - `tests/compile/passes/test_functionalization.py`
  - `tests/compile/passes/test_rope_kvcache_fusion.py`
  - `tests/compile/passes/test_scatter_split_replace.py`
  - `tests/v1/attention/test_attention_backends.py`
  - `vllm/_aiter_ops.py`
  - `vllm/compilation/passes/fusion/rope_kvcache_fusion.py`
  - `vllm/compilation/passes/pass_manager.py`
  - `vllm/compilation/passes/utility/fix_functionalization.py`
  - `vllm/compilation/passes/utility/scatter_split_replace.py`
  - `vllm/config/compilation.py`
  - `vllm/config/vllm.py`
  - `vllm/model_executor/layers/attention/attention.py`
  - `vllm/model_executor/layers/attention/kv_transfer_utils.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/attention/backends/rocm_aiter_unified_attn.py`
  - `vllm/v1/attention/backends/rocm_attn.py`
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
This PR introduces a `fuse_rope_kvcache` Inductor pass that fuses QK rotary position embedding (RoPE) and KV cache update operations into a single kernel for ROCm AITER. It builds on prior work (#25954) to improve performance by reducing kernel launches and intermediate memory operations during decode.

**Technical impact**  
The changes add a new fusion pass (`RopeKVCacheFusionPass`) that matches RoPE and KV cache update patterns and replaces them with a fused custom op (`fused_rope_and_unified_kv_cache_update`). Support is added to ROCm attention backends (AITER unified, Triton, and ROCm attn) via a new `do_rope_and_kv_cache_update` method. The pass is conditionally applied based on token count thresholds to optimize for small-batch decode scenarios. Auxiliary passes (`ScatterSplitReplacementPass`) and functionalization fixes ensure correct pattern matching and side-effect handling.

**Potential risks**  
- The fusion is currently limited to ROCm platforms and requires AITER support.  
- Incorrect functionalization handling could lead to silent errors in side-effect ordering.  
- The pattern matching relies on specific graph structures; changes to RoPE or KV cache implementations may break fusion.  
- The pass is only applied below a token threshold (`rope_kvcache_fusion_max_token_num`), which may cause performance cliffs.

**Key insights**  
- The fusion significantly reduces kernel launch overhead for decode, but is not recommended for compute-bound prefill.  
- Developers must ensure the `fused_rope_kvcache` flag is only enabled on ROCm with AITER; the config validates this.  
- The `ScatterSplitReplacementPass` is a prerequisite for correct pattern matching and should remain enabled.  
- Test coverage is comprehensive but should be extended to edge cases like FP8 KV cache and cross-attention scenarios.

---

## 9. [[Mamba1] - Change supports_update_block_table to True](https://github.com/vllm-project/vllm/pull/35054)


### Base Information

- **PR Number:** #35054
- **Author:** [Josephasafg](https://github.com/Josephasafg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 19:05:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35054/files) (1):**
  - `vllm/v1/attention/backends/mamba1_attn.py`

### Summary

**What changed and why**  
The change removes the `supports_update_block_table: bool = False` flag from the `Mamba1AttentionMetadataBuilder` class. This modification enables Mamba1 models to support block table updates, which was previously disabled due to a bug that has now been fixed in a related PR (#34874).

**Technical impact**  
This change allows Mamba1 models to utilize dynamic block table updates during inference, potentially improving memory management and performance for sequences that change over time. It aligns Mamba1's capabilities with other model architectures that already support this feature, creating more consistent behavior across different attention backends.

**Potential risks**  
If the bug fix in PR #34874 is incomplete or has undiscovered edge cases, enabling this feature could introduce subtle correctness issues during sequence generation. There's also a risk that the APC tests might not cover all scenarios where block table updates occur, potentially missing failure modes in production workloads.

**Key insights**  
This is a straightforward enabling change that depends entirely on the correctness of the underlying bug fix. Developers should verify that all Mamba1 model variants are thoroughly tested with dynamic sequence lengths and block allocation patterns. Consider adding integration tests specifically for block table update scenarios to ensure robustness.

---

## 10. [[Bugfix] Fix lora_ids in FusedMoE LoRA test](https://github.com/vllm-project/vllm/pull/35135)


### Base Information

- **PR Number:** #35135
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-23 18:49:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35135/files) (1):**
  - `tests/lora/test_fused_moe_lora_kernel.py`

### Summary

**What changed and why**  
This PR fixes the FusedMoE LoRA test by correcting the `lora_ids` creation logic and tightening the assertion tolerance. The test previously created `lora_ids` as a simple range (`torch.arange(max_loras + 2)`), which did not match the actual active LoRA IDs derived from `token_lora_mapping`. Now, `sample_data` generates `active_lora_ids` by extracting unique values from `token_lora_mapping` and padding with `-1`, aligning with the kernel's expected input format. Additionally, the `assert_close` tolerance is reduced from `1e-1` to `1e-2` for stricter validation.

**Technical impact**  
The changes ensure the test accurately simulates real-world LoRA usage by generating `lora_ids` that reflect active adapters, improving test fidelity. The stricter tolerance increases the test's sensitivity to numerical errors, potentially catching subtle kernel bugs. The modifications are confined to test code, so they do not affect production behavior but enhance the reliability of the FusedMoE LoRA kernel validation.

**Potential risks**  
The tighter assertion tolerance (`1e-2`) may cause test failures in environments with higher numerical variability (e.g., different hardware or software versions). The updated `lora_ids` generation assumes `token_lora_mapping` contains valid LoRA IDs; if this mapping is malformed (e.g., out-of-range values), it could lead to incorrect test behavior. Additionally, the changes introduce a new return value from `sample_data`, which must be correctly propagated in all test calls.

**Key insights**  
Developers should verify that the `token_lora_mapping` generation in tests remains consistent with production logic. The stricter tolerance may require attention to floating-point precision in kernel implementations. Ensure all test variants (e.g., tensor-parallel and naive block assignment) correctly integrate the updated `lora_ids` parameter to maintain comprehensive coverage.

---

## 11. [[Perf] Enable FlashInfer DeepGEMM swapAB on SM90 by default](https://github.com/vllm-project/vllm/pull/34924)


### Base Information

- **PR Number:** #34924
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 17:34:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34924/files) (2):**
  - `tests/compile/fusions_e2e/test_tp1_quant.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
The PR enables FlashInfer's DeepGEMM swapAB pathway by default for SM90+ GPUs by changing the `VLLM_BLOCKSCALE_FP8_GEMM_FLASHINFER` environment variable default from `False` to `True`. This aims to unlock performance gains observed in benchmarks, such as a ~10% improvement at low batch sizes. A corresponding test skip is added to prevent conflicts with FlashInfer's internal quantization in FP8 fusion tests.

**Technical impact**  
This change activates optimized FP8 GEMM kernels (using TensorRT-LLM) for linear layers on Hopper+ architectures, potentially improving throughput and latency across various batch sizes. The test modification ensures compatibility by skipping fusion tests when FlashInfer's block FP8 GEMM is supported, avoiding quantization mismatches.

**Potential risks**  
Enabling this pathway by default could introduce regressions on untested hardware or model configurations, especially if the kernel has hidden dependencies or edge cases. The test skip might reduce coverage for FP8 fusion scenarios, and performance gains may vary across different workloads or GPU architectures beyond the tested H100 setup.

**Key insights**  
Benchmarks show tangible improvements (e.g., ~8% higher token throughput), but thorough validation across diverse environments is recommended before wider deployment. Developers should monitor for any performance degradation or failures in production, particularly on non-Hopper GPUs or with atypical model architectures. Consider adding conditional fallback logic if stability issues arise.

---

## 12. [[Bugfix] Fix DSV3 kernels breaking _C and _moe_C on unsupported arches](https://github.com/vllm-project/vllm/pull/35123)


### Base Information

- **PR Number:** #35123
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 17:11:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35123/files) (5):**
  - `CMakeLists.txt`
  - `csrc/dsv3_fused_a_gemm.cu`
  - `csrc/moe/dsv3_router_gemm_entry.cu`
  - `csrc/moe/torch_bindings.cpp`
  - `csrc/torch_bindings.cpp`

### Summary

**What changed and why**  
The fix moves the PyTorch kernel implementation (`impl()`) registration for `dsv3_fused_a_gemm` and `dsv3_router_gemm` from the central `torch_bindings.cpp` files into their respective `.cu` source files using `TORCH_LIBRARY_IMPL_EXPAND`. This resolves a linking failure on unsupported GPU architectures (e.g., SM121) where the `.cu` files are excluded by CMake’s arch filtering, but the symbols were still required in the bindings, causing the entire `_C`/`_moe_C` extension to fail.

**Technical impact**  
This change decouples the kernel registration from the central binding files, ensuring that implementations are only compiled and linked when their corresponding `.cu` files are included for the target architecture. It aligns with the existing pattern used by other kernels (e.g., marlin, cutlass) and maintains conditional compilation without hard symbol dependencies.

**Potential risks**  
If the `TORCH_LIBRARY_IMPL_EXPAND` macro is not correctly defined or supported in all build configurations, registration could fail silently. Additionally, future developers might inadvertently re-add `impl()` calls to the central bindings, reintroducing the linking issue. The removal of the `-DENABLE_DSV3_FUSED_A_GEMM=1` flag from CMake could affect any conditional logic relying on it.

**Key insights**  
Always keep kernel implementation registration within the same compilation unit as the kernel itself when conditional compilation is required. This pattern improves modularity and prevents linking errors on unsupported architectures. Verify that all similar kernels follow this convention to avoid regressions, and ensure build flags are cleaned up to avoid unused definitions.

---

## 13. [[Perf] Improve default triton fused moe configs](https://github.com/vllm-project/vllm/pull/34846)


### Base Information

- **PR Number:** #34846
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 16:01:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34846/files) (2):**
  - `benchmarks/kernels/benchmark_moe_defaults.py`
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
This PR improves the default Triton fused MoE kernel configurations by replacing simplistic hardcoded values with data-driven, batch-size-aware defaults. The changes were derived from analyzing 298 tuned configuration files to address deficiencies where old defaults used suboptimal tile sizes (`BLOCK_SIZE_K=32` matched <1% of tuned configs) and lacked explicit warp/stage settings.

**Technical impact**  
The new defaults introduce adaptive tile sizing based on batch size and model characteristics, with `BLOCK_SIZE_M` scaling from 16 to 128 as batch grows, `BLOCK_SIZE_K` increased to 64/128, and explicit `num_warps`/`num_stages` settings. This improves kernel performance significantly for many-expert models (up to 2x speedup at large batch sizes) while maintaining near-tuned performance for smaller batches.

**Potential risks**  
The increased complexity of conditional logic in `get_default_config()` could introduce subtle bugs for edge cases not covered in benchmarking. The batch-size thresholds (32, 96, 512) may need adjustment for different hardware architectures. The ROCm-specific `num_stages` handling remains a platform-specific concern that requires ongoing maintenance.

**Key insights**  
The data-driven approach successfully bridges the gap between generic defaults and tuned configurations, with new defaults typically within 5% of tuned performance. Developers should note the batch-size awareness is critical for optimal performance, and the benchmarking script provides a valuable tool for validating configurations across different models and batch sizes.

---

## 14. [[RL] Validation for pause_mode='keep'](https://github.com/vllm-project/vllm/pull/34992)


### Base Information

- **PR Number:** #34992
- **Author:** [hao-aaron](https://github.com/hao-aaron)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-23 13:30:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34992/files) (2):**
  - `.buildkite/test_areas/distributed.yaml`
  - `examples/offline_inference/new_weight_syncing/rlhf_async_new_apis.py`

### Summary

**What changed and why**  
The PR modifies an RLHF example to validate weight synchronization with `pause_mode='keep'`. It adds validation logic that compares tokens generated after weight updates against tokens from a fresh vLLM engine with new weights. The changes also relocate the test from Buildkite's distributed tests to a 2-GPU distributed test suite.

**Technical impact**  
The example now demonstrates proper validation of weight synchronization by comparing outputs from paused/resumed generation against a reference engine. Key changes include: replacing placement group management with Ray's internal DP resource handling, adding deterministic batch invariance for reproducible outputs, implementing token counting to trigger pauses, and restructuring the test flow into distinct phases for validation.

**Potential risks**  
The validation depends on deterministic generation (enabled via `VLLM_BATCH_INVARIANT=1`), which may not hold across all hardware/configurations. The pause mechanism relies on token counting thresholds that could race with request completion. Removing the test from distributed.yaml could reduce coverage if the new location isn't executed equivalently.

**Key insights**  
This provides a robust validation pattern for weight synchronization scenarios. Developers should note the shift from manual placement groups to Ray's internal DP management. The deterministic validation approach is valuable but requires careful environment configuration. The pause/resume logic demonstrates proper handling of inflight requests during weight updates.

---

## 15. [[Misc] Monitor interface changes](https://github.com/vllm-project/vllm/pull/35113)


### Base Information

- **PR Number:** #35113
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-23 09:14:52
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35113/files) (1):**
  - `.github/CODEOWNERS`

### Summary

**What changed and why**  
The change adds @NickLucche as a code owner for the file `/vllm/v1/worker/kv_connector_model_runner_mixin.py` in the CODEOWNERS file. This ensures they are automatically notified (e.g., via pull request reviews) when modifications are made to this key-value (KV) interface component, aligning with the PR description's goal of being mentioned for KV interface changes.

**Technical impact**  
This update only affects the code review and notification process; it does not alter the codebase's functionality, architecture, or runtime behavior. It enhances collaboration by ensuring relevant stakeholders are involved in changes to the KV connector mixin, which is part of the KV offloading or caching system.

**Potential risks**  
There is minimal risk since this is a metadata change. However, if @NickLucche is not actively monitoring notifications or lacks context on the KV interface, it could slow down review cycles. Additionally, future changes to this file might require coordination between multiple owners, potentially complicating approval processes.

**Key insights**  
This is a straightforward administrative update that improves oversight of KV interface changes. Developers should ensure all listed code owners are aware of their responsibilities and maintain familiarity with the component. For similar critical interfaces, consider documenting ownership roles to clarify review expectations.

---

## 16. [Enforce that `model` is the first positional arg when `--served-model-name` is used](https://github.com/vllm-project/vllm/pull/34973)


### Base Information

- **PR Number:** #34973
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 08:38:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34973/files) (2):**
  - `tests/entrypoints/openai/test_cli_args.py`
  - `vllm/utils/argparse_utils.py`

### Summary

**What changed and why**  
This change enforces that when using `--served-model-name` with `vllm serve`, the model must be provided as the first positional argument. It prevents users from accidentally serving the default model instead of their intended model by catching cases where `--served-model-name` is used without a positional model argument.

**Technical impact**  
The argument parsing logic now validates the command structure more strictly, raising a `ValueError` when `--served-model-name` is present but no positional model argument is provided (unless a config file specifies the model). This improves CLI safety but modifies the parser's behavior for edge-case command invocations.

**Potential risks**  
The regex-based detection (`--served[-_]model[-_]name`) may not cover all possible hyphen/underscore variations if the argument definition changes. Additionally, the check could incorrectly flag valid commands if users rely on config files without a positional model argument but forget to include the model in the config.

**Key insights**  
Developers should ensure the model is always the first positional argument after `serve` when using `--served-model-name`. The validation logic depends on regex patterns matching the argument names exactly as defined elsewhere in the codebase, so any future changes to those names must be synchronized here.

---

## 17. [Fix custom processors that use deleted import for Transformers v5](https://github.com/vllm-project/vllm/pull/35101)


### Base Information

- **PR Number:** #35101
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 08:38:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35101/files) (1):**
  - `vllm/transformers_utils/processor.py`

### Summary

**What changed and why**  
Added a backward compatibility patch for Transformers v5 that creates an alias `ChatTemplateLoadKwargs` in `processing_utils` if it doesn't exist. This resolves import errors in custom processors (like `HCXVisionForCausalLM`) that still reference the removed v4 class, which was merged into `ProcessorChatTemplateKwargs`.

**Technical impact**  
The change ensures vLLM remains compatible with custom processors relying on Transformers v4 APIs, preventing runtime import failures. It modifies the module state dynamically at import time, allowing existing remote code to function without modification while using Transformers v5.

**Potential risks**  
Dynamic modification of imported modules could lead to subtle side effects if other code depends on the absence of `ChatTemplateLoadKwargs`. The patch is conditional but may mask underlying issues if custom processors are not updated. There is also a risk of this alias persisting longer than necessary if upstreaming is delayed.

**Key insights**  
This is a temporary workaround that should be removed once `HCXVisionForCausalLM` is upstreamed to Transformers. Developers should verify that custom processors transition to using `ProcessorChatTemplateKwargs` directly. The patch is narrowly scoped but highlights the importance of tracking upstream API changes in dependencies.

---

## 18. [[Bugfix] Fix prefix caching for Mamba 'all' mode (Nemotron models)](https://github.com/vllm-project/vllm/pull/34874)


### Base Information

- **PR Number:** #34874
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [tdoublep](https://github.com/tdoublep)
- **Merged time:** 2026-02-23 08:31:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34874/files) (2):**
  - `tests/v1/attention/test_mamba_update_block_table.py`
  - `vllm/v1/attention/backends/mamba_attn.py`

### Summary

**What changed and why**  
The fix addresses a bug in prefix caching for Nemotron hybrid models using Mamba's 'all' cache mode. When multiple KV cache groups share identical MambaSpecs, metadata caching reuses metadata via `update_block_table()`. However, this method previously failed to copy `block_idx_last_scheduled_token` and `block_idx_last_computed_token` to the current builder's persistent buffers, causing CUDA graph replay to read stale indices and produce NaN logprobs.

**Technical impact**  
This change ensures that during metadata reuse, block index tensors are correctly copied to the current builder's persistent buffers, maintaining proper memory alignment for CUDA graph execution. It specifically affects the `'all'` cache mode, which relies on these indices during decode, while `'align'` and `'none'` modes remain unaffected. The fix preserves the metadata caching optimization while correcting buffer ownership.

**Potential risks**  
The fix introduces a conditional check for `mamba_cache_mode == "all"`, which could be overlooked if new cache modes are added. Additionally, the non-blocking copies (`non_blocking=True`) assume proper synchronization elsewhere, which may pose risks if the surrounding code lacks explicit synchronization. Edge cases involving mixed cache modes or partial metadata updates should be validated.

**Key insights**  
Developers should ensure that any new cache modes or metadata optimizations account for persistent buffer ownership, particularly when CUDA graphs are involved. The regression test provides a clear template for validating buffer sharing across builders. Consider adding assertions or documentation to highlight the dependency between cache mode and required buffer updates.

---

## 19. [Use Xet high performance mode for Transformers v5](https://github.com/vllm-project/vllm/pull/35098)


### Base Information

- **PR Number:** #35098
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 08:19:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35098/files) (1):**
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
The code now conditionally enables either Xet high performance mode (for Transformers v5) or falls back to `hf_transfer` (for Transformers v4). This optimizes download performance based on the installed Transformers library version, leveraging newer capabilities when available.

**Technical impact**  
This introduces version-aware performance optimization for model downloading. The system automatically detects Transformers v5 via the presence of `HF_XET_HIGH_PERFORMANCE` constant and enables Xet mode, otherwise it defaults to the existing `hf_transfer` method for backward compatibility.

**Potential risks**  
If the `HF_XET_HIGH_PERFORMANCE` constant exists but Xet high performance mode isn't fully supported in the current environment, it could cause unexpected behavior. There's also a risk of silent failures if `enable_hf_transfer()` has side effects beyond environment variable setting that are now skipped for v5 users.

**Key insights**  
The implementation correctly maintains backward compatibility while adding forward-looking optimization. Developers should verify that Xet high performance mode works correctly in their deployment environments and consider adding logging to track which optimization path is being used for debugging purposes.

---

## 20. [[Bugfix] Fix MRotaryEmbedding missing `truncate` attr with YaRN scaling](https://github.com/vllm-project/vllm/pull/35080)


### Base Information

- **PR Number:** #35080
- **Author:** [haosdent](https://github.com/haosdent)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-23 08:05:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35080/files) (1):**
  - `vllm/model_executor/layers/rotary_embedding/mrope.py`

### Summary

**What changed and why**  
The PR adds a missing `truncate` parameter to `MRotaryEmbedding.__init__` and sets it as an instance attribute. This fixes an `AttributeError` that occurred when `MRotaryEmbedding` delegated cache computation to `YaRNScalingRotaryEmbedding` methods, which rely on `self.truncate` being defined.

**Technical impact**  
This change ensures compatibility between `MRotaryEmbedding` and YaRN scaling for extended context models like Qwen3.5. The `truncate` attribute is now properly initialized before `super().__init__()` triggers cache computation, preventing runtime errors during model loading.

**Potential risks**  
If downstream code relies on `MRotaryEmbedding` not having a `truncate` attribute, this could introduce subtle behavioral changes. Additionally, the default value `True` must align with `YaRNScalingRotaryEmbedding`'s default to avoid inconsistent scaling behavior.

**Key insights**  
Always verify that parent and child classes share required attributes when using delegation patterns. The fix highlights the importance of initializing all necessary attributes before calling `super().__init__()` in inheritance chains. Developers should ensure similar attributes are synchronized across related rotary embedding implementations.

---

## 21. [[CI] Skip Responses API](https://github.com/vllm-project/vllm/pull/34990)


### Base Information

- **PR Number:** #34990
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 07:46:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34990/files) (1):**
  - `tests/entrypoints/openai/responses/test_harmony.py`

### Summary

**What changed and why**  
A `@pytest.mark.skip` decorator was added to the `test_mcp_code_interpreter_streaming` test. The change is a temporary measure to skip a flaky test in the CI pipeline, as noted in the PR description. This is intended to stabilize the CI environment while the underlying issue in the code interpreter MCP implementation is investigated.

**Technical impact**  
This change removes the test from the CI execution suite, which will eliminate its intermittent failures and improve overall pipeline stability. It does not modify any application logic, only the test runner's behavior for this specific test case. The test code itself remains in the codebase for future reactivation.

**Potential risks**  
The primary risk is that a legitimate bug in the code interpreter MCP implementation could go undetected while the test is skipped. There is also a risk that the skip becomes permanent if the required investigation and fixes are not prioritized, leading to technical debt in the test suite.

**Key insights**  
This is a standard, short-term tactic for handling flaky tests. The skip reason is well-documented, clearly stating the need for investigation. Developers should treat this as a temporary workaround and ensure a ticket is created to track the root cause analysis and fix for the MCP implementation, with the goal of re-enabling the test.

---

## 22. [[Metrics] Add Prometheus counters for Model FLOPs Utilization (MFU)](https://github.com/vllm-project/vllm/pull/30950)


### Base Information

- **PR Number:** #30950
- **Author:** [markmc](https://github.com/markmc)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-02-23 07:01:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30950/files) (5):**
  - `docs/mkdocs/hooks/generate_metrics.py`
  - `docs/usage/metrics.md`
  - `vllm/v1/metrics/loggers.py`
  - `vllm/v1/metrics/perf.py`
  - `vllm/v1/metrics/ray_wrappers.py`

### Summary

**What changed and why**  
Added Prometheus integration for Model FLOPs Utilization (MFU) metrics, exposing estimated FLOPs and memory bandwidth per GPU via Prometheus counters. This extends existing console logging to provide observability for performance monitoring, addressing feature request #30738.

**Technical impact**  
Three new Prometheus counters (`vllm:estimated_flops_per_gpu_total`, `vllm:estimated_read_bytes_per_gpu_total`, `vllm:estimated_write_bytes_per_gpu_total`) are now exported when `--enable-mfu-metrics` is enabled. The changes integrate seamlessly with the existing metrics architecture, including Ray support via wrapper classes.

**Potential risks**  
If `--enable-mfu-metrics` is not set, the new code paths remain inactive, but there is a minor risk of increased overhead when enabled due to additional counter updates. The `observe` method checks for zero values, but edge cases with negative or extremely large values could affect counter accuracy.

**Key insights**  
Developers can now monitor MFU via Prometheus queries as documented. Ensure `--enable-mfu-metrics` is enabled in deployment configurations to activate these metrics. The implementation follows consistent patterns used for other metrics, making it easy to maintain and extend.

---

## 23. [[kv-cache, ct] Use compressed-tensors as a source of ground-truth for quant strategies](https://github.com/vllm-project/vllm/pull/34254)


### Base Information

- **PR Number:** #34254
- **Author:** [eldarkurtic](https://github.com/eldarkurtic)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-23 06:37:55
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34254/files) (1):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`

### Summary

**What changed and why**  
The changes replace hardcoded string comparisons for KV-cache quantization strategies with `QuantizationStrategy` enum usage from the compressed-tensors library. This delegates validation and strategy handling to compressed-tensors, establishing it as the single source of truth for quantization strategies.

**Technical impact**  
This update centralizes quantization strategy definitions, reducing duplication and potential inconsistencies. The code now relies on the library's enum values (`QuantizationStrategy.TENSOR` and `QuantizationStrategy.ATTN_HEAD`) instead of inline strings, improving maintainability and alignment with upstream updates.

**Potential risks**  
If the compressed-tensors library changes its enum values or validation logic, this could break KV-cache quantization. Additionally, any mismatched library versions might cause runtime errors due to missing or altered enum members.

**Key insights**  
Developers should ensure the compressed-tensors dependency is kept up-to-date and compatible. This change simplifies future updates to quantization strategies but requires monitoring library changes. Using enums enhances type safety and clarity over string literals.

---

## 24. [[Refactor] Decouple TimingContext from InputProcessingContext](https://github.com/vllm-project/vllm/pull/35083)


### Base Information

- **PR Number:** #35083
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-23 06:15:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35083/files) (38):**
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/multimodal/processing/test_gemma3.py`
  - `tests/models/multimodal/processing/test_glm4_1v.py`
  - `tests/models/multimodal/processing/test_h2ovl.py`
  - `tests/models/multimodal/processing/test_idefics3.py`
  - `tests/models/multimodal/processing/test_internvl.py`
  - `tests/models/multimodal/processing/test_llama4.py`
  - `tests/models/multimodal/processing/test_llava_next.py`
  - `tests/models/multimodal/processing/test_llava_onevision.py`
  - `tests/models/multimodal/processing/test_minimax_vl_01.py`
  - `tests/models/multimodal/processing/test_nemotron_vl.py`
  - `tests/models/multimodal/processing/test_phi3v.py`
  - `tests/models/multimodal/processing/test_phi4mm.py`
  - `tests/models/multimodal/processing/test_qwen2_vl.py`
  - `tests/models/multimodal/processing/test_qwen3_omni.py`
  - `tests/models/multimodal/processing/test_smolvlm.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/multimodal/processing/test_transformers.py`
  - `tests/multimodal/test_processing.py`
  - `vllm/benchmarks/mm_processor.py`
  - `vllm/model_executor/models/clip.py`
  - `vllm/model_executor/models/deepseek_vl2.py`
  - `vllm/model_executor/models/h2ovl.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/paligemma.py`
  - `vllm/model_executor/models/pixtral.py`
  - `vllm/model_executor/models/siglip.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/multimodal.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/multimodal/processing/__init__.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/multimodal/processing/dummy_inputs.py`
  - `vllm/multimodal/processing/inputs.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/multimodal/registry.py`
  - `vllm/renderers/base.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This refactor decouples `TimingContext` from `InputProcessingContext` by introducing a separate `MultiModalTimingRegistry` to track processor timing stats. The `apply` method signature is simplified to accept only `ProcessorInputs` and `TimingContext`, consolidating arguments and removing indirect access via `set_request_id`. MM hash generation is moved to `ProcessorInputs.get_mm_hashes`, and timing metric names are standardized (e.g., `*_time` → `*_secs`).

**Technical impact**  
The changes improve modularity by separating timing concerns from processing logic, reducing coupling. The new `ProcessorInputs` class centralizes input data and hash computation, simplifying processor implementations. Timing collection is now explicit via `TimingContext.record()`, making it easier to extend and test. The refactor also fixes missing stats for Terratorch and Transformers backends.

**Potential risks**  
The removal of `set_request_id` context manager could break any external code relying on it. Changes to metric names (`*_time` → `*_secs`) may affect downstream monitoring or dashboards. The move of hash generation to `ProcessorInputs` assumes consistent behavior across all processors; any custom hash logic in subclasses must be updated.

**Key insights**  
Developers should now use `ProcessorInputs` to pass data to processors and employ `TimingContext.record()` for timing measurements. The `__call__` method remains the preferred interface for unit tests. Ensure any custom processor overrides are updated to the new `apply` signature and that timing metric consumers are aware of the renamed fields.

---

## 25. [[Llama4,CI] Bring back Llama-4 bug fixes, and also fix Maverick tests](https://github.com/vllm-project/vllm/pull/35033)


### Base Information

- **PR Number:** #35033
- **Author:** [eldarkurtic](https://github.com/eldarkurtic)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-23 06:04:35
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35033/files) (2):**
  - `tests/models/multimodal/generation/test_maverick.py`
  - `vllm/model_executor/models/llama4.py`

### Summary

**What changed and why**  
This PR reintroduces bug fixes for Llama-4 that were previously reverted, and corrects a Maverick test that was incorrectly passing. The changes simplify the weight permutation logic for rotary embeddings by removing complex quantization-specific handling and replacing it with a unified approach. Additionally, the test fix adjusts weight tensor dimensions to match the expected model architecture.

**Technical impact**  
The refactored `permute_qk_weight_for_rotary` method now uses a consistent permutation strategy for both weights and scales, reducing code complexity and special-case handling for different quantization formats. This improves maintainability and ensures correct weight loading for Llama-4 models. The test update aligns synthetic model weights with the actual weight shapes used in the implementation.

**Potential risks**  
Removing quantization-specific logic (e.g., for compressed-tensors or NVFP4 formats) could break compatibility with certain quantized models if the new unified permutation does not correctly handle all scale types or packed weight layouts. The simplified approach assumes that all multi-element weight scales can be permuted similarly to weights, which may not hold for all quantization schemes.

**Key insights**  
The refactoring prioritizes code clarity and reduces fragmentation, but thorough validation is needed for edge cases like per-block quantization (noted in the code comments). Developers should verify that the permutation works correctly across all supported quantization formats and ensure that the test changes reflect true model behavior rather than masking underlying issues.

---

## 26. [[ModelBash][DSV3] Add TRTLLM DSV3 Router GEMM kernel (6% B1 Speedup)](https://github.com/vllm-project/vllm/pull/34302)


### Base Information

- **PR Number:** #34302
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-23 06:02:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34302/files) (9):**
  - `CMakeLists.txt`
  - `csrc/moe/dsv3_router_gemm_bf16_out.cu`
  - `csrc/moe/dsv3_router_gemm_entry.cu`
  - `csrc/moe/dsv3_router_gemm_float_out.cu`
  - `csrc/moe/dsv3_router_gemm_utils.h`
  - `csrc/moe/moe_ops.h`
  - `csrc/moe/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/models/deepseek_v2.py`

### Summary

**What changed and why**  
Added a specialized CUDA kernel (`dsv3_router_gemm`) for DeepSeek V3 MoE router GEMM operations, optimized for small batch sizes (1–16 tokens) common in decode phases. The kernel computes `output = mat_a @ mat_b.T` with bfloat16 inputs and supports float32 or bfloat16 outputs. It is ported from TensorRT-LLM via SGLang and targets Hopper (SM90+) GPUs with CUDA 12.0+.

**Technical impact**  
This introduces a high-performance, hardware-specific optimization for DeepSeek V3 and similar MoE models (e.g., Kimi K2) with fixed dimensions (hidden_dim=7168, num_experts∈{256,384}). The kernel leverages Programmatic Dependent Launch (PDL) when enabled via `TRTLLM_ENABLE_PDL=1`. Integration includes CMake build support, a new Python binding (`dsv3_router_gemm`), and a custom `DeepSeekV2Gate` class that conditionally routes computations to the optimized kernel.

**Potential risks**  
The kernel is highly specialized—it only works for specific model dimensions, small token counts (≤16), and requires SM90+ GPUs. Incorrect environment or hardware may lead to fallback or runtime errors. The use of PTX assembly and PDL may introduce portability issues across CUDA versions or architectures. The `set_out_dtype` hack for router logits dtype selection is fragile and may break with future changes.

**Key insights**  
This optimization provides measurable performance gains (5.5–6% speedup for batch‑1 decode) but at the cost of generality. Developers should ensure the kernel is only invoked under supported conditions (checked via `allow_dsv3_router_gemm`). The implementation should be viewed as a transitional solution until a more unified MoE kernel framework is established. Consider adding runtime feature detection and clearer fallback logging.

---

## 27. [[XPU] allow TORCH_SDPA/TRITON_ATTN as XPU vit Backend](https://github.com/vllm-project/vllm/pull/35010)


### Base Information

- **PR Number:** #35010
- **Author:** [yma11](https://github.com/yma11)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 05:06:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35010/files) (2):**
  - `vllm/model_executor/layers/attention/mm_encoder_attention.py`
  - `vllm/platforms/xpu.py`

### Summary

**What changed and why**  
The changes enable XPU devices to use `TORCH_SDPA` and `TRITON_ATTN` as vision transformer (ViT) attention backends, in addition to the previously supported `FLASH_ATTN`. This is controlled by the `mm_encoder_attn_backend` configuration, expanding backend options for XPU.

**Technical impact**  
The XPU platform now supports multiple attention backends for vision models, aligning with backend flexibility available on other platforms. The `forward_xpu` method now routes to the appropriate backend implementation (`_forward_fa`, `_forward_triton`, or `_forward_sdpa`) based on the configured `attn_backend`, removing the previous restriction to only Flash Attention.

**Potential risks**  
If `TORCH_SDPA` or `TRITON_ATTN` backends are not fully optimized or tested for XPU, performance or correctness issues may arise. The error handling now covers unsupported backends, but missing validation could lead to runtime failures if an incompatible backend is selected.

**Key insights**  
Developers should verify that the newly enabled backends are properly implemented and optimized for XPU hardware. Ensure thorough testing of each backend with vision models to confirm functional and performance parity. This change enhances flexibility but requires careful backend selection based on model requirements and XPU capabilities.

---

## 28. [[ROCm][CI] Fix spec decode profile assertion and logprob test determinism](https://github.com/vllm-project/vllm/pull/35043)


### Base Information

- **PR Number:** #35043
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 05:05:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35043/files) (1):**
  - `tests/v1/sample/test_logprobs.py`

### Summary

**What changed and why**  
The PR fixes two ROCm-specific issues in speculative decoding tests. First, it resolves a profile assertion failure where speculative decoding's token count exceeded the scheduler's batched token limit. Second, it addresses non-deterministic logprob comparisons by enforcing identical batch sizes (max_num_seqs=1) for both reference and speculative LLMs on ROCm, eliminating floating-point reduction differences.

**Technical impact**  
These changes ensure ROCm compatibility for speculative decoding tests without affecting other platforms. The assertion fix aligns token counting with actual speculative decoding behavior, while the determinism kwargs eliminate numerical noise from non-associative GPU operations, allowing tests to focus on functional correctness.

**Potential risks**  
Enforcing `max_num_seqs=1` could mask concurrency-related bugs that only appear with larger batch sizes. The platform-specific logic adds maintenance overhead and may require similar fixes for other GPU platforms with non-associative floating-point operations.

**Key insights**  
Platform-specific numerical differences should be isolated in tests to avoid false failures. Consider documenting this pattern for other tests sensitive to floating-point determinism. Review whether similar assertions in `gpu_model_runner.py` need updates for speculative decoding edge cases.

---

## 29. [[CLEANING] Remove unused disable_by_batch_size from SpeculativeConfig](https://github.com/vllm-project/vllm/pull/35060)


### Base Information

- **PR Number:** #35060
- **Author:** [VincentG1234](https://github.com/VincentG1234)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 05:05:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35060/files) (1):**
  - `vllm/config/speculative.py`

### Summary

**What changed and why**  
The PR removes the unused `disable_by_batch_size` parameter and its validation logic from `SpeculativeConfig`. This parameter became obsolete after the V0 engine was removed in v0.10.0 and is no longer read anywhere in the codebase, though it was still accepted in configurations.

**Technical impact**  
This change simplifies the configuration schema by eliminating dead code, reducing maintenance overhead. It does not affect runtime behavior since the parameter was already non-functional, but it prevents users from mistakenly relying on it.

**Potential risks**  
Users who have existing configurations with `disable_by_batch_size` set will see validation errors after upgrading, as the parameter will no longer be recognized. This could break backward compatibility for those unaware of the deprecation.

**Key insights**  
Developers should update any configuration files or scripts that reference `disable_by_batch_size` to avoid runtime errors. This cleanup aligns with good software hygiene by removing unused code, but a deprecation warning in a prior release would have eased the transition.

---

## 30. [[Refactor] Remove dead private func `_fp8_perm` and `_extract_mask_for_item`](https://github.com/vllm-project/vllm/pull/35068)


### Base Information

- **PR Number:** #35068
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 05:05:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35068/files) (2):**
  - `vllm/model_executor/layers/fused_moe/utils.py`
  - `vllm/model_executor/models/glmasr_utils.py`

### Summary

**What changed and why**  
Two dead private functions were removed: `_fp8_perm` from `fused_moe/utils.py` and `_extract_mask_for_item` (along with its helper `_normalize_to_tensor`) from `glmasr_utils.py`. These functions were identified as unused code, and their removal is part of a cleanup effort to reduce technical debt and improve codebase maintainability.

**Technical impact**  
The removal has no functional impact on the system as the functions were not being called. It simplifies the codebase by eliminating dead code, which reduces cognitive load for developers and slightly decreases the module's footprint. The changes are purely subtractive, so there is no risk of breaking existing functionality.

**Potential risks**  
The primary risk is if these functions were referenced indirectly or via dynamic imports not caught by static analysis. However, since they are private functions (prefixed with `_`), this risk is minimal. Another consideration is the loss of potentially reusable utility logic, but if it's truly dead code, this is not a concern.

**Key insights**  
This is a safe and beneficial cleanup. Developers should verify via the project's test suite that no tests were relying on these private functions. It's also good practice to ensure the removal aligns with any broader refactoring plans, as the `_fp8_perm` function might have been related to FP8 data type handling, a specialized area.

---

## 31. [Fix pipeline parallel with embed scaling in the Transformers modelling backend](https://github.com/vllm-project/vllm/pull/35094)


### Base Information

- **PR Number:** #35094
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 05:04:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/35094/files) (1):**
  - `vllm/model_executor/models/transformers/base.py`

### Summary

**What changed and why**  
The change adds `self.embed_scale = None` initialization in the `__init__` method of the base Transformer model class. This ensures that `embed_scale` is defined on all pipeline stages, preventing an `AttributeError` when `self.embed_scale is not None` is checked in `Base.forward` on non-first pipeline stages where input embeddings do not exist.

**Technical impact**  
This fix ensures consistent attribute availability across pipeline parallelism stages, allowing the model to scale embeddings correctly when needed. It resolves a runtime error that would occur in distributed inference scenarios where later pipeline stages lack input embedding layers.

**Potential risks**  
If `embed_scale` is later used without proper initialization (e.g., expecting a non-`None` value), it could lead to silent logical errors. Additionally, this change assumes `None` is a safe default for all models; if any model logic depends on `embed_scale` being uninitialized, it may cause unintended behavior.

**Key insights**  
Always ensure attributes referenced in forward passes are initialized in `__init__`, especially in distributed settings. Consider adding a comment or assertion to clarify `embed_scale`'s purpose and valid states. Review other model attributes for similar pipeline parallelism issues.

---

## 32. [[Feature] Lazy import for the "mistral" tokenizer module.](https://github.com/vllm-project/vllm/pull/34651)


### Base Information

- **PR Number:** #34651
- **Author:** [nascheme](https://github.com/nascheme)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 00:43:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34651/files) (14):**
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/reasoning/utils.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/multimodal/processing/context.py`
  - `vllm/sampling_params.py`
  - `vllm/tokenizers/mistral.py`
  - `vllm/tool_parsers/hermes_tool_parser.py`
  - `vllm/tool_parsers/jamba_tool_parser.py`
  - `vllm/tool_parsers/mistral_tool_parser.py`
  - `vllm/utils/mistral.py`
  - `vllm/v1/structured_output/backend_xgrammar.py`

### Summary

**What changed and why**  
This PR implements lazy import for the Mistral tokenizer module to eliminate the mandatory dependency on `mistral_common[image]`. The changes replace direct `isinstance(obj, MistralTokenizer)` checks with a new `is_mistral_tokenizer(obj)` helper function, which avoids importing the Mistral package unless necessary. A new `vllm.utils.mistral` module was added to house the helper and a lazily-loaded reference to the tokenizer module.

**Technical impact**  
The changes reduce startup time and optional dependencies by deferring imports of the Mistral tokenizer until it's actually needed. The architecture now uses a type guard pattern that checks a class attribute (`IS_MISTRAL_TOKENIZER`) before performing the `isinstance` check, ensuring backward compatibility while avoiding eager imports. This affects multiple components including chat completion, tool parsing, multimodal processing, and structured output backends.

**Potential risks**  
If the `IS_MISTRAL_TOKENIZER` attribute is not properly set or maintained on the `MistralTokenizer` class, the type guard may fail to identify valid instances. There's also a risk of subtle bugs if the lazy-loaded module (`mt`) is accessed in a way that assumes it's always available, though the current implementation seems safe. The PR mentions test failures due to missing dependencies, which should be verified to ensure they're unrelated.

**Key insights**  
This is a well-structured dependency optimization that follows the lazy import pattern already used elsewhere in the codebase. Developers should ensure that any new code checking for Mistral tokenizers uses `is_mistral_tokenizer()` instead of direct `isinstance` checks. The addition of the `IS_MISTRAL_TOKENIZER` class attribute provides a lightweight way to identify the tokenizer type without importing the module.

---

## 33. [fix: Apply embedding_multiplier to inputs_embeds](https://github.com/vllm-project/vllm/pull/34813)


### Base Information

- **PR Number:** #34813
- **Author:** [gabe-l-hart](https://github.com/gabe-l-hart)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 00:42:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34813/files) (1):**
  - `vllm/model_executor/models/granitemoehybrid.py`

### Summary

**What changed and why**  
The change moves the `embedding_multiplier` scaling operation outside the conditional branch, ensuring it's applied to both `inputs_embeds` and the embedded `input_ids`. Previously, `inputs_embeds` bypassed scaling, causing incorrect embeddings when `enable_prompt_embeds` was used.

**Technical impact**  
This fix ensures consistent embedding scaling across both input paths, aligning the model's behavior with its intended architecture. It resolves a critical bug where precomputed embeddings were not scaled, leading to garbled or empty outputs in models using the `granitemoehybrid` implementation.

**Potential risks**  
If `embedding_multiplier` is not intended to scale `inputs_embeds` in certain configurations, this change could introduce over-scaling. However, the bug report indicates scaling was missing, so the risk is low. Ensure no other model variants rely on the previous behavior.

**Key insights**  
Always verify that multiplicative factors (like `embedding_multiplier`) are applied uniformly across all input embedding paths. The fix is minimal and corrects a clear oversight, but consider adding a unit test to validate both `input_ids` and `inputs_embeds` paths produce identical scaled embeddings.

---

## 34. [[BugFix]: Fix local mypy issues](https://github.com/vllm-project/vllm/pull/34739)


### Base Information

- **PR Number:** #34739
- **Author:** [hickeyma](https://github.com/hickeyma)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-23 00:40:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34739/files) (6):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`
  - `vllm/entrypoints/openai/chat_completion/protocol.py`
  - `vllm/entrypoints/openai/completion/protocol.py`
  - `vllm/entrypoints/openai/responses/protocol.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/sampling_params.py`

### Summary

**What changed and why**  
This PR fixes mypy type checking errors that occur when running `pre-commit` locally. The changes address three specific issues: Pydantic dataclass incompatibility with `dataclasses.replace()`, a missing Linux-only `os.sched_setaffinity` attribute, and a naming conflict with the `json` module import.

**Technical impact**  
The changes maintain runtime behavior while suppressing mypy errors. Replacing `from dataclasses import replace` with a custom `vllm.config.utils.replace` function provides a type-safe workaround for Pydantic dataclasses. Adding a platform check for `os.sched_setaffinity` improves cross-platform compatibility. Renaming the `json` import avoids conflicts with class properties.

**Potential risks**  
The `# type: ignore[call-arg]` comment could mask legitimate type errors if the `StructuredOutputsParams` class definition changes. The platform check for `sched_setaffinity` might not catch all platform-specific variations. The custom `replace` function must maintain exact compatibility with `dataclasses.replace` to avoid subtle bugs.

**Key insights**  
These are pragmatic fixes for mypy's strict type checking, prioritizing build stability over perfect type coverage. Developers should be aware that the type ignores are intentional workarounds. The `json` import rename is a good hygiene practice that should be applied consistently if similar conflicts exist elsewhere.

---

