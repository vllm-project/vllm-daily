# vLLM Merged PR Report

**Report Date:** 2026-02-20 PST

**Total Merged PRs:** 30

---

## 1. [[ROCM] Optimize ROCM_AITER_FA spec decode eagle performance](https://github.com/vllm-project/vllm/pull/34541)


### Base Information

- **PR Number:** #34541
- **Author:** [jennyyyyzhen](https://github.com/jennyyyyzhen)
- **Merged By:** [zhuohan123](https://github.com/zhuohan123)
- **Merged time:** 2026-02-20 20:32:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34541/files) (1):**
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
This PR optimizes ROCM AITER FA for speculative decoding (EAGLE) by enabling CUDA graph support and removing CPU-GPU synchronization during draft model execution. The changes switch the CUDA graph support mode from `UNIFORM_SINGLE_TOKEN_DECODE` to `UNIFORM_BATCH` and introduce a specialized `build_for_drafting` method that bypasses metadata splitting and CPU syncs.

**Technical impact**  
These modifications allow CUDA graphs to be captured during speculative decoding when `num_speculative_tokens > 0`, significantly reducing latency. The new `build_for_drafting` method assumes uniform decode batches (characteristic of EAGLE drafting) and avoids operations that break graph capture, such as `.cpu()` calls and token-type splitting logic.

**Potential risks**  
The optimization assumes all drafting requests are uniform decodes; if this condition is violated (e.g., mixed request types during drafting), it may lead to incorrect metadata. Removing CPU syncs could mask synchronization issues in non-EAGLE speculative methods. The change also depends on PR #32877 for accuracy fixes, so merging without it risks correctness.

**Key insights**  
The PR demonstrates a ~2x improvement in throughput and reduced latency, as shown in the benchmark results. Developers should ensure the uniform-batch assumption holds for all speculative decoding paths and verify that the dependency PR is merged first. This pattern of avoiding CPU syncs in drafting could be applied to other attention backends for similar gains.

---

## 2. [[ROCm][CI] Fix spec decode logprobs flakiness and parametrize tree attention backends](https://github.com/vllm-project/vllm/pull/34599)


### Base Information

- **PR Number:** #34599
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 20:25:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34599/files) (2):**
  - `tests/v1/sample/test_logprobs.py`
  - `tests/v1/spec_decode/test_tree_attention.py`

### Summary

**What changed and why**  
The PR addresses ROCm-specific test flakiness and improves test coverage. In `test_logprobs.py`, it disables ROCm's non-deterministic skinny GEMM for the speculative decode logprobs test to eliminate intermittent failures, adds clearer assertion messages, and converts `VllmRunner` instances to context managers for proper resource cleanup. In `test_tree_attention.py`, it parametrizes the correctness test over all available reference attention backends on the current platform (including ROCm's `TRITON_ATTN` and the platform default) instead of hardcoding `FLASH_ATTN`, and adds KV cache layout adaptation to support backends with different cache layouts.

**Technical impact**  
The changes enhance test reliability and platform-specific validation. Disabling skinny GEMM for a specific test isolates non-determinism, preventing false positives while preserving performance elsewhere. Parametrizing the tree attention test ensures broader backend coverage on ROCm, improving cross-platform consistency. The KV cache layout adaptation (`flash` ↔ `block`) enables correct comparisons across backends, and the resource management improvements reduce potential memory leaks in tests.

**Potential risks**  
The environment variable `VLLM_ROCM_USE_SKINNY_GEMM=0` is scoped to a single test, but if similar non-determinism affects other tests, they may remain flaky. The KV cache layout adaptation assumes a simple transpose for `TRITON_ATTN`; however, documented incompatibilities with `ROCM_ATTN` (paged layout) and `ROCM_AITER_FA` (head count mismatch) could lead to false negatives if not properly handled. Direct cache updates for certain backends bypass the standard `do_kv_cache_update` path, which might mask integration issues.

**Key insights**  
Developers should note that ROCm's skinny GEMM non-determinism is a known source of flakiness in logprob comparisons; similar issues may require targeted environment variable adjustments. The attention test now dynamically selects reference backends per platform, making it essential to verify new backends against the layout adaptation logic. The TODOs for `ROCM_ATTN` and `ROCM_AITER_FA` highlight important integration gaps that need resolution for full ROCm coverage.

---

## 3. [[ROCm][AITER] Fix aiter paged_attention_v1 decode for sliding window and head_size < 64](https://github.com/vllm-project/vllm/pull/34570)


### Base Information

- **PR Number:** #34570
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 20:25:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34570/files) (1):**
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`

### Summary

**What changed and why**  
The fix restores the `unified_attention` Triton kernel fallback for decode operations when sliding window attention is active or when `head_size < 64`. This addresses a regression where the `ll4mi` kernel in `paged_attention_v1` incorrectly computed `VHELOOP = 0` for small head sizes, causing the softmax × V multiplication loop to be skipped and producing corrupted outputs.

**Technical impact**  
This change reintroduces a conditional dispatch in the ROCm AITER backend, ensuring that decode operations for sliding window models or models with small attention heads are handled correctly. The `paged_attention_v1` kernel no longer processes sliding window decode paths, and the `unified_attention` kernel is used as a fallback, maintaining compatibility and correctness for affected model configurations.

**Potential risks**  
The fallback path does not yet support the shuffle KV cache layout, which could limit performance optimizations for certain configurations. Additionally, the hardcoded `_MIN_HEAD_SIZE_FOR_LL4MI = 64` assumes a fixed warp size (NWARPS=4) on ROCm, which may not be portable if the kernel's warp configuration changes. Edge cases with mixed head sizes or dynamic warp counts could still cause issues.

**Key insights**  
Developers should ensure that any future changes to the `ll4mi` kernel include proper validation for minimum head size requirements. The conditional fallback logic should be documented clearly, and consideration should be given to extending shuffle KV cache support to the `unified_attention` path for completeness. Testing should prioritize models with `head_size < 64` and sliding window attention to prevent similar regressions.

---

## 4. [[CI] Fix ColBERT HF comparison tests on AMD CI + refactor](https://github.com/vllm-project/vllm/pull/34567)


### Base Information

- **PR Number:** #34567
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 20:12:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34567/files) (1):**
  - `tests/models/language/pooling/test_colbert.py`

### Summary

**What changed and why**  
This PR fixes a Triton kernel crash on AMD CI by detecting CPU execution and forcing eager attention implementation, and refactors three near-identical HF comparison tests into a single parametrized test. It also resolves `torch.tensor()` copy-construct warnings by replacing them with `torch.as_tensor()` calls.

**Technical impact**  
The fix ensures cross-platform compatibility by dynamically selecting attention implementation based on device. The refactoring reduces code duplication by ~80 lines, centralizes test logic, and improves maintainability. The `torch.as_tensor()` changes eliminate unnecessary tensor copies and warnings without affecting functionality.

**Potential risks**  
The device detection logic assumes CPU/GPU binary classification; mixed-device scenarios could still cause issues. The `.float()` calls in `_compute_hf_colbert_embeddings` may hide precision issues if models use different dtypes. Centralizing test logic increases coupling—if the shared helpers fail, all parametrized tests fail.

**Key insights**  
Always consider device compatibility when using Triton/flash attention kernels. Use `torch.as_tensor()` instead of `torch.tensor()` when copies aren't needed. Parametrization is effective for reducing duplication in model comparison tests, but ensure shared helpers are robust. The explicit `.float()` conversion maintains consistency across model types but could mask dtype-related bugs.

---

## 5. [[feat] Add per-block extra_keys to KV events](https://github.com/vllm-project/vllm/pull/33304)


### Base Information

- **PR Number:** #33304
- **Author:** [zhongdaor-nv](https://github.com/zhongdaor-nv)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 20:11:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33304/files) (6):**
  - `examples/online_serving/kv_events_subscriber.py`
  - `tests/v1/core/test_kv_cache_utils.py`
  - `vllm/distributed/kv_events.py`
  - `vllm/v1/core/block_pool.py`
  - `vllm/v1/core/kv_cache_utils.py`
  - `vllm/v1/request.py`

### Summary

**What changed and why**  
Added a per-block `extra_keys` field to `BlockStored` KV cache events to expose the extra hash keys (like MM identifiers, LoRA name, cache_salt, and prompt embedding hashes) used in each block's hash computation. This enables external KV cache consumers, such as Dynamo's routing system, to correctly identify and differentiate cached blocks, especially for multi-modal requests where placeholder tokens are identical across different images.

**Technical impact**  
The changes affect the KV event publishing pipeline by adding a new field to the event data structure. The `block_pool.py` now calculates `extra_keys` for each block individually during caching, ensuring the list aligns with `block_hashes`. A caching mechanism for prompt embedding hashes was also introduced in the `Request` object to avoid redundant tensor hashing operations, improving performance.

**Potential risks**  
The new `extra_keys` field is optional (`None` by default), which maintains backward compatibility but requires downstream consumers to handle the `None` case. The caching of prompt embedding hashes on the `Request` object adds memory overhead for long prompts with many blocks. There is a risk of incorrect `extra_keys` list generation if the logic for skipping null blocks does not perfectly match the `new_hashes` generation.

**Key insights**  
This change is critical for enabling accurate multi-modal request routing in distributed systems. Developers integrating with KV events must update their code to parse the `extra_keys` list. The per-block hash caching for prompt embeddings is a good performance optimization but should be monitored for memory usage in high-concurrency scenarios. Ensure all event subscribers are updated to handle the new field gracefully.

---

## 6. [[CI][MCP][Harmony] Heavy refactoring Harmony & MCP response tests and stabilizing with deterministic test infrastructure](https://github.com/vllm-project/vllm/pull/33949)


### Base Information

- **PR Number:** #33949
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 20:03:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33949/files) (10):**
  - `tests/entrypoints/openai/responses/conftest.py`
  - `tests/entrypoints/openai/responses/test_harmony.py`
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`
  - `tests/entrypoints/openai/responses/test_parsable_context.py`
  - `tests/entrypoints/openai/responses/test_simple.py`
  - `tests/utils.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`
  - `vllm/entrypoints/openai/responses/context.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
This PR addresses systemic test flakiness in Harmony and MCP Responses API integration tests by replacing non-deterministic test patterns with robust infrastructure. The core issue was that tests used `@pytest.mark.flaky` which restarted entire tests including server fixtures, causing port conflicts and masking real failures. The solution introduces deterministic testing through pinned system prompts, API-level retries, and clear separation between server invariants (hard assertions) and model behavior (soft/xfail).

**Technical impact**  
The changes introduce a new environment variable `VLLM_SYSTEM_START_DATE` to pin conversation dates in system messages, eliminating a source of non-determinism. Test infrastructure now uses shared retry helpers in `conftest.py` that retry API calls without restarting server fixtures. The codebase gains better consistency checks (MCP tool mappings), improved logging for diagnostics, and more reliable server cleanup with process group management. Architecture-wise, tests are reorganized into logical classes with proper fixture scoping to prevent lifecycle interference.

**Potential risks**  
The pinned date environment variable could accidentally be set in production, though it defaults to `None` maintaining current behavior. The new process group termination in `utils.py` assumes Unix-like systems and may not work correctly on Windows. The retry mechanisms could mask genuine API issues if retry counts are too high. Changes to tool availability filtering in `serving.py` could break existing workflows if tools are expected to be available regardless of request parameters.

**Key insights**  
The PR successfully separates concerns: server behavior is validated with hard assertions while model behavior uses soft assertions (`xfail`). The shared test infrastructure in `conftest.py` eliminates code duplication and ensures consistent retry logic. Developers should note that `VLLM_SYSTEM_START_DATE` can now be used for reproducible inference, and the enhanced logging (`log_response_diagnostics`) provides valuable debugging information in CI. The process group management ensures more reliable GPU memory cleanup between tests.

---

## 7. [[Frontend] Support multimodal inputs for late-interaction scoring (ColQwen3) + NewModel: nvidia/nemotron-colembed](https://github.com/vllm-project/vllm/pull/34574)


### Base Information

- **PR Number:** #34574
- **Author:** [craftsangjae](https://github.com/craftsangjae)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 20:01:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34574/files) (10):**
  - `docs/models/pooling_models.md`
  - `examples/pooling/score/colqwen3_rerank_online.py`
  - `tests/models/multimodal/pooling/test_colqwen3.py`
  - `tests/models/registry.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/pooling/score/utils.py`
  - `vllm/model_executor/models/colqwen3.py`
  - `vllm/model_executor/models/registry.py`
  - `vllm/transformers_utils/config.py`

### Summary

**What changed and why**  
This PR enables multimodal inputs (text + images) for late-interaction scoring and reranking with ColQwen3-style models, removing previous `NotImplementedError` limitations. It also adds support for the new NVIDIA Nemotron-Colembed model. Changes include new utility functions to parse multimodal score data, updates to the offline (`LLM.score()`) and online (`/score`, `/rerank`) scoring paths, and comprehensive documentation and examples.

**Technical impact**  
The changes extend the late-interaction scoring pipeline to handle heterogeneous input types (strings and multimodal content parts) by converting `ScoreData` into `PromptType` objects with attached multimodal metadata. This allows ColBERT-style models to compute MaxSim scores across text-image pairs, marking the first cross-modal scoring support in vLLM. The architecture now distinguishes between cross-encoder (shared multimodal context) and late-interaction (independent encoding) processing paths.

**Potential risks**  
- Increased memory usage due to separate multimodal trackers for each query/document in late-interaction scoring.  
- Edge cases where mixed text and multimodal inputs could cause inconsistent tokenization or processing errors.  
- The new `Qwen3VLNemotronEmbedModel` may have subtle behavioral differences from existing ColQwen3 variants, requiring careful validation.

**Key insights**  
- Developers should use the provided `score_data_to_prompts` utility when handling multimodal inputs for late-interaction models.  
- The PR introduces a clear separation between cross-encoder and late-interaction multimodal parsing, ensuring correct context handling.  
- Extensive tests cover text-vs-image, mixed-document, and image-vs-text scenarios, providing a robust reference for future multimodal scoring implementations.

---

## 8. [[Realtime] Add Qwen3-ASR realtime streaming support](https://github.com/vllm-project/vllm/pull/34613)


### Base Information

- **PR Number:** #34613
- **Author:** [pougetat](https://github.com/pougetat)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:59:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34613/files) (5):**
  - `tests/models/registry.py`
  - `vllm/entrypoints/openai/realtime/connection.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/qwen3_asr_realtime.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds real-time WebSocket streaming transcription support for the Qwen3-ASR model. It introduces a new model class `Qwen3ASRRealtimeGeneration` with a custom audio buffer and multimodal processor to handle streaming audio chunks, enabling low-latency transcription via the existing real-time endpoint.

**Technical impact**  
The changes extend the real-time streaming framework to support Qwen3-ASR by implementing the `SupportsRealtime` protocol. A new audio buffer accumulates and segments incoming audio, while a specialized multimodal processor expands audio placeholder tokens to match the feature length. The `realtime_max_tokens` is increased to 64 to accommodate longer transcriptions per segment, and the model registry is updated to recognize the new class.

**Potential risks**  
The hardcoded segment duration (5.0 seconds) may not be optimal for all latency requirements. The buffer’s dynamic resizing could cause memory spikes under high load. There is no explicit handling for audio sampling rate mismatches or malformed audio chunks. The `realtime_max_tokens=64` might still be insufficient for very long utterances, potentially truncating output.

**Key insights**  
Developers should validate that the audio sampling rate matches the model’s expected rate (16 kHz). Consider making segment duration configurable via model or endpoint parameters. Monitor memory usage during sustained streaming. Ensure the real-time endpoint’s concurrency limits are appropriate for audio workloads. The implementation reuses much of the base Qwen3-ASR infrastructure, maintaining consistency.

---

## 9. [[Kernel] Optimize sample_recovered_tokens_kernel](https://github.com/vllm-project/vllm/pull/34974)


### Base Information

- **PR Number:** #34974
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:59:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34974/files) (2):**
  - `tests/v1/sample/test_rejection_sampler.py`
  - `vllm/v1/sample/rejection_sampler.py`

### Summary

**What changed and why**  
The PR optimizes the `sample_recovered_tokens_kernel` in the rejection sampler by implementing two key changes: replacing a global vocabulary reduction with tiled reduction over chunks (BLOCK_SIZE = 8192) to lower register pressure and improve occupancy, and substituting division (`prob / q`) with multiplication (`prob * inv_q`) for better performance. These changes aim to accelerate kernel execution, with profiling showing a ~5x speedup.

**Technical impact**  
The kernel now processes vocabulary in blocks rather than loading the entire padded vocabulary at once, reducing memory bandwidth and register usage. This improves GPU occupancy and overall kernel efficiency. The division-to-multiplication change leverages faster hardware operations. The optimization is localized to the kernel, preserving the same functional behavior and API.

**Potential risks**  
The fixed `BLOCK_SIZE` (8192) may not be optimal for all vocabulary sizes or hardware configurations, potentially leading to suboptimal performance if vocab_size is not a multiple of BLOCK_SIZE. The tiled reduction introduces additional loop overhead, which could negate benefits for very small vocabularies. Edge cases with extremely large vocabularies may still face memory pressure if BLOCK_SIZE is too large for register limits.

**Key insights**  
The optimization successfully reduces kernel latency significantly (159µs → 29µs) with minimal end-to-end impact (~1% throughput gain) due to infrequent kernel launches. Developers should consider making BLOCK_SIZE configurable or dynamically tuned based on vocabulary size and hardware. The changes maintain numerical equivalence, as validated by comprehensive unit tests. Future optimizations could explore adaptive tiling or further arithmetic optimizations.

---

## 10. [Support prompt_embeds for pooling requests in output processor](https://github.com/vllm-project/vllm/pull/34904)


### Base Information

- **PR Number:** #34904
- **Author:** [laviier](https://github.com/laviier)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:57:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34904/files) (1):**
  - `vllm/v1/engine/output_processor.py`

### Summary

**What changed and why**  
The PR adds support for `prompt_embeds` in pooling requests within the V1 output processor. Previously, pooling requests using `EmbedsPrompt` (with `prompt_token_ids=None`) would crash with an assertion error because the pooling path lacked the fallback logic already present in the completion path. The change moves the fallback logic earlier to create placeholder token IDs from `prompt_embeds` before the request type is determined, ensuring both completion and pooling paths can handle embeds-only requests.

**Technical impact**  
This change unifies the handling of `prompt_embeds` across completion and pooling request types in the output processor. It enables IO processor plugins to use `EmbedsPrompt` for non-text data (like time series or image features) with pooling models, allowing direct injection into a model's `inputs_embeds` parameter. The architecture now consistently supports custom data transport without requiring dummy token IDs for pooling APIs.

**Potential risks**  
The primary risk is that downstream consumers of `prompt_token_ids` in the pooling path (e.g., for statistics) might incorrectly assume the placeholder `[0] * len(prompt_embeds)` contains meaningful token data, though the PR notes they only use `len()`. There's a minor risk if any future code inspects the actual token values. Additionally, the assertion `assert prompt_token_ids is not None` now occurs before the request type check, which could affect error handling flow.

**Key insights**  
The fix is minimal and mirrors the proven pattern from the completion path, reducing risk. Developers should ensure that any new code using `prompt_token_ids` in pooling contexts only relies on length, not content. This change enhances vLLM's flexibility for serving non-text models via pooling APIs, aligning with the growing use of embeddings for diverse data types.

---

## 11. [[Misc] Fix mypy errors in vllm/profiler and remove from exclude list](https://github.com/vllm-project/vllm/pull/34959)


### Base Information

- **PR Number:** #34959
- **Author:** [taneem-ibrahim](https://github.com/taneem-ibrahim)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:56:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34959/files) (2):**
  - `tools/pre_commit/mypy.py`
  - `vllm/profiler/layerwise_profile.py`

### Summary

**What changed and why**  
This PR removes `vllm/profiler` from the mypy exclusion list and fixes type errors in `layerwise_profile.py`. The changes are part of an ongoing effort to enable comprehensive mypy type checking across the codebase, improving type safety and maintainability.

**Technical impact**  
The modifications introduce generic typing (`Generic[StatsEntryT]`) to `_StatsTreeNode` and refine type annotations throughout the profiler module. This ensures type consistency in tree structures and recursive functions, while also updating method signatures to properly handle optional parameters and return types.

**Potential risks**  
The refactoring adds conditional checks (`if summary_node is not None`) that could subtly alter control flow if nodes unexpectedly become `None`. Additionally, the increased type strictness might reveal hidden type mismatches in other parts of the codebase when mypy checks are expanded further.

**Key insights**  
The use of generics elegantly handles polymorphic tree nodes, enhancing code clarity. Developers should verify that the `None` checks align with the profiler's logic and ensure similar patterns are adopted when removing other exclusions. This sets a good precedent for type-safe recursive data structures.

---

## 12. [[ROCm][Bugfix]: Only save unpadded sizes for shared_experts in MoERunner to fix rmsnorm pad fusion](https://github.com/vllm-project/vllm/pull/34636)


### Base Information

- **PR Number:** #34636
- **Author:** [Rohan138](https://github.com/Rohan138)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:56:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34636/files) (1):**
  - `vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`

### Summary

**What changed and why**  
The fix modifies the MoERunner to only preserve the original hidden states tensor (`original_hidden_states`) and its dimension when shared experts are present. Previously, this tensor was always saved, which introduced an extra user that broke ROCm's RMSNorm+padding fusion pattern matching. By conditionally storing the tensor only when needed, the fusion can proceed correctly.

**Technical impact**  
This change restores the RMSNorm+padding fusion on ROCm for GPT-OSS models, improving kernel performance and reducing memory overhead. The fusion pattern now matches because the unpadded hidden states output from RMSNorm no longer has an additional user (`auto_functionalized(moe_forward)`) when shared experts are absent.

**Potential risks**  
If the conditional logic incorrectly identifies when shared experts are present, it could lead to runtime errors or incorrect outputs in MoE layers. Additionally, any future changes to the MoE runner or fusion passes must ensure they remain compatible with this conditional tensor preservation.

**Key insights**  
The fix is minimal and targeted, avoiding a broader rewrite of the fusion pass. Developers should note that preserving intermediate tensors only when strictly necessary is crucial for compiler optimizations like kernel fusion. This also reduces unnecessary memory usage, aligning with performance best practices.

---

## 13. [[Kernel] [Helion] [9/N] Canonicalize GPU variant names to base model names](https://github.com/vllm-project/vllm/pull/34928)


### Base Information

- **PR Number:** #34928
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:55:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34928/files) (4):**
  - `tests/kernels/helion/test_utils.py`
  - `vllm/kernels/helion/config_manager.py`
  - `vllm/kernels/helion/configs/silu_mul_fp8.json`
  - `vllm/kernels/helion/utils.py`

### Summary

**What changed and why**  
Added a `_GPU_NAME_ALIASES` lookup table to `canonicalize_gpu_name()` to map GPU variant names (with form factor, memory, or other suffixes) to their base model names. This allows Helion kernel configurations to be consolidated per GPU model instead of duplicated across variants. For example, various H100 variants now all map to `nvidia_h100`.

**Technical impact**  
This change simplifies configuration management by reducing duplication in kernel config files, as evidenced by the removal of over 27,000 lines from `silu_mul_fp8.json`. The platform detection system now uses canonical names, enabling a single set of configurations to support multiple hardware variants of the same GPU architecture.

**Potential risks**  
If the alias mapping is incomplete or incorrect, certain GPU variants may fail to find appropriate kernel configurations. The error message improvement helps, but manual alias additions are still required for new variants. There's also a risk that different variants with meaningful performance differences could be incorrectly grouped under the same canonical name.

**Key insights**  
The alias approach effectively decouples hardware naming variations from configuration logic. Developers should add new GPU variants to `_GPU_NAME_ALIASES` when encountered, and consider whether performance-critical differences between variants warrant separate configurations despite the canonicalization. The improved error messaging will help users self-diagnose platform detection issues.

---

## 14. [[CI/Build] Add opentelemetry libs in default vllm build (requirements/common.txt)](https://github.com/vllm-project/vllm/pull/34466)


### Base Information

- **PR Number:** #34466
- **Author:** [vladmihailescu](https://github.com/vladmihailescu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:54:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34466/files) (2):**
  - `examples/online_serving/opentelemetry/README.md`
  - `requirements/common.txt`

### Summary

**What changed and why**  
This PR adds OpenTelemetry libraries (`opentelemetry-sdk`, `opentelemetry-api`, `opentelemetry-exporter-otlp`, `opentelemetry-semantic-conventions-ai`) to the default vLLM build by including them in `requirements/common.txt`. The purpose is to eliminate the need for users to manually patch OpenTelemetry dependencies in production environments, reducing support churn and preventing import failures when users attempt to enable OTLP tracing without the required libraries.

**Technical impact**  
The change makes OpenTelemetry a core dependency of vLLM, ensuring that all builds automatically include telemetry capabilities. This simplifies deployment for production use cases requiring observability and aligns with the project's move toward production-ready defaults. The documentation has been updated to reflect that manual installation is no longer necessary.

**Potential risks**  
Adding these dependencies increases the attack surface and could introduce version conflicts with other packages in users' environments. The version constraints are specified as minimum versions (`>=`), which may lead to unexpected behavior if newer major versions introduce breaking changes. There's also a slight increase in image size, though minimal (~5MB).

**Key insights**  
This is a strategic move to improve the out-of-the-box production experience for vLLM users. Developers should ensure that the OpenTelemetry integration remains optional at runtime to avoid performance overhead for users who don't need it. Consider adding runtime checks or configuration flags to disable telemetry if not configured, maintaining flexibility while providing robust defaults.

---

## 15. [[LoRA] Support Quantized Adapters](https://github.com/vllm-project/vllm/pull/30286)


### Base Information

- **PR Number:** #30286
- **Author:** [yugong333](https://github.com/yugong333)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 19:54:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30286/files) (2):**
  - `vllm/lora/ops/triton_ops/__init__.py`
  - `vllm/lora/ops/triton_ops/fused_moe_lora_fp8_op.py`

### Summary

**What changed and why**  
This PR adds support for FP8-quantized LoRA adapters to the fused MoE LoRA kernel, introducing three new operations (`fused_moe_lora_fp8`, `fused_moe_lora_shrink_fp8`, `fused_moe_lora_expand_fp8`). The changes enable per-channel, per-tensor, and block-wise quantization schemes for LoRA weights, aiming to reduce memory usage and improve inference efficiency for quantized adapters.

**Technical impact**  
The implementation extends the existing fused MoE LoRA kernel with FP8 quantization support, maintaining the same interface while adding quantization-specific parameters (e.g., `group_n`, `group_k` for block-wise quantization). It introduces a new kernel (`_fused_moe_lora_kernel_fp8`) that handles quantized weight loading and dequantization during computation, leveraging Triton's capabilities for optimized GPU execution.

**Potential risks**  
The kernel includes complex conditional logic for different quantization schemes and assignment strategies (`naive_block_assignment`), which could lead to subtle bugs if not thoroughly tested. The use of a global cache (`_LORA_PTR_DICT`) for weight pointers may cause issues in multi-device or multi-process scenarios. Additionally, the extensive use of Triton JIT specialization could increase compilation overhead.

**Key insights**  
Developers should ensure proper testing across all quantization modes and assignment strategies. The global pointer cache should be reviewed for thread-safety and device compatibility. Consider adding validation for quantization parameters (e.g., `group_n`, `group_k`) to prevent misconfiguration. The kernel's performance should be benchmarked against non-quantized versions to verify efficiency gains.

---

## 16. [Revert "[Llama4,Quantization] Simplify and generalize logic for Q/K permutations in quantized self-attn layers "](https://github.com/vllm-project/vllm/pull/34997)


### Base Information

- **PR Number:** #34997
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 17:19:19
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34997/files) (1):**
  - `vllm/model_executor/models/llama4.py`

### Summary

**What changed and why**  
This PR reverts a previous change that simplified Q/K weight permutation logic for quantized self-attention layers in Llama4 models. The revert restores more complex, specialized handling for different quantization formats (NVFP4, CompressedTensors INT8/FP8) to fix CI failures caused by the earlier simplification.

**Technical impact**  
The code now reintroduces format-specific logic for weight permutations, including special handling for packed uint8 FP4 weights and block-scaled quantization. This increases maintenance complexity but ensures compatibility with multiple quantization schemes that the simplified version failed to support correctly.

**Potential risks**  
The restored logic is more fragile due to hardcoded assumptions (e.g., block size of 16) and dtype/layout checks that may not generalize to future quantization formats. There is also a risk of regression if the quantization configuration or weight packing strategies change.

**Key insights**  
Developers should note that quantization-aware weight permutation requires careful, format-specific handling. Consider abstracting this logic into a quantization-format dispatcher to improve maintainability. Ensure thorough testing across all supported quantization types when modifying this code.

---

## 17. [Bump Flashinfer Version and Re-enable DeepSeek NVFP4 AR+Norm Fusion](https://github.com/vllm-project/vllm/pull/34899)


### Base Information

- **PR Number:** #34899
- **Author:** [wzhao18](https://github.com/wzhao18)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 13:37:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34899/files) (5):**
  - `docker/Dockerfile`
  - `docker/Dockerfile.nightly_torch`
  - `docker/versions.json`
  - `requirements/cuda.txt`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
This PR bumps FlashInfer from version 0.6.3 to 0.6.4 and removes the workaround that disabled the AllReduce-RMS-Norm fusion pass for DeepSeek-V3 models using NVFP4 quantization. The update incorporates a patch from FlashInfer that fixes a previously identified accuracy issue (#34395), allowing the fusion to be safely re-enabled by default.

**Technical impact**  
The changes update the FlashInfer dependency across the build system (Dockerfiles, versions.json, requirements) and remove the model-specific configuration logic that forced the fusion pass to be disabled for DeepSeek-V3 with NVFP4. This restores the performance benefits of the fusion optimization for these models without compromising accuracy.

**Potential risks**  
The primary risk is regression if the FlashInfer 0.6.4 patch does not fully resolve the accuracy issue under all conditions or introduces new incompatibilities. The removal of the `DeepseekV3ForCausalLM` configuration class also means that any future model-specific adjustments for DeepSeek-V3 would need to be re-implemented elsewhere.

**Key insights**  
Always verify that upstream dependency fixes are comprehensive before removing local workarounds. The test results show successful accuracy restoration on the GSM8K benchmark, but broader validation across different workloads and model variants is recommended. Ensure the FlashInfer version bump is synchronized across all configuration files to maintain build consistency.

---

## 18. [[AMD][CI] Fix test_custom_allreduce for A100 testgroup](https://github.com/vllm-project/vllm/pull/34735)


### Base Information

- **PR Number:** #34735
- **Author:** [rjrock](https://github.com/rjrock)
- **Merged By:** [gshtras](https://github.com/gshtras)
- **Merged time:** 2026-02-20 13:33:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34735/files) (1):**
  - `tests/distributed/test_custom_all_reduce.py`

### Summary

**What changed and why**  
The PR adds deletion of the `HIP_VISIBLE_DEVICES` environment variable in two test functions (`graph_allreduce` and `eager_allreduce`) to fix a test failure on AMD A100 hardware. This is necessary because ROCm's ray modifies `HIP_VISIBLE_DEVICES` (not `CUDA_VISIBLE_DEVICES`) when allocating GPUs, and leaving it set causes an "invalid device ordinal" error.

**Technical impact**  
The change ensures compatibility with AMD/ROCm platforms by clearing the correct environment variable that controls GPU visibility. It maintains existing behavior for CUDA platforms while preventing device assignment conflicts when tests run under ROCm with ray remote GPU allocation.

**Potential risks**  
If other ROCm-specific environment variables influence device visibility, similar issues could arise. The fix assumes `HIP_VISIBLE_DEVICES` is the only variable needing cleanup; unexpected interactions between CUDA and HIP environment variables on mixed-platform CI setups could still occur.

**Key insights**  
This highlights the importance of platform-specific environment handling in heterogeneous CI environments. Developers should verify similar patterns in other distributed tests and consider adding a helper function to abstract environment cleanup for both CUDA and ROCm backends.

---

## 19. [[CI] Revert PRs 34818 and 33600](https://github.com/vllm-project/vllm/pull/34979)


### Base Information

- **PR Number:** #34979
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 13:25:50
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34979/files) (16):**
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/utils.py`
  - `tests/v1/spec_decode/test_eagle.py`
  - `vllm/config/cache.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/layers/attention/chunked_local_attention.py`
  - `vllm/model_executor/layers/attention/mla_attention.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/interface.py`
  - `vllm/v1/attention/backend.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/executor/ray_executor.py`
  - `vllm/v1/executor/uniproc_executor.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR reverts two previous changes (PRs 34818 and 33600) that introduced a new dynamic block-size selection mechanism based on attention backends. The revert was necessary because the changes broke basic model tests and caused accuracy regressions in GSM8K evaluations for specific models. The rollback restores the previous behavior where block size defaults to 16 unless explicitly overridden.

**Technical impact**  
The revert removes the `update_block_size_for_backend()` platform method and associated logic that automatically adjusted block sizes per attention backend. Block size validation for DCP and Mamba constraints is moved back into `VllmConfig.check_and_update_config()`. The `CacheConfig.block_size` type is now restricted to a literal set (`BlockSize`), and default block size is set to 16 in CUDA platform initialization.

**Potential risks**  
Reverting may reintroduce earlier issues that PRs 34818 and 33600 aimed to fix, such as suboptimal block-size choices for certain attention backends. The hardcoded default of 16 could hurt performance for backends that prefer larger block sizes (e.g., FlashMLA prefers 64). There is also a risk of inconsistency if future changes reintroduce similar logic without proper testing.

**Key insights**  
The dynamic block-size selection approach was too disruptive and needs more thorough validation before reimplementation. Developers should ensure any future block-size adjustments are accompanied by comprehensive testing across model types and backends. Consider adding a dedicated test suite for block-size compatibility to prevent regression.

---

## 20. [[Test] Add FP8 KV Cache Testing for MLA Backends](https://github.com/vllm-project/vllm/pull/34473)


### Base Information

- **PR Number:** #34473
- **Author:** [wzhao18](https://github.com/wzhao18)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-20 10:51:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34473/files) (1):**
  - `tests/v1/attention/test_mla_backends.py`

### Summary

**What changed and why**  
This PR adds FP8 KV cache testing support for MLA backends by extending the existing test infrastructure. The changes enable testing with different FP8 variants (`"fp8"` and `"fp8_e4m3"`) alongside the existing `"auto"` dtype, and refactor the KV cache creation logic to handle multiple FP8 formats.

**Technical impact**  
The test now dynamically filters backends based on their supported KV cache dtypes, ensuring only compatible backends are tested. The KV cache creation and population logic has been generalized to support both `fp8_ds_mla` and other FP8 formats, with proper tensor reshaping for non-DeepSeek MLA FP8 layouts. The reference implementation in `TestMLAAttention` has been updated to handle FP8 query concatenation when supported.

**Potential risks**  
The assumption that `kv_cache_dtype.startswith("fp8")` reliably identifies all FP8 variants may be fragile if new dtypes are introduced. The `_decode_concat_quant_fp8_op` initialization in the test class could cause issues if the underlying operator expects specific configurations not covered by the test. There's also a risk of missing edge cases when `kv_cache_dtype` is `None` or an unsupported string.

**Key insights**  
Developers should verify that all intended FP8 variants are correctly detected by the `startswith("fp8")` logic. The test now provides better coverage for FP8 KV cache paths, but ensure the mock `_decode_concat_quant_fp8_op` aligns with production behavior. The dynamic backend filtering is a good practice that should be adopted in other similar tests to maintain compatibility.

---

## 21. [[CI] Remove failing prime-rl integration test](https://github.com/vllm-project/vllm/pull/34843)


### Base Information

- **PR Number:** #34843
- **Author:** [mgoin](https://github.com/mgoin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 10:17:42
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34843/files) (3):**
  - `.buildkite/scripts/run-prime-rl-test.sh`
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test_areas/e2e_integration.yaml`

### Summary

**What changed and why**  
This PR removes a failing Prime-RL integration test that has been consistently failing for several weeks. The changes delete the test script and remove all references to it from the CI pipeline configuration files. The stated purpose is to clean up the CI pipeline by removing a broken test, with the option to restore it later if needed.

**Technical impact**  
The removal eliminates a blocking or flaky test from the CI pipeline, which should improve pipeline reliability and reduce noise from test failures. This affects both AMD and NVIDIA GPU test configurations, as the test was included in multiple pipeline definitions. The integration test environment setup and execution logic are completely removed from the codebase.

**Potential risks**  
There's a risk of losing test coverage for Prime-RL integration functionality, which could allow regressions to go undetected. The test was marked as optional and had soft_fail enabled in some configurations, but its complete removal means no one will be alerted if Prime-RL compatibility breaks. Additionally, if the test failure was symptomatic of a deeper integration issue, that problem may now go unaddressed.

**Key insights**  
This is a pragmatic cleanup of a consistently failing test, but consider documenting the removal in a tracking issue to facilitate future restoration. Developers should be aware that Prime-RL integration is no longer being validated in CI, so manual verification may be needed when making changes that could affect reinforcement learning integrations. The removal follows good practice by cleaning up associated configuration entries alongside the main test script.

---

## 22. [[compile] Fix torch.compile time discrepancy in logging.](https://github.com/vllm-project/vllm/pull/34912)


### Base Information

- **PR Number:** #34912
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 08:47:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34912/files) (3):**
  - `vllm/compilation/backends.py`
  - `vllm/compilation/monitor.py`
  - `vllm/compilation/piecewise_backend.py`

### Summary

**What changed and why**  
The changes fix a torch.compile time logging discrepancy by switching from summing individual compilation phases to measuring end-to-end wall time. This addresses user confusion where reported compilation times didn't match actual elapsed time between log entries. The PR also replaces `time.time()` with `time.perf_counter()` for more accurate, monotonic benchmarking.

**Technical impact**  
These modifications ensure compilation time metrics now reflect the true total elapsed time from start to finish of the torch.compile process. The logging output will show more accurate timing information, including the time spent saving compiler manager cache when it exceeds 1 second. The switch to `perf_counter()` provides better precision and monotonic behavior for performance measurements.

**Potential risks**  
The cache saving operation timing could introduce overhead if called frequently, though the 1-second threshold mitigates this. There's a risk that the new total time calculation might include unexpected overhead not previously measured, potentially showing longer times than before. The changes assume `perf_counter()` is available and suitable across all deployment environments.

**Key insights**  
Developers should verify that the new timing calculations align with actual user experience across different hardware configurations. The cache saving logging threshold (1 second) should be monitored to ensure it's appropriate for various deployment scenarios. These changes improve transparency but may reveal previously hidden compilation overhead that warrants further optimization.

---

## 23. [[compile] Move torch_aot_compile directory under torch_compile_cache](https://github.com/vllm-project/vllm/pull/34831)


### Base Information

- **PR Number:** #34831
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 08:46:45
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34831/files) (1):**
  - `vllm/compilation/decorators.py`

### Summary

**What changed and why**  
The change moves the `torch_aot_compile` cache directory to be a subdirectory under `torch_compile_cache`. This consolidation addresses user confusion where deleting only `torch_compile_cache/` did not invalidate all compilation caches, as `torch_aot_compile` was previously a separate, parallel directory.

**Technical impact**  
This modifies the filesystem layout for AOT compilation cache artifacts. The cache path changes from `VLLM_CACHE_ROOT/torch_aot_compile/{hash}/` to `VLLM_CACHE_ROOT/torch_compile_cache/torch_aot_compile/{hash}/`. Existing caches at the old location will become orphaned and will not be used after this change.

**Potential risks**  
There is a risk of cache invalidation for users upgrading, as existing AOT compilation caches will not be found at the new location, potentially causing recompilation overhead. The change does not include migration logic for old cache directories, which could lead to disk space waste if old caches are not cleaned up.

**Key insights**  
This is a sensible consolidation for cache management. Developers should be aware that this is a breaking change for the cache location. Consider adding a note in release documentation about potential recompilation. For future similar changes, implementing cache migration or cleanup logic could improve the user experience.

---

## 24. [[Kernel] [Helion] [6/N] Add num_tokens dimension to silu_mul autotuning and dispatching](https://github.com/vllm-project/vllm/pull/34185)


### Base Information

- **PR Number:** #34185
- **Author:** [gmagogsfm](https://github.com/gmagogsfm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 08:36:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34185/files) (3):**
  - `tests/kernels/helion/test_silu_mul_fp8.py`
  - `vllm/kernels/helion/configs/silu_mul_fp8.json`
  - `vllm/kernels/helion/ops/silu_mul_fp8.py`

### Summary

**What changed and why**  
The PR adds a `num_tokens` dimension to the silu_mul_fp8 kernel's autotuning and dispatching logic. Previously, the kernel was only tuned for a fixed `num_tokens=256`, which caused suboptimal performance for smaller token counts. The changes introduce config keys formatted as `intermediate_{size}_numtokens_{tokens}` and update the config picker to select the best configuration based on both intermediate size and token count.

**Technical impact**  
The kernel now supports fine‑grained autotuning across 306 combinations of intermediate sizes and token counts, improving performance across varying input shapes. The config picker uses a ceiling strategy for token counts (selecting the smallest available `num_tokens` ≥ input tokens) and falls back to the largest available if the input exceeds all tuned values. This enhances runtime efficiency, especially for smaller batch sizes.

**Potential risks**  
The strict validation of config key format may break existing configurations that do not follow the new naming convention. Additionally, the ceiling selection strategy could lead to suboptimal performance if the tuned `num_tokens` values are sparse or if the input shape distribution is not well‑covered by the autotuned set. The large number of new configs (+55k lines) may increase memory overhead for config management.

**Key insights**  
Developers should ensure all existing configs are migrated to the new `intermediate_{int}_numtokens_{int}` format. The addition of H200 autotuned configs and updated device‑name validation improves hardware compatibility. The unit tests comprehensively validate the selection logic, including edge cases for malformed keys and fallback behavior, which should be maintained as the kernel evolves.

---

## 25. [Ensure that MkDocs v2 does not get installed](https://github.com/vllm-project/vllm/pull/34958)


### Base Information

- **PR Number:** #34958
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [hmellor](https://github.com/hmellor)
- **Merged time:** 2026-02-20 07:38:12
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34958/files) (1):**
  - `requirements/docs.txt`

### Summary

**What changed and why**  
The change pins MkDocs to versions below 2.0.0 in the documentation requirements file. This ensures compatibility with existing plugins that may not yet support MkDocs v2, as mentioned in the PR description. The team is considering a future migration to Zensical instead.

**Technical impact**  
This prevents accidental installation of MkDocs 2.0.0 or higher when setting up the documentation environment, maintaining current plugin functionality. It's a defensive version constraint that avoids breaking the documentation build process due to incompatible dependencies.

**Potential risks**  
If MkDocs 2.0.0 introduces critical security fixes or essential features, this constraint could block those updates. There's also a risk that other dependencies might eventually require MkDocs >=2.0.0, creating version conflicts. The team should monitor plugin compatibility timelines.

**Key insights**  
This is a temporary compatibility measure that should be documented in the project's upgrade plan. Consider adding a comment in the requirements file explaining why MkDocs is pinned, and establish a timeline for evaluating Zensical or updating plugins to support MkDocs v2.

---

## 26. [[perf] Avoid dtype promotion sync in mamba_get_block_table_tensor](https://github.com/vllm-project/vllm/pull/34870)


### Base Information

- **PR Number:** #34870
- **Author:** [hl475](https://github.com/hl475)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 06:21:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34870/files) (1):**
  - `vllm/v1/attention/backends/utils.py`

### Summary

**What changed and why**  
The change modifies the `offsets` tensor in `mamba_get_block_table_tensor` from `int64` to `int32` to avoid a costly dtype promotion and synchronization when adding `int32` (`start_indices`) to `int64` (`offsets`). The result is then explicitly cast to `int64` before the `torch.gather` operation, which requires Long (int64) indices.

**Technical impact**  
This eliminates an implicit `aten::to` dtype promotion that was causing GPU synchronization overhead, improving performance in linear attention workloads. The arithmetic now occurs entirely in `int32`, with a single explicit cast to `int64` only where necessary for downstream operations.

**Potential risks**  
If `offsets` values exceed the `int32` range (~2.1 billion), overflow could occur, though this is unlikely given typical block table sizes. The explicit `.to(torch.int64)` adds a minimal operation but ensures compatibility with `torch.gather`, which expects `int64` indices.

**Key insights**  
Always be mindful of implicit dtype promotions in performance-critical paths, as they can introduce unexpected synchronization. Explicitly managing dtypes in tensor operations—especially when mixing types—can yield significant performance gains. Verify that any dtype changes do not risk overflow in the intended use cases.

---

## 27. [[Refactor] Extract Harmony streaming SSE event builders into streaming_events.py](https://github.com/vllm-project/vllm/pull/34909)


### Base Information

- **PR Number:** #34909
- **Author:** [sfeng33](https://github.com/sfeng33)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 06:20:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34909/files) (2):**
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/openai/responses/streaming_events.py`

### Summary

**What changed and why**  
This PR extracts pure-function event builders from the `serving.py` class into a new `streaming_events.py` module. The primary goals are to reduce the size of `serving.py` (by ~800 lines), make event builders independently testable, and prepare for future unification of SSE event emission across different streaming paths (Harmony, Simple, and upcoming Parsable).

**Technical impact**  
The refactor moves ~900 lines of event-generation logic into a dedicated module, transforming instance methods into standalone functions. This improves modularity and separation of concerns—event building is now decoupled from the streaming processor logic. The `HarmonyStreamingState` dataclass and helper functions like `is_mcp_tool_by_namespace` are also moved, enabling reuse across different streaming contexts.

**Potential risks**  
The extraction changes the function signatures from instance methods to pure functions, which could introduce subtle bugs if any hidden dependencies on class state remain. Additionally, the `TOOL_NAME_TO_MCP_SERVER_LABEL` mapping is now a module-level constant; changes to this mapping must be synchronized across all usages. The refactor also increases the number of imports and could affect startup time slightly.

**Key insights**  
This is a well-scoped refactor that sets the stage for a more maintainable and testable codebase. Developers should verify that all event builders are truly stateless and that no implicit dependencies on the original class exist. Future PRs can now build on this to unify streaming event processors, reducing duplication and complexity.

---

## 28. [[V0 Deprecation] Remove unused MM placeholders in request output](https://github.com/vllm-project/vllm/pull/34944)


### Base Information

- **PR Number:** #34944
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 06:19:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34944/files) (1):**
  - `vllm/outputs.py`

### Summary

**What changed and why**  
Removed unused `multi_modal_placeholders` field and its import from the `RequestOutput` class. This field was only used in the deprecated V0 API and is no longer needed.

**Technical impact**  
This change simplifies the `RequestOutput` class by removing dead code, reducing memory usage slightly per request, and eliminating an unnecessary dependency on `MultiModalPlaceholderDict`. The class interface is now cleaner and more focused on current functionality.

**Potential risks**  
If any downstream code still references `multi_modal_placeholders` attribute, it will now raise `AttributeError`. The removal of the import could break other modules if they rely on `vllm.multimodal.inputs` being imported via this file, though this seems unlikely.

**Key insights**  
This is a straightforward cleanup of deprecated functionality. Developers should verify no external codebases depend on the removed field. The change aligns with good maintenance practices by removing unused code paths after API version transitions.

---

## 29. [[BUGFIX] Fix `_dummy_run` missing `prepare_inputs_event` synchronization](https://github.com/vllm-project/vllm/pull/34866)


### Base Information

- **PR Number:** #34866
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-20 05:54:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34866/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The fix adds synchronization around `_dummy_run`'s pinned CPU buffer operations by wrapping them in `self.synchronize_input_prep()`. This ensures `_dummy_run` participates in the same event protocol (`prepare_inputs_event`) used by `execute_model`, preventing race conditions where back-to-back dummy or real steps overwrite buffers while non-blocking DMA transfers are still in progress.

**Technical impact**  
This change aligns `_dummy_run` with the synchronization mechanism of `execute_model`, ensuring safe concurrent access to shared pinned CPU buffers (e.g., `seq_lens`, `query_start_loc`). It eliminates `cudaErrorIllegalAddress` errors caused by asynchronous DMA reads from corrupted buffer contents, improving stability in async scheduling scenarios with idle DP workers.

**Potential risks**  
Introducing synchronization may slightly increase latency for dummy runs, though this is negligible given their coordination purpose. If `synchronize_input_prep()` has side effects or dependencies not fully accounted for, it could inadvertently affect other operations. The fix assumes the event protocol is thread-safe and correctly implemented for all buffer access patterns.

**Key insights**  
Always extend synchronization protocols to all code paths sharing hardware resources like pinned CPU buffers. The root cause highlights a subtle bug in async GPU workflows where missing synchronization on auxiliary paths (`_dummy_run`) can corrupt main execution. Developers should audit similar helper functions for consistent use of resource-protection mechanisms.

---

## 30. [[Kernel] Optimize grouped topk kernel](https://github.com/vllm-project/vllm/pull/34206)


### Base Information

- **PR Number:** #34206
- **Author:** [xyang16](https://github.com/xyang16)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-20 01:34:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34206/files) (3):**
  - `csrc/moe/grouped_topk_kernels.cu`
  - `csrc/moe/moeTopKFuncs.cuh`
  - `tests/kernels/moe/test_grouped_topk.py`

### Summary

**What changed and why**  
A new CUDA kernel `grouped_topk_fused_small_expert_count_kernel` was added, optimized for MoE models with small expert counts (≤512 experts per group). It uses warp-level reductions and shared memory to accelerate top‑K selection for models like DeepSeek‑R1, Kimi‑K2.5, and Nemotron. The existing kernel remains as a fallback for larger expert counts.

**Technical impact**  
The kernel reduces top‑K computation latency by ~43% (from 7µs to 4µs in profiling) for supported models. This improves end‑to‑end throughput by 1–5% across benchmarks. The changes introduce a new header (`moeTopKFuncs.cuh`) with reusable top‑K reduction utilities and maintain backward compatibility via a conditional dispatch.

**Potential risks**  
The optimization relies on compile‑time template parameters (e.g., `MaxNumExperts`), which could limit flexibility if future models exceed the current expert‑count thresholds. Edge cases with very large `topk` values (up to 22) are supported but may stress shared‑memory limits. The fallback to the original kernel must be correctly triggered to avoid performance regression for unsupported configurations.

**Key insights**  
The kernel effectively leverages warp‑centric parallelism and shared‑memory buffers for small expert counts, a common pattern in modern MoE architectures. Developers should ensure the dispatch logic matches the documented expert‑count limits. The added unit tests cover key configurations, but profiling on diverse hardware is recommended to validate performance gains across platforms.

---

