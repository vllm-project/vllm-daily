# vLLM Merged PR Report

**Report Date:** 2026-02-04 PST

**Total Merged PRs:** 41

---

## 1. [[KV Connector][Metrics] Do not count local prefix cache hits in connector queries](https://github.com/vllm-project/vllm/pull/30522)


### Base Information

- **PR Number:** #30522
- **Author:** [markmc](https://github.com/markmc)
- **Merged By:** [orozery](https://github.com/orozery)
- **Merged time:** 2026-02-04 23:57:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30522/files) (3):**
  - `tests/v1/core/test_scheduler.py`
  - `tests/v1/kv_connector/unit/test_invalid_blocks_correctness.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The PR fixes double-counting in the `vllm:external_prefix_cache_queries` metric by ensuring it only counts tokens actually queried from the KV connector, excluding those already found in the local prefix cache. Previously, the metric incorrectly counted all prompt tokens, leading to inflated query counts. The change also adjusts the timing of metric recording to capture connector hits even if subsequent loads fail.

**Technical impact**  
The scheduler now tracks connector-specific queries and hits separately from local cache stats, ensuring accurate external cache performance measurement. Metrics are recorded earlier in the request lifecycle (during scheduling) rather than after completion, which better reflects attempted external cache usage. The `connector_prefix_cache_stats` are now managed directly in `make_stats()` instead of via helper methods.

**Potential risks**  
If local cache hits are miscalculated (e.g., due to incorrect token accounting), external query counts could be underreported. The early recording of hits before load completion might count tokens that ultimately fail to transfer, though this is intentional per the PR description. Changes to stat reset logic in `make_stats()` could affect metric aggregation if called multiple times per cycle.

**Key insights**  
Developers should verify that local and external cache token calculations align with actual cache layers. The new approach provides clearer insight into external cache effectiveness but requires careful validation of token accounting in edge cases (e.g., preempted requests). The updated tests comprehensively cover scenarios with and without local cache hits, ensuring metric accuracy across configurations.

---

## 2. [[Perf] Optimize the performance of structured output + reasoning](https://github.com/vllm-project/vllm/pull/33557)


### Base Information

- **PR Number:** #33557
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-04 23:45:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33557/files) (4):**
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/v1/engine/__init__.py`
  - `vllm/v1/request.py`
  - `vllm/v1/structured_output/__init__.py`

### Summary

**What changed and why**  
This PR optimizes structured output + reasoning performance by moving the `reasoning_parser` and its `is_reasoning_end()` check from the core engine to the frontend (OpenAI API server). This addresses an inconsistency where `enable_thinking` parameter wasn't properly considered in `StructuredOutputManager`, causing mismatched behavior between frontend and engine.

**Technical impact**  
The reasoning parser is now instantiated once per request in the frontend, with proper chat template kwargs, and its `reasoning_ended` state is passed to the engine via a new `reasoning_ended` field in `EngineCoreRequest` and `Request`. This reduces redundant computations in the core engine and ensures consistent parser behavior across components.

**Potential risks**  
The change introduces a new field (`reasoning_ended`) that must be properly synchronized between frontend and engine. There's a risk of regression if other code paths (like `openai_gptoss`) don't handle this field correctly. The conditional logic in `structured_output/__init__.py` indicates transitional complexity that needs eventual cleanup.

**Key insights**  
This is a performance optimization that also fixes a bug by ensuring consistent reasoning parser initialization. Developers should verify that all API entrypoints properly propagate the `reasoning_ended` field. The commented code about `openai_gptoss` unification suggests future refactoring is planned once code paths are consolidated.

---

## 3. [[CI/Build] Fix CPU CI test case title](https://github.com/vllm-project/vllm/pull/33870)


### Base Information

- **PR Number:** #33870
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-04 23:07:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33870/files) (1):**
  - `.buildkite/hardware_tests/cpu.yaml`

### Summary

**What changed and why**  
The change modifies a Buildkite pipeline step label from "CPU-TP/DP/PP Tests" to "CPU-Distributed Tests". This addresses a potential naming rule violation in Buildkite, likely related to special characters or formatting constraints in step labels.

**Technical impact**  
This is a minor metadata change that only affects the display name of the CI test step in Buildkite's UI. It does not alter the actual test execution, dependencies, or pipeline logic, so there is no functional impact on the build process or test results.

**Potential risks**  
The risk is minimal since it's purely a label change. However, if any external systems or documentation reference the old label name, they may become out of sync. There is also a slight chance the new label could conflict with existing step names if not unique.

**Key insights**  
Always validate CI/CD configuration against platform-specific naming rules to avoid pipeline failures. Consider updating any related documentation or scripts that might reference the old label. For future changes, include a brief note on the specific naming rule being addressed to improve clarity.

---

## 4. [[CPU][BugFix] Allow w8a8 oneDNN quantized matmul to support 3D inputs](https://github.com/vllm-project/vllm/pull/33727)


### Base Information

- **PR Number:** #33727
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-04 22:26:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33727/files) (1):**
  - `vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py`

### Summary

**What changed and why**  
The fix modifies the `_apply_weights_onednn` method to support 3D inputs for w8a8 quantized matrix multiplication on CPU. It reshapes 3D tensors to 2D before the quantized matmul operation and then reshapes the output back to the original 3D structure, enabling support for models like RedHatAI/whisper-large-v3-quantized.w8a8.

**Technical impact**  
This change extends the oneDNN quantized matmul kernel's compatibility to handle batched inputs (3D tensors) by adding pre- and post-processing reshape operations. The core matmul operation continues to work with 2D tensors as expected by the underlying oneDNN library, maintaining computational correctness while expanding input dimensionality support.

**Potential risks**  
The reshape operations assume the last dimension is the feature dimension and flatten all preceding dimensions, which could cause issues with 4D+ inputs. There's also a performance overhead from the extra reshape operations, though this is necessary for compatibility. The fix doesn't address the underlying oneDNN kernel's limitation with 3D inputs directly.

**Key insights**  
This is a pragmatic workaround that enables immediate support for 3D inputs without modifying the low-level oneDNN kernels. Developers should be aware that this approach may need refinement if support for higher-dimensional inputs (4D+) is required. The reshape logic correctly preserves the original tensor structure for 3D inputs while enabling the quantized matmul to function.

---

## 5. [[Bugfix] Fix ScoreMultiModalParam multi-document scoring returning single result](https://github.com/vllm-project/vllm/pull/33837)


### Base Information

- **PR Number:** #33837
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-04 22:17:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33837/files) (1):**
  - `tests/models/multimodal/pooling/test_jinavl_reranker.py`

### Summary

**What changed and why**  
The PR fixes a regression where a single `ScoreMultiModalParam` containing multiple content items (e.g., two images) passed to `LLM.score()` incorrectly returned only one score instead of one per content item. The root cause was in `validate_score_input()` incorrectly grouping multiple content items into a single scoring request. The fix modifies the test helper `create_score_multimodal_param` to return a list of individual `ScoreMultiModalParam` objects (one per content item), aligning the test's input format with the expected behavior.

**Technical impact**  
This change ensures the test suite correctly validates that the scoring system processes each content item within a multimodal parameter as a separate document. The test now iterates through documents individually when calling the Hugging Face reference implementation, matching the expected per-item scoring behavior and fixing assertion errors about output length mismatches.

**Potential risks**  
The test changes assume the underlying bug fix in `validate_score_input()` correctly expands a single multi-content `ScoreMultiModalParam` into multiple requests. If that fix is incomplete or regresses, the tests may pass incorrectly because they now manually split the content, masking the original issue. The simplified HF scoring loop loses the parallel batching of same-type documents, potentially affecting performance comparisons in benchmarks.

**Key insights**  
The test refactor correctly mirrors the intended API behavior: each content item should be scored independently. Developers should verify the production fix in `validate_score_input()` handles the expansion logic robustly, including edge cases with mixed content types. The removal of type-based batching in the test is acceptable for correctness validation but should be noted if performance regression testing is needed.

---

## 6. [[CI/Build] Parallelize CPU CI tests](https://github.com/vllm-project/vllm/pull/33778)


### Base Information

- **PR Number:** #33778
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-04 21:53:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33778/files) (6):**
  - `.buildkite/hardware_tests/arm.yaml`
  - `.buildkite/hardware_tests/cpu.yaml`
  - `.buildkite/hardware_tests/intel.yaml`
  - `.buildkite/scripts/hardware_ci/run-cpu-distributed-smoke-test.sh`
  - `.buildkite/scripts/hardware_ci/run-cpu-test.sh`
  - `vllm/v1/worker/cpu_worker.py`

### Summary

**What changed and why**  
This PR restructures CPU CI testing to enable parallel execution by splitting monolithic tests into specialized job categories (kernel, model, quantization, distributed, multimodal). It removes the unified CPU test from intel.yaml, creates a dedicated cpu.yaml with parallel test groups, adds a distributed smoke test script, simplifies the main test runner, and enhances CPU worker logic to simulate multi-NUMA environments for testing.

**Technical impact**  
The changes significantly improve CI efficiency through parallel test execution, reducing overall test runtime. The architecture now supports targeted test dependencies via `source_file_dependencies`, enabling smarter CI triggering. The simplified test runner becomes more maintainable by delegating specific test commands to the pipeline definition, while the CPU worker modification allows better testing of NUMA-aware scheduling in constrained environments.

**Potential risks**  
The removal of AVX2-specific testing could miss architecture-specific issues. The distributed smoke test uses hardcoded model paths and parameters that may become outdated. The NUMA simulation logic introduces complexity that could mask real NUMA binding issues. Parallel test execution may increase resource contention on shared CI infrastructure.

**Key insights**  
Developers should ensure new CPU-related tests are added to appropriate job categories in cpu.yaml. The source dependency tracking is a powerful feature for optimizing CI runs—maintain accurate file mappings. Consider adding back AVX2 testing if architecture coverage is critical. Monitor test flakiness due to increased parallelism and resource sharing.

---

## 7. [[2/N] move responses/serving _make_response_output_items logic to parser](https://github.com/vllm-project/vllm/pull/33281)


### Base Information

- **PR Number:** #33281
- **Author:** [qandrew](https://github.com/qandrew)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-04 21:46:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33281/files) (2):**
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/parser/abstract_parser.py`

### Summary

**What changed and why**  
This PR moves the logic for parsing model outputs (reasoning, content, and tool calls) from the OpenAI Responses serving module into the abstract parser class. The goal is to centralize output parsing so that different models can implement custom parsers, enabling support for parallel tool calling, interleaved reasoning, and easier migration of GPT-OSS into the parser structure.

**Technical impact**  
The `_make_response_output_items` method in `serving.py` is significantly simplified, delegating parsing to the parser’s new `extract_response_outputs` method. The abstract parser now includes a concrete implementation that handles reasoning extraction, tool call parsing, and construction of `ResponseOutputItem` objects. This change promotes separation of concerns and makes the parsing logic reusable across different model types.

**Potential risks**  
If a custom parser does not properly implement `extract_response_outputs`, fallback behavior may break or produce incorrect output structures. The refactored tool-call parsing logic must align with existing behavior for forced, required, and automatic tool calls. Additionally, error handling in the parser layer needs to be robust to avoid silent failures in response generation.

**Key insights**  
Developers should ensure custom parsers inherit from `AbstractParser` and correctly override `extract_response_outputs`. The parser now centrally manages output item creation, reducing duplication. When adding new model-specific parsers, verify that reasoning extraction and tool-call parsing integrate seamlessly with the existing request handling flow.

---

## 8. [[CI][AMD][BugFix] Ensure VLLM_ROCM_USE_AITER is set so test_rocm_aiter_topk.py can run correctly](https://github.com/vllm-project/vllm/pull/33840)


### Base Information

- **PR Number:** #33840
- **Author:** [rasmith](https://github.com/rasmith)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-04 21:05:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33840/files) (1):**
  - `tests/kernels/moe/test_rocm_aiter_topk.py`

### Summary

**What changed and why**  
The PR fixes a broken test by ensuring the `VLLM_ROCM_USE_AITER` environment variable is set to "1" before importing ROCm AITER operations. This is necessary because the test `test_rocm_aiter_topk.py` validates the correct loading of these custom ops, which requires either this environment variable or a specific backend selection to be active.

**Technical impact**  
The changes restructure the test's import logic and conditional skipping. The environment variable is now set at the module level before any relevant imports, guaranteeing the AITER ops are registered. The platform check (`is_rocm()`) and package availability check (`aiter_available`) have been separated into distinct `pytest.skip` calls for clearer failure reporting.

**Potential risks**  
If other tests in the same module run later and depend on a different state of the `VLLM_ROCM_USE_AITER` variable, this module-level assignment could cause side effects or pollution. The test now assumes the AITER package is the only required condition after confirming the ROCm platform, which might mask other environmental or configuration issues.

**Key insights**  
Always set required environment variables before importing modules that depend on them, especially for conditional op registration. Separating skip conditions improves test diagnostics. Developers should be cautious of module-level state changes that could affect other tests and consider using fixtures or teardown procedures for environment isolation.

---

## 9. [[docs] fix unintentional misspellings](https://github.com/vllm-project/vllm/pull/33863)


### Base Information

- **PR Number:** #33863
- **Author:** [rinbaro](https://github.com/rinbaro)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 20:50:59
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33863/files) (3):**
  - `docs/contributing/model/basic.md`
  - `docs/contributing/model/multimodal.md`
  - `docs/getting_started/quickstart.md`

### Summary

**What changed and why**  
This PR fixes three unintentional spelling errors in documentation files: "posses" corrected to "possess" in a technical description, "promopt" corrected to "prompt" in a multimodal processing example, and "vlm-dev" corrected to "vllm-dev" in a Docker image reference.

**Technical impact**  
These changes have no impact on the codebase, runtime behavior, or system architecture. They are purely cosmetic documentation corrections that improve readability and professionalism.

**Potential risks**  
There are no technical risks. The only minor risk is if the corrected term "vllm-dev" was actually an incorrect reference, but the fix appears accurate as it aligns the Docker image name with the project name (vLLM).

**Key insights**  
Maintaining correct spelling in documentation is important for clarity and project credibility. While these are minor fixes, they demonstrate good attention to detail. Developers should continue to proofread documentation changes, as even small errors can confuse readers, especially in technical contexts.

---

## 10. [[Minor] Include `StreamingInput` in inputs package](https://github.com/vllm-project/vllm/pull/33856)


### Base Information

- **PR Number:** #33856
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-04 20:38:21
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33856/files) (4):**
  - `tests/v1/e2e/test_streaming_input.py`
  - `tests/v1/streaming_input/test_async_llm_streaming.py`
  - `vllm/inputs/__init__.py`
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
The changes move `StreamingInput` from a direct import within the `vllm.inputs.data` module to being exported through the main `vllm.inputs` package. This is a follow-on refactoring from recently introduced streaming inputs support, making the import path consistent with other input types and simplifying the import statements.

**Technical impact**  
This refactoring improves API consistency by exposing `StreamingInput` through the canonical `vllm.inputs` public interface. It reduces import complexity across the codebase and aligns with standard Python packaging patterns where the main package module (`__init__.py`) re-exports important public symbols.

**Potential risks**  
Minimal risk since this is primarily an import path change. However, any external code that directly imports from `vllm.inputs.data` (outside the changed files) would break. The changes appear limited to internal test files and one engine file, suggesting this risk is contained.

**Key insights**  
This is a clean-up change that enhances maintainability by centralizing exports. Developers should now import `StreamingInput` exclusively from `vllm.inputs`. The TODO comment about avoiding re-validation of sampling parameters is unrelated but worth noting for future optimization work.

---

## 11. [Revert "[Attention][FA3] Update FA3 to include new swizzle optimization"](https://github.com/vllm-project/vllm/pull/33841)


### Base Information

- **PR Number:** #33841
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 19:54:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33841/files) (3):**
  - `cmake/external_projects/vllm_flash_attn.cmake`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`

### Summary

**What changed and why**  
This PR reverts a previous optimization (#23465) that introduced a swizzle optimization for Flash Attention v3 (FA3). The revert was necessary because the optimization broke distributed tests on 2-GPU H100 systems, specifically causing failures in `test_async_tp.py::test_async_pass_correctness`. The changes roll back the Flash Attention git commit and adjust the `scheduler_metadata` tensor size calculation to its original state.

**Technical impact**  
Reverting the optimization removes the swizzle-related changes, which likely affected memory layout or kernel execution patterns in multi-GPU setups. The `scheduler_metadata` tensor size is reduced from `max(max_cudagraph_size, max_num_seqs) * 4 + 1` to simply `max_num_seqs + 1`, simplifying memory allocation but potentially losing optimizations for CUDA graph captures. The Flash Attention dependency is rolled back to an earlier, stable commit.

**Potential risks**  
The revert may reintroduce performance regressions or inefficiencies that the original optimization aimed to address, particularly in CUDA graph scenarios. There is also a risk that the underlying issue causing test failures is not fully understood, which could surface in other configurations. The change in tensor size might affect systems where `max_cudagraph_size` significantly exceeds `max_num_seqs`.

**Key insights**  
This highlights the sensitivity of distributed multi-GPU tests to low-level attention kernel optimizations. Developers should ensure that performance optimizations are rigorously tested in distributed environments before merging. Consider isolating the root cause of the test failure—possibly related to tensor size or memory alignment—before attempting a revised optimization. Monitoring performance metrics post-revert is advisable to quantify any trade-offs.

---

## 12. [[CI][Bugfix]: return McpCall for built-in MCP tools in non-streaming mode](https://github.com/vllm-project/vllm/pull/32762)


### Base Information

- **PR Number:** #32762
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-04 19:14:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32762/files) (6):**
  - `tests/entrypoints/openai/responses/test_harmony.py`
  - `tests/entrypoints/openai/responses/test_mcp_tools.py`
  - `tests/entrypoints/openai/responses/test_parsable_context.py`
  - `tests/entrypoints/openai/responses/test_simple.py`
  - `tests/utils.py`
  - `vllm/entrypoints/openai/parser/harmony_utils.py`

### Summary

**What changed and why**  
The changes fix inconsistent behavior between streaming and non-streaming responses for built-in MCP tools (python, browser, container). Previously, these tools correctly returned `McpCall` items in streaming mode but incorrectly returned `ResponseReasoningItem` items in non-streaming mode. The fix ensures both modes consistently produce `McpCall` output items for built-in tools.

**Technical impact**  
The core change introduces a mapping (`_BUILTIN_TOOL_TO_MCP_SERVER_LABEL`) to properly route built-in tool recipients to the MCP call parser. This affects the `parse_output_message()` and `parse_remaining_state()` functions in `harmony_utils.py`, ensuring unified output item generation. Test updates include dependency management, flaky test handling, and improved server cleanup with GPU memory monitoring.

**Potential risks**  
The mapping approach assumes all built-in tool recipients are covered; new built-in tools would need explicit additions. Changes to `parse_remaining_state()` could affect edge cases where recipient parsing differs between streaming and non-streaming paths. The GPU memory cleanup logic introduces new failure modes if memory doesn't stabilize within the timeout.

**Key insights**  
The fix highlights the importance of consistent output item types across API modes. Developers should verify that any new built-in tools are added to the `_BUILTIN_TOOL_TO_MCP_SERVER_LABEL` mapping. The test improvements demonstrate proactive handling of model variability and resource cleanup, which are critical for reliable CI/CD pipelines.

---

## 13. [[release] Minor fixes to release annotation](https://github.com/vllm-project/vllm/pull/33849)


### Base Information

- **PR Number:** #33849
- **Author:** [khluu](https://github.com/khluu)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-04 18:07:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33849/files) (1):**
  - `.buildkite/scripts/annotate-release.sh`

### Summary

**What changed and why**  
The changes reorganize the release annotation script to improve logical flow and fix image tagging for ROCm repositories. Specifically, the script now groups commands by architecture type (CUDA, ROCm, CPU) and removes the redundant `-rocm` suffix from tags in the `vllm-openai-rocm` repository, while maintaining proper base image tagging.

**Technical impact**  
This restructuring improves the readability and maintainability of the release process by grouping related operations together. The removal of the `-rocm` suffix from version tags standardizes the tagging convention across repositories, potentially simplifying downstream consumption of these images.

**Potential risks**  
If any existing automation or documentation references the old `v${RELEASE_VERSION}-rocm` tag format, it could break. The manifest creation commands were moved after all tagging operations, which is correct, but the script's execution order must be preserved to avoid pushing manifests before their constituent images exist.

**Key insights**  
The reorganization is a positive change for script clarity. Developers should verify that no external systems depend on the old `-rocm` suffix in tag names. The grouping of operations by architecture (CUDA, ROCm, CPU) followed by manifest creation is a logical and recommended pattern for multi-arch Docker workflows.

---

## 14. [[Bugfix] fix DeepSeek R1 with CUTLASS MLA Broken on B200](https://github.com/vllm-project/vllm/pull/33637)


### Base Information

- **PR Number:** #33637
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-04 17:28:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33637/files) (1):**
  - `vllm/model_executor/layers/attention/mla_attention.py`

### Summary

**What changed and why**  
The PR removes the `q_pad_num_heads` parameter from the `__init__` method of the MLA attention layer and instead retrieves it from the `impl` attribute after initialization. This fixes an issue where DeepSeek R1 with CUTLASS MLA was broken on B200 hardware, likely due to a mismatch in how padding parameters are handled between the layer and its underlying implementation.

**Technical impact**  
This change decouples the padding configuration from the layer's constructor, making it dependent on the internal implementation's attribute. The architecture now allows the implementation object (`self.impl`) to determine the appropriate padding, which could vary based on hardware backend or kernel selection. This improves abstraction by letting implementations manage their own padding requirements.

**Potential risks**  
If the `impl` object doesn't have a `q_pad_num_heads` attribute, `getattr` will return `None`, which might cause issues if downstream code expects a specific value. There's also a risk of inconsistent behavior if different implementations handle this attribute differently. The change assumes all relevant implementations properly set this attribute.

**Key insights**  
This fix demonstrates proper abstraction by moving hardware-specific configuration to the implementation layer. Developers should ensure that any custom attention implementations properly expose the `q_pad_num_heads` attribute when needed. The pattern of using `getattr` with a default provides flexibility but requires careful testing across different hardware platforms.

---

## 15. [[Bugfix] Disable TRTLLM attention when KV transfer is enabled](https://github.com/vllm-project/vllm/pull/33192)


### Base Information

- **PR Number:** #33192
- **Author:** [ZhanqiuHu](https://github.com/ZhanqiuHu)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-04 16:49:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33192/files) (1):**
  - `vllm/v1/attention/backends/flashinfer.py`

### Summary

**What changed and why**  
The fix automatically disables TRTLLM attention kernels when KV transfer (P/D disaggregation) is enabled. This addresses a crash on Blackwell GPUs where NixlConnector creates non-contiguous KV cache views, which TRTLLM kernels cannot handle, causing an `AssertionError` on `is_strictly_contiguous(kv_cache_permute)`.

**Technical impact**  
When `kv_transfer_config` is set, the code now falls back to FlashInfer's native attention kernels for both prefill and decode phases. This ensures compatibility with non-contiguous KV cache tensors introduced by disaggregation, maintaining system stability without altering the external API or performance for standard configurations.

**Potential risks**  
Disabling TRTLLM kernels may reduce attention performance on Blackwell GPUs when KV transfer is active, as native kernels might be slower. Additionally, if other features in the future also create non-contiguous tensors, similar issues could arise unless this logic is extended or generalized.

**Key insights**  
The change is a targeted workaround that prioritizes correctness over performance for a specific hardware/configuration combination. Developers should monitor performance impacts and consider whether a more robust solution—such as detecting tensor contiguity dynamically—is needed long-term. Ensure any future attention kernel optimizations account for KV transfer compatibility.

---

## 16. [[CI][torch.compile] Reduce e2e fusion test time](https://github.com/vllm-project/vllm/pull/33293)


### Base Information

- **PR Number:** #33293
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-04 16:09:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33293/files) (17):**
  - `.buildkite/test-amd.yaml`
  - `.buildkite/test-pipeline.yaml`
  - `.buildkite/test_areas/compile.yaml`
  - `.buildkite/test_areas/distributed.yaml`
  - `.buildkite/test_areas/pytorch.yaml`
  - `tests/compile/distributed/test_fusions_e2e.py`
  - `tests/compile/fusion_test_utils.py`
  - `tests/compile/fusions_e2e/__init__.py`
  - `tests/compile/fusions_e2e/common.py`
  - `tests/compile/fusions_e2e/conftest.py`
  - `tests/compile/fusions_e2e/models.py`
  - `tests/compile/fusions_e2e/test_tp1_quant.py`
  - `tests/compile/fusions_e2e/test_tp2_ar_rms.py`
  - `tests/compile/fusions_e2e/test_tp2_async_tp.py`
  - `tests/compile/test_fusion_attn.py`
  - `tests/test_config.py`
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
This PR replaces the monolithic E2E fusion tests with a new modular test structure in `tests/compile/fusions_e2e/`. The old tests (`test_fusions_e2e.py` and `fusion_test_utils.py`) were removed, and CI configurations were updated to split tests into "quick" (all models, single config) and "sweep" (single model, all configs) runs. The changes reduce test time by using fewer hidden layers (`n_layers=6` or `4`) and dummy weights, while improving coverage and maintainability.

**Technical impact**  
The new test architecture separates concerns into reusable components (`common.py`, `conftest.py`, `models.py`) and per-fusion-type test files (`test_tp1_quant.py`, `test_tp2_ar_rms.py`, `test_tp2_async_tp.py`). This modularity makes it easier to add new models and fusions. CI pipelines now have targeted jobs with stricter timeouts (e.g., quick tests <15 minutes), and distributed compilation tests are moved from the distributed area to compile-specific configurations.

**Potential risks**  
Reducing the number of hidden layers (`n_layers`) could miss edge cases that only appear in deeper models. The use of dummy weights may skip real weight‑loading issues. Some tests are conditionally skipped (e.g., Blackwell‑only, FlashInfer‑required), which could mask platform‑specific regressions. The removal of old tests without a phased migration risks losing coverage if the new tests have gaps.

**Key insights**  
Developers should leverage the new utilities (`ModelFusionInfo`, `AttentionBackendCase`) when adding models or fusions. The split between quick and sweep tests balances CI speed and coverage—quick tests should run on most changes, while sweep tests are reserved for compilation/model‑forward changes. Ensure any new fusion passes are integrated into the appropriate test file and that `matches` expectations are correctly defined per layer count.

---

## 17. [feat: Add ColBERT late interaction model support](https://github.com/vllm-project/vllm/pull/33686)


### Base Information

- **PR Number:** #33686
- **Author:** [ieBoytsov](https://github.com/ieBoytsov)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-04 16:05:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33686/files) (13):**
  - `docs/models/pooling_models.md`
  - `examples/pooling/score/colbert_rerank_online.py`
  - `tests/entrypoints/pooling/score/test_online_colbert.py`
  - `tests/models/language/pooling/test_colbert.py`
  - `tests/models/registry.py`
  - `vllm/config/model.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/pooling/__init__.py`
  - `vllm/entrypoints/pooling/score/serving.py`
  - `vllm/entrypoints/pooling/score/utils.py`
  - `vllm/model_executor/models/colbert.py`
  - `vllm/model_executor/models/interfaces.py`
  - `vllm/model_executor/models/registry.py`

### Summary

**What changed and why**  
This PR adds support for ColBERT late-interaction models to vLLM, enabling retrieval and reranking tasks using per-token embeddings with MaxSim scoring. The implementation includes a new model class (`ColBERTModel`), integration with existing pooling and scoring endpoints, and comprehensive testing.

**Technical impact**  
The changes extend vLLM's pooling framework to handle a new class of models that output token-level embeddings rather than single-vector representations. The scoring pipeline now supports late-interaction scoring via a dedicated `_late_interaction_score` method, and the API routers have been updated to expose rerank and score endpoints for ColBERT models. This introduces a new model configuration flag (`is_late_interaction`) and updates the model registry accordingly.

**Potential risks**  
The implementation assumes standard BERT-based ColBERT architectures; models with custom encoders (e.g., rotary embeddings) are explicitly unsupported and may fail. There is also a risk of performance overhead due to the per-token embedding computations and MaxSim scoring, especially for long documents. The error handling for multimodal inputs in late-interaction scoring is currently limited to raising `NotImplementedError`.

**Key insights**  
Developers should note that ColBERT models only support the `token_embed` task, not the standard `embed` task. When serving ColBERT models, the `--hf-overrides` flag may be required if the model's config doesn't specify `HF_ColBERT` architecture. The MaxSim scoring is implemented in `compute_maxsim_score` and integrated into both offline (`LLM`) and online (`ServingScores`) scoring paths.

---

## 18. [[Core] Don't schedule spec tokens with prefill chunks](https://github.com/vllm-project/vllm/pull/33652)


### Base Information

- **PR Number:** #33652
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-04 15:40:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33652/files) (5):**
  - `tests/v1/core/test_scheduler.py`
  - `vllm/v1/core/sched/async_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/request.py`
  - `vllm/v1/worker/gpu/spec_decode/utils.py`

### Summary

**What changed and why**  
This PR prevents speculative (draft) tokens from being scheduled during chunked prefill operations. The fix addresses a bug where draft tokens were incorrectly included with prefill chunks, which could cause scheduling errors and interfere with linear attention prefix caching. Additionally, it resolves ModelRunner V2 spec decode compatibility with non-async scheduling.

**Technical impact**  
The changes introduce a `is_prefill_chunk` flag on requests to track when prefill is incomplete, ensuring draft tokens are ignored until prefill finishes. This modifies scheduling logic in both async and base schedulers, and updates draft token handling in the GPU worker to support non-async scheduling by providing placeholder draft tokens when needed.

**Potential risks**  
If the `is_prefill_chunk` flag is incorrectly set or cleared, it could lead to missed draft tokens during decode or premature speculative scheduling. The placeholder draft tokens (`-1` values) in non-async mode require careful handling downstream to avoid processing invalid token IDs. Edge cases involving structured outputs during prefill chunks need thorough validation.

**Key insights**  
Developers should ensure all scheduling paths respect the `is_prefill_chunk` flag, particularly when integrating new features. The fix highlights the importance of separating prefill and decode phases for speculative decoding. Test coverage should be expanded to include mixed scenarios with structured outputs and chunked prefill.

---

## 19. [Change the type signature of MixtureOfExperts.expert_weights to MutableSequence[Sequence[Tensor]]](https://github.com/vllm-project/vllm/pull/33573)


### Base Information

- **PR Number:** #33573
- **Author:** [SageMoore](https://github.com/SageMoore)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-04 14:02:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33573/files) (2):**
  - `vllm/distributed/eplb/rebalance_execute.py`
  - `vllm/model_executor/models/interfaces.py`

### Summary

**What changed and why**  
The PR changes the type signature of `MixtureOfExperts.expert_weights` from `MutableSequence[Iterable[Tensor]]` to `MutableSequence[Sequence[Tensor]]`. This removes support for generators and ensures the sequence supports `__len__` and `__getitem__` methods, which are required for downstream operations like indexing and length checks.

**Technical impact**  
This change enforces a stricter contract on the `expert_weights` attribute, guaranteeing it is a concrete sequence rather than a lazy iterable. It affects the EPLB (expert parallel load balancing) implementation, which now relies on direct indexing (`expert_weights[0][0]`) instead of using `next(iter(...))` to access tensor dimensions. This improves code clarity and eliminates potential runtime errors from unsupported operations on generators.

**Potential risks**  
If any existing code passes generators or non-sequence iterables to `expert_weights`, it will now fail at runtime due to the stricter type requirement. The change assumes all callers already provide sequences; however, if generators were previously used, this could break backward compatibility. Additionally, the `assert len(expert_weights[0]) >= 1` added may fail if empty sequences are provided.

**Key insights**  
The update simplifies the code by removing generator handling, making the API more predictable. Developers should ensure that any code providing `expert_weights` now uses lists or other sequences instead of generators. The added assertions help catch invalid inputs early, but thorough testing is recommended to confirm no hidden dependencies on lazy evaluation exist.

---

## 20. [Revert "[torch.compile] Significantly speed up cold start times"](https://github.com/vllm-project/vllm/pull/33820)


### Base Information

- **PR Number:** #33820
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-04 14:00:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33820/files) (3):**
  - `tests/compile/test_cold_start.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/compiler_interface.py`

### Summary

**What changed and why**  
This PR reverts a previous optimization that aimed to speed up cold start times by caching compiled subgraphs based on their hash. The revert changes the cache key from `(runtime_shape, graph_hash, backend_name)` to `(runtime_shape, graph_index, backend_name)`, effectively removing the deduplication of identical subgraphs across different positions in the model graph.

**Technical impact**  
The change reduces compilation efficiency by eliminating subgraph deduplication. Previously, identical subgraphs (e.g., repeated transformer layers) were compiled once and reused via hash-based caching. Now, each subgraph is compiled separately based on its index, leading to redundant compilations and increased cold start times. The test expectations are updated to reflect that cache hits now occur for duplicate subgraphs.

**Potential risks**  
Cold start performance will degrade significantly for models with repeated layers, as each layer triggers a separate compilation. The cache may become less effective if subgraph indices change due to model modifications, even if the actual computation remains identical. There is also a risk of increased memory usage from storing multiple compiled artifacts for identical subgraphs.

**Key insights**  
This revert prioritizes simplicity and correctness over performance, possibly due to issues with hash stability or cache invalidation in the original implementation. Developers should monitor cold start times and consider re-introducing deduplication with a more robust mechanism if performance becomes critical. The test changes highlight the shift from expecting zero cache hits to expecting many, indicating a fundamental change in caching strategy.

---

## 21. [[Model] Add transcription support for Qwen3-Omni](https://github.com/vllm-project/vllm/pull/29828)


### Base Information

- **PR Number:** #29828
- **Author:** [mu-hashmi](https://github.com/mu-hashmi)
- **Merged By:** [NickLucche](https://github.com/NickLucche)
- **Merged time:** 2026-02-04 13:17:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29828/files) (3):**
  - `docs/contributing/model/transcription.md`
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
This PR adds transcription and translation support for the Qwen3-Omni model by implementing the `SupportsTranscription` interface. The changes include updating documentation to list the new model and modifying the model class to handle audio input and generate appropriate prompts for speech-to-text tasks.

**Technical impact**  
The integration extends vLLM's transcription capabilities to a fifth model, enabling Qwen3-Omni to process audio via the existing `/v1/audio/transcriptions` and `/v1/audio/translations` endpoints. The model uses its multimodal audio embeddings and follows a pattern similar to other supported models, requiring no extra registration beyond the class implementation.

**Potential risks**  
The implementation assumes the model's processor supports the required audio placeholder format and chunking parameters. There is a risk if the audio preprocessing (e.g., sample rate, chunk length) mismatches between the config and the actual model expectations. Additionally, the language mapping relies on a hardcoded dictionary, which may become outdated if the model's supported languages change.

**Key insights**  
Developers should verify that the audio preprocessing in `get_speech_to_text_config` aligns with the model's feature extractor. The language support mapping should be maintained if the model expands its language capabilities. Testing with diverse audio inputs and languages is recommended to ensure robustness.

---

## 22. [[Bugfix] Support `RotaryEmbedding` CustomOp for gpt-oss](https://github.com/vllm-project/vllm/pull/33800)


### Base Information

- **PR Number:** #33800
- **Author:** [simondanielsson](https://github.com/simondanielsson)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-04 12:17:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33800/files) (4):**
  - `tests/compile/test_rotary_embedding_compile.py`
  - `vllm/model_executor/layers/rotary_embedding/base.py`
  - `vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py`
  - `vllm/model_executor/layers/rotary_embedding/mrope.py`

### Summary

**What changed and why**  
The fix modifies `RotaryEmbedding` to prevent mutation of the `cos_sin_cache` buffer during `torch.compile` with cudagraphs. Previously, `_match_cos_sin_cache_dtype` updated the buffer in-place, which caused compilation failures. Now, it returns a casted view without mutating the buffer when compiling, and a regression test verifies that no `update` operation appears in the compiled bytecode.

**Technical impact**  
This change ensures that the rotary embedding cache remains immutable during graph compilation, which is required for cudagraphs to work correctly. The cache is now conditionally updated only at runtime, preserving the performance of dtype/device conversions while maintaining compatibility with torch.compile. The fix applies consistently across CUDA, HIP, XPU, and CPU backends, as well as specialized variants like DeepSeek and mRoPE.

**Potential risks**  
If the conditional logic in `_match_cos_sin_cache_dtype` incorrectly identifies compilation state, it could lead to dtype mismatches or unnecessary copies. The test relies on environment variables (`VLLM_USE_BYTECODE_HOOK`, `VLLM_USE_AOT_COMPILE`) which must be set appropriately in other test scenarios. There is also a risk that the returned cache view might not be properly tracked by autograd in edge cases.

**Key insights**  
Developers should note that buffer mutation during compilation is prohibited when using cudagraphs. The fix demonstrates a pattern of returning a transformed tensor instead of mutating module state during compilation. The added test provides a robust check for buffer mutation by inspecting compiled bytecode, which can be adapted for similar issues in other custom ops.

---

## 23. [Implement zero-copy GQA for multimodal and CPU](https://github.com/vllm-project/vllm/pull/33732)


### Base Information

- **PR Number:** #33732
- **Author:** [voidbag](https://github.com/voidbag)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-04 12:11:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33732/files) (4):**
  - `vllm/model_executor/layers/attention/mm_encoder_attention.py`
  - `vllm/model_executor/models/molmo2.py`
  - `vllm/v1/attention/backends/cpu_attn.py`
  - `vllm/v1/attention/ops/vit_attn_wrappers.py`

### Summary

**What changed and why**  
This PR implements zero-copy Grouped Query Attention (GQA) for multimodal and CPU backends by removing explicit tensor copying (via `torch.repeat_interleave`) when the number of query heads exceeds key/value heads. Instead, it passes an `enable_gqa` flag to the underlying SDPA implementation, allowing the backend to handle head repetition internally without data duplication.

**Technical impact**  
The changes affect attention layers across three components: multimodal encoder attention (`mm_encoder_attention.py`), the Molmo2 model (`molmo2.py`), and CPU attention (`cpu_attn.py`). By eliminating redundant tensor copies, memory usage and computational overhead are reduced, particularly for GQA configurations. The SDPA wrapper functions are updated to propagate the `enable_gqa` flag, ensuring consistent behavior across backends.

**Potential risks**  
If the underlying SDPA implementation does not fully support the `enable_gqa` flag on all hardware or software versions, it could lead to incorrect attention computations or runtime errors. Additionally, the removal of explicit head repetition assumes that the backend correctly handles GQA; any mismatch in head counts without proper backend support may produce silent errors.

**Key insights**  
This optimization is crucial for memory efficiency in GQA scenarios, but thorough validation is required to ensure compatibility with all target platforms (especially CPU and multimodal backends). Developers should verify that the `enable_gqa` flag is supported in the PyTorch or backend versions used, and consider adding fallback logic or validation checks to maintain robustness.

---

## 24. [[rocm][ray] Fix: Unify Ray device visibility handling across CUDA and ROCm](https://github.com/vllm-project/vllm/pull/33308)


### Base Information

- **PR Number:** #33308
- **Author:** [kouroshHakha](https://github.com/kouroshHakha)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 10:09:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33308/files) (8):**
  - `docker/Dockerfile.rocm`
  - `tests/config/test_config_generation.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/interface.py`
  - `vllm/platforms/rocm.py`
  - `vllm/v1/executor/ray_executor.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/worker_base.py`

### Summary

**What changed and why**  
This PR unifies Ray device visibility handling across CUDA and ROCm platforms by preventing Ray from automatically setting device visibility environment variables. Instead, vLLM now controls these variables through platform-specific configurations, ensuring workers start with all GPUs visible and use `local_rank` for device indexing, similar to multiprocessing mode.

**Technical impact**  
The changes introduce a platform abstraction (`ray_noset_device_env_vars`) to declare which Ray environment variables should be set to disable automatic device management. This ensures consistent behavior across CUDA and ROCm, eliminates platform-specific workarounds, and unifies the Ray and multiprocessing codepaths for device initialization.

**Potential risks**  
If Ray's behavior changes regarding `RAY_EXPERIMENTAL_NOSET_*` variables, the fix may become ineffective. Additionally, setting `CUDA_VISIBLE_DEVICES` to all GPUs after worker construction could introduce subtle issues if any libraries cache device visibility earlier than expected, though this is mitigated by the new initialization order.

**Key insights**  
Developers should note that ROCm no longer requires special environment variables in Docker images, simplifying deployment. The unified approach reduces maintenance burden and improves user experience by providing consistent error messages and behavior across platforms. Ensure any new platform implementations define appropriate `ray_noset_device_env_vars`.

---

## 25. [[Bugfix] Fix interns1-pro initialization and PP](https://github.com/vllm-project/vllm/pull/33793)


### Base Information

- **PR Number:** #33793
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-04 09:54:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33793/files) (6):**
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/interns1_pro.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/qwen3_vl_moe.py`

### Summary

**What changed and why**  
This PR fixes initialization and pipeline parallelism (PP) issues for the InternS1-Pro model. The changes address broken initialization caused by a previous PR being based on an outdated vLLM version, and correct PP-related assertions in Qwen3 VL models that were incorrectly applied to models without deepstack visual indexes.

**Technical impact**  
The modifications update the InternS1-Pro model to use the correct base class initialization (`super(Qwen3MoeForCausalLM, self).__init__()` instead of `super().__init__()`), adjust weight loading to handle FoPE rotary embeddings dynamically, and fix PP assertions in Qwen3 VL models to conditionally check `deepstack_visual_indexes`. Additionally, test skips are added for InternS1-Pro due to unresolved tokenization issues.

**Potential risks**  
The dynamic FoPE parameter mapping in `get_frope_params_map()` may introduce overhead or complexity during weight loading. Skipping tests for InternS1-Pro leaves tokenization and tensor schema issues unvalidated, potentially hiding regressions. The conditional PP assertions rely on `hasattr`, which could mask configuration errors if `vision_config` is malformed.

**Key insights**  
Developers should prioritize resolving the skipped tests for InternS1-Pro to ensure full functionality. The FoPE parameter mapping approach, while a workaround, should be reviewed for performance implications. The PP fixes in Qwen3 VL models improve robustness but should be accompanied by comprehensive testing across different model configurations.

---

## 26. [[Misc] Delay deprecation of CommonAttentionMetadata properties](https://github.com/vllm-project/vllm/pull/33801)


### Base Information

- **PR Number:** #33801
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 08:41:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33801/files) (1):**
  - `vllm/v1/attention/backend.py`

### Summary

**What changed and why**  
The deprecation warnings for `seq_lens_cpu` and `num_computed_tokens_cpu` properties in `CommonAttentionMetadata` have been updated to remove the specific version (v0.15.0) and replace it with a generic message. This change delays the planned removal due to performance issues and CI instability encountered during migration efforts.

**Technical impact**  
This maintains backward compatibility for existing code using these properties while signaling that migration is still required. The change avoids breaking user code immediately but does not alter the underlying functionality or performance characteristics of the properties.

**Potential risks**  
Without a clear removal timeline, users may delay migration indefinitely, potentially causing sudden breakages when the properties are eventually removed. Additionally, the performance issues that prompted the deprecation (implicit host-to-device synchronization) will persist until users transition to the recommended alternatives.

**Key insights**  
Developers should proactively migrate to using `seq_lens` directly or deriving CPU copies from `query_start_loc_cpu` and `seq_lens` to avoid future disruptions. The team should establish and communicate a new deprecation timeline once the performance and CI stability issues are resolved.

---

## 27. [[Bugfix] Fix ubatch wrapper num_tokens calculate](https://github.com/vllm-project/vllm/pull/33694)


### Base Information

- **PR Number:** #33694
- **Author:** [jiangkuaixue123](https://github.com/jiangkuaixue123)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-02-04 08:41:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33694/files) (1):**
  - `vllm/v1/worker/gpu_ubatch_wrapper.py`

### Summary

**What changed and why**  
The fix corrects the calculation of `num_tokens` in the GPU ubatch wrapper. Previously, it incorrectly calculated tokens by doubling the token count from only the first slice (`ubatch_slices[0]`). The new implementation correctly sums the `num_tokens` from all `ubatch_slice` objects in the `ubatch_slices` list, aligning with the variable ubatch-size support introduced in PR #30120.

**Technical impact**  
This change ensures the `num_tokens` variable accurately reflects the total number of tokens being processed across all micro-batches. This is critical for downstream operations that depend on this count, such as attention computation or tensor shape validations, preventing potential shape mismatches or incorrect processing logic.

**Potential risks**  
If the `ubatch_slice.num_tokens` attribute itself is incorrectly populated or if `ubatch_slices` is empty, this could lead to a zero or erroneous `num_tokens` value. Additionally, any code that previously relied on the flawed doubling logic might now behave unexpectedly, though such dependencies would likely have been buggy.

**Key insights**  
Always validate that aggregated calculations consider all elements in a collection, not just the first. When extending systems to support variable-sized batches, ensure all related metadata calculations are updated accordingly. Reviewers should check for similar patterns where slice-specific data needs aggregation across a list.

---

## 28. [[Bugfix] Fix `normalize` still being passed to `PoolerConfig`](https://github.com/vllm-project/vllm/pull/33794)


### Base Information

- **PR Number:** #33794
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-02-04 07:56:02
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33794/files) (2):**
  - `tests/models/language/pooling/test_embedding.py`
  - `vllm/entrypoints/llm.py`

### Summary

**What changed and why**  
The PR fixes a bug where the deprecated `normalize` parameter was still being passed to `PoolerConfig` in test code and documentation. The parameter was removed in a previous change (#33477), causing test failures.

**Technical impact**  
This change aligns the codebase with the updated `PoolerConfig` API by replacing `normalize=False` with `use_activation=False`. It ensures tests pass and documentation accurately reflects the current interface.

**Potential risks**  
If other instances of the deprecated `normalize` parameter exist elsewhere in the codebase, they could cause similar runtime errors. The changes are minimal and focused, reducing the risk of introducing new issues.

**Key insights**  
Always update all references when deprecating or removing API parameters. The fix is straightforward but highlights the importance of comprehensive cleanup after API changes. Developers should verify no other usages of `normalize` remain.

---

## 29. [[Perf] Optimize spec decoding + async scheduling, 1.5% Throughput improvement](https://github.com/vllm-project/vllm/pull/33612)


### Base Information

- **PR Number:** #33612
- **Author:** [yewentao256](https://github.com/yewentao256)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-02-04 06:34:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33612/files) (2):**
  - `vllm/v1/core/sched/async_scheduler.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The optimization replaces repeated list creation `[-1] * self.num_spec_tokens` in `_update_after_schedule` with a reusable placeholder list stored in `AsyncScheduler.__init__`. This reduces memory allocations during speculative decoding with async scheduling, yielding a measured 1.5% throughput improvement.

**Technical impact**  
This change improves performance by eliminating per-request list allocations for speculative token placeholders. The base `Scheduler.schedule` method is also updated to safely handle the shared placeholder list by creating slices instead of mutating it, ensuring thread safety and correct token tracking.

**Potential risks**  
The shared placeholder list could cause issues if mutated elsewhere, but the code protects against this by assigning slices. There's a minor risk if `num_spec_tokens` changes dynamically after scheduler initialization, as the placeholder list wouldn't be updated. The preemption logic change ensures `spec_token_ids` is cleared properly when non-empty.

**Key insights**  
This is a clean micro-optimization that demonstrates measurable performance gains from reducing Python object allocations. Developers should note the pattern of reusing immutable data structures in hot paths. Ensure any future changes to `num_spec_tokens` also update `_spec_token_placeholders` accordingly.

---

## 30. [[Bugfix][ROCm] Include float8_e4m3fnuz in NCCL Dtype Dispatching](https://github.com/vllm-project/vllm/pull/33713)


### Base Information

- **PR Number:** #33713
- **Author:** [micah-wil](https://github.com/micah-wil)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 05:36:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33713/files) (1):**
  - `vllm/distributed/device_communicators/pynccl_wrapper.py`

### Summary

**What changed and why**  
The change replaces a hardcoded check for `torch.float8_e4m3fn` with a call to `current_platform.fp8_dtype()`. This ensures that both `float8_e4m3fn` and `float8_e4m3fnuz` (used on AMD MI300/325) are correctly mapped to the same NCCL dtype (`ncclFloat8e4m3`), fixing a dtype dispatching error that prevented the server from starting with FP8 models on ROCm.

**Technical impact**  
This update aligns the PyNCCL wrapper with ROCm's RCCL implementation, which uses a single `ncclFloat8e4m3` type for both FP8 variants. It enables proper tensor communication for FP8 models on AMD hardware and resolves the specific failure in the `Qwen3-30B-A3B-FP8-block Accuracy` CI test.

**Potential risks**  
The change assumes `current_platform.fp8_dtype()` correctly returns the appropriate FP8 dtype for the active platform. If this function is not implemented or returns an unexpected dtype on some platforms, it could lead to runtime errors. Additionally, any future FP8 dtype variants must be handled by the platform abstraction to maintain compatibility.

**Key insights**  
Always use platform-aware abstractions (like `current_platform.fp8_dtype()`) instead of hardcoding dtype checks to ensure cross-platform compatibility. Developers should verify that similar dtype mappings elsewhere in the codebase are also updated to avoid inconsistencies, especially when adding support for new hardware or data types.

---

## 31. [Apply #33621 to main](https://github.com/vllm-project/vllm/pull/33758)


### Base Information

- **PR Number:** #33758
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 05:35:40
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33758/files) (3):**
  - `requirements/common.txt`
  - `requirements/rocm-test.txt`
  - `requirements/test.txt`

### Summary

**What changed and why**  
Updated the aiohttp dependency from version 3.13.0 to 3.13.3 across three requirement files (common.txt, rocm-test.txt, test.txt). This change applies a fix previously targeted only at a release branch to the main branch, ensuring consistency and incorporating bug fixes or security patches from the minor version update.

**Technical impact**  
The update aligns all development and testing environments with the same aiohttp version, reducing potential inconsistencies between branches. Since this is a patch version change (3.13.0 → 3.13.3), it likely includes backward-compatible bug fixes without introducing breaking API changes, minimizing disruption to existing functionality.

**Potential risks**  
While patch updates are generally low-risk, there is a slight chance of undiscovered regressions in aiohttp 3.13.3 that could affect asynchronous HTTP operations. Additionally, if any code implicitly depends on behaviors specific to version 3.13.0, subtle issues may arise in edge cases related to HTTP client/server interactions.

**Key insights**  
Ensure thorough testing of asynchronous HTTP features, especially in integration scenarios, to validate compatibility. Consider documenting the reason for this specific version bump (e.g., linking to relevant fixes) to provide context for future maintenance. Regularly synchronize dependency updates between release and main branches to prevent drift and technical debt.

---

## 32. [[Perf] Optimize chat completion streaming performance](https://github.com/vllm-project/vllm/pull/33782)


### Base Information

- **PR Number:** #33782
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-04 04:30:36
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33782/files) (1):**
  - `vllm/entrypoints/openai/chat_completion/serving.py`

### Summary

**What changed and why**  
The PR optimizes chat completion streaming performance by caching the result of `is_reasoning_end(res.prompt_token_ids)` for each choice. Previously, this check was executed multiple times per choice during streaming; now it's computed once and stored in `prompt_is_reasoning_end_arr` to avoid redundant computations.

**Technical impact**  
This change reduces computational overhead in the streaming path, particularly for scenarios involving reasoning parsers and tool calls. The optimization is localized to the streaming generator and maintains the same logical behavior while improving efficiency for each generated token delta.

**Potential risks**  
If `res.prompt_token_ids` changes mid-stream for a choice (though unlikely), the cached value could become stale. Additionally, the array initialization with `None` and conditional checks introduce slight complexity, requiring careful handling of state transitions. Edge cases around `reasoning_end_arr` updates must align with the cached values.

**Key insights**  
The optimization is a classic memoization technique applied to a hot path. Developers should ensure that `prompt_token_ids` are indeed immutable per choice throughout the stream. Consider extending similar caching to `output.token_ids` if they are also repeatedly evaluated, and verify that all code paths correctly respect the `reasoning_end_arr` state.

---

## 33. [[Model] Apply #32631 for recent models](https://github.com/vllm-project/vllm/pull/33785)


### Base Information

- **PR Number:** #33785
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-04 04:23:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33785/files) (4):**
  - `vllm/model_executor/models/eagle2_5_vl.py`
  - `vllm/model_executor/models/funaudiochat.py`
  - `vllm/model_executor/models/openpangu_vl.py`
  - `vllm/model_executor/models/qwen3_asr.py`

### Summary

**What changed and why**  
This PR applies changes from #32631 to recent multimodal models (Eagle2.5-VL, FunAudioChat, OpenPangu-VL, Qwen3-ASR). The primary modifications involve wrapping vision/audio tower initialization and language model initialization within context managers (`_mark_tower_model` and `_mark_language_model`) and removing the now-redundant `get_language_model` methods.

**Technical impact**  
The changes standardize multimodal model initialization by explicitly marking tower and language model components. This likely enables better model partitioning, quantization, or profiling by providing clear boundaries between modality-specific encoders and the core language model. The removal of `get_language_model` suggests the method is no longer needed, possibly due to refactored access patterns.

**Potential risks**  
If `_mark_tower_model` or `_mark_language_model` have side effects (e.g., registration, state changes), improper nesting or missing calls could break model behavior. The removal of `get_language_model` may affect any external code that relied on this method to access the language model submodule, though this is likely internal.

**Key insights**  
This is a systematic refactoring to align multiple models with a new initialization pattern. Developers should verify that the context managers do not introduce performance overhead or initialization order dependencies. When updating similar models, ensure all tower components are correctly wrapped and that no legacy access patterns remain.

---

## 34. [[Bugfix][Model] Fix audio-in-video support for Qwen2.5-Omni and Qwen3-Omni](https://github.com/vllm-project/vllm/pull/33605)


### Base Information

- **PR Number:** #33605
- **Author:** [linyueqian](https://github.com/linyueqian)
- **Merged By:** [ywang96](https://github.com/ywang96)
- **Merged time:** 2026-02-04 04:15:29
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33605/files) (2):**
  - `vllm/model_executor/models/qwen2_5_omni_thinker.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
The PR fixes audio-in-video support for Qwen2.5-Omni and Qwen3-Omni models by addressing embedding misalignment when `use_audio_in_video=True`. The HF processor interleaves video and audio tokens, but embeddings were provided as separate contiguous tensors, causing incorrect scattering. The solution adds helper functions to detect interleaved sequences and merge embeddings per modality using token ID-based masks.

**Technical impact**  
Changes are localized to model-specific `embed_input_ids` overrides, preserving the shared multimodal interface. For Qwen3-Omni, the deepstack vision path is also updated to use correct vision masks during interleaving. This ensures embeddings align with token positions, enabling proper multimodal processing without altering core infrastructure.

**Potential risks**  
The modality categorization logic in `merge_interleaved_embeddings` assumes embeddings are grouped by modality and ordered consistently with input kwargs. If future changes alter embedding ordering or introduce new modalities, this logic may break. Additionally, the interleaving detection relies on token ID comparisons, which could fail if token IDs are reused or misconfigured.

**Key insights**  
Developers should note that interleaved audio-video sequences require special handling in embedding merge logic. The solution avoids modifying shared utilities, reducing regression risk. When extending support to other models with similar interleaving, consider adopting the same pattern of token ID-based masking and per-modality scattering.

---

## 35. [[PERF] Change GDN Attention State Layout from [N, HV, K, V] to [N, HV, V, K]](https://github.com/vllm-project/vllm/pull/33291)


### Base Information

- **PR Number:** #33291
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-02-04 03:20:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33291/files) (6):**
  - `vllm/model_executor/layers/fla/ops/chunk.py`
  - `vllm/model_executor/layers/fla/ops/chunk_delta_h.py`
  - `vllm/model_executor/layers/fla/ops/chunk_o.py`
  - `vllm/model_executor/layers/fla/ops/fused_recurrent.py`
  - `vllm/model_executor/layers/fla/ops/kda.py`
  - `vllm/model_executor/layers/mamba/mamba_utils.py`

### Summary

**What changed and why**  
The PR changes the recurrent state memory layout in GDN attention from `[N, HV, K, V]` to `[N, HV, V, K]` across multiple kernel implementations. This modification aims to improve memory access patterns and throughput, and enables compatibility with FlashInfer's GDN kernels.

**Technical impact**  
The layout change affects all state tensors in GDN attention operations, requiring updates to tensor allocations, pointer arithmetic, and dot product computations in the Triton kernels. This impacts how states are loaded, stored, and processed during attention computation, potentially altering memory locality and kernel performance characteristics.

**Potential risks**  
The changes introduce risk of layout mismatches with existing saved states or external integrations. Performance improvements are inconsistent across batch sizes (some show minor regressions), and there may be hidden edge cases in variable-length sequences or speculative decoding scenarios. The modifications are extensive and could affect correctness if any tensor dimension calculations are misaligned.

**Key insights**  
Developers must update any code that creates, passes, or persists GDN state tensors to use the new `[V, K]` layout. Performance testing shows significant gains at large batch sizes (+4.8% at 1024) but minor regressions at smaller batches, suggesting the optimization benefits high-throughput scenarios. The change is necessary for FlashInfer kernel compatibility, indicating future integration plans.

---

## 36. [[BugFix] scheduler: Delay freeing blocks of aborted async loads](https://github.com/vllm-project/vllm/pull/32255)


### Base Information

- **PR Number:** #32255
- **Author:** [orozery](https://github.com/orozery)
- **Merged By:** [markmc](https://github.com/markmc)
- **Merged time:** 2026-02-04 03:16:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32255/files) (3):**
  - `tests/v1/core/test_scheduler.py`
  - `tests/v1/kv_connector/unit/test_offloading_connector.py`
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The scheduler now delays freeing KV cache blocks for aborted requests that are still waiting for remote KV loading to complete. This prevents premature block deallocation while asynchronous KV transfers are in progress, ensuring system stability.

**Technical impact**  
The `finish_requests` method now checks if an aborted request is in `WAITING_FOR_REMOTE_KVS` status and hasn't finished receiving KVs, delaying block freeing until the transfer completes. The `_update_from_kv_xfer_finished` method also handles cleanup for already-finished requests when transfers complete.

**Potential risks**  
If `finished_recving_kv_req_ids` tracking becomes inconsistent, blocks may leak or be freed prematurely. The assertion in `_update_from_kv_xfer_finished` assumes request statuses are correctly maintained, which could fail if there are race conditions in state transitions.

**Key insights**  
This fix addresses a critical edge case in async KV loading workflows. Developers should ensure all abort pathways properly synchronize with KV connector state, and consider adding metrics to monitor block lifecycle consistency. The test coverage is comprehensive but should be extended to concurrent abort scenarios.

---

## 37. [[compile] Remove runner type from ignored caching factor list.](https://github.com/vllm-project/vllm/pull/33712)


### Base Information

- **PR Number:** #33712
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-02-04 02:56:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33712/files) (1):**
  - `vllm/config/model.py`

### Summary

**What changed and why**  
The change removes the `runner` field from the ignored caching factor list in the `ModelConfig.compute_hash` method. This ensures that the draft runner type is considered when computing the cache key for Dynamo artifacts, preventing draft models from incorrectly reusing cached compilation artifacts intended for other runner types.

**Technical impact**  
This modification affects the cache key generation for model compilation artifacts. Previously, different runner types could share the same cached Dynamo artifacts, which caused correctness issues. Now, each runner type (including draft runners) will have distinct cache entries, ensuring proper isolation of compiled artifacts.

**Potential risks**  
If other configuration factors that should affect compilation are still being ignored, similar caching issues could occur. The change might increase cache size since identical models with different runner types will now create separate cache entries. There's also a risk if the runner type changes dynamically at runtime without proper cache invalidation.

**Key insights**  
The fix addresses a specific correctness issue with draft model compilation. Developers should verify that all configuration factors influencing model compilation are properly included in the cache key computation. Consider implementing cache cleanup strategies for obsolete runner-specific artifacts to manage storage growth.

---

## 38. [[compile] Clean up AOT compile bypass on evaluate_guards.](https://github.com/vllm-project/vllm/pull/33578)


### Base Information

- **PR Number:** #33578
- **Author:** [zhxchen17](https://github.com/zhxchen17)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 02:12:53
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33578/files) (1):**
  - `vllm/compilation/wrapper.py`

### Summary

**What changed and why**  
Removed a conditional assertion that disabled AOT compilation when using backed dynamic shapes (`ds_type == DynamicShapesType.BACKED`). This change is possible because a PyTorch PR (#169239) has been integrated, resolving the underlying issue that previously required the bypass.

**Technical impact**  
AOT compilation can now be enabled for backed dynamic shapes, potentially improving performance or enabling new compilation optimizations. The removal simplifies the code by eliminating a workaround that was contingent on an external PyTorch fix.

**Potential risks**  
If the integrated PyTorch change does not fully address the original issue or introduces regressions, AOT compilation with backed shapes could fail or produce incorrect results. Additionally, there may be edge cases in shape evaluation that were not covered by the upstream fix.

**Key insights**  
This cleanup improves code maintainability by removing a temporary restriction. Developers should verify that AOT compilation works correctly with backed dynamic shapes in all relevant scenarios and monitor for any related compilation errors or performance anomalies.

---

## 39. [[XPU][2/N] add support unquantized moe support for xpu](https://github.com/vllm-project/vllm/pull/33659)


### Base Information

- **PR Number:** #33659
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-04 02:12:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33659/files) (6):**
  - `.buildkite/scripts/hardware_ci/run-xpu-test.sh`
  - `requirements/xpu.txt`
  - `vllm/model_executor/layers/fused_moe/__init__.py`
  - `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`
  - `vllm/model_executor/layers/fused_moe/xpu_fused_moe.py`

### Summary

**What changed and why**  
This PR adds unquantized MoE (Mixture of Experts) support for XPU devices by modularizing the implementation and removing the monolithic XPU path. It introduces a new `XPUExperts` class that integrates with the existing fused MoE framework, updates backend selection logic, and adds XPU-specific test cases.

**Technical impact**  
The changes integrate XPU MoE support into the modular kernel architecture, aligning it with CUDA and CPU implementations. This improves code maintainability by eliminating platform-specific monolithic code paths. The XPU kernel now uses the same interface as other backends, with proper handling for FP8 quantization and expert parallelism.

**Potential risks**  
The XPU implementation currently lacks chunking support and has limited activation function compatibility (only silu, gelu, swigluoai). There may be performance implications due to the `supports_chunking()` returning false. The FP8 quantization support is marked with a TODO for device-specific dispatch, which could lead to inconsistencies.

**Key insights**  
Developers should note that XPU MoE now follows the modular pattern, making future enhancements easier. The removal of monolithic code reduces technical debt. However, the limited chunking support and activation functions may restrict model compatibility, and the FP8 quantization implementation requires further refinement for production readiness.

---

## 40. [use ORJSONResponse when available to improve the efficiency of request process](https://github.com/vllm-project/vllm/pull/33548)


### Base Information

- **PR Number:** #33548
- **Author:** [staugust](https://github.com/staugust)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-04 02:04:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33548/files) (1):**
  - `vllm/entrypoints/pooling/embed/api_router.py`

### Summary

**What changed and why**  
The PR replaces `JSONResponse` with `ORJSONResponse` for embedding endpoints when the `orjson` library is available, falling back to `JSONResponse` otherwise. This change aims to improve request processing efficiency by leveraging a faster JSON serialization library, as demonstrated by benchmark results showing increased throughput and reduced latency.

**Technical impact**  
The modification introduces a runtime check for `orjson` availability via `importlib.util.find_spec`, caching the result with `@lru_cache` to avoid repeated checks. This optimizes JSON serialization for embedding responses without adding a hard dependency, maintaining backward compatibility. The change is localized to the embedding API router, minimizing broader system impact.

**Potential risks**  
If `orjson` is installed but incompatible with the FastAPI version or system environment, it could cause serialization errors or crashes. The fallback mechanism relies on a warning log, which might be overlooked if logging is not monitored. Additionally, the caching approach assumes `orjson` availability does not change during runtime, which is generally safe but could be problematic in dynamic dependency scenarios.

**Key insights**  
The implementation effectively balances performance gains with dependency flexibility. Developers should ensure `orjson` is included in production environments to maximize benefits. Consider extending this optimization to other JSON-heavy endpoints if similar performance improvements are observed. Monitoring for serialization-related errors post-deployment is advisable.

---

## 41. [[XPU] remove common path warning log](https://github.com/vllm-project/vllm/pull/33769)


### Base Information

- **PR Number:** #33769
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [jikunshang](https://github.com/jikunshang)
- **Merged time:** 2026-02-04 00:40:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33769/files) (2):**
  - `vllm/platforms/__init__.py`
  - `vllm/platforms/xpu.py`

### Summary

**What changed and why**  
This PR removes a warning log about xccl availability from the common platform initialization path and moves it to the XPU-specific device communicator class. The change eliminates unnecessary warning noise for non-XPU platforms while preserving the warning for XPU users when communication functionality is actually needed.

**Technical impact**  
The warning log is now only triggered when XPU platforms attempt to initialize device communication, rather than during general platform detection. This reduces log clutter for GPU/CPU users and better aligns the warning with its actual relevance—only XPU users need to know about xccl availability for distributed operations.

**Potential risks**  
If the `supports_xccl()` check has different behavior or timing compared to the original check in `xpu_platform_plugin()`, it might cause the warning to appear at a different stage or not appear when needed. Additionally, changing the default `dist_backend` from "ccl" to "xccl" could affect existing XPU configurations that rely on the previous default.

**Key insights**  
The warning relocation is a good practice to avoid alarming non-XPU users. However, developers should verify that the `supports_xccl()` function accurately reflects the original detection logic and that the "xccl only" backend change is intentional and compatible with all supported XPU environments. Consider adding a comment explaining why "ccl" is no longer an option.

---

