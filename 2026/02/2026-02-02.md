# vLLM Merged PR Report

**Report Date:** 2026-02-02 PST

**Total Merged PRs:** 38

---

## 1. [[CI/Build] Investigate torchrun distributed tests hanging issue](https://github.com/vllm-project/vllm/pull/33650)


### Base Information

- **PR Number:** #33650
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 23:49:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33650/files) (2):**
  - `tests/distributed/test_torchrun_example.py`
  - `tests/distributed/test_torchrun_example_moe.py`

### Summary

**What changed and why**  
Two distributed test files were modified to disable async scheduling (`async_scheduling=False`) when running with torchrun. This is a temporary workaround to fix a deadlock issue (#33533) that occurs when enabling async scheduling with pipeline parallelism (PP), specifically within the `get_pp_group().send_tensor_dict(...)` call.

**Technical impact**  
This change prevents the deadlock in the test environment, allowing the distributed tests to pass. It does not fix the underlying issue but ensures CI stability. The tests now run with synchronous scheduling, which may affect performance measurements but maintains functional correctness for validation.

**Potential risks**  
The root cause of the deadlock remains unaddressed, meaning the async scheduling feature is effectively disabled for PP in these tests. If async scheduling is critical for production performance, this workaround could mask performance regressions. There is also a risk that developers might overlook the `FIXME` comment and not investigate the underlying issue.

**Key insights**  
This is a stopgap solution to unblock CI. The `FIXME` comment correctly signals that a deeper investigation is required. Developers should prioritize diagnosing the deadlock in `send_tensor_dict` with async scheduling. Consider adding a test-specific skip or conditional logic for async scheduling with PP until the core issue is resolved.

---

## 2. [[torch.compile] Document the workaround to standalone_compile failing](https://github.com/vllm-project/vllm/pull/33571)


### Base Information

- **PR Number:** #33571
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-02 23:16:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33571/files) (2):**
  - `docs/design/debug_vllm_compile.md`
  - `vllm/compilation/compiler_interface.py`

### Summary

**What changed and why**  
Added documentation and runtime validation to handle cases where compiled artifacts are not serializable by torch.compile. The changes provide a workaround for users encountering serialization errors by documenting the issue and implementing a check that fails early with a helpful error message.

**Technical impact**  
The compiler interface now validates compiled artifacts for serializability before attempting to save them to the cache. This prevents silent failures and undefined behavior when loading corrupted cache entries, making the system more robust. The documentation update informs users about serialization requirements and available workarounds.

**Potential risks**  
The `is_saveable_2_10` function relies on internal torch.compile implementation details that may change in future PyTorch versions (as noted in the comment about PyTorch 2.11). Disabling the compilation cache via environment variable could significantly impact warm start performance for production deployments.

**Key insights**  
The implementation provides a safer alternative to previous behavior by failing fast with clear guidance. Developers should be aware that serialization issues often stem from non-serializable elements in model code. Consider adding logging or metrics to track how frequently this validation fails in production environments.

---

## 3. [[Misc] Remove deprecated VLLM_ALL2ALL_BACKEND environment variable](https://github.com/vllm-project/vllm/pull/33535)


### Base Information

- **PR Number:** #33535
- **Author:** [carlory](https://github.com/carlory)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 23:01:59
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33535/files) (4):**
  - `.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh`
  - `tests/kernels/moe/modular_kernel_tools/common.py`
  - `vllm/config/parallel.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
This PR removes the deprecated `VLLM_ALL2ALL_BACKEND` environment variable, which was scheduled for removal in v0.15.0. The functionality is now exclusively available via the `--all2all-backend` command-line argument, completing the deprecation cycle.

**Technical impact**  
The changes eliminate legacy environment variable handling, simplifying configuration logic in `ParallelConfig` and removing the variable definition from `envs.py`. Integration tests and internal test utilities have been updated to use the CLI argument directly, ensuring consistent configuration pathways.

**Potential risks**  
Users or scripts still relying on the environment variable will experience failures, as the variable is no longer recognized. There is a risk of configuration errors if migration documentation was insufficient, though the deprecation warning should have provided adequate notice.

**Key insights**  
This cleanup reduces technical debt and enforces a single configuration method. Developers should verify all deployment scripts and CI configurations use `--all2all-backend` instead of the environment variable. The removal aligns with semantic versioning, as it was planned for v0.15.0.

---

## 4. [[Minor] Some code simplification in `scheduler.py`](https://github.com/vllm-project/vllm/pull/33597)


### Base Information

- **PR Number:** #33597
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 23:00:00
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33597/files) (1):**
  - `vllm/v1/core/sched/scheduler.py`

### Summary

**What changed and why**  
The changes simplify code in `scheduler.py` by reducing repeated dictionary key lookups. Instead of repeatedly accessing `request.request_id` for dictionary operations, the request ID is stored in a local variable (`request_id` or `preempted_req_id`) and reused. This improves readability and potentially reduces minor overhead.

**Technical impact**  
These changes are purely refactoring with no functional impact. The scheduler's behavior remains identical, but the code becomes slightly more maintainable by eliminating duplicate attribute accesses and reducing line noise in dictionary key references.

**Potential risks**  
Low risk, as the changes are straightforward and preserve all original logic. However, there is a minor risk if any variable is incorrectly reassigned or used outside its intended scope, though the current changes appear safe.

**Key insights**  
This is a clean, non-breaking refactor that follows good practices for reducing code duplication. Developers should adopt similar patterns to improve code clarity, especially in performance-critical paths like the scheduler, though the performance gain here is likely negligible.

---

## 5. [[Misc] Remove deprecated profiler environment variables](https://github.com/vllm-project/vllm/pull/33536)


### Base Information

- **PR Number:** #33536
- **Author:** [carlory](https://github.com/carlory)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 22:58:45
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33536/files) (2):**
  - `vllm/config/profiler.py`
  - `vllm/envs.py`

### Summary

**What changed and why**  
Removed deprecated environment variables for profiler configuration that were scheduled for removal in v0.15.0. The changes eliminate 11 environment variables and their associated parsing logic, simplifying the profiler configuration to use only command-line arguments or direct configuration objects.

**Technical impact**  
The profiler configuration now exclusively uses the `ProfilerConfig` class and command-line arguments, removing the dual configuration path through environment variables. This simplifies the codebase by eliminating 95 lines of code and reduces maintenance burden for the configuration system.

**Potential risks**  
Users who still rely on the deprecated environment variables will experience breakage, as their profiler settings will no longer be recognized. The removal could affect automated deployment scripts or container configurations that haven't migrated to the new configuration methods.

**Key insights**  
This is a clean technical debt removal following proper deprecation cycles. Developers should ensure all deployment configurations have migrated to using `--profiler-config` command-line arguments or direct `ProfilerConfig` instantiation. The simplification improves code maintainability and reduces configuration complexity.

---

## 6. [[XPU][1/N] Deprecate ipex and switch to vllm-xpu-kernels for xpu platform](https://github.com/vllm-project/vllm/pull/33379)


### Base Information

- **PR Number:** #33379
- **Author:** [jikunshang](https://github.com/jikunshang)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-02 22:46:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33379/files) (18):**
  - `.buildkite/scripts/hardware_ci/run-xpu-test.sh`
  - `docker/Dockerfile.xpu`
  - `requirements/xpu.txt`
  - `vllm/_ipex_ops.py`
  - `vllm/config/model.py`
  - `vllm/model_executor/layers/activation.py`
  - `vllm/model_executor/layers/layernorm.py`
  - `vllm/model_executor/layers/linear.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/inc.py`
  - `vllm/model_executor/layers/quantization/ipex_quant.py`
  - `vllm/model_executor/layers/rotary_embedding/base.py`
  - `vllm/platforms/__init__.py`
  - `vllm/platforms/xpu.py`
  - `vllm/v1/attention/backends/fa_utils.py`
  - `vllm/v1/attention/backends/registry.py`
  - `vllm/v1/worker/xpu_worker.py`

### Summary

**What changed and why**  
This PR initiates a major platform migration for Intel XPU support, replacing IPEX-based implementations with the new `vllm-xpu-kernels` library. It upgrades dependencies to OneAPI 2025.3 and PyTorch 2.10, removes IPEX-specific quantization and kernel code, and redirects XPU operations to use the same CUDA-like kernels via the new library.

**Technical impact**  
The changes fundamentally alter the XPU backend architecture by deprecating IPEX dependency and introducing a unified kernel interface. Activation functions, normalization layers, and attention backends now route XPU calls through `_custom_ops` (backed by `vllm-xpu-kernels`), simplifying the codebase and aligning XPU with CUDA kernel patterns. Quantization support for XPU is temporarily disabled during migration.

**Potential risks**  
- Removal of IPEX quantization (`ipex` method) and FP8 support may break existing XPU quantized models.
- The migration is marked as incomplete (part 1/N), so some features may be non-functional until subsequent PRs.
- Dependency upgrades (OneAPI 2025.3, PyTorch 2.10) could introduce compatibility issues with existing systems or models.
- The `vllm-xpu-kernels` library is a new external dependency with potential stability risks.

**Key insights**  
- Developers must update their environments to use the new dependencies and kernel library.
- Quantized models using IPEX methods will not work until quantization support is restored in future PRs.
- The changes streamline kernel dispatch by treating XPU more like CUDA, reducing platform-specific code.
- Ensure thorough testing of all XPU workflows, as this is a foundational change with broad impact.

---

## 7. [[Bugfix] Interleaved thinking keeps compatibility with reasoning_content](https://github.com/vllm-project/vllm/pull/33635)


### Base Information

- **PR Number:** #33635
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 22:46:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33635/files) (1):**
  - `vllm/entrypoints/chat_utils.py`

### Summary

**What changed and why**  
Added a duplicate `reasoning_content` field alongside the existing `reasoning` field when processing interleaved thinking messages. This maintains backward compatibility for code that may depend on the `reasoning_content` key, addressing a regression introduced by a previous PR.

**Technical impact**  
The change ensures that both `reasoning` and `reasoning_content` fields contain identical string values in parsed message objects. This prevents breaking existing integrations or downstream consumers that rely on the deprecated `reasoning_content` field name while aligning with the newer `reasoning` field.

**Potential risks**  
Duplicating data increases payload size slightly and could lead to inconsistency if future modifications update one field but not the other. There is also a risk of perpetuating technical debt if the deprecated field is not eventually removed.

**Key insights**  
This is a straightforward compatibility fix, but consider adding a deprecation warning for `reasoning_content` and documenting a timeline for its removal. Ensure future changes to reasoning content update both fields simultaneously to avoid silent bugs.

---

## 8. [[CI/Build] Remove hardcoded America/Los_Angeles timezone from Dockerfiles](https://github.com/vllm-project/vllm/pull/33553)


### Base Information

- **PR Number:** #33553
- **Author:** [carlory](https://github.com/carlory)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-02 22:32:39
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33553/files) (2):**
  - `docker/Dockerfile`
  - `docker/Dockerfile.nightly_torch`

### Summary

**What changed and why**  
Removed hardcoded `debconf-set-selections` commands that set the timezone to `America/Los_Angeles` in Dockerfiles. This corrects an accidental reintroduction of a previously fixed issue, ensuring containers default to UTC—the standard for production environments—while allowing custom timezones via the `TZ` environment variable or file mounts.

**Technical impact**  
Docker images will now inherit the base system’s timezone (UTC by default) instead of forcing Pacific Time. This aligns with production best practices, reduces configuration drift, and simplifies timezone management for users who can override it via environment variables or volume mounts.

**Potential risks**  
If downstream systems or tests implicitly depend on the `America/Los_Angeles` timezone, they may encounter unexpected behavior. However, this risk is minimal as UTC is the standard and the change restores the intended behavior. Users relying on the hardcoded timezone must now explicitly set it via `TZ` or mounted timezone files.

**Key insights**  
This fix reinforces the importance of avoiding hardcoded environment-specific configurations in Dockerfiles. Developers should ensure CI/CD pipelines and tests validate timezone-sensitive operations against UTC or configurable timezones. Consider adding a linting rule to prevent similar regressions in Dockerfile changes.

---

## 9. [Fix quantized Falcon-H1 model loading issues](https://github.com/vllm-project/vllm/pull/32728)


### Base Information

- **PR Number:** #32728
- **Author:** [shengliangxu](https://github.com/shengliangxu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-02 22:31:27
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32728/files) (1):**
  - `vllm/model_executor/models/falcon_h1.py`

### Summary

**What changed and why**  
This PR fixes quantized Falcon-H1 model loading by addressing two issues: passing quantization configuration to submodules (attention and MLP blocks) and remapping key-value cache scaling factor names. The changes ensure compatibility with Nvidia Model Optimizer and other quantization libraries that store scaling factors in different locations within the model architecture.

**Technical impact**  
The modifications enable proper loading of quantized Falcon-H1 models by ensuring quantization configurations propagate to all relevant components and allowing flexible mapping of kv-cache scaling factors. This maintains model accuracy while supporting different quantization toolchains, as evidenced by comparable GSM8K benchmark results between quantized and non-quantized versions.

**Potential risks**  
The kv-scale remapping logic could potentially skip loading legitimate parameters if the `maybe_remap_kv_scale_name` function returns `None` for non-scale parameters containing "scale" in their names. Additionally, the changes assume consistent quantization configuration requirements across all submodules, which may not hold for future architectural variations.

**Key insights**  
Developers should verify that the `maybe_remap_kv_scale_name` function handles all edge cases correctly and consider adding logging when scale parameters are skipped. The successful benchmark results demonstrate the fix effectively preserves model performance while enabling quantization support, making this a critical enhancement for deployment efficiency.

---

## 10. [[Frontend] Add sampling parameters to Responses API](https://github.com/vllm-project/vllm/pull/32609)


### Base Information

- **PR Number:** #32609
- **Author:** [DanielMe](https://github.com/DanielMe)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-02 21:51:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32609/files) (3):**
  - `tests/entrypoints/openai/responses/test_sampling_params.py`
  - `tests/entrypoints/openai/responses/test_simple.py`
  - `vllm/entrypoints/openai/responses/protocol.py`

### Summary

**What changed and why**  
This PR adds five core sampling parameters (`repetition_penalty`, `seed`, `stop`, `ignore_eos`, `vllm_xargs`) to the `/v1/responses` API by extending the `ResponsesRequest` protocol. The goal is to align the Responses API with the `/v1/chat/completions` API, providing users with essential generation control for text sampling.

**Technical impact**  
The changes extend the request validation and mapping logic in `protocol.py`, ensuring new parameters are correctly passed to the underlying `SamplingParams`. The integration maintains backward compatibility by setting sensible defaults (e.g., `repetition_penalty` defaults to 1.0, `stop` defaults to an empty list) and includes bounds validation for the `seed` parameter using PyTorch’s `long` integer limits.

**Potential risks**  
- The `seed` parameter validation relies on PyTorch’s integer bounds, which may differ across platforms or PyTorch versions.  
- The `stop` parameter accepts both string and list inputs, but the conversion logic could introduce edge cases if a user passes `None` (currently defaults to `[]`).  
- The `vllm_xargs` dictionary allows arbitrary custom extensions, which may lead to unexpected behavior if malformed or unsupported keys are provided.

**Key insights**  
- The implementation wisely focuses on a minimal set of essential parameters, avoiding over-engineering while ensuring parity with the chat completions API.  
- Developers should note that `repetition_penalty` is now supported, whereas `presence_penalty` and `frequency_penalty` are not—this distinction should be documented for users.  
- The added unit and integration tests provide strong coverage for parameter mapping and validation, but future work should include testing for cross-API consistency with `/v1/chat/completions`.

---

## 11. [[Bugfix] Fix mm budget setting for Qwen Omni models](https://github.com/vllm-project/vllm/pull/33634)


### Base Information

- **PR Number:** #33634
- **Author:** [ywang96](https://github.com/ywang96)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 20:56:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33634/files) (1):**
  - `vllm/multimodal/budget.py`

### Summary

**What changed and why**  
Added a dictionary comprehension filter and explanatory comment to handle models where multiple modalities share the same placeholder tokens (e.g., Qwen3Omni with `use_audio_in_video=True`). This prevents KeyErrors when active modalities aren't present in `all_mm_max_toks_per_item`.

**Technical impact**  
The change makes modality budget calculation more robust by gracefully handling shared placeholders across modalities. It ensures `mm_max_toks_per_item` only includes modalities with independent tokens, maintaining compatibility with existing budget computation logic.

**Potential risks**  
If a modality is incorrectly filtered out due to missing from `all_mm_max_toks_per_item` (e.g., from a configuration error), budget allocation could become incomplete. The fix assumes shared-placeholder behavior is intentional—misconfigured models might silently skip modalities.

**Key insights**  
Always validate that shared-placeholder configurations align with model specifications. Consider adding logging or assertions to detect unexpected modality exclusions during development. This pattern may be relevant for other multimodal systems with token reuse.

---

## 12. [[Feature][CPU Backend]: Optimize ARM vectorization backend](https://github.com/vllm-project/vllm/pull/30329)


### Base Information

- **PR Number:** #30329
- **Author:** [Radu2k](https://github.com/Radu2k)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-02 20:17:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30329/files) (5):**
  - `csrc/cpu/cpu_attn_impl.hpp`
  - `csrc/cpu/cpu_types_arm.hpp`
  - `csrc/cpu/dnnl_kernels.cpp`
  - `csrc/cpu/mla_decode.cpp`
  - `csrc/cpu/utils.hpp`

### Summary

**What changed and why**  
This PR implements ARM vectorization using PyTorch's `at::vec::Vectorized<T>` abstraction, introducing a new `VectorizedRegWrapper` class to unify vector register handling across FP32, FP16, and BF16 data types. It removes platform-specific BF16 guards and provides optimized SIMD reductions and memory operations.

**Technical impact**  
The changes centralize vectorized operations, reducing code duplication and improving maintainability. By leveraging PyTorch's vectorization backend, the implementation gains better portability and potential performance optimizations. The removal of BF16 conditionals simplifies the codebase and ensures consistent behavior across ARM platforms.

**Potential risks**  
The removal of BF16 feature detection may cause issues on older ARM hardware without native BF16 support, though a scalar fallback is provided. Heavy reliance on template metaprogramming could increase compilation times and obscure error messages. Changes to low-level vector operations risk introducing subtle performance regressions or correctness issues.

**Key insights**  
Developers should verify that the scalar fallback for BF16 is robust on all target ARM architectures. The new abstraction layer improves code reuse but requires careful testing to ensure performance parity. Future extensions to integer types should follow the same `VectorizedRegWrapper` pattern for consistency.

---

## 13. [[torch.compile] Don't do the fast moe cold start optimization if there is speculative decoding](https://github.com/vllm-project/vllm/pull/33624)


### Base Information

- **PR Number:** #33624
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-02 19:38:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33624/files) (2):**
  - `vllm/config/compilation.py`
  - `vllm/forward_context.py`

### Summary

**What changed and why**  
Added a `fast_moe_cold_start` flag to `CompilationConfig` (default `True`) to control an optimization that reduces cold‑start time for MoE models. In `create_forward_context`, the optimization is now automatically disabled when speculative decoding is enabled, because the underlying assumptions (only one model runs, MoE layers execute in initialization order) are violated by speculative decoding, which could cause silent correctness issues.

**Technical impact**  
The change conditionally sets `all_moe_layers` to `None` when speculative decoding is active, effectively turning off the fast MoE cold‑start optimization in that scenario. This prevents incorrect behavior when a draft model with MoE layers is present, while preserving the optimization’s benefits for non‑speculative decoding.

**Potential risks**  
If speculative decoding is enabled but not properly detected (e.g., due to misconfiguration), the optimization might still be applied incorrectly. The warning logged when speculative decoding is active may be missed in logs. Additionally, the optimization remains enabled by default, so users unaware of its assumptions could encounter issues in other unsupported scenarios (e.g., multiple MoE models).

**Key insights**  
Always disable `fast_moe_cold_start` when using speculative decoding; the code now does this automatically. Consider making the optimization opt‑in rather than default‑on to avoid hidden correctness risks. The warning should be elevated to an error in development environments to ensure visibility.

---

## 14. [[CI/Build] add directions for CPU image upload to Docker Hub](https://github.com/vllm-project/vllm/pull/32032)


### Base Information

- **PR Number:** #32032
- **Author:** [nathan-weinberg](https://github.com/nathan-weinberg)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-02-02 18:48:07
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32032/files) (1):**
  - `.buildkite/scripts/annotate-release.sh`

### Summary

**What changed and why**  
The PR adds CPU image publishing instructions to Docker Hub in the `annotate-release.sh` script. This enables public availability of CPU-based Docker images for CI and developer use, addressing a gap where previously only GPU images were published to Docker Hub while CPU images were limited to ECR.

**Technical impact**  
This change extends the release pipeline to create multi-architecture CPU images (`x86_64` and `arm64`) with proper version tagging (`v${RELEASE_VERSION}` and `latest`). It establishes a consistent tagging pattern parallel to the existing GPU image workflow, making CPU images accessible through Docker Hub under the `vllm/vllm-openai-cpu` repository.

**Potential risks**  
The script assumes CPU images are already built and available in ECR, which could fail if upstream build steps don't complete successfully. There's also a risk of tag conflicts if the manifest removal command (`docker manifest rm`) fails silently, though the `\|\| true` mitigates this. No authentication or permission validation is included in the script itself.

**Key insights**  
Developers should verify that ECR image builds succeed before running this script. The implementation correctly follows established patterns from GPU image publishing, maintaining consistency. Consider adding validation steps to ensure Docker Hub credentials are properly configured in the CI environment before attempting pushes.

---

## 15. [[BugFix] DPMetadata raises assert error for dense model](https://github.com/vllm-project/vllm/pull/32739)


### Base Information

- **PR Number:** #32739
- **Author:** [River12](https://github.com/River12)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-02-02 16:56:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32739/files) (1):**
  - `vllm/forward_context.py`

### Summary

**What changed and why**  
The fix addresses an assertion error that occurs when running non-MoE (Mixture of Experts) models with Data Parallelism (DP) enabled. Previously, the code incorrectly triggered an assertion that expected `is_moe_model` to be `False` in certain DP scenarios. The change modifies the condition to check if `is_moe_model` is not `False` before proceeding with DP metadata initialization.

**Technical impact**  
This change prevents unnecessary DP metadata initialization for non-MoE models when DP is enabled, ensuring the code path only executes when relevant. It maintains backward compatibility for MoE models while fixing the assertion failure for dense models, allowing DP to function correctly across both model types without runtime errors.

**Potential risks**  
The condition `is_moe_model is not False` uses identity comparison (`is`) rather than equality (`==`), which could behave unexpectedly if `is_moe_model` is a boolean value that may be `True` or `False`. This might lead to subtle bugs if the variable is not strictly a boolean literal. Additionally, the logic now depends on three combined conditions, increasing cognitive load for future maintenance.

**Key insights**  
Use equality operators (`==`) instead of identity (`is`) for boolean comparisons to ensure robustness. Consider simplifying the condition or adding a comment explaining why `is not False` is necessary. Verify that the fix handles all DP configurations correctly, especially edge cases where `is_moe_model` might be `None` or another non-boolean value.

---

## 16. [[Release] Fix format and cherry-pick](https://github.com/vllm-project/vllm/pull/33618)


### Base Information

- **PR Number:** #33618
- **Author:** [zhewenl](https://github.com/zhewenl)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-02 16:19:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33618/files) (2):**
  - `vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py`
  - `vllm/model_executor/models/step3p5.py`

### Summary

**What changed and why**  
This PR fixes two issues: a file deletion that wasn't properly cherry-picked, and an import ordering violation. The `nvfp4_moe_support.py` file was removed because it references non-existent attributes after a previous change, and the import order in `step3p5.py` was corrected to maintain code style standards.

**Technical impact**  
Removing the `nvfp4_moe_support.py` file eliminates mypy errors about missing attributes and resolves pre-commit failures. The import reordering in `step3p5.py` has no functional impact but ensures compliance with the project's linting rules, specifically maintaining consistent import organization.

**Potential risks**  
The file deletion could break dependencies if other modules still import from `nvfp4_moe_support.py`. The import change is low-risk but should be verified to not introduce circular dependencies, though the original order already imported from `vllm.attention.layer`.

**Key insights**  
Always verify cherry-picks include file deletions to avoid lingering references. The import fix highlights the importance of consistent import ordering for maintainability. Developers should run full pre-commit checks after cherry-picking to catch similar issues early.

---

## 17. [[Release] patch step3p5 attention class in v0.15.1 release](https://github.com/vllm-project/vllm/pull/33602)


### Base Information

- **PR Number:** #33602
- **Author:** [zhewenl](https://github.com/zhewenl)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-02 14:54:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33602/files) (1):**
  - `vllm/model_executor/models/step3p5.py`

### Summary

**What changed and why**  
The change updates the import path for the `Attention` class in `step3p5.py` from `vllm.model_executor.layers.attention` to `vllm.attention.layer`. This fixes a CI failure caused by a model support PR being behind an attention class refactoring, ensuring the code aligns with the new module structure.

**Technical impact**  
This update ensures the `step3p5` model correctly references the refactored attention module, maintaining compatibility with the rest of the codebase. It resolves import errors that would otherwise break model initialization and inference workflows.

**Potential risks**  
If other model files still use the old import path, they may encounter similar CI failures. Additionally, any downstream dependencies or custom extensions relying on the previous module structure could be affected.

**Key insights**  
Developers should verify that all model implementations use the updated attention import path to prevent similar issues. This change highlights the importance of synchronizing model support PRs with architectural refactorings to avoid CI breaks.

---

## 18. [[Voxtral Realtime] Introduce global log mel max](https://github.com/vllm-project/vllm/pull/33574)


### Base Information

- **PR Number:** #33574
- **Author:** [patrickvonplaten](https://github.com/patrickvonplaten)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-02 14:01:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33574/files) (4):**
  - `tests/entrypoints/openai/test_realtime_validation.py`
  - `tests/models/multimodal/generation/test_voxtral_realtime.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/transformers_utils/configs/mistral.py`

### Summary

**What changed and why**  
The changes introduce a `global_log_mel_max` configuration parameter to replace per-frame normalization of log-mel spectrograms in Voxtral's realtime audio processing. This aims to stabilize transcriptions by using a consistent normalization value instead of normalizing based on short, variable audio frames.

**Technical impact**  
The `compute_whisper_melspec` function now uses a configurable global maximum value for log-mel normalization when `global_log_mel_max` is provided, falling back to the previous per-frame max behavior if not set. This affects audio feature extraction consistency across streaming chunks and improves transcription stability as evidenced by updated test expectations.

**Potential risks**  
If `global_log_mel_max` is incorrectly calibrated for different audio environments, it could degrade transcription quality. The type checking (`float`) is minimal, and there's no validation for reasonable value ranges. The changes to test expectations suggest the normalization shift meaningfully alters output text, which could affect downstream applications relying on specific transcriptions.

**Key insights**  
The removal of test normalization adjustments indicates the new approach successfully aligns streaming and offline processing. Developers should validate `global_log_mel_max` values across diverse audio datasets and consider making this parameter tunable per deployment environment. The change demonstrates that consistent audio normalization is critical for realtime ASR quality.

---

## 19. [fix cutlass_3x_gemm_fp8_blockwise on sm103a](https://github.com/vllm-project/vllm/pull/32224)


### Base Information

- **PR Number:** #32224
- **Author:** [IwakuraRein](https://github.com/IwakuraRein)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-02 11:47:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32224/files) (7):**
  - `csrc/cutlass_extensions/common.hpp`
  - `csrc/quantization/w8a8/cutlass/c3x/scaled_mm.cuh`
  - `csrc/quantization/w8a8/cutlass/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh`
  - `csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm100_fp8_dispatch.cuh`
  - `csrc/quantization/w8a8/cutlass/scaled_mm_c2x.cuh`
  - `csrc/quantization/w8a8/cutlass/scaled_mm_c2x_sm89_fp8_dispatch.cuh`
  - `csrc/quantization/w8a8/cutlass/scaled_mm_c2x_sm89_int8_dispatch.cuh`

### Summary

**What changed and why**  
The PR fixes a bug where `cutlass_3x_gemm_fp8_blockwise` produced incorrect results on sm103a GPUs. It updates architecture guards in `common.hpp` to include sm103a support and adds runtime error messages for unsupported architectures. The changes also consolidate architecture-specific wrappers and apply them consistently across multiple kernel definitions.

**Technical impact**  
This refactors the architecture guard system to provide better error handling and support for newer GPU architectures. The `enable_sm100f_only` wrapper now supports both sm1000 (100a) and sm1030 (103a) architectures, ensuring FP8 kernels work correctly on sm103a. The changes also remove duplicate guard definitions from `scaled_mm_c2x.cuh` and standardize their usage across the codebase.

**Potential risks**  
The `asm("trap;")` statements in the new error handling could cause abrupt kernel termination without proper cleanup. The architecture range checks (e.g., `sm[80, 89)`) might not handle future architecture versions correctly. There's also a risk that the consolidated guards could inadvertently affect other kernels if not properly tested.

**Key insights**  
Developers should verify that all architecture-specific kernels use the appropriate guard wrappers. The error messages will help debug runtime architecture mismatches but should be complemented with compile-time checks where possible. Future architecture additions should follow the same pattern of updating the guard helpers in `common.hpp` rather than creating new ad-hoc solutions.

---

## 20. [fix memory for online fp8 quantization with streaming weight load](https://github.com/vllm-project/vllm/pull/31914)


### Base Information

- **PR Number:** #31914
- **Author:** [vkuzo](https://github.com/vkuzo)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-02-02 11:17:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31914/files) (5):**
  - `tests/quantization/test_fp8.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/model_loader/base_loader.py`
  - `vllm/model_executor/model_loader/dummy_loader.py`
  - `vllm/model_executor/model_loader/weight_utils.py`

### Summary

**What changed and why**  
This PR fixes a memory inefficiency in online FP8 quantization where linear and MoE weights were initially created in bf16 and held in memory throughout the weight-loading loop, causing high peak memory usage. The solution defers weight materialization by creating weights on the `meta` device and materializing them just-in-time during loading. Additionally, logging for peak memory usage and a unit test for MoE models were added, along with a workaround to maintain `--load_format dummy` functionality.

**Technical impact**  
The changes significantly reduce peak GPU memory during model loading for online FP8 quantization—from 40.48 GiB to 14.98 GiB in the provided example—while maintaining the same final memory footprint. The architecture now supports streaming weight post-processing more effectively by avoiding unnecessary bf16 weight retention. The modifications are localized to FP8 quantization logic and weight initialization routines.

**Potential risks**  
The just-in-time materialization relies on internal bookkeeping (e.g., `_loaded_numel`, `_load_device`), which could break if future changes alter the weight-loading flow. The workaround for `--load_format dummy` adds complexity and may not generalize to other quantization methods. Edge cases, such as expert parallelism where weight loaders run without copying data, require careful handling to avoid re-initialization.

**Key insights**  
Developers should note that online quantization now uses `meta` device tensors during creation, with materialization deferred to `patched_weight_loader`. The added peak memory logging aids in monitoring and regression testing. Ensure any new quantization methods follow a similar pattern to avoid memory bloat. The `_copy_missing_attrs` helper is critical for preserving tensor metadata during materialization.

---

## 21. [[UX] Format attention backend log line](https://github.com/vllm-project/vllm/pull/33570)


### Base Information

- **PR Number:** #33570
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-02 10:57:13
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33570/files) (1):**
  - `vllm/platforms/cuda.py`

### Summary

**What changed and why**  
The PR modifies the attention backend selection log line to use a consistent list format with square brackets and quotes, matching the existing MoE backend logging style. This changes the output from a Python tuple representation to a formatted string list.

**Technical impact**  
This change only affects log formatting and has no functional impact on the attention backend selection logic. It improves log consistency by aligning the attention backend logging format with the MoE backend logging format already in use elsewhere in the codebase.

**Potential risks**  
The risk is minimal since this is purely a cosmetic logging change. However, any downstream systems parsing these log lines may need to adjust their parsing logic if they were relying on the tuple format. The change also introduces string concatenation where tuple formatting was previously used.

**Key insights**  
This is a straightforward consistency improvement that enhances log readability and uniformity. Developers should verify that no log parsing tools depend on the previous tuple format, and consider applying similar formatting consistency to other log messages if needed.

---

## 22. [Reduce the kernel overhead when num of active loras is smaller than max loras. Multiple cuda graphs are captured for each num of active-loras.](https://github.com/vllm-project/vllm/pull/32005)


### Base Information

- **PR Number:** #32005
- **Author:** [yugong333](https://github.com/yugong333)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-02-02 09:30:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32005/files) (15):**
  - `tests/lora/test_fused_moe_lora_kernel.py`
  - `tests/lora/test_punica_ops.py`
  - `tests/v1/cudagraph/test_cudagraph_dispatch.py`
  - `vllm/config/lora.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/forward_context.py`
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`
  - `vllm/lora/ops/triton_ops/lora_expand_op.py`
  - `vllm/lora/ops/triton_ops/lora_kernel_metadata.py`
  - `vllm/lora/ops/triton_ops/lora_shrink_op.py`
  - `vllm/lora/punica_wrapper/punica_gpu.py`
  - `vllm/lora/utils.py`
  - `vllm/v1/cudagraph_dispatcher.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/lora_model_runner_mixin.py`

### Summary

**What changed and why**  
This PR introduces specialization for different counts of active LoRA adapters to reduce kernel overhead when fewer adapters are used. It adds a `specialize_active_lora` configuration option and captures multiple CUDA graphs for powers-of-two active LoRA counts up to `max_loras`. The changes propagate `num_active_loras` through the dispatch system and kernel metadata, allowing Triton kernels to size their launch grids based on the actual number of active adapters.

**Technical impact**  
The system now captures separate CUDA graphs for different active LoRA counts (including zero), which improves performance for variable LoRA usage patterns by reducing grid overhead. Kernels like `fused_moe_lora`, `lora_shrink`, and `lora_expand` accept `num_active_loras` to adjust their grid dimensions. The dispatch logic rounds up to the nearest captured LoRA count to match graphs, ensuring compatibility while minimizing wasted threads.

**Potential risks**  
Increased startup time and memory usage due to capturing multiple graphs (especially with high `max_loras`). The rounding logic may dispatch to a larger graph than necessary, potentially leaving some threads idle. Edge cases include incorrect `num_active_loras` computation when mixing LoRA and non-LoRA tokens, and the risk of graph mismatches if the capture counts list is inconsistent between components.

**Key insights**  
This optimization is beneficial for workloads with varying LoRA usage, as it reduces kernel overhead when fewer adapters are active. Developers should enable `specialize_active_lora` only when performance gains outweigh the capture overhead. Ensure `captured_lora_counts` is consistently derived (using `get_captured_lora_counts`) across all components to avoid dispatch errors. Test with realistic LoRA activation patterns to validate performance improvements.

---

## 23. [Update huggingface-hub again](https://github.com/vllm-project/vllm/pull/33567)


### Base Information

- **PR Number:** #33567
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-02 09:20:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33567/files) (2):**
  - `requirements/rocm-test.txt`
  - `requirements/test.txt`

### Summary

**What changed and why**  
Updated huggingface-hub from version 0.36.0 to 0.36.1 in both `requirements/rocm-test.txt` and `requirements/test.txt`. The change aims to leverage improved HTTP error logging in the new version, which will help the Hub team diagnose timeout issues encountered in CI environments.

**Technical impact**  
This is a minor version bump that primarily enhances debugging capabilities without introducing breaking changes. The update affects testing dependencies only, so it won't impact production deployments but should provide more detailed error information during CI runs involving Hugging Face Hub interactions.

**Potential risks**  
Minimal risk given it's a patch release focused on logging improvements. However, any dependency change could introduce subtle behavioral differences or new transient bugs. Ensure compatibility with existing pinned versions of related packages like `transformers` and `accelerate`.

**Key insights**  
The update is a targeted improvement for CI debuggability. Developers should verify that the enhanced logging doesn't produce excessive noise in CI outputs. Consider documenting any timeout patterns revealed by the new logs to inform future infrastructure adjustments.

---

## 24. [Remove incorrect tokenizer info test](https://github.com/vllm-project/vllm/pull/33565)


### Base Information

- **PR Number:** #33565
- **Author:** [hmellor](https://github.com/hmellor)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 09:11:44
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33565/files) (1):**
  - `tests/entrypoints/openai/test_tokenization.py`

### Summary

**What changed and why**  
The PR removes a test that validated the structure of the `added_tokens_decoder` field in the `/tokenizer_info` endpoint. This test was incorrect because the serialization logic intentionally returns only the string `content` from `AddedToken` objects, not full dictionaries. The test was passing only because the test model lacked `added_tokens_decoder` entries in Transformers v4, but it fails under Transformers v5 where such entries exist.

**Technical impact**  
Removing the test ensures compatibility with Transformers v5 while preserving the existing API behavior. The endpoint will continue to return string values for `added_tokens_decoder` instead of structured dictionaries, aligning with the serialization design. No functional changes are made to the codebase.

**Potential risks**  
If downstream consumers rely on the test’s expected dictionary structure (despite it never being implemented), they may encounter issues. Additionally, the removal reduces test coverage for the `added_tokens_decoder` field, though the field’s actual behavior remains untested. Future changes to the serialization logic could introduce regressions without detection.

**Key insights**  
The test was inherently flawed and misaligned with the implemented serialization. Developers should verify that any API expectations match the actual serialization behavior, especially for nested objects. Consider adding a targeted test for the `added_tokens_decoder` field that validates the correct string output, or document the API’s behavior to prevent confusion.

---

## 25. [[Model] Use mm_position to compute mrope positions for GLM-4.xV](https://github.com/vllm-project/vllm/pull/33039)


### Base Information

- **PR Number:** #33039
- **Author:** [KKSK-DON](https://github.com/KKSK-DON)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-02 08:55:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33039/files) (3):**
  - `examples/offline_inference/vision_language_multi_image.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/glm4v.py`

### Summary

**What changed and why**  
The PR refactors the `get_mrope_input_positions` method in GLM-4V and GLM-4.1V models to compute multi-modal rotary position embeddings using `mm_position` metadata instead of scanning token IDs. It fixes a bug where video tokens were incorrectly classified as text due to outdated token ID checks, and adds a multi-image example for GLM-4.1V.

**Technical impact**  
The changes simplify position calculation by directly iterating over `mm_features` sorted by offset, eliminating complex token-type detection logic. This improves maintainability and correctness, especially for video modalities. The addition of `iter_mm_grid_thw` abstracts grid dimension extraction, and the example script supports multi-image inference for GLM-4.1V.

**Potential risks**  
The refactored logic assumes `mm_features` are correctly populated and sorted by offset; any misalignment could lead to incorrect position mappings. The switch from PyTorch to NumPy for index operations may introduce subtle performance or type differences, though the final output is converted back to a tensor. The GLM-4V implementation lacks video support, which could cause errors if video features are unexpectedly provided.

**Key insights**  
The fix addresses a critical bug in video token handling by leveraging `mm_position` metadata, which is more reliable than token ID scanning. Developers should ensure `mm_features` are properly structured and validated. The new example demonstrates multi-image usage, but the GLM-4V model should be updated if video support is required in the future.

---

## 26. [[CI] Add DeepSeek V3.2 nightly eval](https://github.com/vllm-project/vllm/pull/33566)


### Base Information

- **PR Number:** #33566
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-02-02 08:10:02
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33566/files) (3):**
  - `tests/evals/gsm8k/configs/DeepSeek-V3.2-DP.yaml`
  - `tests/evals/gsm8k/configs/DeepSeek-V3.2-TP.yaml`
  - `tests/evals/gsm8k/configs/models-h200.txt`

### Summary

**What changed and why**  
This PR adds DeepSeek-V3.2 to the nightly LM evaluation suite on H200 hardware. Two configuration files were created (one for tensor parallel and one for data parallel execution) to test the model under different parallelization strategies. The change aims to proactively catch potential issues with this new model, as referenced in the linked GitHub issue.

**Technical impact**  
The addition enables automated testing of DeepSeek-V3.2 in the existing evaluation framework, specifically for GSM8K benchmark evaluations. The configurations specify expert parallel execution (--enable-expert-parallel) and speculative decoding with MTP method, indicating this model utilizes MoE (Mixture of Experts) architecture. This expands test coverage for MoE models in the vLLM codebase.

**Potential risks**  
The 20-minute startup timeout (1200 seconds) suggests potential initialization complexity for this model. The accuracy threshold of 0.95 is quite high for GSM8K and may be difficult to achieve consistently. There's no documentation update mentioned for this new model support, which could lead to user confusion if the model becomes available but isn't documented.

**Key insights**  
This is a preventive measure to catch integration issues early through automated testing. Developers should monitor the test results to ensure the model performs within the specified accuracy threshold. The parallel configurations (TP vs DP) will provide valuable data on scaling behavior for this MoE model. Consider adding documentation updates if this model support is intended for production use.

---

## 27. [[Refactor] Move profiling methods to MM budget](https://github.com/vllm-project/vllm/pull/33559)


### Base Information

- **PR Number:** #33559
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-02 07:27:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33559/files) (8):**
  - `vllm/lora/model_manager.py`
  - `vllm/multimodal/budget.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/core/encoder_cache_manager.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
The changes move profiling and budget calculation logic for multi-modal models from the MM registry into a dedicated `MultiModalBudget` class. This refactor centralizes budget computation and simplifies the registry by removing related methods. The motivation is to handle text-only encoder-decoder models uniformly as multi-modal models for consistency.

**Technical impact**  
The `MultiModalBudget` class now encapsulates all budget-related calculations, including encoder cache sizing and item limits per modality. This improves modularity and reduces coupling between the registry and other components like the scheduler and input processor. The registry becomes cleaner, focusing solely on processor creation and dummy input generation.

**Potential risks**  
Removing the `get_encdec_max_encoder_len` method from the registry could break existing code that relies on it for encoder-decoder models. The new logic depends on `mm_max_toks_per_item`, which assumes at least one modality is active; edge cases with zero active modalities need careful handling. Additionally, the refactor changes how encoder budgets are computed in several places, which could introduce subtle bugs if not thoroughly tested.

**Key insights**  
The refactor enhances code organization by separating concerns: the registry manages processors, while `MultiModalBudget` handles resource allocation. Developers should ensure all multimodal and encoder-decoder models are tested with the new budget calculations. Pay attention to the removal of `mm_processor_info` in `model_manager.py`—it's now replaced by direct access to `mm_budget.mm_max_items_per_prompt`.

---

## 28. [[Feature][Core] Support Fabric detection to adapt the MNNVL protocol for the GB series](https://github.com/vllm-project/vllm/pull/33540)


### Base Information

- **PR Number:** #33540
- **Author:** [kebe7jun](https://github.com/kebe7jun)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-02-02 06:55:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33540/files) (1):**
  - `csrc/cumem_allocator.cpp`

### Summary

**What changed and why**  
The change adds Fabric handle support for GPU memory allocation when multi-node NVLink (MNNVL) is available, specifically targeting GB-series GPUs. It detects Fabric support via a CUDA device attribute and attempts allocation with `CU_MEM_HANDLE_TYPE_FABRIC`. If that fails due to missing MNNVL hardware, it gracefully falls back to POSIX file descriptor handles.

**Technical impact**  
This enables the use of the MNNVL protocol for cross-node communication in GB-series environments lacking RoCE networks, improving performance and stability. The fallback mechanism ensures compatibility with systems where Fabric allocation isn't permitted or supported, maintaining backward compatibility.

**Potential risks**  
The fallback logic only triggers for specific CUDA errors (`CUDA_ERROR_NOT_PERMITTED` or `CUDA_ERROR_NOT_SUPPORTED`). Other allocation failures could be incorrectly handled if they produce different error codes. Additionally, the change assumes Fabric handle support implies MNNVL availability, which may not hold true in all configurations.

**Key insights**  
The PR significantly improves throughput in GB200 environments without RoCE, as shown by benchmark results. Developers should ensure that sleep-mode is enabled when using custom allocators, as noted in the test plan. Consider extending error handling to cover a broader range of allocation failures for robustness.

---

## 29. [move spec decode slow test to test_areas.yaml](https://github.com/vllm-project/vllm/pull/33365)


### Base Information

- **PR Number:** #33365
- **Author:** [shanjiaz](https://github.com/shanjiaz)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-02 06:28:36
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33365/files) (2):**
  - `.buildkite/test_areas/misc.yaml`
  - `tests/v1/spec_decode/test_acceptance_length.py`

### Summary

**What changed and why**  
The PR moves a previously approved slow test for spec decode from the main test suite to a dedicated section in `test_areas.yaml`. This change excludes slow tests from regular spec decode test runs by adding the `-m 'not slow_test'` filter and creates a new optional Buildkite job to run only the slow tests separately.

**Technical impact**  
This improves CI efficiency by separating slow acceptance length tests from faster unit tests, allowing the main spec decode tests to run more quickly while still enabling optional execution of resource-intensive tests on H100 GPUs. The test logic remains unchanged; only the test orchestration is modified.

**Potential risks**  
If the `slow_test` pytest marker is not consistently applied to all intended tests, some slow tests could inadvertently run in the main suite or be omitted from the optional job. The optional job's 25-minute timeout may need adjustment if test execution time grows.

**Key insights**  
Developers should ensure all slow tests are properly marked with `@pytest.mark.slow_test`. The optional job provides a controlled environment for running heavy tests without blocking routine CI pipelines. Verify that the `VLLM_ALLOW_INSECURE_SERIALIZATION` environment variable is appropriate for the test context.

---

## 30. [[Bugfix] Enable Kimi k25 processor test](https://github.com/vllm-project/vllm/pull/33562)


### Base Information

- **PR Number:** #33562
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 06:25:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33562/files) (4):**
  - `examples/offline_inference/vision_language.py`
  - `tests/models/multimodal/processing/test_common.py`
  - `vllm/model_executor/models/kimi_k25.py`
  - `vllm/multimodal/parse.py`

### Summary

**What changed and why**  
This PR enables testing for the Kimi-K2.5 vision-language model by fixing its processor's handling of `vision_chunk` inputs. The changes add support for a new `vision_chunk` modality in the example script and test suite, and correct the processor logic to properly expand media token placeholders with the correct number of tokens.

**Technical impact**  
The modifications extend the multimodal input system to support `vision_chunk` as a first-class modality alongside `image` and `video`. The processor now correctly calculates and inserts the appropriate number of media tokens when encountering the media placeholder token, ensuring proper token alignment between text and visual inputs for the Kimi-K2.5 model.

**Potential risks**  
The `random_vision_chunk` function's logic for handling `num_frames > 1` returns a dictionary with `"type": "video_chunk"`, but the modality key is `"vision_chunk"`. This inconsistency could cause parsing errors. Additionally, the processor assumes `num_tokens_per_chunk` list length matches the number of media placeholder tokens, which may fail if the input text contains unexpected media tokens.

**Key insights**  
The fix correctly addresses the core issue of media token expansion, but developers should verify the `vision_chunk`/`video_chunk` type consistency in test data generation. The processor's dependency on exact token matching (`token == self.media_token_id`) is fragile; consider adding validation to ensure media token counts align with provided vision chunks.

---

## 31. [[MoE] Enable Shared/Routed Overlap For Latent MoE (Nemotron-H)](https://github.com/vllm-project/vllm/pull/32790)


### Base Information

- **PR Number:** #32790
- **Author:** [danielafrimi](https://github.com/danielafrimi)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-02-02 06:18:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32790/files) (4):**
  - `tests/kernels/moe/test_shared_fused_moe_routed_transform.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/layers/fused_moe/shared_fused_moe.py`
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
The changes enable parallel CUDA stream execution between shared and routed experts in latent MoE architectures like Nemotron-H. Previously, the dimension mismatch between shared experts (needing original hidden size) and routed experts (needing compressed latent size) prevented streaming optimization. The solution introduces a `routed_input_transform` hook to apply transformations (e.g., latent projection) within the MoE layer, saving the original input for shared experts while routed experts use the transformed input, allowing concurrent execution.

**Technical impact**  
This modifies the `FusedMoE` base class to support latent MoE transformations via a new hook and context manager, enabling shared and routed experts to run in parallel on separate CUDA streams. The `SharedFusedMoE` class now accepts a `routed_input_transform` parameter, and the Nemotron-H model passes its latent projection module to leverage this optimization. The changes maintain backward compatibility for non-latent MoE models.

**Potential risks**  
The optimization shows neutral or minor performance gains in benchmarks, with some configurations showing a slight regression in mean TTFT (+3.2%). There is a risk of subtle correctness issues if the `_shared_experts_input` state management is mishandled, especially in torch.compile scenarios where the custom op bypasses normal setup. The cloning of inputs for the auxiliary stream adds memory overhead, which could impact memory-bound workloads.

**Key insights**  
Developers should verify that the `routed_input_transform` hook returns only the transformed tensor (not tuples) unless explicitly handled. The changes are most beneficial for batch sizes where shared and routed expert workloads are balanced; performance gains are modest and scenario-dependent. Future optimizations could move the latent projection's all-reduce to the smaller latent dimension for bandwidth savings, as noted in the TODO comments.

---

## 32. [fix[ROCm]: Remove unconditional aiter import](https://github.com/vllm-project/vllm/pull/32902)


### Base Information

- **PR Number:** #32902
- **Author:** [rabi](https://github.com/rabi)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-02-02 06:10:03
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32902/files) (3):**
  - `vllm/_aiter_ops.py`
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
The changes fix an issue where the AITER library was being imported unconditionally at module load time on ROCm platforms, triggering unnecessary JIT compilation and warnings even when the `VLLM_ROCM_USE_AITER` environment variable was set to 0 (disabled). The fix ensures AITER imports are gated behind explicit checks for both platform support and the environment variable.

**Technical impact**  
This modifies the initialization logic for the ROCm AITER backend. The `is_aiter_found_and_supported()` function now also checks `envs.VLLM_ROCM_USE_AITER`, and conditional imports in `rocm_aiter_fa.py` and `eagle.py` are deferred until after verifying AITER is enabled via `rocm_aiter_ops.is_enabled()`. A placeholder `AITER_FP8_DTYPE` is also defined to prevent `NameError` when the module is loaded but AITER is disabled.

**Potential risks**  
The placeholder `AITER_FP8_DTYPE` variable could be misleading if inspected during debugging, as it's a dummy value. There's a minor risk if code paths that don't use the `@if_aiter_supported` decorator try to access AITER modules directly, though the changes appear comprehensive. The dependency on the correct evaluation order of `rocm_aiter_ops.is_enabled()` in module-level code should be verified.

**Key insights**  
The fix correctly addresses the root cause by moving environment variable checks to the earliest point (`is_aiter_found_and_supported`). Developers should ensure all future AITER-related imports follow the same pattern of checking `rocm_aiter_ops.is_enabled()` before import. The use of a placeholder constant is a clean solution to avoid runtime errors while maintaining module load integrity.

---

## 33. [[Model] Use explicit types in `get_generation_prompt`](https://github.com/vllm-project/vllm/pull/33551)


### Base Information

- **PR Number:** #33551
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 04:38:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33551/files) (8):**
  - `vllm/entrypoints/openai/translations/speech_to_text.py`
  - `vllm/model_executor/models/gemma3n_mm.py`
  - `vllm/model_executor/models/glmasr.py`
  - `vllm/model_executor/models/granite_speech.py`
  - `vllm/model_executor/models/qwen3_asr.py`
  - `vllm/model_executor/models/voxtral.py`
  - `vllm/model_executor/models/voxtral_realtime.py`
  - `vllm/model_executor/models/whisper.py`

### Summary

**What changed and why**  
This PR replaces implicit dictionary-based prompt construction with explicit typed prompt objects (`TextPrompt`, `TokensPrompt`, `ExplicitEncoderDecoderPrompt`) across multiple speech-to-text model implementations. The change improves code readability and provides stronger type safety for upcoming Renderer refactoring work.

**Technical impact**  
The changes standardize prompt creation using dedicated data classes instead of loosely-typed dictionaries. This enables better static type checking, clearer intent in the code, and easier maintenance when the prompt schema evolves. The speech-to-text preprocessing logic was also enhanced to properly handle the new explicit encoder-decoder prompt structure.

**Potential risks**  
The `_preprocess_verbose_prompt` method now assumes specific prompt structures that may not be fully validated at runtime. Changes to the `ExplicitEncoderDecoderPrompt` type definition could break the preprocessing logic. There's also a risk of incomplete migration if other parts of the codebase still expect dictionary-based prompts.

**Key insights**  
Developers should use the explicit prompt types (`TextPrompt`, `TokensPrompt`, `ExplicitEncoderDecoderPrompt`) for all new prompt construction. The `is_explicit_encoder_decoder_prompt` helper provides proper type checking. When modifying prompt-related code, ensure compatibility with both the new typed approach and any remaining dictionary-based usage during the transition period.

---

## 34. [Update get_expert_mapping to include self parameter](https://github.com/vllm-project/vllm/pull/33525)


### Base Information

- **PR Number:** #33525
- **Author:** [Otsutsukii](https://github.com/Otsutsukii)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 04:29:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33525/files) (1):**
  - `vllm/model_executor/models/glm4_moe_lite.py`

### Summary

**What changed and why**  
The change adds a missing `self` parameter to the `make_expert_params_mapping` method call within the `get_expert_mapping` function. This fixes a runtime error where the method was being called incorrectly without the required instance reference.

**Technical impact**  
This is a critical bug fix that resolves a `TypeError` when the `get_expert_mapping` method is executed. The correction ensures the `SharedFusedMoE.make_expert_params_mapping` class method receives the proper `self` instance, allowing it to correctly access instance attributes needed for parameter mapping generation in the GLM4 MoE Lite model.

**Potential risks**  
The risk is minimal as this is a straightforward syntax correction. However, it's essential to verify that the `SharedFusedMoE.make_expert_params_mapping` method signature indeed expects `self` as its first parameter and that no other similar calls elsewhere in the codebase have the same issue.

**Key insights**  
This fix highlights the importance of consistent method call signatures, especially when mixing instance and class methods. Developers should ensure all method calls provide the correct number and type of arguments. A broader check for similar patterns in other MoE model files could prevent related issues.

---

## 35. [Fix accessing hidden_act from model config](https://github.com/vllm-project/vllm/pull/32686)


### Base Information

- **PR Number:** #32686
- **Author:** [grzegorz-k-karch](https://github.com/grzegorz-k-karch)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 03:11:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32686/files) (1):**
  - `vllm/model_executor/models/nemotron_nas.py`

### Summary

**What changed and why**  
The code modifies how `hidden_act` (activation function) is retrieved in the Nemotron-NAS model. Previously, it assumed a global `config.hidden_act` value applied to all layers. Now, it first checks if `hidden_act` is defined per-layer within `block_config.ffn`; if not found, it falls back to the global config value. This supports modern heterogeneous models where activation functions can vary by layer while maintaining backward compatibility with older homogeneous models.

**Technical impact**  
This change enables correct activation function selection for each layer in heterogeneous Nemotron-NAS architectures, ensuring proper model execution. The fallback mechanism preserves compatibility with existing models that use a single global `hidden_act` value, preventing breaking changes. The adjustment is localized to the MLP initialization within individual transformer blocks.

**Potential risks**  
If a model incorrectly defines `hidden_act` at both global and per-layer levels with conflicting values, the per-layer definition will take precedence, which may cause unexpected behavior if not intended. Additionally, there is a risk of silent failures if `block_config.ffn` exists but lacks `hidden_act` while `config.hidden_act` is also undefined, though this scenario is unlikely given typical model configurations.

**Key insights**  
Always validate model configurations for consistency between global and per-layer parameters when supporting heterogeneous architectures. Consider adding a warning or log when falling back to the global config to aid debugging. This pattern of per-layer configuration with global fallbacks is reusable for other model parameters that may become layer-specific in future architectures.

---

## 36. [[CI][Bugfix] Fix flaky `tests/v1/kv_connector/unit/test_multi_connector.py::test_multi_example_connector_consistency`](https://github.com/vllm-project/vllm/pull/33555)


### Base Information

- **PR Number:** #33555
- **Author:** [NickLucche](https://github.com/NickLucche)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-02 03:01:30
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33555/files) (1):**
  - `tests/v1/kv_connector/unit/test_multi_connector.py`

### Summary

**What changed and why**  
The test was modified to run generation with single prompts instead of the full PROMPTS list to eliminate race conditions. The flakiness occurred because the test's behavior depended on whether two prompts were scheduled in the same batch or sequentially, which affected KV cache operations.

**Technical impact**  
This change makes the test deterministic by ensuring prompts are processed one at a time, removing scheduling dependencies. It maintains the same verification logic for multi-connector consistency but with controlled execution order, improving test reliability.

**Potential risks**  
The test now covers a narrower scenario (single prompt processing) which might miss edge cases related to batch processing. There's a risk that the fix addresses symptoms rather than underlying issues in the KV connector's batch handling logic.

**Key insights**  
When testing systems with scheduling dependencies, isolate operations to create deterministic test conditions. Consider whether the underlying system should be made more robust to batch scheduling variations, or if additional tests are needed to cover batch scenarios separately.

---

## 37. [[Chore] Remove redundant input parsing methods](https://github.com/vllm-project/vllm/pull/33542)


### Base Information

- **PR Number:** #33542
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-02-02 02:50:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33542/files) (7):**
  - `tests/renderers/test_completions.py`
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/inputs/data.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/utils.py`

### Summary

**What changed and why**  
This PR removes redundant input parsing methods (`is_tokens_prompt`, `is_embeds_prompt`, and most calls to `get_prompt_components`) to simplify the codebase in preparation for a Renderer refactor. The changes replace type-checking functions with direct dictionary access and consolidate prompt text extraction logic.

**Technical impact**  
The removal of type-checking functions (`is_tokens_prompt`, `is_embeds_prompt`) simplifies the input handling interface and reduces abstraction layers. The `EmbedsPrompt` type now includes an optional `prompt` field, allowing more uniform handling of different prompt types. Most code now accesses prompt text directly via `engine_prompt.get("prompt")` instead of using the three-tuple return from `get_prompt_components`.

**Potential risks**  
Direct dictionary access (`engine_prompt.get("prompt")`) assumes consistent dictionary structure across all prompt types, which could break if future changes modify the prompt interface. The removal of explicit type checking might make debugging more difficult when dealing with malformed prompt objects. The test changes remove validation of `is_embeds_prompt` but add direct tensor comparison, which could miss edge cases where prompt structure is valid but type identification is needed.

**Key insights**  
The refactor moves toward a more uniform prompt handling approach where different prompt types share common interfaces. Developers should ensure all prompt types consistently implement the `prompt` field for backward compatibility. When working with prompt objects, prefer direct field access over type-checking functions, but maintain awareness of the underlying data structure requirements.

---

## 38. [[cohere] [misc] support arbitrary MM datasets in spec dec bench](https://github.com/vllm-project/vllm/pull/33486)


### Base Information

- **PR Number:** #33486
- **Author:** [kkt-cohere](https://github.com/kkt-cohere)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-02 00:49:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33486/files) (3):**
  - `docs/benchmarking/cli.md`
  - `examples/offline_inference/spec_decode.py`
  - `vllm/benchmarks/datasets.py`

### Summary

**What changed and why**  
This PR adds support for arbitrary multimodal datasets in both online and offline speculative decoding benchmarks. Previously, only specific datasets like VisionChatArena/MMStar were supported. The changes enable loading custom JSONL files with `prompt` and `image_files` fields, supporting both remote URLs and local image paths (with `--allowed-local-media-path` for security).

**Technical impact**  
The modifications extend the benchmarking infrastructure to handle multimodal inputs generically. A new `CustomMMDataset` class processes JSONL entries, and the `spec_decode.py` script now supports `--backend openai-chat` for multimodal chat transformations. The changes also propagate `enable_multimodal_chat` and `allowed_local_media_path` parameters through the dataset pipeline and LLM initialization.

**Potential risks**  
- Only the first image in `image_files` is used per sample, which may truncate multi-image prompts without clear user notification.  
- Local image paths require explicit `--allowed-local-media-path`; incorrect paths could cause silent failures or security issues.  
- The `prompt_len` calculation may be inaccurate when `enable_multimodal_chat` is true, as noted in the code comments.

**Key insights**  
- Use `--backend openai-chat` and the `/v1/chat/completions` endpoint for multimodal benchmarking.  
- Ensure local image directories are correctly specified with `--allowed-local-media-path` to avoid access errors.  
- Developers should be aware that multi-image samples are not fully supported—only the first image is processed.

---

