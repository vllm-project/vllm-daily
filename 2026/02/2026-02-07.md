# vLLM Merged PR Report

**Report Date:** 2026-02-07 PST

**Total Merged PRs:** 18

---

## 1. [[ROCm] [CI] Reduce Resource of two test groups](https://github.com/vllm-project/vllm/pull/34059)


### Base Information

- **PR Number:** #34059
- **Author:** [tjtanaa](https://github.com/tjtanaa)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-07 23:17:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34059/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The PR modifies the CI configuration to change the agent pool for two test groups ("Benchmarks" and "Benchmarks CLI Test") from `mi325_8` to `mi325_1`. This reduces the GPU resources allocated to these tests, freeing up 14 GPUs for other `amd_mi325_1` jobs.

**Technical impact**  
These changes lower the computational resources allocated to the benchmark tests, which may increase their runtime due to reduced parallelism. The overall CI pipeline remains functional, but test execution could become slower depending on the workload scaling characteristics.

**Potential risks**  
If the benchmarks are resource-intensive, running them on fewer GPUs might cause timeouts or performance regressions not caught in CI. There is also a risk of resource contention if `mi325_1` agents are oversubscribed, leading to flaky tests or increased queue times.

**Key insights**  
Monitor CI runtimes and failure rates for these test groups to ensure they remain stable. Consider adding performance thresholds or adjusting timeouts if tests become slower. This change is a resource optimization but requires validation under sustained load.

---

## 2. [[Perf] Simplify DeepseekV32 tokenizer, ensure fast detokenization used](https://github.com/vllm-project/vllm/pull/33855)


### Base Information

- **PR Number:** #33855
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-02-07 23:16:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33855/files) (4):**
  - `tests/tokenizers_/test_basic.py`
  - `vllm/renderers/deepseek_v32.py`
  - `vllm/tokenizers/deepseek_v32.py`
  - `vllm/v1/structured_output/backend_xgrammar.py`

### Summary

**What changed and why**  
The PR simplifies the DeepSeek-V3.2 tokenizer implementation by replacing a custom wrapper class (`DeepseekV32Tokenizer`) with a lightweight wrapper function (`get_deepseek_v32_tokenizer`). This ensures the tokenizer is recognized as a fast tokenizer (`PreTrainedTokenizerFast`), enabling the fast detokenization path. It also removes special-case handling for xGrammar integration, as the new wrapper maintains compatibility with the base `HfTokenizer`.

**Technical impact**  
The tokenizer now inherits directly from `HfTokenizer` (a fast tokenizer), which improves performance by enabling incremental detokenization. The architecture shifts from a heavy subclass to a dynamic class modification via `__class__` reassignment, reducing code duplication and aligning with existing tokenizer patterns. The xGrammar backend no longer needs custom logic, as the wrapped tokenizer exposes standard Hugging Face attributes.

**Potential risks**  
Dynamic class reassignment (`dsv32_tokenizer.__class__ = _DeepseekV32Tokenizer`) may cause subtle issues with serialization or inheritance checks. The `__reduce__` method is added to support pickling, but this approach is less conventional. There’s a risk of breaking changes if the underlying `HfTokenizer` API changes, though the wrapper delegates most operations to the base tokenizer.

**Key insights**  
The change prioritizes performance by ensuring fast tokenizer detection. Developers should verify that the wrapped tokenizer retains all expected behaviors (e.g., special token handling, vocabulary methods). The removal of xGrammar special-case logic simplifies maintenance, but the dynamic class wrapper should be monitored for edge cases in multi-process or distributed environments.

---

## 3. [[ROCm][Bugfix] fix act_quant_fusion module import error](https://github.com/vllm-project/vllm/pull/34069)


### Base Information

- **PR Number:** #34069
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 19:21:12
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34069/files) (1):**
  - `vllm/compilation/passes/fusion/rocm_aiter_fusion.py`

### Summary

**What changed and why**  
The change fixes a module import error by updating the import path for `ActivationQuantPattern`. The import was incorrectly referencing a parent directory (`..activation_quant_fusion`) and has been corrected to a relative import from the current directory (`.act_quant_fusion`). This resolves a `ModuleNotFoundError` that occurred during test execution on the ROCm platform.

**Technical impact**  
This change ensures the `rocm_aiter_fusion.py` module can correctly locate its dependency, `ActivationQuantPattern`, which is essential for activation quantization fusion passes. The fix is minimal and localized, affecting only the import resolution without altering the underlying functionality or architecture of the fusion logic.

**Potential risks**  
The risk is very low as this is a straightforward path correction. However, if other files in the codebase use similar incorrect relative imports, they may encounter the same error. The change assumes that `act_quant_fusion.py` is indeed located in the same directory (`vllm/compilation/passes/fusion/`), which should be validated.

**Key insights**  
Always verify relative import paths, especially when modules are moved or restructured. Developers should run import-related tests on all target platforms (like ROCm) to catch such issues early. Consider adding a linting rule or CI check to validate import statements across the codebase.

---

## 4. [[CI/Build] Skip GCS test](https://github.com/vllm-project/vllm/pull/34057)


### Base Information

- **PR Number:** #34057
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 08:52:39
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34057/files) (1):**
  - `tests/model_executor/model_loader/runai_streamer_loader/test_runai_model_streamer_loader.py`

### Summary

**What changed and why**  
A single test (`test_runai_model_loader_download_files_gcs`) has been temporarily skipped by adding a `pytest.mark.skip` decorator. This is due to persistent GCS access failures when trying to fetch a public model from `gs://vertex-model-garden-public-us/codegemma/codegemma-2b/`, which is causing CI pipeline failures.

**Technical impact**  
The change prevents the failing GCS test from blocking CI runs, allowing other tests to proceed. It does not alter any production logic or model-loading behavior—only the test execution is affected. The test suite will now pass in CI, but coverage for GCS-based model loading is temporarily reduced.

**Potential risks**  
If the skip remains indefinitely, regressions in GCS model loading could go undetected. There is also a risk that the underlying GCS access issue (e.g., permissions, network, or bucket availability) might be mistaken for a code defect when the test is eventually re-enabled. The skip comment includes a TODO, but without a clear owner or timeline, it may be forgotten.

**Key insights**  
This is a reasonable short-term mitigation to unblock CI, but the team should prioritize investigating and resolving the GCS access issue. Consider adding a tracking issue or ticket linked to the TODO to ensure the test is re-enabled. In the meantime, monitor other GCS-related tests or functionality to ensure no silent regressions occur.

---

## 5. [[Doc] Fix run_batch docs](https://github.com/vllm-project/vllm/pull/34056)


### Base Information

- **PR Number:** #34056
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 06:18:16
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34056/files) (1):**
  - `vllm/entrypoints/openai/run_batch.py`

### Summary

**What changed and why**  
This PR fixes documentation issues in the `run_batch.py` file by adding a `TypeAlias` definition for `WrapperFn` and updating type hints to use this alias. The changes improve code clarity and maintainability by creating a reusable type definition for wrapper functions.

**Technical impact**  
The changes introduce a `WrapperFn` type alias for `Callable[[Callable], Callable]`, making the code more readable and consistent. This alias is now used in function signatures throughout the module, reducing duplication and improving type safety.

**Potential risks**  
The main risk is minimal - if the `TypeAlias` import is missing or incorrectly defined, it could cause runtime errors. However, since this is primarily a documentation/type hint improvement, functional impact should be negligible.

**Key insights**  
Using type aliases for complex callable signatures enhances code maintainability and developer experience. Consider extending this pattern to other complex type definitions in the codebase to improve overall type safety and readability.

---

## 6. [Perf tuning and expansion of cases covered for wvSplitKrc](https://github.com/vllm-project/vllm/pull/33493)


### Base Information

- **PR Number:** #33493
- **Author:** [amd-hhashemi](https://github.com/amd-hhashemi)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:33:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33493/files) (3):**
  - `csrc/rocm/skinny_gemms.cu`
  - `tests/kernels/quantization/test_rocm_skinny_gemms.py`
  - `vllm/model_executor/layers/utils.py`

### Summary

**What changed and why**  
This PR introduces performance tuning and expands coverage for the `wvSplitKrc` kernel, a wave-split K reduction kernel for skinny GEMM operations on ROCm GPUs. The changes modify the kernel to handle more cases by adding a `CHUNKK` parameter for better workload distribution, adjust memory access patterns, and update the selection logic to dynamically determine when to use this kernel based on CU occupancy.

**Technical impact**  
The kernel now supports a wider range of matrix dimensions (N, K, M) by introducing configurable chunking (`CHUNKK`) and refining the LDS usage patterns. The selection logic in `utils.py` has been updated to more accurately assess whether the problem fits within available compute units, enabling the kernel for more scenarios while maintaining performance. Performance measurements show significant speedups for many configurations, especially at larger N values (e.g., 32–128).

**Potential risks**  
The assumption `doRdc = true` may not hold for all K sizes, potentially causing incorrect behavior for small K. The increased complexity in index calculations (e.g., with `CHUNKK`) could introduce off-by-one errors or out-of-bounds memory accesses. The dynamic selection logic relies on accurate CU count and dimension rounding, which might fail on some hardware or edge cases.

**Key insights**  
The kernel's expansion is driven by a more flexible workload distribution strategy, which improves utilization on underfilled CUs. Developers should verify that the `CHUNKK` parameter aligns with hardware capabilities and test edge cases thoroughly. The updated test suite now covers a broader parameter space, but additional validation for non-power-of-two dimensions and very small K is recommended.

---

## 7. [Make directory exist ok for ray spinning up multiple replicas on a single instance](https://github.com/vllm-project/vllm/pull/33604)


### Base Information

- **PR Number:** #33604
- **Author:** [jiangwu300](https://github.com/jiangwu300)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:30:50
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33604/files) (1):**
  - `vllm/transformers_utils/runai_utils.py`

### Summary

**What changed and why**  
The change modifies directory creation logic in the RunAI object storage initializer. Previously, it would delete and recreate an existing directory; now it uses `os.makedirs(dir_name, exist_ok=True)` to allow the directory to already exist without raising an error. This fixes a race condition when multiple Ray replicas attempt to initialize the same directory simultaneously.

**Technical impact**  
This eliminates the failure mode where concurrent replicas on a single instance conflict over directory existence, allowing Ray to spin up multiple vLLM replicas successfully. The change is minimal and localized, affecting only the initialization behavior of the RunAI streamer's cache directory.

**Potential risks**  
If the existing directory contains stale or corrupted data, it will no longer be automatically cleaned up, which could lead to unexpected behavior if the cache assumes a fresh state. Additionally, any permissions issues with the existing directory could now surface as runtime errors instead of being resolved by recreation.

**Key insights**  
This fix addresses a specific race condition in distributed deployments. Developers should ensure that the directory's contents are managed appropriately elsewhere if freshness is required. Consider adding validation or cleanup logic at a higher level if the cache must be empty on each initialization.

---

## 8. [Update DeepGEMM version pin in Dockerfile to match #32479](https://github.com/vllm-project/vllm/pull/33935)


### Base Information

- **PR Number:** #33935
- **Author:** [zifeitong](https://github.com/zifeitong)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:30:23
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33935/files) (3):**
  - `docker/Dockerfile`
  - `docker/versions.json`
  - `tools/install_deepgemm.sh`

### Summary

**What changed and why**  
Updated the DeepGEMM version pin from `594953acce41793ae00a1233eb516044d604bcb6` to `477618cd51baffca09c4b0b87e97c03fe827ef03` in three files: `docker/Dockerfile`, `docker/versions.json`, and `tools/install_deepgemm.sh`. This change ensures consistency across the codebase after a previous update (#32479) only modified the install script, leaving the Dockerfile and version configuration out of sync.

**Technical impact**  
The Docker build will now use the same DeepGEMM commit as the standalone installation script, eliminating version mismatches that could cause runtime errors or inconsistent behavior between containerized and non-containerized environments. The centralized version management in `versions.json` ensures future updates can be coordinated more easily.

**Potential risks**  
If the new commit (`477618cd...`) introduces breaking changes or regressions, all builds (both Docker and local installations) will be affected simultaneously. There is also a risk that the previous commit (`594953ac...`) was intentionally kept in the Dockerfile for compatibility reasons, though the PR description suggests this was an oversight.

**Key insights**  
Always verify version consistency across all configuration files when updating dependencies. Consider adding a CI check to ensure version pins in Dockerfile, install scripts, and version manifests remain synchronized. The change is minimal and focused, reducing the chance of unintended side effects.

---

## 9. [move checks out of `unified_kv_cache_update` custom op](https://github.com/vllm-project/vllm/pull/33943)


### Base Information

- **PR Number:** #33943
- **Author:** [Rohan138](https://github.com/Rohan138)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:30:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33943/files) (7):**
  - `vllm/model_executor/layers/attention/attention.py`
  - `vllm/model_executor/layers/attention/cross_attention.py`
  - `vllm/model_executor/models/whisper_causal.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/rocm_aiter_unified_attn.py`
  - `vllm/v1/attention/backends/rocm_attn.py`
  - `vllm/v1/attention/backends/triton_attn.py`

### Summary

**What changed and why**  
The PR moves validation checks for KV cache updates out of the `unified_kv_cache_update` custom op and into the `Attention.forward` method and related cross-attention implementations. Specifically, it adds checks for `kv_sharing_target_layer_name` and `key`/`value` tensors being non-None before invoking the cache update, eliminating redundant checks in backend-specific `do_kv_cache_update` methods.

**Technical impact**  
This change centralizes the validation logic, simplifying the backend implementations by removing duplicate conditionals. It ensures that KV cache updates are only attempted when appropriate (i.e., when not sharing caches and when tensors are valid), improving code maintainability and reducing the risk of inconsistent behavior across backends.

**Potential risks**  
If the validation conditions are incorrectly applied or omitted in any attention variant (e.g., cross-attention or specialized models), it could lead to missed cache updates or unnecessary operations. Additionally, the removal of checks from backends assumes all callers now enforce the same preconditions, which must be verified across all attention implementations.

**Key insights**  
Developers should ensure that all attention layers (including future additions) adhere to the same validation pattern. This refactor is a step toward eventually removing the `kv_sharing_target_layer_name` parameter entirely, as indicated in the PR description. Consistency in validation across attention variants is critical to avoid runtime errors.

---

## 10. [[PluggableLayer][3/N] Apply PluggableLayer to mamba layers.](https://github.com/vllm-project/vllm/pull/33660)


### Base Information

- **PR Number:** #33660
- **Author:** [whx-sjtu](https://github.com/whx-sjtu)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:26:06
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33660/files) (3):**
  - `vllm/model_executor/layers/mamba/mamba_mixer.py`
  - `vllm/model_executor/layers/mamba/mamba_mixer2.py`
  - `vllm/model_executor/models/plamo2.py`

### Summary

**What changed and why**  
This PR migrates three Mamba layer implementations (`mamba_mixer`, `mamba_mixer2`, and `plamo2_mamba_mixer`) from inheriting `CustomOp` to `PluggableLayer`. The changes involve updating class inheritance, replacing `forward_cuda` with `forward_impl`, and removing unused `forward_native` methods. This is part of a larger effort to adopt a pluggable layer architecture for improved modularity and maintainability.

**Technical impact**  
The refactor standardizes Mamba layers under the `PluggableLayer` abstraction, aligning them with a consistent interface for custom operations. This reduces code duplication by eliminating empty `forward_native` methods and consolidates the forward logic into `forward_impl`. The external API remains unchanged, as the public `forward` methods still delegate to the underlying implementation via helper functions.

**Potential risks**  
If `PluggableLayer` does not fully support all behaviors previously provided by `CustomOp`, subtle regressions could occur. The removal of `forward_native` may break compatibility with code paths that relied on its existence, though it appears unused. Additionally, any downstream dependencies expecting the old class hierarchy or method names could be affected.

**Key insights**  
This is a clean refactoring step that enhances code consistency and reduces technical debt. Developers should verify that `PluggableLayer` offers equivalent functionality to `CustomOp` for all Mamba use cases. Future work should ensure similar migrations for other custom layers to fully realize the benefits of the pluggable architecture.

---

## 11. [[Model] Enable Step3p5ForCausalLM testing](https://github.com/vllm-project/vllm/pull/33755)


### Base Information

- **PR Number:** #33755
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:25:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33755/files) (3):**
  - `docs/models/supported_models.md`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/step3p5.py`

### Summary

**What changed and why**  
This PR enables testing for the Step3p5ForCausalLM model by updating the model registry configuration and fixing the model implementation. The changes correct the model name case in the documentation, adjust the registry to properly initialize MoE layers, and simplify the LM head initialization by removing unnecessary LoRA-related logic.

**Technical impact**  
The modifications allow the Step3p5ForCausalLM model to be tested in the CI pipeline by ensuring the model can be loaded with the correct configuration (including MoE layer initialization). The refactoring in the model executor consolidates LM head setup and removes redundant vocabulary padding calculations, aligning it more closely with standard patterns used in other models.

**Potential risks**  
If the model relies on specific LoRA configurations that were previously handled by the removed logic, those features may break. Additionally, the hardcoded `num_hidden_layers`: 4 override in the registry might not match all model variants, potentially causing initialization errors or incorrect behavior for models with different architectures.

**Key insights**  
Developers should verify that the Step3p5 model works correctly with LoRA if that feature is required. The registry configuration should be reviewed to ensure the layer count override is appropriate for all intended model checkpoints. The simplification in the model executor is positive but must be validated with comprehensive testing.

---

## 12. [[Frontend]Add support for transcriptions and translations to run_batch](https://github.com/vllm-project/vllm/pull/33934)


### Base Information

- **PR Number:** #33934
- **Author:** [pooyadavoodi](https://github.com/pooyadavoodi)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:24:58
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33934/files) (3):**
  - `tests/entrypoints/openai/test_run_batch.py`
  - `vllm/entrypoints/openai/run_batch.py`
  - `vllm/entrypoints/openai/speech_to_text/serving.py`

### Summary

**What changed and why**  
This PR adds support for audio transcription and translation endpoints to the vLLM OpenAI batch API (`run_batch.py`). It introduces new request types (`BatchTranscriptionRequest` and `BatchTranslationRequest`) that use `file_url` instead of `file` to handle audio data via URLs or base64-encoded data URLs, enabling batch processing of audio files. The changes include updated request/response handling, audio data downloading, and new unit tests.

**Technical impact**  
The batch API now supports two new endpoint types (`/v1/audio/transcriptions` and `/v1/audio/translations`), extending its compatibility with audio models like Whisper. The architecture integrates these endpoints through a generic handler pattern, reusing existing speech-to-text serving logic with modifications to accept `file_url` and optional `raw_request` parameters. This expands the batch processing capabilities beyond text-based tasks.

**Potential risks**  
- Data URL parsing may fail on malformed base64 strings or unsupported encodings.  
- HTTP URL downloads could time out or fail, impacting batch job reliability.  
- The `raw_request` parameter being optional in serving methods might affect existing non-batch usage if not properly handled.  
- Increased memory usage when processing large audio files in parallel batches.

**Key insights**  
- Ensure robust error handling for URL fetching and base64 decoding to prevent batch failures.  
- Consider adding timeout and retry logic for HTTP downloads to improve resilience.  
- Verify that the optional `raw_request` change does not break existing real-time API endpoints.  
- Monitor performance when scaling audio batch jobs, as audio processing is more resource-intensive than text.

---

## 13. [Enable Eagle3 speculative decoding for Mistral3ForConditionalGeneration to support eagle3](https://github.com/vllm-project/vllm/pull/33939)


### Base Information

- **PR Number:** #33939
- **Author:** [TundeAtSN](https://github.com/TundeAtSN)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:24:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33939/files) (1):**
  - `vllm/model_executor/models/mistral3.py`

### Summary

**What changed and why**  
The PR adds Eagle3 speculative decoding support to the Mistral3ForConditionalGeneration model by implementing the SupportsEagle3 interface. This includes adding the interface to the class inheritance and implementing two required methods: `set_aux_hidden_state_layers` and `get_eagle3_aux_hidden_state_layers`.

**Technical impact**  
This change enables the Mistral3 model to work with Eagle3 speculative decoding, which can improve inference throughput by using a draft model to predict multiple tokens ahead. The implementation follows the existing pattern for Eagle3 support in other models, maintaining consistency across the codebase.

**Potential risks**  
The hardcoded layer selection in `get_eagle3_aux_hidden_state_layers` (layers 2, middle, and third-from-last) may not be optimal for all Mistral3 variants or configurations. Additionally, there's no validation that the specified layers exist within the model's actual layer count, which could cause runtime errors if the model has fewer than 4 layers.

**Key insights**  
The implementation correctly follows the established Eagle3 interface pattern. However, consider making the layer selection configurable or adding validation to ensure robustness across different model sizes. The changes are minimal and focused, reducing the risk of unintended side effects while enabling a valuable performance optimization feature.

---

## 14. [[torch.compile] Stop compiling identical artifacts](https://github.com/vllm-project/vllm/pull/34003)


### Base Information

- **PR Number:** #34003
- **Author:** [zou3519](https://github.com/zou3519)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:24:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34003/files) (2):**
  - `tests/compile/test_cold_start.py`
  - `vllm/compilation/backends.py`

### Summary

**What changed and why**  
This PR modifies the compilation backend to avoid compiling duplicate subgraph artifacts. Previously, identical subgraphs were compiled separately, relying on PyTorch's autograd cache for speed but still generating redundant artifacts. Now, the system detects duplicate subgraphs during compilation and reuses existing in-memory artifacts, reducing cold-start compilation time by eliminating unnecessary work.

**Technical impact**  
The changes introduce an artifact deduplication mechanism by intercepting PyTorch's autograd cache key generation. When a cache hit is detected, compilation is halted early via a custom `StopCompiling` exception, and the pre-existing artifact is returned. This reduces the number of compiled artifacts from N (total subgraphs) to X (unique subgraphs), improving cold-start performance and reducing disk I/O.

**Potential risks**  
Monkey-patching PyTorch's internal `autograd_cache_key` function is fragile and may break with future PyTorch updates. The `StopCompiling` exception, while effective, is a non-standard control flow mechanism that could complicate debugging. Edge cases where `cache_key` is `None` or artifacts are improperly stored could lead to silent failures or incorrect graph mappings.

**Key insights**  
The optimization significantly reduces compilation time for models with repeated layers (e.g., from 24s to 16s for LLaMA-70B). However, the implementation should be considered temporary; a more stable solution would involve PyTorch exposing a public cache key API. Developers should monitor test stability and be prepared to adapt if PyTorch internals change.

---

## 15. [[Kernel] Add KernelConfig flag to enable/disable FlashInfer autotune](https://github.com/vllm-project/vllm/pull/34006)


### Base Information

- **PR Number:** #34006
- **Author:** [mmangkad](https://github.com/mmangkad)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:24:44
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34006/files) (5):**
  - `vllm/config/__init__.py`
  - `vllm/config/kernel.py`
  - `vllm/config/vllm.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/model_executor/warmup/kernel_warmup.py`

### Summary

**What changed and why**  
Added a `KernelConfig` class with an `enable_flashinfer_autotune` flag to control FlashInfer autotuning during kernel warmup. The flag is integrated into `VllmConfig` and exposed via CLI arguments, allowing users to enable/disable autotuning. Optimization levels now set default values for this flag.

**Technical impact**  
The changes introduce a new configuration layer for kernel behavior, decoupling FlashInfer autotune control from hardware detection. This allows finer-grained performance tuning and avoids unnecessary autotuning overhead when disabled. The flag is validated during config initialization and respects optimization-level defaults.

**Potential risks**  
If `enable_flashinfer_autotune` is left uninitialized (None) after applying optimization defaults, it raises a `ValueError`. The CLI exposes both `--kernel-config` and `--enable-flashinfer-autotune`, which are mutually exclusive and could cause confusion or conflicts if both are set. The hash computation in `KernelConfig` currently returns a constant value, which may affect caching if future fields are added.

**Key insights**  
Always set `enable_flashinfer_autotune` explicitly or rely on optimization-level defaults. Avoid mixing CLI arguments for kernel config to prevent validation errors. When extending `KernelConfig`, update `compute_hash()` to include new fields that affect the computation graph. The flag provides a clear path to disable autotuning for debugging or performance reasons.

---

## 16. [[Renderer] Define `render_cmpl` and `render_chat`](https://github.com/vllm-project/vllm/pull/34039)


### Base Information

- **PR Number:** #34039
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:24:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34039/files) (7):**
  - `tests/models/multimodal/generation/test_phi4mm.py`
  - `tests/models/multimodal/generation/test_qwen2_vl.py`
  - `tests/models/quantization/test_awq.py`
  - `vllm/entrypoints/llm.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/renderers/inputs/preprocess.py`
  - `vllm/renderers/protocol.py`

### Summary

**What changed and why**  
This PR introduces top-level render methods `render_cmpl` and `render_chat` (and their async variants) in the renderer protocol, consolidating prompt preprocessing for Completions and Chat APIs. It refactors both offline (`vllm/entrypoints/llm.py`) and online (`vllm/entrypoints/openai/engine/serving.py`) API handlers to use these new methods, reducing code duplication and centralizing logic for applying prompt extras like `mm_processor_kwargs` and `cache_salt`.

**Technical impact**  
The changes streamline prompt processing by unifying scattered preprocessing steps into dedicated renderer methods. This improves maintainability and ensures consistent handling of multimodal models and prompt extras across APIs. The architecture now clearly separates rendering, tokenization, and extra-application phases, making the flow more modular and testable.

**Potential risks**  
The async and sync paths for Completions API differ in multimodal handling (`is_multimodal_model` check only in sync path), which could cause inconsistencies. Removing empty `size_factors` test cases might reduce coverage for no-image scenarios. There’s also a TODO about moving multimodal processor integration, indicating incomplete functionality.

**Key insights**  
Developers should verify multimodal model behavior aligns between async/sync Completions APIs. The new render methods simplify future extensions but require careful testing of edge cases, especially for encoder-decoder and multimodal models. Ensure prompt extras are applied correctly in all code paths, and monitor performance due to added abstraction layers.

---

## 17. [[CI][Build]  Pin grpcio-tools==1.78.0](https://github.com/vllm-project/vllm/pull/34048)


### Base Information

- **PR Number:** #34048
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 05:24:35
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34048/files) (5):**
  - `pyproject.toml`
  - `requirements/build.txt`
  - `requirements/rocm.txt`
  - `requirements/test.in`
  - `requirements/test.txt`

### Summary

**What changed and why**  
The PR pins `grpcio-tools` to version 1.78.0 across multiple dependency files to fix a unit test failure in the MAIN Entrypoints tests. The change ensures consistent versioning between build, test, and ROCm environments.

**Technical impact**  
This change enforces a specific version of `grpcio-tools` across the entire development and testing pipeline, eliminating potential version mismatches that could cause gRPC-related build or runtime errors. It also updates the transitive dependency `grpcio` to match in the locked test requirements.

**Potential risks**  
Pinning to an exact version may cause future dependency conflicts if other packages require different versions of `grpcio-tools`. There is also a risk that the underlying issue is not fully resolved if the test failure was caused by a transient or environment-specific problem unrelated to versioning.

**Key insights**  
The fix addresses an immediate test failure by standardizing the `grpcio-tools` version, but consider whether a version range (e.g., `>=1.78.0,<1.79.0`) would provide more flexibility. Ensure that the pinned version is compatible with all other dependencies, especially `grpcio`, to avoid future breakage.

---

## 18. [[Misc] Simplify `get_max_tokens`](https://github.com/vllm-project/vllm/pull/34036)


### Base Information

- **PR Number:** #34036
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-07 00:59:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34036/files) (5):**
  - `vllm/entrypoints/openai/chat_completion/serving.py`
  - `vllm/entrypoints/openai/completion/serving.py`
  - `vllm/entrypoints/openai/engine/serving.py`
  - `vllm/entrypoints/openai/responses/serving.py`
  - `vllm/entrypoints/utils.py`

### Summary

**What changed and why**  
The PR simplifies the `get_max_tokens` function by removing its internal logic to determine the appropriate max token property from different request types. Instead, callers now directly pass the specific max token value (e.g., `request.max_tokens`, `request.max_completion_tokens`, or `request.max_output_tokens`) to the function, eliminating the need for complex if-else or `getattr` chains.

**Technical impact**  
This change reduces the coupling between `get_max_tokens` and the request protocol types, making the function more generic and focused solely on computing the max tokens based on provided inputs. It also streamlines the code by removing type-checking imports and conditional attribute lookups, potentially improving performance slightly due to fewer runtime checks.

**Potential risks**  
If callers incorrectly pass `None` or an invalid value for `max_tokens` (when a valid integer is expected), it could lead to unexpected behavior or errors in token calculation. Additionally, any future request types with different max token property names would require updates at all call sites rather than just within `get_max_tokens`.

**Key insights**  
The refactor enhances code clarity and maintainability by delegating responsibility for extracting the max token value to the caller. Developers should ensure that the passed `max_tokens` parameter is appropriately validated or handled (e.g., defaulting to a sensible value) before invoking `get_max_tokens`. This change aligns with a more functional design, where each function has a single, well-defined responsibility.

---

