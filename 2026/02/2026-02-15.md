# vLLM Merged PR Report

**Report Date:** 2026-02-15 PST

**Total Merged PRs:** 11

---

## 1. [[Renderer] Move InputPreprocessor into Renderer (1.5/2)](https://github.com/vllm-project/vllm/pull/34598)


### Base Information

- **PR Number:** #34598
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 23:46:33
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34598/files) (11):**
  - `tests/renderers/test_completions.py`
  - `vllm/inputs/data.py`
  - `vllm/inputs/parse.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/model_executor/models/llava.py`
  - `vllm/model_executor/models/terratorch.py`
  - `vllm/model_executor/models/transformers/multimodal.py`
  - `vllm/multimodal/inputs.py`
  - `vllm/multimodal/media/base.py`
  - `vllm/multimodal/processing/processor.py`
  - `vllm/renderers/base.py`

### Summary

**What changed and why**  
This PR refactors input preprocessing logic by moving encoder-decoder handling utilities from `InputPreprocessor` into shared input data modules. Key changes include adding a `prompt` field to input types, updating `EncoderDecoderInputs` keys, introducing factory methods (`mm_inputs`, `mm_enc_dec_inputs`), and relocating decoder start token ID logic to the renderer. The goal is to isolate performance regressions from a larger PR (#34560) and improve code organization.

**Technical impact**  
The changes centralize encoder-decoder input construction logic in `vllm/inputs/data.py`, reducing duplication and making the codebase more modular. The renderer now provides decoder start token ID resolution, while input preprocessing is simplified by using the new factory methods. This improves maintainability and aligns with separation of concerns.

**Potential risks**  
There is a risk of breaking existing encoder-decoder model integrations if the new key names (`encoder_prompt`/`decoder_prompt`) are not correctly handled downstream. The removal of validation logic from `InputPreprocessor` could lead to uncaught errors if the new factory methods are misused. Additionally, the `prompt` field addition may cause serialization issues if not properly optional.

**Key insights**  
Developers should use the new factory methods (`mm_inputs`, `mm_enc_dec_inputs`) for creating multimodal inputs. The `prompt` field should be populated when available to support future extraction features. Ensure all encoder-decoder model tests pass with the updated key names, and verify that the decoder start token ID fallback logic works correctly for all relevant models.

---

## 2. [[CI] Write bake config to temp directory instead of repo root](https://github.com/vllm-project/vllm/pull/34569)


### Base Information

- **PR Number:** #34569
- **Author:** [amrmahdi](https://github.com/amrmahdi)
- **Merged By:** [khluu](https://github.com/khluu)
- **Merged time:** 2026-02-15 22:15:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34569/files) (1):**
  - `.buildkite/image_build/image_build.sh`

### Summary

**What changed and why**  
The bake configuration file is now written to a temporary directory instead of the repository root. This prevents the file from being included in the Docker build context and subsequently copied into the built image, eliminating duplicate artifact uploads that caused "Multiple artifacts found" errors in downstream CI steps.

**Technical impact**  
This change isolates the bake config from the Docker build context, ensuring it doesn't become part of the container image. Downstream steps that upload artifacts will no longer encounter duplicate files, resolving failures in pipelines like rebuild-cpu-build-ami that depend on unique artifact uploads.

**Potential risks**  
If the temporary directory is cleaned up prematurely before the artifact upload completes, the bake config could be missing. Additionally, any other scripts relying on the bake config being in the repo root may need updates, though this seems unlikely given the isolated usage shown.

**Key insights**  
Always consider how files in the build context affect Docker images and downstream CI processes. Using temporary directories for intermediate CI artifacts is a best practice to avoid unintended side effects. Verify that no other processes depend on the previous file location.

---

## 3. [[Bugfix] Add method to swap quant_method on FusedMoE to fix LoRA issues](https://github.com/vllm-project/vllm/pull/34453)


### Base Information

- **PR Number:** #34453
- **Author:** [bnellnm](https://github.com/bnellnm)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 20:10:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34453/files) (2):**
  - `vllm/lora/layers/fused_moe.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`

### Summary

**What changed and why**  
Added a `_replace_quant_method` method to the `FusedMoE` layer to safely swap the `quant_method` attribute and reinitialize the `runner`. This fixes LoRA test failures caused by the MoE runner not being updated after `quant_method` changes during LoRA operations.

**Technical impact**  
The change centralizes the logic for replacing `quant_method` and ensures the runner is properly reconstructed. This maintains consistency between the quant method and runner state, preventing runtime errors in LoRA-enabled MoE models.

**Potential risks**  
The temporary hook (`_replace_quant_method`) introduces a public-ish method that may be misused. If `FusedMoEModularMethod` is not fully eliminated as planned, this workaround could become a permanent maintenance burden. Edge cases around concurrent modifications to `quant_method` are not addressed.

**Key insights**  
The fix is a pragmatic solution for immediate test failures but should be replaced with a cleaner long-term design. Developers should treat `_replace_quant_method` as internal and avoid calling it directly. The TODO comments highlight the need for a less intrusive architecture once `FusedMoEModularMethod` is deprecated.

---

## 4. [[BugFix] Fix Python 3.13 FlashMLA import error](https://github.com/vllm-project/vllm/pull/34548)


### Base Information

- **PR Number:** #34548
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 20:09:18
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34548/files) (1):**
  - `cmake/external_projects/flashmla.cmake`

### Summary

**What changed and why**  
The change updates the Git commit hash (GIT_TAG) for the FlashMLA dependency from `c2afa9cb93e674d5a9120a170a6da57b89267208` to `692917b1cda61b93ac9ee2d846ec54e75afe87b1`. This picks up a specific fix from the FlashMLA repository intended to resolve a Python 3.13 import error referenced in issue #34504.

**Technical impact**  
This modifies the pinned version of the external FlashMLA project fetched during CMake configuration. The build system will now pull the updated commit, which should include the necessary changes to support Python 3.13, potentially resolving compatibility issues in the vLLM codebase.

**Potential risks**  
If the new FlashMLA commit introduces any regressions or breaks compatibility with other Python versions, it could affect vLLM's stability. Additionally, since the change is tightly coupled to a specific commit, future updates may require similar manual synchronization.

**Key insights**  
This is a targeted dependency fix that addresses a Python version compatibility issue. Developers should verify that the new FlashMLA commit resolves the reported import error without introducing new issues. Consider monitoring FlashMLA releases for more stable version tags to reduce reliance on specific commits.

---

## 5. [[Doc] Add Mistral-7b-v0.3 model to the batch invariance validated model](https://github.com/vllm-project/vllm/pull/34584)


### Base Information

- **PR Number:** #34584
- **Author:** [banparth](https://github.com/banparth)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-15 20:09:00
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34584/files) (1):**
  - `docs/features/batch_invariance.md`

### Summary

**What changed and why**  
Added the `mistralai/Mistral-7B-v0.3` model to the batch invariance documentation as a validated model. This change addresses a community request to expand the list of validated models for batch invariance testing.

**Technical impact**  
This update does not affect code or system behavior; it only updates documentation to reflect that Mistral-7B-v0.3 has passed batch invariance validation tests. It helps users identify supported models for deterministic batch processing.

**Potential risks**  
No technical risks, as this is purely a documentation change. However, if the model's behavior changes in future versions, the documentation may become outdated unless revalidated.

**Key insights**  
Documentation updates like this enhance user confidence and transparency. Ensure that any future model validations follow the same testing protocol and are promptly documented to maintain accuracy.

---

## 6. [[CI][Frontend] Return 422 instead of 500 for invalid Anthropic tool_choice](https://github.com/vllm-project/vllm/pull/34590)


### Base Information

- **PR Number:** #34590
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 20:06:48
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34590/files) (2):**
  - `tests/entrypoints/openai/test_openai_schema.py`
  - `vllm/entrypoints/anthropic/protocol.py`

### Summary

**What changed and why**
The PR adds validation to ensure that when Anthropic's `tool_choice.type` is `"tool"`, a `name` field is required. This prevents a 500 Internal Server Error caused by Pydantic validation failure when `name=None` is passed to `ChatCompletionNamedToolChoiceParam`. The fix ensures a proper 422 validation error is returned. Additionally, the `/v1/messages` endpoint is added to the longer timeout map for schema tests.

**Technical impact**
The changes enforce API spec compliance at the request parsing layer by adding a `model_validator` to `AnthropicToolChoice`. This shifts error handling from an uncaught exception during Pydantic model conversion to a controlled validation error, improving error reporting and API robustness. The test timeout update ensures the `/v1/messages` endpoint is treated consistently with other completion endpoints during testing.

**Potential risks**
If the validator logic has any edge cases (e.g., handling of empty strings or whitespace for `name`), it might still allow invalid data. There's also a risk that similar validation might be needed elsewhere in the codebase for other Anthropic request fields. The change assumes the validator runs in the correct mode and order relative to other validators.

**Key insights**
This fix correctly addresses a spec violation and improves error handling from 500 to 422, which is more appropriate for client errors. Developers should ensure that all API endpoints handling external requests have similar validation to prevent unhandled exceptions. Consider adding unit tests for the validator to cover various `tool_choice` scenarios.

---

## 7. [[CI/Build] Enable tests for recent day-0 new models](https://github.com/vllm-project/vllm/pull/34585)


### Base Information

- **PR Number:** #34585
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 18:17:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34585/files) (4):**
  - `tests/models/multimodal/processing/test_common.py`
  - `tests/models/multimodal/processing/test_tensor_schema.py`
  - `tests/models/registry.py`
  - `vllm/model_executor/models/interns1_pro.py`

### Summary

**What changed and why**  
This PR enables initialization and processor tests for recently released "day-0" models by updating test configurations and fixing model-specific issues. The changes include adjusting test skip conditions, updating model registry availability flags, and correcting inheritance in the Intern-S1-Pro model implementation.

**Technical impact**  
The modifications allow previously skipped models (like `lfm2_vl` and `Intern-S1-Pro`) to be tested by resolving tokenization and processor initialization issues. Removing `is_available_online=False` flags for certain models indicates they are now accessible for testing, while the inheritance fix ensures the Intern-S1-Pro model uses its correct MoE implementation.

**Potential risks**  
Skipping the `Fun-ASR-Nano-2512-vllm` test due to cached audio feature mismatches could mask underlying data processing bugs. The inheritance change in `interns1_pro.py` might introduce subtle behavioral differences if the base `Qwen3VLForConditionalGeneration` class assumptions diverge from the new `InternS1ProMoeMixtureOfExperts` logic.

**Key insights**  
Developers should prioritize resolving the skipped Fun-ASR test to prevent technical debt. The model registry updates suggest improved external model availability, but verifying download and initialization stability post-change is advisable. The MoE inheritance correction aligns the implementation with the model's architecture, reducing maintenance complexity.

---

## 8. [[torch.compile] Disable ar-rms fusion for ds3-fp4 & DP, fix CI test](https://github.com/vllm-project/vllm/pull/34392)


### Base Information

- **PR Number:** #34392
- **Author:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 06:33:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34392/files) (3):**
  - `vllm/config/model.py`
  - `vllm/config/vllm.py`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
This PR disables the AR+RMS (AllReduce + RMSNorm) fusion by default for DeepSeekV3 models using NVFP4 quantization and when Data Parallel (DP) is used, due to accuracy issues and DP-related failures. The changes add detection for NVFP4 quantization, update the fusion enablement logic to exclude DP scenarios, and introduce model-specific config validation to disable the fusion for affected models.

**Technical impact**  
The `enable_allreduce_rms_fusion` function now requires `data_parallel_size == 1` and supports both Hopper (capability 90) and Blackwell (capability 100) GPUs. A new `is_nvfp4_quantized()` method identifies NVFP4 quantization, and the `DeepseekV3ForCausalLM` config class automatically disables the fusion for NVFP4 models unless explicitly overridden, with appropriate warnings.

**Potential risks**  
If users manually enable `fuse_allreduce_rms` for DeepSeekV3 with NVFP4, they may encounter unresolved accuracy issues despite the warning. The DP restriction might affect performance for multi-GPU inference setups using DP instead of TP. There is a risk that the NVFP4 detection logic could miss future quantization formats or configurations.

**Key insights**  
Developers should note that AR+RMS fusion remains disabled for DeepSeekV3 NVFP4 and DP scenarios; manual enabling is not recommended. The fusion is now supported on Blackwell GPUs. Ensure that quantization detection aligns with actual model checkpoint formats to avoid unintended behavior.

---

## 9. [[CPU][ARM] Add ARM BF16 cross-compilation support and improve documen…](https://github.com/vllm-project/vllm/pull/33079)


### Base Information

- **PR Number:** #33079
- **Author:** [maryamtahhan](https://github.com/maryamtahhan)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 06:33:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/33079/files) (3):**
  - `cmake/cpu_extension.cmake`
  - `docker/Dockerfile.cpu`
  - `docs/getting_started/installation/cpu.arm.inc.md`

### Summary

**What changed and why**  
This PR adds ARM BF16 cross-compilation support and improves ARM CPU build documentation. It introduces a new `VLLM_CPU_ARM_BF16` build argument to enable BF16-optimized builds on systems without native BF16 support, extends the existing cross-compilation framework from x86 to ARM, and adds validation to prevent mixing incompatible ISA flags across architectures.

**Technical impact**  
The changes enable consistent cross-compilation workflows across x86 and ARM platforms by adding ARM BF16 support to the CMake configuration and Docker build process. The new validation logic ensures build-time safety by preventing architecture-specific ISA flag mismatches, while OCI labels improve traceability of build configurations.

**Potential risks**  
The validation logic relies on `TARGETARCH` environment variable availability during Docker build, which may not be set in all CI/CD environments. Cross-compilation with `VLLM_CPU_ARM_BF16=true` could produce binaries that fail on older ARM processors lacking BF16 support, though this is documented. The hard-coded error messages might need localization if the project expands its user base.

**Key insights**  
Developers should use the new `VLLM_CPU_ARM_BF16` flag for ARM cross-compilation scenarios and leverage the improved documentation for build examples. The validation errors provide clear guidance, but ensure `TARGETARCH` is properly set in custom build scripts. Always verify target CPU compatibility when force-enabling BF16 support.

---

## 10. [[MM Encoder] Add Triton ViT attention backend](https://github.com/vllm-project/vllm/pull/32183)


### Base Information

- **PR Number:** #32183
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-02-15 06:32:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/32183/files) (14):**
  - `tests/kernels/attention/test_mha_attn.py`
  - `vllm/model_executor/layers/attention/mm_encoder_attention.py`
  - `vllm/model_executor/models/dots_ocr.py`
  - `vllm/model_executor/models/ernie45_vl.py`
  - `vllm/model_executor/models/glm4_1v.py`
  - `vllm/model_executor/models/paddleocr_vl.py`
  - `vllm/model_executor/models/qwen2_5_vl.py`
  - `vllm/model_executor/models/qwen2_vl.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`
  - `vllm/model_executor/models/qwen3_vl.py`
  - `vllm/model_executor/models/vision.py`
  - `vllm/platforms/cuda.py`
  - `vllm/platforms/rocm.py`
  - `vllm/v1/attention/ops/vit_attn_wrappers.py`

### Summary

**What changed and why**  
This PR adds a Triton attention backend for ViT encoder attention (`MMEncoderAttention`) to improve performance when using FP32 precision, where Torch SDPA may be inefficient. The changes introduce a new Triton-based attention wrapper, integrate it into the attention dispatch logic, and update multiple vision models to support the new backend.

**Technical impact**  
The Triton backend is now prioritized over Torch SDPA in the CUDA platform’s backend selection logic (after FlashAttention). It requires `max_seqlen` computation similar to FlashAttention, so multiple vision models have been updated to include `TRITON_ATTN` in their `compute_attn_mask_seqlen` conditions. The backend selection now iterates through supported backends, checking compatibility with head size, dtype, and compute capability.

**Potential risks**  
The relaxed tolerance in tests (`rtol=1e-3, atol=1e-3`) for Triton attention may mask numerical discrepancies. The backend selection loop could introduce subtle performance regressions if the order of backends changes. Additionally, error handling in `get_vit_attn_backend` now catches `AttributeError` alongside `AssertionError`, which might obscure configuration issues.

**Key insights**  
Developers should verify that the Triton backend’s numerical accuracy is acceptable for all supported models, as test tolerances have been increased. The backend priority (FlashAttention → Triton → Torch SDPA) should be validated against performance benchmarks across different hardware and precision settings. Ensure that the `max_seqlen` computation is consistently applied for all backends that require it.

---

## 11. [[Doc] Update Encoder-Decoder models support doc with Florence-2](https://github.com/vllm-project/vllm/pull/34581)


### Base Information

- **PR Number:** #34581
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-02-15 04:18:57
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/34581/files) (2):**
  - `docs/models/supported_models.md`
  - `docs/usage/v1_guide.md`

### Summary

**What changed and why**  
This PR updates vLLM's documentation to reflect that Florence-2 (`Florence2ForConditionalGeneration`) is now supported via the existing `bart-plugin`. The changes add Florence-2 to the supported models table and the encoder-decoder usage guide, following the same pattern as BART.

**Technical impact**  
The updates are purely documentation changes—no code modifications were made. They inform users that Florence-2 can be used with vLLM through the plugin system, enhancing the visibility of supported encoder-decoder architectures without altering system behavior.

**Potential risks**  
Since only documentation files were modified, there is no risk to code functionality. However, if the plugin implementation for Florence-2 is incomplete or unstable, users might encounter issues not covered in the docs. The PR description lacks test results or validation details, which could mask integration problems.

**Key insights**  
Developers should verify that the `bart-plugin` fully supports Florence-2's features before relying on it. Future PRs for model support should include test plans and results to ensure reliability. Maintaining consistent documentation helps users leverage vLLM's plugin ecosystem effectively.

---

